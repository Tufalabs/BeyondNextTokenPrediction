
<<<PAGE 1>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
Information Theory ,Inference, andLearning Algorithms
David J.C.MacKa y

<<<PAGE 2>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
Information Theory ,
Inference,
andLearning Algorithms
David J.C.MacKa y
mackay@mrao.cam.ac.uk
c1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003
Version 6.0(aspublished) June26,2003
Please sendfeedbac konthisbookvia
http://www.inference.ph y.cam.ac.uk/mackay/itila/
Thisbookwillbepublished byC.U.P .inSeptem ber2003. Itwillremain
viewableon-screen ontheabovewebsite, inpostscript, djvu, andpdf
formats.
(C.U.P. replacethispagewiththeirownpageii.)

<<<PAGE 3>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
Contents
Preface ................................ v
1Introduction toInformation Theory ................ 3
2Probabilit y,Entropy,andInference .................22
3More aboutInference ........................48
I DataCompression ........................65
4TheSource CodingTheorem ....................67
5SymbolCodes ............................91
6Stream Codes.............................110
7CodesforIntegers ..........................132
II Noisy-Channel Coding......................137
8Correlated Random Variables ....................138
9Comm unication overaNoisy Channel ...............146
10TheNoisy-Channel CodingTheorem ................162
11Error-Correcting CodesandRealChannels ............177
IIIFurther Topics inInformation Theory .............191
12Hash Codes:CodesforEcien tInformation Retriev al.....193
13Binary Codes ............................206
14VeryGoodLinear CodesExist ...................229
15Further Exercises onInformation Theory .............233
16Message Passing ...........................241
17Comm unication overConstrained Noiseless Channels ......248
18Crossw ordsandCodebreaking ...................260
19WhyhaveSex?Information Acquisition andEvolution .....269
IV Probabilities andInference ...................281
20AnExample Inference Task:Clustering ..............284
21Exact Inference byComplete Enumeration ............293
22Maxim umLikelihoodandClustering ................300
23Useful Probabilit yDistributions ..................311
24Exact Marginalization ........................319
25Exact Marginalization inTrellises .................324
26Exact Marginalization inGraphs ..................334

<<<PAGE 4>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
27Laplace's Metho d...........................341
28ModelComparison andOccam's Razor ..............343
29MonteCarlo Metho ds........................357
30Ecien tMonteCarlo Metho ds...................387
31IsingModels .............................400
32Exact MonteCarlo Sampling ....................413
33Variational Metho ds.........................422
34Indep enden tComp onentAnalysis andLatentVariable Mod-
elling .................................437
35Random Inference Topics ......................445
36Decision Theory ...........................451
37Bayesian Inference andSampling Theory .............457
V Neural networks.........................467
38Introduction toNeural Networks ..................468
39TheSingle Neuron asaClassier ..................471
40Capacit yofaSingle Neuron .....................483
41Learning asInference ........................492
42Hopeld Networks ..........................505
43Boltzmann Machines .........................522
44Supervised Learning inMultila yerNetworks ............527
45Gaussian Processes .........................535
46Decon volution ............................549
VI Sparse Graph Codes ......................555
47Low-Densit yParity-Chec kCodes .................557
48Convolutional CodesandTurboCodes...............574
49Repeat{Accum ulate Codes .....................582
50Digital FountainCodes .......................589
VIIAppendices ............................597
ANotation ................................598
BSome Physics .............................601
CSome Mathematics ..........................605
Bibliograph y................................613
Index ....................................620

<<<PAGE 5>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
Preface
Thisbookisaimed atsenior undergraduates andgraduate studen tsinEngi-
neering, Science, Mathematics, andComputing. Itexpectsfamiliarit ywith
calculus, probabilit ytheory ,andlinear algebra astaughtinarst-orsecond-
yearundergraduate course onmathematics forscientistsandengineers.
Conventional courses oninformation theory covernotonlythebeauti-
fultheoreticalideas ofShannon, butalsopracticalsolutions tocomm unica-
tionproblems. Thisbookgoesfurther, bringing inBayesian datamodelling,
MonteCarlo metho ds,variational metho ds,clustering algorithms, andneural
networks.
Whyunify information theory andmachinelearning? Because theyare
twosides ofthesame coin. Inthe1960s, asingle eld, cybernetics, was
populated byinformation theorists, computer scientists, andneuroscien tists,
allstudying common problems. Information theory andmachinelearning still
belong together. Brains aretheultimate compression andcomm unication
systems. Andthestate-of-the-art algorithms forbothdatacompression and
error-correcting codesusethesame toolsasmachine-learning.
Howtousethisbook
Theessentialdependencies betweenchapters areindicated inthegure onthe
nextpage. Anarrowfromonechapter toanother indicates thatthesecond
chapter requires some oftherst.
Within PartsI,II,IV,andVofthisbook,chapters onadvanced oroptional
topics aretowardstheend. Allchapters ofPartIIIareoptional onarst
reading, except perhaps forChapter 16(Message Passing).
Thesamesystem sometimes applies within achapter: thenalsections of-
tendealwithadvanced topics thatcanbeskippedonarstreading. Forexam-
pleintwokeychapters {Chapter 4(TheSource CodingTheorem) andChap-
ter10(The Noisy-Channel CodingTheorem) {therst-time reader should
detour atsection 4.5andsection 10.4respectively.
Pagesvii{xshowafewwaystousethisbook.First, Igivetheroadmap for
acourse thatIteachinCambridge: `Information theory ,pattern recognition,
andneural networks'. Thebookisalsointended asatextbookfortraditional
courses ininformation theory .Thesecond roadmap showsthechapters foran
introductory information theory course andthethirdforacourse aimed atan
understanding ofstate-of-the-art error-correcting codes.Thefourth roadmap
showshowtousethetextinaconventional course onmachinelearning.
v

<<<PAGE 6>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
vi Preface
1 Introduction toInformation Theory
2 Probabilit y,Entropy,andInference
3 More aboutInference
IDataCompression
4 TheSource CodingTheorem
5 SymbolCodes
6 Stream Codes
7 CodesforIntegers
IINoisy-Channel Coding
8 Correlated Random Variables
9 Comm unication overaNoisy Channel
10 TheNoisy-Channel CodingTheorem
11 Error-Correcting CodesandRealChannels
IIIFurther Topics inInformation Theory
12 Hash Codes
13 Binary Codes
14 VeryGoodLinear CodesExist
15 Further Exercises onInformation Theory
16 Message Passing
17 Constrained Noiseless Channels
18 Crossw ordsandCodebreaking
19 WhyhaveSex?IVProbabilities andInference
20 AnExample Inference Task:Clustering
21 Exact Inference byComplete Enumeration
22 Maxim umLikelihoodandClustering
23 Useful Probabilit yDistributions
24 Exact Marginalization
25 Exact Marginalization inTrellises
26 Exact Marginalization inGraphs
27 Laplace's Metho d
28 ModelComparison andOccam's Razor
29 MonteCarlo Metho ds
30 Ecien tMonteCarlo Metho ds
31 IsingModels
32 Exact MonteCarlo Sampling
33 Variational Metho ds
34 Indep enden tComp onentAnalysis
35 Random Inference Topics
36 Decision Theory
37 Bayesian Inference andSampling Theory
VNeural networks
38 Introduction toNeural Networks
39 TheSingle Neuron asaClassier
40 Capacit yofaSingle Neuron
41 Learning asInference
42 Hopeld Networks
43 Boltzmann Machines
44 Supervised Learning inMultila yerNetworks
45 Gaussian Processes
46 Decon volution
VISparse Graph Codes
47 Low-Densit yParity-Chec kCodes
48 Convolutional CodesandTurboCodes
49 Repeat-Accum ulate Codes
50 Digital FountainCodesDependencies

<<<PAGE 7>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
Preface vii
1 Introduction toInformation Theory
2 Probabilit y,Entropy,andInference
3 More aboutInference
IDataCompression
4 TheSource CodingTheorem
5 SymbolCodes
6 Stream Codes
7 CodesforIntegers
IINoisy-Channel Coding
8 Correlated Random Variables
9 Comm unication overaNoisy Channel
10 TheNoisy-Channel CodingTheorem
11 Error-Correcting CodesandRealChannels
IIIFurther Topics inInformation Theory
12 Hash Codes
13 Binary Codes
14 VeryGoodLinear CodesExist
15 Further Exercises onInformation Theory
16 Message Passing
17 Constrained Noiseless Channels
18 Crossw ordsandCodebreaking
19 WhyhaveSex?IVProbabilities andInference
20 AnExample Inference Task:Clustering
21 Exact Inference byComplete Enumeration
22 Maxim umLikelihoodandClustering
23 Useful Probabilit yDistributions
24 Exact Marginalization
25 Exact Marginalization inTrellises
26 Exact Marginalization inGraphs
27 Laplace's Metho d
28 ModelComparison andOccam's Razor
29 MonteCarlo Metho ds
30 Ecien tMonteCarlo Metho ds
31 IsingModels
32 Exact MonteCarlo Sampling
33 Variational Metho ds
34 Indep enden tComp onentAnalysis
35 Random Inference Topics
36 Decision Theory
37 Bayesian Inference andSampling Theory
VNeural networks
38 Introduction toNeural Networks
39 TheSingle Neuron asaClassier
40 Capacit yofaSingle Neuron
41 Learning asInference
42 Hopeld Networks
43 Boltzmann Machines
44 Supervised Learning inMultila yerNetworks
45 Gaussian Processes
46 Decon volution
VISparse Graph Codes
47 Low-Densit yParity-Chec kCodes
48 Convolutional CodesandTurboCodes
49 Repeat-Accum ulate Codes
50 Digital FountainCodes1 Introduction toInformation Theory
2 Probabilit y,Entropy,andInference
3 More aboutInference
4 TheSource CodingTheorem
5 SymbolCodes
6 Stream Codes
8 Correlated Random Variables
9 Comm unication overaNoisy Channel
10 TheNoisy-Channel CodingTheorem
11 Error-Correcting CodesandRealChannels20 AnExample Inference Task:Clustering
21 Exact Inference byComplete Enumeration
22 Maxim umLikelihoodandClustering
24 Exact Marginalization
27 Laplace's Metho d
29 MonteCarlo Metho ds
30 Ecien tMonteCarlo Metho ds
31 IsingModels
32 Exact MonteCarlo Sampling
33 Variational Metho ds
38 Introduction toNeural Networks
39 TheSingle Neuron asaClassier
40 Capacit yofaSingle Neuron
41 Learning asInference
42 Hopeld Networks
47 Low-Densit yParity-Chec kCodesMyCambridge Course on,
Information Theory ,
Pattern Recognition,
andNeural Networks

<<<PAGE 8>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
viii Preface
1 Introduction toInformation Theory
2 Probabilit y,Entropy,andInference
3 More aboutInference
IDataCompression
4 TheSource CodingTheorem
5 SymbolCodes
6 Stream Codes
7 CodesforIntegers
IINoisy-Channel Coding
8 Correlated Random Variables
9 Comm unication overaNoisy Channel
10 TheNoisy-Channel CodingTheorem
11 Error-Correcting CodesandRealChannels
IIIFurther Topics inInformation Theory
12 Hash Codes
13 Binary Codes
14 VeryGoodLinear CodesExist
15 Further Exercises onInformation Theory
16 Message Passing
17 Constrained Noiseless Channels
18 Crossw ordsandCodebreaking
19 WhyhaveSex?IVProbabilities andInference
20 AnExample Inference Task:Clustering
21 Exact Inference byComplete Enumeration
22 Maxim umLikelihoodandClustering
23 Useful Probabilit yDistributions
24 Exact Marginalization
25 Exact Marginalization inTrellises
26 Exact Marginalization inGraphs
27 Laplace's Metho d
28 ModelComparison andOccam's Razor
29 MonteCarlo Metho ds
30 Ecien tMonteCarlo Metho ds
31 IsingModels
32 Exact MonteCarlo Sampling
33 Variational Metho ds
34 Indep enden tComp onentAnalysis
35 Random Inference Topics
36 Decision Theory
37 Bayesian Inference andSampling Theory
VNeural networks
38 Introduction toNeural Networks
39 TheSingle Neuron asaClassier
40 Capacit yofaSingle Neuron
41 Learning asInference
42 Hopeld Networks
43 Boltzmann Machines
44 Supervised Learning inMultila yerNetworks
45 Gaussian Processes
46 Decon volution
VISparse Graph Codes
47 Low-Densit yParity-Chec kCodes
48 Convolutional CodesandTurboCodes
49 Repeat-Accum ulate Codes
50 Digital FountainCodes1 Introduction toInformation Theory
2 Probabilit y,Entropy,andInference
4 TheSource CodingTheorem
5 SymbolCodes
6 Stream Codes
8 Correlated Random Variables
9 Comm unication overaNoisy Channel
10 TheNoisy-Channel CodingTheorem
Short Course on
Information Theory

<<<PAGE 9>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
Preface ix
1 Introduction toInformation Theory
2 Probabilit y,Entropy,andInference
3 More aboutInference
IDataCompression
4 TheSource CodingTheorem
5 SymbolCodes
6 Stream Codes
7 CodesforIntegers
IINoisy-Channel Coding
8 Correlated Random Variables
9 Comm unication overaNoisy Channel
10 TheNoisy-Channel CodingTheorem
11 Error-Correcting CodesandRealChannels
IIIFurther Topics inInformation Theory
12 Hash Codes
13 Binary Codes
14 VeryGoodLinear CodesExist
15 Further Exercises onInformation Theory
16 Message Passing
17 Constrained Noiseless Channels
18 Crossw ordsandCodebreaking
19 WhyhaveSex?IVProbabilities andInference
20 AnExample Inference Task:Clustering
21 Exact Inference byComplete Enumeration
22 Maxim umLikelihoodandClustering
23 Useful Probabilit yDistributions
24 Exact Marginalization
25 Exact Marginalization inTrellises
26 Exact Marginalization inGraphs
27 Laplace's Metho d
28 ModelComparison andOccam's Razor
29 MonteCarlo Metho ds
30 Ecien tMonteCarlo Metho ds
31 IsingModels
32 Exact MonteCarlo Sampling
33 Variational Metho ds
34 Indep enden tComp onentAnalysis
35 Random Inference Topics
36 Decision Theory
37 Bayesian Inference andSampling Theory
VNeural networks
38 Introduction toNeural Networks
39 TheSingle Neuron asaClassier
40 Capacit yofaSingle Neuron
41 Learning asInference
42 Hopeld Networks
43 Boltzmann Machines
44 Supervised Learning inMultila yerNetworks
45 Gaussian Processes
46 Decon volution
VISparse Graph Codes
47 Low-Densit yParity-Chec kCodes
48 Convolutional CodesandTurboCodes
49 Repeat-Accum ulate Codes
50 Digital FountainCodes11 Error-Correcting CodesandRealChannels
12 Hash Codes
13 Binary Codes
14 VeryGoodLinear CodesExist
15 Further Exercises onInformation Theory
16 Message Passing
17 Constrained Noiseless Channels24 Exact Marginalization
25 Exact Marginalization inTrellises
26 Exact Marginalization inGraphs
47 Low-Densit yParity-Chec kCodes
48 Convolutional CodesandTurboCodes
49 Repeat-Accum ulate Codes
50 Digital FountainCodesAdvanced Course on
Information Theory andCoding

<<<PAGE 10>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
x Preface
1 Introduction toInformation Theory
2 Probabilit y,Entropy,andInference
3 More aboutInference
IDataCompression
4 TheSource CodingTheorem
5 SymbolCodes
6 Stream Codes
7 CodesforIntegers
IINoisy-Channel Coding
8 Correlated Random Variables
9 Comm unication overaNoisy Channel
10 TheNoisy-Channel CodingTheorem
11 Error-Correcting CodesandRealChannels
IIIFurther Topics inInformation Theory
12 Hash Codes
13 Binary Codes
14 VeryGoodLinear CodesExist
15 Further Exercises onInformation Theory
16 Message Passing
17 Constrained Noiseless Channels
18 Crossw ordsandCodebreaking
19 WhyhaveSex?IVProbabilities andInference
20 AnExample Inference Task:Clustering
21 Exact Inference byComplete Enumeration
22 Maxim umLikelihoodandClustering
23 Useful Probabilit yDistributions
24 Exact Marginalization
25 Exact Marginalization inTrellises
26 Exact Marginalization inGraphs
27 Laplace's Metho d
28 ModelComparison andOccam's Razor
29 MonteCarlo Metho ds
30 Ecien tMonteCarlo Metho ds
31 IsingModels
32 Exact MonteCarlo Sampling
33 Variational Metho ds
34 Indep enden tComp onentAnalysis
35 Random Inference Topics
36 Decision Theory
37 Bayesian Inference andSampling Theory
VNeural networks
38 Introduction toNeural Networks
39 TheSingle Neuron asaClassier
40 Capacit yofaSingle Neuron
41 Learning asInference
42 Hopeld Networks
43 Boltzmann Machines
44 Supervised Learning inMultila yerNetworks
45 Gaussian Processes
46 Decon volution
VISparse Graph Codes
47 Low-Densit yParity-Chec kCodes
48 Convolutional CodesandTurboCodes
49 Repeat-Accum ulate Codes
50 Digital FountainCodes2 Probabilit y,Entropy,andInference
3 More aboutInference20 AnExample Inference Task:Clustering
21 Exact Inference byComplete Enumeration
22 Maxim umLikelihoodandClustering
24 Exact Marginalization
27 Laplace's Metho d
28 ModelComparison andOccam's Razor
29 MonteCarlo Metho ds
30 Ecien tMonteCarlo Metho ds
31 IsingModels
32 Exact MonteCarlo Sampling
33 Variational Metho ds
34 Indep enden tComp onentAnalysis
38 Introduction toNeural Networks
39 TheSingle Neuron asaClassier
40 Capacit yofaSingle Neuron
41 Learning asInference
42 Hopeld Networks
43 Boltzmann Machines
44 Supervised Learning inMultila yerNetworks
45 Gaussian Processes
ACourse onBayesian Inference
andMachineLearning

<<<PAGE 11>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
Preface xi
Abouttheexercises
Youcanunderstand asubjectonlybycreating itforyourself. Theexercises
playanessentialroleinthisbook.Forguidance, eachhasarating (similar to
thatusedbyKnuth(1968)) from1to5toindicate itsdicult y.
Inaddition, exercises thatareespecially recommended aremarkedbya
marginal encouraging rat.Some exercises thatrequire theuseofacomputer
aremarkedwithaC.
Answerstomanyexercises areprovided. Usethem wisely .Where asolu-
tionisprovided, thisisindicated byincluding itspagenumberalongside the
dicult yrating.
Solutions tomanyoftheother exercises willbesupplied toinstructors
using thisbookintheirteaching; please emailsolutions@cambridge.org .
Summa ryofcodesforexercises
Especially recommended
. Recommended
C Partsrequire acomputer
[p.42]Solution provided onpage42[1]Simple (oneminute)
[2]Medium (quarter hour)
[3]Moderately hard
[4]Hard
[5]Researc hproject
Internet resources
Thewebsite
http://www.inference.p hy.cam.ac.uk/mackay/itila
containsseveralresources:
1.Softwar e.TeachingsoftwarethatIuseinlectures, interactiv esoftware,
andresearc hsoftware,written inperl,octave ,tcl,C,andgnuplot .
Alsosome animations.
2.Corrections tothebook.Thank youinadvanceforemailing these!
3.Thisbook.Thebookisprovided inpostscript ,pdf,anddjvuformats
foron-screen viewing. Thesame copyrightrestrictions apply astoa
normal book.
Acknowledgmen ts
Iammost grateful totheorganizations whohavesupported mewhile this
bookgestated: theRoyalSocietyandDarwin College whogavemeafantas-
ticresearc hfellowship intheearly years; theUniversityofCambridge; the
KeckCentreattheUniversityofCalifornia inSanFrancisco, where Ispenta
productiv esabbatical; andtheGatsb yCharitable Foundation, whose support
gavemethefreedom tobreak outoftheEscherstaircase thatbook-writing
hadbecome.
Myworkhasdepended onthegenerosit yoffreesoftwareauthors. Iwrote
thebookinLATEX2".Three cheers forDonald KnuthandLeslie Lamp ort!
Ourcomputers runtheGNU/Lin uxoperating system. Iuseemacs ,perl,and

<<<PAGE 12>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
xii Preface
gnuplot everyday.Thank youRichardStallman, thank youLinusTorvalds,
thank youeveryone.
Manyreaders, toonumerous toname here, havegivenfeedbac konthe
book,andtothem allIextend mysincere acknowledgmen ts.Iespecially wish
tothank allthestuden tsandcolleagues atCambridge Universitywhohave
attended mylectures oninformation theory andmachinelearning overthelast
nineyears.
ThemembersoftheInference researc hgroup havegivenimmense support,
andIthank them allfortheirgenerosit yandpatience overthelasttenyears:
Mark Gibbs, MichellePovinelli, Simon Wilson, Coryn Bailer-Jones, Matthew
Davey,Katriona Macphee, James Miskin, DavidWard,EdRatzer, SebWills,
JohnBarry,JohnWinn, PhilCowans,Hanna Wallach,Matthew Garrett, and
especially SanjoyMaha jan.Thank youtootoGraeme Mitchison, MikeCates,
andDavinYap.
Finally Iwouldliketoexpress mydebttomypersonal heroes,thementors
fromwhom Ihavelearned somuch:YaserAbu-Mostafa, Andrew Blake,John
Bridle, PeterCheeseman, SteveGull,Geo Hinton,JohnHopeld, SteveLut-
trell,RobertMacKa y,BobMcEliece, Radford Neal, Roger Sewell,andJohn
Skilling.
Dedication
Thisbookisdedicated tothecampaign against thearmstrade.
www.caat.org.uk
Peacecannot bekeptbyforce.
Itcanonlybeachievedthrough understanding.
{AlbertEinstein

<<<PAGE 13>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
AboutChapter 1
Intherstchapter, youwillneedtobefamiliar withthebinomial distribution.
Andtosolvetheexercises inthetext{whichIurgeyoutodo{youwillneed
toknowStirling's approximation forthefactorial function,x!'xxe x,and
beabletoapply itto N
r=N!
(N r)!r!.These topics arereview edbelow. Unfamiliar notation?
SeeAppendix A,p.598.
Thebinomial distribution
Example 1.1.Abentcoinhasprobabilit yfofcoming upheads. Thecoinis
tossedNtimes. What istheprobabilit ydistribution ofthenumberof
heads,r?What arethemean andvariance ofr?
00.050.10.150.20.250.3
012345678910
r
Figure 1.1.Thebinomial
distribution P(rjf=0:3;N=10).Solution. Thenumberofheads hasabinomial distribution.
P(rjf;N)= 
N
r!
fr(1 f)N r: (1.1)
Themean,E[r],andvariance, var[r],ofthisdistribution aredened by
E[r]NX
r=0P(rjf;N)r (1.2)
var[r]Eh
(r E[r])2i
(1.3)
=E[r2] (E[r])2=NX
r=0P(rjf;N)r2 (E[r])2: (1.4)
Rather thanevaluating thesumsoverrin(1.2)and(1.4)directly ,itiseasiest
toobtain themean andvariance bynoting thatristhesumofNindependent
random variables, namely ,thenumberofheads inthersttoss(whichiseither
zeroorone),thenumberofheads inthesecond toss,andsoforth. Ingeneral,
E[x+y]=E[x]+E[y] foranyrandom variablesxandy;
var[x+y]=var[x]+var[y]ifxandyareindependen t:(1.5)
Sothemean ofristhesumofthemeans ofthose random variables, andthe
variance ofristhesumoftheirvariances. Themean numberofheads ina
single tossisf1+(1 f)0=f,andthevariance ofthenumberofheads
inasingle tossis
h
f12+(1 f)02i
 f2=f f2=f(1 f); (1.6)
sothemean andvariance ofrare:
E[r]=Nf and var[r]=Nf(1 f): 2 (1.7)
1

<<<PAGE 14>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
2 AboutChapter 1
Approximating x!andN
r
00.020.040.060.080.10.12
0510152025
r
Figure 1.2.ThePoisson
distribution P(rj=15).Let's deriveStirling's approximation byanuncon ventional route. Westart
fromthePoisson distribution withmean,
P(rj)=e r
r!r2f0;1;2;:::g: (1.8)
Forlarge,thisdistribution iswellapproximated {atleastinthevicinit yof
r'{byaGaussian distribution withmeanandvariance:
e r
r!'1p
2e (r )2
2: (1.9)
Let'splugr=intothisformula.
e 
!'1p
2(1.10)
)!'e p
2: (1.11)
ThisisStirling's approximation forthefactorial function.
x!'xxe xp
2x,lnx!'xlnx x+1
2ln2x: (1.12)
Wehavederivednotonlytheleading order behaviour,x!'xxe x,butalso,
atnocost, thenext-order correction termp
2x.Wenowapply Stirling's
approximation toln N
r:
ln 
N
r!
lnN!
(N r)!r!'(N r)lnN
N r+rlnN
r:(1.13)
Since alltheterms inthisequation arelogarithms, thisresult canberewritten
inanybase. Wewilldenote natural logarithms (loge)by`ln',andlogarithms Recall thatlog2x=logex
loge2.
Notethat@log2x
@x=1
loge21
x.tobase2(log2)by`log'.
Ifweintroducethebinary entropyfunction ,
H2(x)xlog1
x+(1 x)log1
(1 x); (1.14)
thenwecanrewrite theapproximation (1.13) as
H2(x)
00.20.40.60.81
00.20.40.60.81x
Figure 1.3.Thebinary entropy
function.log 
N
r!
'NH2(r=N); (1.15)
or,equivalently, 
N
r!
'2NH2(r=N): (1.16)
Ifweneedamore accurate approximation, wecaninclude terms ofthenext
order fromStirling's approximation (1.12):
log 
N
r!
'NH2(r=N) 1
2log
2NN r
Nr
N
: (1.17)

<<<PAGE 15>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
1
Introduction toInformation Theory
Thefundamen talproblem ofcomm unication isthatofreproducing
atonepointeither exactly orapproximately amessage selected at
another point.
(Claude Shannon, 1948)
Inthersthalfofthisbookwestudy howtomeasure information content;we
learnhowtocompress data; andwelearnhowtocomm unicate perfectly over
imperfect comm unication channels.
Westartbygetting afeeling forthislastproblem.
1.1Howcanweachieveperfect comm unication overanimperfect,
noisy commm unication channel?
Some examples ofnoisy comm unication channels are:
ananalogue telephone line,overwhichtwomodemscomm unicate digitalmodemphone
linemodem--
information;
theradio comm unication linkfromGalileo, theJupiter-orbiting space-GalileoradiowavesEarth--
craft, toearth;
parent
cell
daugh ter
celldaugh ter
cell
  
@@Rreproducing cells, inwhichthedaugh tercells's DNA containsinforma-
tionfromtheparentcells;
computer
memorydisk
drivecomputer
memory--adiskdrive.
Thelastexample showsthatcomm unication doesn'thavetoinvolveinforma-
tiongoing fromoneplacetoanother. When wewrite aleonadiskdrive,
we'llreaditointhesame location {butatalatertime.
These channels arenoisy.Atelephone linesuers fromcross-talk with
other lines; thehardw areinthelinedistorts andaddsnoisetothetransmitted
signal. Thedeepspace networkthatlistens toGalileo's punytransmitter
receivesbackground radiation fromterrestrial andcosmic sources. DNA is
subjecttomutations anddamage. Adiskdrive,whichwrites abinary digit
(aoneorzero,alsoknownasabit)byaligning apatchofmagnetic material
inoneoftwoorientations, maylaterfailtoreadoutthestored binary digit:
thepatchofmaterial mightspontaneously ipmagnetization, oraglitchof
background noise mightcause thereading circuit toreportthewrong value
forthebinary digit, orthewriting headmightnotinduce themagnetization
intherstplace because ofinterference fromneighbouring bits.
Inallthesecases, ifwetransmit data,e.g.,astring ofbits,overthechannel,
there issomeprobabilit ythatthereceivedmessage willnotbeidenticaltothe
3

<<<PAGE 16>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
4 1|Introduction toInformation Theory
transmitted message. Wewouldprefer tohaveacomm unication channel for
whichthisprobabilit ywaszero{orsoclosetozerothatforpractical purposes
itisindistinguishable fromzero.
Let's consider anoisy diskdrivethattransmits eachbitcorrectly with
probabilit y(1 f)andincorrectly withprobabilit yf.Thismodelcomm uni-
cation channel isknownasthebinary symmetric channel (gure 1.4).
x--
  @@R10
10yP(y=0jx=0)=1 f;
P(y=1jx=0)=f;P(y=0jx=1)=f;
P(y=1jx=1)=1 f:Figure 1.4.Thebinary symmetric
channel. Thetransmitted symbol
isxandthereceiv edsymboly.
Thenoiselevel,theprobabilit yof
abit'sbeingipped,isf.
(1 f)(1 f)
f
--
    @
@
@@R
10
10
Figure 1.5.Abinary data
sequence oflength 10000
transmitted overabinary
symmetric channel withnoise
levelf=0:1.[Dilbertimage
Copyrightc1997United Feature
Syndicate, Inc.,usedwith
permission.]
Asanexample, let'simagine thatf=0:1,thatis,tenpercentofthebitsare
ipped(gure 1.5). Auseful diskdrivewouldipnobitsatallinitsentire
lifetime. Ifweexpecttoreadandwrite agigabyteperdayfortenyears,we
require abiterrorprobabilit yoftheorder of10 15,orsmaller. There aretwo
approac hestothisgoal.
Thephysicalsolution
Thephysical solution istoimpro vethephysical characteristics ofthecomm u-
nication channel toreduce itserror probabilit y.Wecould impro veourdisk
driveby
1.using more reliable componentsinitscircuitry;
2.evacuating theairfromthediskenclosure soastoeliminate theturbu-
lence thatperturbs thereading headfromthetrack;
3.using alarger magnetic patchtorepresen teachbit;or
4.using higher-p owersignals orcooling thecircuitry inorder toreduce
thermal noise.
These physical modications typically increase thecostofthecomm unication
channel.
The`system' solution
Information theory andcodingtheory oeranalternativ e(andmuchmoreex-
citing) approac h:weaccept thegivennoisy channel asitisandaddcomm uni-
cation systems toitsothatwecandetect andcorrect theerrors introduced by
thechannel. Asshowningure 1.6,weaddanencoderbeforethechannel and
adecoderafterit.Theencoderencodesthesource message sintoatransmit-
tedmessage t,adding redundancy totheoriginal message insome way.The
channel addsnoisetothetransmitted message, yielding areceivedmessage r.
Thedecoderusestheknownredundancy introduced bytheencodingsystem
toinferboththeoriginal signalsandtheadded noise.

<<<PAGE 17>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
1.2:Error-correcting codesforthebinary symmetric channel 5
Noisy
channelEncoder DecoderSource
ts
r^s
-66
?Figure 1.6.The`system' solution
forachieving reliable
comm unication overanoisy
channel. Theencodingsystem
introduces systematic redundancy
intothetransmitted vectort.The
decodingsystem usesthisknown
redundancy todeduce fromthe
receiv edvectorrboththeoriginal
source vectorandthenoise
introduced bythechannel.
Whereas physical solutions giveincremen talchannel impro vementsonlyat
anever-increasing cost,system solutions canturnnoisy channels intoreliable
comm unication channels withtheonlycostbeingacomputational requiremen t
attheencoderanddecoder.
Information theory isconcerned withthetheoretical limitations andpo-
tentialsofsuchsystems. `What isthebesterror-correcting performance we
could achieve?'
Codingtheory isconcerned withthecreation ofpractical encodingand
decodingsystems.
1.2Error-correcting codesforthebinary symmetric channel
Wenowconsider examples ofencodinganddecodingsystems. What isthe
simplest waytoadduseful redundancy toatransmission? [Tomaketherules
ofthegame clear: wewanttobeabletodetect andcorrect errors; andre-
transmission isnotanoption. Wegetonlyonechance toencode,transmit,
anddecode.]
Repetition codes
Astraigh tforwardideaistorepeateverybitofthemessage aprearranged
numberoftimes {forexample, three times, asshownintable 1.7.Wecall
thisrepetition code`R3'.Source Transmitted
sequence sequence
s t
0 000
1 111
Table1.7.Therepetition codeR3.Imagine thatwetransmit thesource message
s=0010110
overabinary symmetric channel withnoiselevelf=0:1using thisrepetition
code.Wecandescrib ethechannel as`adding' asparse noise vectorntothe
transmitted vector {adding inmodulo2arithmetic, i.e.,thebinary algebra
inwhich1+1=0.Apossible noise vectornandreceivedvectorr=t+nare
showningure 1.8.
s0010110
tz}|{
000z}|{
000z}|{
111z}|{
000z}|{
111z}|{
111z}|{
000
n000001000000101000000
r000001111000010111000Figure 1.8.Anexample
transmission using R3.
Howshould wedecodethisreceivedvector? Theoptimal algorithm looks
atthereceivedbitsthree atatimeandtakesamajorityvote(algorithm 1.9).

<<<PAGE 18>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
6 1|Introduction toInformation Theory
Receiv edsequence rLikelihoodratioP(rjs=1)
P(rjs=0)Decodedsequence ^s
000  30
001  10
010  10
100  10
101 11
110 11
011 11
111 31Algorithm 1.9.Majority-vote
decodingalgorithm forR3.Also
shownarethelikelihoodratios
(1.23), assuming thechannel isa
binary symmetric channel;
(1 f)=f.
Attheriskofexplaining theobvious, let'sprovethisresult. Theoptimal decoding
decision (optimal inthesense ofhavingthesmallest probabilit yofbeingwrong) is
tondwhichvalueofsismostprobable, givenr.Consider thedecodingofasingle
bits,whichwasencodedast(s)andgaverisetothree receiv edbitsr=r1r2r3.By
Bayes'theorem, theposterior probabilit yofsis
P(sjr1r2r3)=P(r1r2r3js)P(s)
P(r1r2r3): (1.18)
Wecanspellouttheposterior probabilit yofthetwoalternativ esthus:
P(s=1jr1r2r3)=P(r1r2r3js=1)P(s=1)
P(r1r2r3); (1.19)
P(s=0jr1r2r3)=P(r1r2r3js=0)P(s=0)
P(r1r2r3): (1.20)
Thisposterior probabilit yisdetermined bytwofactors: theprior probabilit yP(s),
andthedata-dep enden tterm P(r1r2r3js),whichiscalled thelikelihoodofs.The
normalizing constan tP(r1r2r3)needn't becomputed when nding theoptimal decod-
ingdecision, whichistoguess ^s=0ifP(s=0jr)>P(s=1jr),and^s=1otherwise.
TondP(s=0jr)andP(s=1jr),wemustmakeanassumption abouttheprior
probabilities ofthetwohypotheses s=0ands=1,andwemustmakeanassumption
abouttheprobabilit yofrgivens.Weassume thatthepriorprobabilities areequal:
P(s=0)=P(s=1)=0:5;thenmaximizing theposterior probabilit yP(sjr)is
equivalenttomaximizing thelikelihoodP(rjs).Andweassume thatthechannel is
abinary symmetric channel withnoise levelf<0:5,sothatthelikelihoodis
P(rjs)=P(rjt(s))=NY
n=1P(rnjtn(s)); (1.21)
where N=3isthenumberoftransmitted bitsintheblockweareconsidering, and
P(rnjtn)=
(1 f)ifrn=tn
f ifrn6=tn:(1.22)
Thusthelikelihoodratioforthetwohypotheses is
P(rjs=1)
P(rjs=0)=NY
n=1P(rnjtn(1))
P(rnjtn(0)); (1.23)
eachfactorP(rnjtn(1))
P(rnjtn(0))equals(1 f)
fifrn=1andf
(1 f)ifrn=0.Theratio(1 f)
f
isgreater than1,since f<0:5,sothewinning hypothesis istheonewiththemost
`votes', eachvotecountingforafactor ofinthelikelihoodratio.
Thusthemajority-votedecodershowninalgorithm 1.9istheoptimal decoderifwe
assume thatthechannel isabinary symmetric channel andthatthetwopossible
source messages 0and1haveequal priorprobabilit y.
Wenowapply themajorityvotedecodertothereceivedvectorofgure 1.8.
Therstthree receivedbitsareall0,sowedecodethistriplet asa0.Inthe
second triplet ofgure 1.8,therearetwo0sandone1,sowedecodethistriplet

<<<PAGE 19>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
1.2:Error-correcting codesforthebinary symmetric channel 7
asa0{whichinthiscasecorrects theerror. Notallerrors arecorrected,
however.Ifweareunluckyandtwoerrors fallinasingle block,asinthefth
triplet ofgure 1.8,thenthedecodingrulegetsthewrong answer,asshown
ingure 1.10.
s0010110
tz}|{
000z}|{
000z}|{
111z}|{
000z}|{
111z}|{
111z}|{
000
n000001000000101000000
r000|{z}001|{z}111|{z}000|{z}010|{z}111|{z}000|{z}
^s0010010
corrected errors ?
undetected errors ?Figure 1.10.Decodingthereceiv ed
vectorfromgure 1.8.
Exercise 1.2.[2,p.16]Showthattheerrorprobabilit yisreduced bytheuseof Theexercise's rating, e.g.`[2]',indi-
cates itsdicult y:`1'exercises are
theeasiest. Exercises thatareaccom-
panied byamarginal ratareespe-
cially recommended. Ifasolution or
partial solution isprovided, thepage
isindicated afterthedicult yrating;
forexample, thisexercise's solution is
onpage16.R3bycomputing theerrorprobabilit yofthiscodeforabinary symmetric
channel withnoise levelf.
Theerror probabilit yisdominated bytheprobabilit ythattwobitsin
ablockofthree areipped,whichscales asf2.Inthecaseofthebinary
symmetric channel withf=0:1,theR3codehasaprobabilit yoferror, after
decoding,ofpb'0:03perbit.Figure 1.11showstheresult oftransmitting a
binary image overabinary symmetric channel using therepetition code.
Therepetition codeR3hastherefore reduced theprobabilit yoferror, as
desired. Yetwehavelostsomething: ourrateofinformation transfer has
fallen byafactor ofthree. Soifweusearepetition codetocomm unicate data
overatelephone line,itwillreduce theerrorfrequency ,butitwillalsoreduce
ourcomm unication rate. Wewillhavetopaythree times asmuchforeach
phone call.Similarly ,wewouldneedthree oftheoriginal noisy gigabytedisk
drivesinorder tocreate aone-gigab ytediskdrivewithpb=0:03.
Canwepushtheerrorprobabilit ylower,tothevalues required forasell-
ablediskdrive{10 15?Wecould achievelowererrorprobabilities byusing
repetition codeswithmore repetitions.
Exercise 1.3.[3,p.16](a)Showthattheprobabilit yoferrorofRN,therepe-
tition codewithNrepetitions, is
pb=NX
n=(N+1)=2 
N
n!
fn(1 f)N n; (1.24)
foroddN.
(b)Assumingf=0:1,whichoftheterms inthissumisthebiggest?
Howmuchbigger isitthanthesecond-biggest term?
(c)UseStirling's approximation (p.2)toapproximate the N
ninthe
largest term, andnd,approximately ,theprobabilit yoferror of
therepetition codewithNrepetitions.
(d)Assumingf=0:1,ndhowmanyrepetitions arerequired toget
theprobabilit yoferrordownto10 15.[Answ er:about60.]
Sotobuildasingle gigabytediskdrivewiththerequired reliabilit yfromnoisy
gigabytedriveswithf=0:1,wewouldneedsixty ofthenoisy diskdrives.
Thetradeo betweenerrorprobabilit yandrateforrepetition codesisshown
ingure 1.12.

<<<PAGE 20>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
8 1|Introduction toInformation Theory
s
-encoder
 t channel
f=10%
-
r decoder
-
^s
Figure 1.11.Transmitting 10000
source bitsoverabinary
symmetric channel withf=10%
using arepetition codeandthe
majorityvotedecodingalgorithm.
Theprobabilit yofdecodedbit
errorhasfallen toabout3%;the
ratehasfallen to1/3.
00.020.040.060.080.1
00.20.40.60.81
Ratemore useful codes R5R3
R61R1
pb0.1
0.01
1e-05
1e-10
1e-15
00.20.40.60.81
Ratemore useful codesR5
R3
R61R1Figure 1.12.Error probabilit ypb
versusrateforrepetition codes
overabinary symmetric channel
withf=0:1.Theright-hand
gure showspbonalogarithmic
scale. Wewouldliketherateto
belargeandpbtobesmall.

<<<PAGE 21>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
1.2:Error-correcting codesforthebinary symmetric channel 9
Blockcodes{the(7;4)Hamming code
Wewouldliketocomm unicate withtinyprobabilit yoferrorandatasubstan-
tialrate.Canweimpro veonrepetition codes?What ifweaddredundancy to
blocksofdatainstead ofencodingonebitatatime? Wenowstudy asimple
blockcode.
Ablockcodeisaruleforconverting asequence ofsource bitss,oflength
K,say,intoatransmitted sequence toflengthNbits.Toaddredundancy ,
wemakeNgreater thanK.Inalinear blockcode,theextraN Kbitsare
linear functions oftheoriginalKbits;these extra bitsarecalled parity-check
bits.Anexample ofalinear blockcodeisthe(7;4)Hamming code,which
transmitsN=7bitsforeveryK=4source bits.
(a)ss s
t tt
7 65
4s32 1
(b)1 0
0
01
0 1Figure 1.13.Pictorial
represen tation ofencodingforthe
(7;4)Hamming code.
Theencodingoperation forthecodeisshownpictorially ingure 1.13.We
arrange theseventransmitted bitsinthree intersecting circles. Therstfour
transmitted bits,t1t2t3t4,aresetequal tothefoursource bits,s1s2s3s4.The
parity-checkbitst5t6t7aresetsothattheparitywithin eachcircle iseven:
therstparity-checkbitistheparityoftherstthree source bits(that is,it
is0ifthesumofthose bitsiseven,and1ifthesumisodd);thesecond is
theparityofthelastthree; andthethirdparitybitistheparityofsource bits
one,three andfour.
Asanexample, gure 1.13b showsthetransmitted codewordforthecase
s=1000.Table1.14showsthecodewordsgenerated byeachofthe24=
sixteen settings ofthefoursource bits. These codewordshavethespecial
propertythatanypairdier fromeachother inatleastthree bits.
s t
00000000000
00010001011
00100010111
00110011100s t
01000100110
01010101101
01100110001
01110111010s t
10001000101
10011001110
10101010010
10111011001s t
11001100011
11011101000
11101110100
11111111111Table1.14.Thesixteen codewords
ftgofthe(7;4)Hamming code.
Anypairofcodewordsdier from
eachother inatleastthree bits.
Because theHamming codeisalinear code,itcanbewritten compactly interms of
matrices asfollows.Thetransmitted codewordtisobtained fromthesource sequence
sbyalinear operation,
t=GTs; (1.25)
where Gisthegenerator matrix ofthecode,
GT=2
66666641000
0100
0010
0001
1110
0111
10113
7777775; (1.26)
andtheencodingoperation (1.25) usesmodulo-2 arithmetic (1+1=0,0+1=1,
etc.).

<<<PAGE 22>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
10 1|Introduction toInformation Theory
Intheencodingoperation (1.25) Ihaveassumed thatsandtarecolumn vectors. If
instead theyarerowvectors, thenthisequation isreplaced by
t=sG; (1.27)
where
G=2
641000101
0100110
0010111
00010113
75: (1.28)
Inditeasier torelate totheright-multiplication (1.25) thantheleft-m ultiplication
(1.27). Manycodingtheory texts usetheleft-m ultiplying conventions (1.27{1.28),
however.
Therowsofthegenerator matrix (1.28) canbeviewedasdening fourbasisvectors
lying inaseven-dimensional binary space. Thesixteen codewordsareobtained by
making allpossible linear combinations ofthese vectors.
Decodingthe(7;4)Hamming code
When weinventamore complex encoders!t,thetaskofdecodingthe
receivedvectorrbecomes lessstraigh tforward.Remem berthatanyofthe
bitsmayhavebeenipped,including theparitybits.
Ifweassume thatthechannel isabinary symmetric channel andthatall
source vectors areequiprobable, thentheoptimal decoderidentiesthesource
vectorswhose encodingt(s)diers fromthereceivedvectorrinthefewest
bits.[Refer tothelikelihoodfunction (1.23) toseewhythisisso.]Wecould
solvethedecodingproblem bymeasuring howfarrisfromeachofthesixteen
codewordsintable1.14,thenpickingtheclosest. Isthere amoreecien tway
ofnding themostprobable source vector?
SyndromedecodingfortheHamming code
Forthe(7;4)Hamming codethere isapictorial solution tothedecoding
problem, based ontheencodingpicture, gure 1.13.
Asarstexample, let'sassume thetransmission wast=1000101 andthe
noise ipsthesecond bit,sothereceivedvector isr=10001010100000 =
1100101 .Wewrite thereceivedvector intothethree circles asshownin
gure 1.15a, andlookateachofthethree circles toseewhether itsparity
iseven.Thecircles whose parityisnotevenareshownbydashed linesin
gure 1.15b. Thedecodingtaskistondthesmallest setofippedbitsthat
canaccoun tforthese violations oftheparityrules. [Thepattern ofviolations
oftheparitychecksiscalled thesyndrome ,andcanbewritten asabinary
vector {forexample, ingure 1.15b, thesyndrome isz=(1;1;0),because
thersttwocircles are`unhapp y'(parity1)andthethird circle is`happy'
(parity0).]
Tosolvethedecodingtask,weaskthequestion: canwendaunique bit
thatliesinside allthe`unhapp y'circles andoutside allthe`happy'circles? If
so,theipping ofthatbitwouldaccoun tfortheobserv edsyndrome. Inthe
caseshowningure 1.15b, thebitr2liesinside thetwounhapp ycircles and
outside thehappycircle; noother single bithasthisproperty,sor2istheonly
single bitcapable ofexplaining thesyndrome.
Let's workthrough acouple more examples. Figure 1.15c showswhat
happensifoneoftheparitybits,t5,isippedbythenoise. Justoneofthe
checksisviolated. Onlyr5liesinside thisunhapp ycircle andoutside theother
twohappycircles, sor5isidentiedastheonlysingle bitcapable ofexplaining
thesyndrome.

<<<PAGE 23>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
1.2:Error-correcting codesforthebinary symmetric channel 11
(a)rr r
r rr
7 65
4r32 1
(b)1*11
0 10
0
(c)*0
1
0 10
00
(d)1 0
01
0 11*
(e)1*
0*11
000-
(e0)1*
0*11
001Figure 1.15.Pictorial
represen tation ofdecodingofthe
Hamming (7;4)code.The
receiv edvectoriswritten intothe
diagram asshownin(a).In
(b,c,d,e), thereceiv edvectoris
shown,assuming thatthe
transmitted vectorwasasin
gure 1.13b andthebitslabelled
by?wereipped.Theviolated
paritychecksarehighligh tedby
dashed circles. Oneoftheseven
bitsisthemostprobable suspect
toaccoun tforeach`syndrome',
i.e.,eachpattern ofviolated and
satised paritychecks.
Inexamples (b),(c),and(d),the
mostprobable suspectistheone
bitthatwasipped.
Inexample (e),twobitshavebeen
ipped,s3andt7.Themost
probable suspectisr2,markedby
acircle in(e0),whichshowsthe
output ofthedecodingalgorithm.
Syndrome z000001010011100101110111
Unip thisbitnoner7r6r4r5r1r2r3Algorithm 1.16.Actions takenby
theoptimal decoderforthe(7;4)
Hamming code,assuming a
binary symmetric channel with
small noiselevelf.Thesyndrome
vectorzlistswhether eachparity
checkisviolated (1)orsatised
(0),going through thechecksin
theorder ofthebitsr5,r6,andr7.Ifthecentralbitr3isreceivedipped,gure 1.15d showsthatallthree
checksareviolated; onlyr3liesinside allthree circles, sor3isidentied as
thesuspectbit.
Ifyoutryipping anyoneofthesevenbits,you'llndthatadieren t
syndrome isobtained ineachcase{sevennon-zero syndromes, oneforeach
bit.There isonlyoneother syndrome, theall-zero syndrome. Soifthe
channel isabinary symmetric channel withasmall noiselevelf,theoptimal
decoderunips atmost onebit,depending onthesyndrome, asshownin
algorithm 1.16.Eachsyndrome could havebeencaused byother noisepatterns
too,butanyother noise pattern thathasthesame syndrome mustbeless
probable because itinvolvesalarger numberofnoise events.
What happensifthenoise actually ipsmore thanonebit?Figure 1.15e
showsthesituation when twobits,r3andr7,arereceivedipped.Thesyn-
drome,110,makesussuspectthesingle bitr2;soouroptimal decodingal-
gorithm ipsthisbit,giving adecodedpattern withthree errors asshown
ingure 1.15e0.Ifweusetheoptimal decodingalgorithm, anytwo-biterror
pattern willleadtoadecodedseven-bit vector thatcontainsthree errors.
Generalviewofdecodingforlinearcodes:syndromedecoding
Wecanalsodescrib ethedecodingproblem foralinear codeinterms ofmatrices. The
rstfourreceiv edbits,r1r2r3r4,purporttobethefoursource bits;andthereceiv ed
bitsr5r6r7purporttobetheparities ofthesource bits,asdened bythegenerator
matrix G.Weevaluate thethree parity-checkbitsforthereceiv edbits,r1r2r3r4,
andseewhether theymatchthethree receiv edbits,r5r6r7.Thedierences (modulo

<<<PAGE 24>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
12 1|Introduction toInformation Theory
s
-encoder
paritybits8
>>><
>>>:t channel
f=10%
-
r decoder
-
^s
Figure 1.17.Transmitting 10000
source bitsoverabinary
symmetric channel withf=10%
using a(7;4)Hamming code.The
probabilit yofdecodedbiterroris
about7%.
2)betweenthese twotriplets arecalled thesyndrome ofthereceiv edvector. Ifthe
syndrome iszero{ifallthree paritychecksarehappy{thenthereceiv edvectorisa
codeword,andthemostprobable decodingisgivenbyreading outitsrstfourbits.
Ifthesyndrome isnon-zero, thenthenoisesequence forthisblockwasnon-zero, and
thesyndrome isourpointertothemostprobable errorpattern.
Thecomputation ofthesyndrome vectorisalinear operation. Ifwedene the34
matrix Psuchthatthematrix ofequation (1.26) is
GT=
I4
P
; (1.29)
where I4isthe44identitymatrix, thenthesyndrome vectorisz=Hr,where the
parity-checkmatrix HisgivenbyH=
 PI3
;inmodulo2arithmetic,  11,
so
H=
PI3
="1110100
0111010
1011001#
: (1.30)
Allthecodewordst=GTsofthecodesatisfy
Ht="0
0
0#
: (1.31)
.Exercise 1.4.[1]Provethatthisissobyevaluating the34matrix HGT.
Since thereceiv edvectorrisgivenbyr=GTs+n,thesyndrome-deco dingproblem
istondthemostprobable noise vectornsatisfying theequation
Hn=z: (1.32)
Adecodingalgorithm thatsolvesthisproblem iscalled amaxim um-lik elihooddecoder.
Wewilldiscuss decodingproblems likethisinlaterchapters.
Summary ofthe(7;4)Hamming code'sproperties
Everypossible receivedvectoroflength 7bitsiseither acodeword,orit'sone
ipawayfromacodeword.
Since there arethree parityconstrain ts,eachofwhichmightormightnot
beviolated, there are222=8distinct syndromes. They canbedivided
intosevennon-zero syndromes {oneforeachoftheone-bit errorpatterns {
andtheall-zero syndrome, corresp onding tothezero-noise case.
Theoptimal decodertakesnoaction ifthesyndrome iszero,otherwise it
usesthismapping ofnon-zero syndromes ontoone-bit errorpatterns tounip
thesuspectbit.
There isadecodingerror ifthefourdecodedbits^s1;^s2;^s3;^s4donotall
matchthesource bitss1;s2;s3;s4.Theprobabilit yofblockerrorpBisthe

<<<PAGE 25>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
1.2:Error-correcting codesforthebinary symmetric channel 13
probabilit ythatoneormoreofthedecodedbitsinoneblockfailtomatchthe
corresp onding source bits,
pB=P(^s6=s): (1.33)
Theprobabilit yofbiterrorpbistheaverage probabilit ythatadecodedbit
failstomatchthecorresp onding source bit,
pb=1
KKX
k=1P(^sk6=sk): (1.34)
InthecaseoftheHamming code,adecodingerror willoccurwhenev er
thenoise hasippedmore thanonebitinablockofseven.Theprobabilit y
ofblockerror isthustheprobabilit ythattwoormore bitsareippedina
block.Thisprobabilit yscales asO(f2),asdidtheprobabilit yoferrorforthe
repetition codeR3.Butnotice thattheHamming codecomm unicates ata
greater rate,R=4=7.
Figure 1.17showsabinary image transmitted overabinary symmetric
channel using the(7;4)Hamming code.About7%ofthedecodedbitsare
inerror. Notice thattheerrors arecorrelated: often twoorthree successiv e
decodedbitsareipped.
Exercise 1.5.[1]Thisexercise andthenextthree refertothe(7;4)Hamming
code.Decodethereceivedstrings:
(a)r=1101011
(b)r=0110110
(c)r=0100111
(d)r=1111111 .
Exercise 1.6.[2,p.17](a)Calculate theprobabilit yofblockerrorpBofthe
(7;4)Hamming codeasafunction ofthenoise levelfandshow
thattoleading order itgoesas21f2.
(b)[3]Showthattoleading order theprobabilit yofbiterrorpbgoes
as9f2.
Exercise 1.7.[2,p.19]Findsome noise vectors thatgivetheall-zero syndrome
(that is,noise vectors thatleavealltheparitychecksunviolated). How
manysuchnoise vectors arethere?
.Exercise 1.8.[2]Iasserted abovethatablockdecodingerrorwillresult when-
evertwoormore bitsareippedinasingle block.Showthatthisis
indeed so.[Inprinciple, there mightbeerror patterns that, afterde-
coding,ledonlytothecorruption oftheparitybits,withnosource bits
incorrectly decoded.]
Summary ofcodes'performanc es
Figure 1.18showstheperformance ofrepetition codesandtheHamming code.
Italsoshowstheperformance ofafamily oflinear blockcodesthataregen-
eralizations ofHamming codes,called BCH codes.
Thisgure showsthatwecan,using linear blockcodes,achievebetter
performance thanrepetition codes;buttheasymptotic situation stilllooks
grim.

<<<PAGE 26>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
14 1|Introduction toInformation Theory
00.020.040.060.080.1
00.20.40.60.81
RateH(7,4)
more useful codes R5R3BCH(31,16)R1
BCH(15,7)pb0.1
0.01
1e-05
1e-10
1e-15
00.20.40.60.81
RateH(7,4)
more useful codesR5
BCH(511,76)
BCH(1023,101)R1Figure 1.18.Error probabilit ypb
versusrateRforrepetition codes,
the(7;4)Hamming codeand
BCH codeswithblocklengths up
to1023overabinary symmetric
channel withf=0:1.The
righthand gure showspbona
logarithmic scale.
Exercise 1.9.[4,p.19]Design anerror-correcting codeandadecodingalgorithm
forit,estimate itsprobabilit yoferror, andaddittogure 1.18. [Don't
worryifyounditdicult tomakeacodebetterthantheHamming
code,orifyounditdicult tondagooddecoderforyourcode;that's
thepointofthisexercise.]
Exercise 1.10.[3,p.20]A(7;4)Hamming codecancorrect anyoneerror; might
there bea(14;8)codethatcancorrect anytwoerrors?
Optional extra: Doestheanswertothisquestion dependonwhether the
codeislinear ornonlinear?
Exercise 1.11.[4,p.21]Design anerror-correcting code,other thanarepetition
code,thatcancorrect anytwoerrors inablockofsizeN.
1.3What performance canthebestcodesachieve?
There seems tobeatrade-o betweenthedecodedbit-error probabilit ypb
(whichwewouldliketoreduce) andtherateR(whichwewouldliketokeep
large). Howcanthistrade-o becharacterized? What pointsinthe(R;pb)
plane areachievable? Thisquestion wasaddressed byClaude Shannon inhis
pioneering paperof1948, inwhichhebothcreated theeldofinformation
theory andsolvedmostofitsfundamen talproblems.
Atthattimethere wasawidespread beliefthattheboundary between
achievableandnonac hievablepointsinthe(R;pb)plane wasacurvepassing
through theorigin (R;pb)=(0;0);ifthiswereso,then, inorder toachieve
avanishingly small error probabilit ypb,onewouldhavetoreduce therate
corresp ondingly closetozero. `Nopain,nogain.'
However,Shannon provedtheremark ableresult thattheboundary be-
tweenachievableandnonac hievablepointsmeets theRaxisatanon-zer o
valueR=C,asshowningure 1.19. Foranychannel, there existcodesthat
makeitpossible tocomm unicate witharbitrarilysmallprobabilit yoferrorpb
atnon-zero rates. Thersthalfofthisbook(PartsI{III)willbedevotedto
understanding thisremark ableresult, whichiscalled thenoisy-c hannel coding
theorem .
Example: f=0:1
Themaxim umrateatwhichcomm unication ispossible witharbitrarily small
pbiscalled thecapacit yofthechannel. Theformulaforthecapacit yofa

<<<PAGE 27>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
1.4:Summary 15
00.020.040.060.080.1
00.20.40.60.81
Ratenot achievableH(7,4)
achievableR5R3R1
Cpb0.1
0.01
1e-05
1e-10
1e-15
00.20.40.60.81
Ratenot achievable achievableR5 R1
CFigure 1.19.Shannon's
noisy-c hannel codingtheorem.
Thesolidcurveshowsthe
Shannon limitonachievable
valuesof(R;pb)forthebinary
symmetric channel withf=0:1.
Rates uptoR=Careachievable
witharbitrarily smallpb.The
pointsshowtheperformance of
sometextbookcodes,asin
gure 1.18.
Theequation dening the
Shannon limit(thesolidcurve)is
R=C=(1 H2(pb));whereCand
H2aredened inequation (1.35).
binary symmetric channel withnoise levelfis
C(f)=1 H2(f)=1 
flog21
f+(1 f)log21
1 f
; (1.35)
thechannel wewerediscussing earlier withnoise levelf=0:1hascapacit y
C'0:53.Letusconsider whatthismeans interms ofnoisy diskdrives.The
repetition codeR3could comm unicate overthischannel withpb=0:03ata
rateR=1=3.Thusweknowhowtobuild asingle gigabytediskdrivewith
pb=0:03fromthree noisy gigabytediskdrives.Wealsoknowhowtomakea
single gigabytediskdrivewithpb'10 15fromsixtynoisy one-gigab ytedrives
(exercise 1.3,p.7).AndnowShannon passes by,notices usjuggling withdisk
drivesandcodesandsays:
`What performance areyoutrying toachieve?10 15?Youdon't
needsixty diskdrives{youcangetthatperformance withjust
twodiskdrives(since 1/2islessthan0:53). Andifyouwant
pb=10 18or10 24oranything, youcangetthere withtwodisk
drivestoo!'
[Strictly ,theabovestatemen tsmightnotbequite right,since, asweshallsee,
Shannon provedhisnoisy-c hannel codingtheorem bystudying sequences of
blockcodeswithever-increasing blocklengths, andtherequired blocklength
mightbebigger thanagigabyte(thesizeofourdiskdrive),inwhichcase,
Shannon mightsay`well,youcan'tdoitwiththosetinydiskdrives,butifyou
hadtwonoisyterabyte drives,youcould makeasingle highqualityterabyte
drivefromthem'.]
1.4Summary
The(7;4)Hamming Code
Byincluding three parity-checkbitsinablockof7bitsitispossible todetect
andcorrect anysingle biterrorineachblock.
Shannon 'snoisy-channel codingtheorem
Information canbecommunic atedoveranoisychannel atanon-zer oratewith
arbitrarilysmallerrorprobability.
Information theory addresses boththelimitations andthepossibilities of
comm unication. Thenoisy-c hannel codingtheorem, whichwewillprovein

<<<PAGE 28>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
16 1|Introduction toInformation Theory
Chapter 10,asserts boththatreliable comm unication atanyratebeyondthe
capacit yisimpossible, andthatreliable comm unication atallrates upto
capacit yispossible.
Thenextfewchapters laythefoundations forthisresult bydiscussing
howtomeasure information contentandtheintimately related topic ofdata
compression .
1.5Further exercises
.Exercise 1.12.[2,p.21]Consider therepetition codeR9.Onewayofviewing
thiscodeisasaconcatenation ofR3withR3.Werstencodethe
source stream withR3,thenencodetheresulting output withR3.We
could callthiscode`R2
3'.Thisideamotivatesanalternativ edecoding
algorithm, inwhichwedecodethebitsthree atatimeusing thedecoder
forR3;thendecodethedecodedbitsfromthatrstdecoderusing the
decoderforR3.
Evaluate theprobabilit yoferror forthisdecoderandcompare itwith
theprobabilit yoferrorfortheoptimal decoderforR9.
Dotheconcatenated encoderanddecoderforR2
3haveadvantages over
those forR9?
1.6Solutions
Solution toexercise 1.2(p.7).Anerrorismade byR3iftwoormorebitsare
ippedinablockofthree. Sotheerror probabilit yofR3isasumoftwo
terms: theprobabilit ythatallthree bitsareipped,f3;andtheprobabilit y
thatexactly twobitsareipped,3f2(1 f).[Ifthese expressions arenot
obvious, seeexample 1.1(p.1):theexpressions areP(r=3jf;N=3)and
P(r=2jf;N=3).]
pb=pB=3f2(1 f)+f3=3f2 2f3: (1.36)
Thisprobabilit yisdominated forsmallfbytheterm3f2.
Seeexercise 2.38(p.39)forfurther discussion ofthisproblem.
Solution toexercise 1.3(p.7).Theprobabilit yoferrorfortherepetition code
RNisdominated bytheprobabilit yofdN=2ebits'beingipped,whichgoes
(foroddN)as Notation:
N=2
denotes thesmallest
integer greater thanorequal toN=2. 
N
dN=2e!
f(N+1)=2(1 f)(N 1)=2: (1.37)
Theterm N
Kcanbeapproximated using thebinary entropyfunction:
1
N+12NH2(K=N) 
N
K!
2NH2(K=N)) 
N
K!
'2NH2(K=N);(1.38)
where thisapproximation introduces anerror oforderp
N{asshownin
equation (1.17). So
pb=pB'2N(f(1 f))N=2=(4f(1 f))N=2: (1.39)
Setting thisequal totherequired valueof10 15wendN'2log10 15
log4f(1 f)=68.
Thisanswerisalittleoutbecause theapproximation weusedoverestimated N
Kandwedidnotdistinguish betweendN=2eandN=2.

<<<PAGE 29>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
1.6:Solutions 17
Aslightlymorecareful answer(short ofexplicit computation) goesasfollows.Taking
theapproximation for N
K
tothenextorder, wend:

N
N=2
'2N 1p
2N=4: (1.40)
Thisapproximation canbeprovedfromanaccurate version ofStirling's approxima-
tion(1.12), orbyconsidering thebinomial distribution withp=1=2andnoting
1=X
K
N
K
2 N'2 N
N
N=2 N=2X
r= N=2e r2=22'2 N
N
N=2p
2;(1.41)
where =p
N=4,fromwhichequation (1.40) follows.Thedistinction betweendN=2e
andN=2isnotimportantinthistermsince N
K
hasamaxim umatK=N=2.
Then theprobabilit yoferror(foroddN)istoleading order
pb'
N
(N+1)=2
f(N+1)=2(1 f)(N 1)=2(1.42)
'2N1p
N=2f[f(1 f)](N 1)=2'1p
N=8f[4f(1 f)](N 1)=2(1.43)
Theequation pb=10 15canbewritten
(N 1)=2'log10 15+logp
N=8
f
log4f(1 f)(1.44)
whichmaybesolvedforNiterativ ely,therstiteration starting from ^N1=68:
(^N2 1)=2' 15+1:7
 0:44=29:9)^N2'60:9 (1.45)
Thisanswerisfound tobestable, soN'61istheblocklength atwhichpb'10 15.
Solution toexercise 1.6(p.13).
(a)Theprobabilit yofblockerroroftheHamming codeisasumofsixterms
{theprobabilities that2,3,4,5,6,or7errors occurinoneblock.
pB=7X
r=2 
7
r!
fr(1 f)7 r: (1.46)
Toleading order, thisgoesas
pB' 
7
2!
f2=21f2: (1.47)
(b)Theprobabilit yofbiterror oftheHamming codeissmaller thanthe
probabilit yofblockerrorbecause ablockerrorrarely corrupts allbitsin
thedecodedblock.Theleading-order behaviour isfound byconsidering
theoutcome inthemostprobable casewhere thenoisevectorhasweight
two.Thedecoderwillerroneously ipathirdbit,sothatthemodied
receivedvector (oflength 7)diers inthree bitsfromthetransmitted
vector. Thatmeans, ifweaverage overallsevenbits,theprobabilit ythat
arandomly chosen bitisippedis3=7times theblockerrorprobabilit y,
toleading order. Now,whatwereally careaboutistheprobabilit ythat
asource bitisipped.Areparitybitsorsource bitsmore likelytobe
among these three ippedbits,orareallsevenbitsequally likelytobe
corrupted when thenoise vector hasweighttwo?TheHamming code
isinfactcompletely symmetric intheprotection itaords totheseven
bits(assuming abinary symmetric channel). [This symmetry canbe

<<<PAGE 30>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
18 1|Introduction toInformation Theory
provedbyshowingthattheroleofaparitybitcanbeexchanged with
asource bitandtheresulting codeisstilla(7;4)Hamming code;see
below.]Theprobabilit ythatanyonebitendsupcorrupted isthesame
forallsevenbits.Sotheprobabilit yofbiterror(forthesource bits)is
simply three seventhsoftheprobabilit yofblockerror.
pb'3
7pB'9f2: (1.48)
Symmetry oftheHamming (7;4)code
Toprovethatthe(7;4)codeprotects allbitsequally ,westartfromtheparity-
checkmatrix
H=2
641110100
0111010
10110013
75: (1.49)
Thesymmetry among theseventransmitted bitswillbeeasiest toseeifwe
reorder thesevenbitsusing thepermutation (t1t2t3t4t5t6t7)!(t5t2t3t4t1t6t7).
Then wecanrewrite Hthus:
H=2
641110100
0111010
00111013
75: (1.50)
Now,ifwetakeanytwoparityconstrain tsthattsatises andaddthem
together, wegetanother parityconstrain t.Forexample, row1assertst5+
t2+t3+t1=even,androw2assertst2+t3+t4+t6=even,andthesumof
these twoconstrain tsis
t5+2t2+2t3+t1+t4+t6=even; (1.51)
wecandroptheterms 2t2and2t3,sincetheyareevenwhatev ert2andt3are;
thuswehavederivedtheparityconstrain tt5+t1+t4+t6=even,whichwe
canifwewishaddintotheparity-checkmatrix asafourth row.[Thesetof
vectors satisfying Ht=0willnotbechanged.] Wethusdene
H0=2
66641110100
0111010
0011101
10011103
7775: (1.52)
Thefourth rowisthesum(modulotwo)ofthetoptworows.Notice thatthe
second,third,andfourth rowsareallcyclic shifts ofthetoprow.If,having
added thefourth redundan tconstrain t,wedroptherstconstrain t,weobtain
anewparity-checkmatrix H00,
H00=2
640111010
0011101
10011103
75; (1.53)
whichstillsatises H00t=0forallcodewords, andwhichlooksjustlike
thestarting Hin(1.50), except thatallthecolumns haveshifted along one
totheright,andtherightmost column hasreappeared attheleft(acyclic
permutation ofthecolumns).
Thisestablishes thesymmetry among thesevenbits.Iterating theabove
procedure vemore times, wecanmakeatotalofsevendieren tHmatrices
forthesame original code,eachofwhichassigns eachbittoadieren trole.

<<<PAGE 31>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
1.6:Solutions 19
Wemayalsoconstruct thesuper-redundan tseven-rowparity-checkmatrix
forthecode,
H000=2
666666666641110100
0111010
0011101
1001110
0100111
1010011
11010013
77777777775: (1.54)
Thismatrix is`redundan t'inthesense thatthespace spanned byitsrowsis
onlythree-dimensional, notseven.
Thismatrix isalsoacyclic matrix. Everyrowisacyclic permutation of
thetoprow.
Cyclic codes:ifthere isanordering ofthebitst1:::tNsuchthatalinear
codehasacyclic parity-checkmatrix, thenthecodeiscalled acyclic
code.
Thecodewordsofsuchacodealsohavecyclic properties: anycyclic
permutation ofacodewordisacodeword.
Forexample, theHamming (7;4)code,withitsbitsordered asabove,
consists ofallsevencyclic shifts ofthecodewords1110100 and1011000 ,
andthecodewords0000000 and1111111 .
Cyclic codesareacornerstone ofthealgebraic approac htoerror-correcting
codes.Wewon'tusethem again inthisbook,however,astheyhavebeen
superceded bysparse graph codes(PartVI).
Solution toexercise 1.7(p.13).There arefteen non-zero noisevectors which
givetheall-zero syndrome; these areprecisely thefteen non-zero codewords
oftheHamming code.Notice thatbecause theHamming codeislinear,the
sumofanytwocodewordsisacodeword.
Graphscorresponding tocodes
Solution toexercise 1.9(p.14).When answering thisquestion, youwillprob-
ablyndthatitiseasier toinventnewcodesthantondoptimal decoders
forthem. There aremanywaystodesign codes,andwhat followsisjustone
possible trainofthough t.Wemakealinear blockcodethatissimilar tothe
(7;4)Hamming code,butbigger.
Figure 1.20.Thegraph ofthe
(7;4)Hamming code.The7
circles arethebitnodesandthe3
squares aretheparity-check
nodes.Manycodescanbeconvenientlyexpressed interms ofgraphs. Ing-
ure1.13,weintroduced apictorial represen tation ofthe(7;4)Hamming code.
Ifwereplace thatgure's bigcircles, eachofwhichshowsthattheparityof
fourparticular bitsiseven,bya`parity-checknode'thatisconnected tothe
fourbits,thenweobtain therepresen tation ofthe(7;4)Hamming codebya
bipartite graph asshowningure 1.20. The7circles arethe7transmitted
bits.The3squares aretheparity-checknodes(nottobeconfused withthe
3parity-checkbits,whicharethethree most peripheral circles). Thegraph
isa`bipartite' graph because itsnodesfallintotwoclasses {bitsandchecks
{andthere areedges onlybetweennodesindieren tclasses. Thegraph and
thecode'sparity-checkmatrix (1.30) aresimply related toeachother: each
parity-checknodecorresp ondstoarowofHandeachbitnodecorresp ondsto
acolumn ofH;forevery1inH,there isanedgebetweenthecorresp onding
pairofnodes.

<<<PAGE 32>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
20 1|Introduction toInformation Theory
Havingnoticed thisconnection betweenlinear codesandgraphs, oneway
toinventlinear codesissimply tothink ofabipartite graph. Forexample,
aprettybipartite graph canbeobtained fromadodecahedron bycalling the
vertices ofthedodecahedron theparity-checknodes,andputting atransmitted
bitoneachedgeinthedodecahedron. Thisconstruction denes aparity-
Figure 1.21.Thegraph dening
the(30;11)dodecahedron code.
Thecircles arethe30transmitted
bitsandthetriangles arethe20
paritychecks.Oneparitycheckis
redundan t.checkmatrix inwhicheverycolumn hasweight2andeveryrowhasweight3.
[Theweightofabinary vector isthenumberof1sitcontains.]
ThiscodehasN=30bits,anditappearstohaveMapparen t=20parity-
checkconstrain ts.Actually ,there areonlyM=19independent constrain ts;
the20thconstrain tisredundan t(that is,if19constrain tsaresatised, then
the20thisautomatically satised); sothenumberofsource bitsisK=
N M=11.Thecodeisa(30;11)code.
Itishardtondadecodingalgorithm forthiscode,butwecanestimate
itsprobabilit yoferror bynding itslowestweightcodewords. Ifweipall
thebitssurrounding onefaceoftheoriginal dodecahedron, thenalltheparity
checkswillbesatised; sothecodehas12codewordsofweight5,oneforeach
face. Since thelowest-weightcodewordshaveweight5,wesaythatthecode
hasdistanced=5;the(7;4)Hamming codehaddistance 3andcould correct
allsingle bit-ip errors. Acodewithdistance 5cancorrect alldouble bit-ip
errors, butthere aresome triple bit-ip errors thatitcannot correct. Sothe
errorprobabilit yofthiscode,assuming abinary symmetric channel, willbe
dominated, atleastforlownoise levelsf,byaterm oforderf3,perhaps
something like
12 
5
3!
f3(1 f)27: (1.55)
Ofcourse, there isnoobligation tomakecodeswhose graphs canberep-
resentedonaplane, asthisonecan;thebestlinear codes,whichhavesimple
graphical descriptions, havegraphs thataremore tangled, asillustrated by
thetiny(16;4)codeofgure 1.22.Figure 1.22.Graph ofarate-1/4
low-densit yparity-checkcode
(Gallager code)withblocklength
N=16,andM=12parity-check
constrain ts.Eachwhite circle
represen tsatransmitted bit.Each
bitparticipates inj=3
constrain ts,represen tedby
squares. Theedges betweennodes
wereplaced atrandom. (See
Chapter 47formore.) Furthermore, there isnoreason forstickingtolinear codes;indeed some
nonlinear codes{codeswhose codewordscannot bedened byalinear equa-
tionlikeHt=0{haveverygoodproperties. Buttheencodinganddecoding
ofanonlinear codeareeventrickiertasks.
Solution toexercise 1.10(p.14).First let'sassume wearemaking alinear
codeanddecodingitwithsyndrome decoding. Ifthere areNtransmitted
bits,thenthenumberofpossible errorpatterns ofweightuptotwois
 
N
2!
+ 
N
1!
+ 
N
0!
: (1.56)
ForN=14,that's 91+14+1=106patterns. Now,everydistinguishable
error pattern mustgiverisetoadistinct syndrome; andthesyndrome isa
listofMbits,sothemaxim umpossible numberofsyndromes is2M.Fora
(14;8)code,M=6,sothere areatmost26=64syndromes. Thenumberof
possible errorpatterns ofweightuptotwo,106,isbigger thanthenumberof
syndromes, 64,sowecanimmediately ruleoutthepossibilit ythatthere isa
(14;8)codethatis2-error-correcting.
Thesame countingargumen tworksnefornonlinear codestoo.When
thedecoderreceivesr=t+n,hisaimistodeduce bothtandnfromr.If
itisthecasethatthesender canselect anytransmission tfromacodeofsize
St,andthechannel canselect anynoisevectorfromasetofsizeSn,andthose
twoselections canberecoveredfromthereceivedbitstringr,whichisoneof

<<<PAGE 33>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
1.6:Solutions 21
atmost2Npossible strings, thenitmustbethecasethat
StSn2N: (1.57)
So,fora(N;K)two-error-correcting code,whether linear ornonlinear,
2K" 
N
2!
+ 
N
1!
+ 
N
0!#
2N: (1.58)
Solution toexercise 1.11(p.14).There arevarious strategies formaking codes
thatcancorrect multiple errors, andIstrongly recommend youthink outone
ortwoofthem foryourself.
Ifyourapproac husesalinear code,e.g.,onewithacollection ofMparity
checks,itishelpful tobearinmindthecountingargumen tgivenintheprevious
exercise, inorder toanticipate howmanyparitychecks,M,youmightneed.
Examples ofcodesthatcancorrect anytwoerrors arethe(30;11)dodec-
ahedron codeintheprevious solution, andthe(15;6)pentagonful codetobe
introduced onp.221.Further simple ideas formaking codesthatcancorrect
multiple errors fromcodesthatcancorrect onlyoneerror arediscussed in
section 13.7.
Solution toexercise 1.12(p.16).Theprobabilit yoferror ofR2
3is,toleading
order,
pb(R2
3)'3[pb(R3)]2=3(3f2)2+=27f4+; (1.59)
whereas theprobabilit yoferrorofR9isdominated bytheprobabilit yofve
ips,
pb(R9)' 
9
5!
f5(1 f)4'126f5+: (1.60)
TheR2
3decodingprocedure istherefore suboptimal, sincethere arenoisevec-
torsofweightfourwhichcause ittomakeadecodingerror.
Ithastheadvantage, however,ofrequiring smaller computational re-
sources: onlymemorization ofthree bits,andcountinguptothree, rather
thancountinguptonine.
Thissimple codeillustrates animportantconcept. Concatenated codes
arewidely usedinpractice because concatenation allowslarge codestobe
implemen tedusing simple encodinganddecodinghardw are.Some ofthebest
knownpractical codesareconcatenated codes.

<<<PAGE 34>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
2
Probabilit y,Entropy,andInference
Thischapter, anditssibling, Chapter 8,devotesome timetonotation. Just
astheWhite Knigh tdistinguished betweenthesong, thename ofthesong,
andwhat thename ofthesongwascalled (Carroll, 1998), wewillsometimes
needtobecareful todistinguish betweenarandom variable, thevalueofthe
random variable, andtheproposition thatasserts thattherandom variable
hasaparticular value. Inanyparticular chapter, however,Iwillusethemost
simple andfriendly notation possible, attheriskofupsetting pure-minded
readers. Forexample, ifsomething is`truewithprobabilit y1',Iwillusually
simply saythatitis`true'.
2.1Probabilities andensem bles
Anensem bleXisatriple (x;AX;PX),where theoutcomexisthevalue
ofarandom variable, whichtakesononeofasetofpossible values,
AX=fa1;a2;:::;ai;:::;aIg,havingprobabilitiesPX=fp1;p2;:::;pIg,
withP(x=ai)=pi,pi0andP
ai2AXP(x=ai)=1.
ThenameAismnemonic for`alphab et'.Oneexample ofanensem bleisa
letter thatisrandomly selected fromanEnglish documen t.Thisensem bleis
showningure 2.1.There aretwenty-sevenpossible letters:a{z,andaspace
character `-'.iaipi
1a0.0575
2b0.0128
3c0.0263
4d0.0285
5e0.0913
6f0.0173
7g0.0133
8h0.0313
9i0.0599
10j0.0006
11k0.0084
12l0.0335
13m0.0235
14n0.0596
15o0.0689
16p0.0192
17q0.0008
18r0.0508
19s0.0567
20t0.0706
21u0.0334
22v0.0069
23w0.0119
24x0.0073
25y0.0164
26z0.0007
27{0.1928a
b
c
d
e
f
g
h
i
j
k
l
m
n
o
p
q
r
s
t
u
v
w
x
y
z
{
Figure 2.1.Probabilit y
distribution overthe27outcomes
forarandomly selected letter in
anEnglish language documen t
(estimated fromTheFrequently
AskedQuestions Manual for
Linux ).Thepicture showsthe
probabilities bytheareas ofwhite
squares.Abbreviations .Briefer notation willsometimes beused. Forexample,
P(x=ai)maybewritten asP(ai)orP(x).
Probabilit yofasubset .IfTisasubset ofAXthen:
P(T)=P(x2T)=X
ai2TP(x=ai): (2.1)
Forexample, ifwedeneVtobevowelsfrom gure 2.1,V=
fa;e;i;o;ug,then
P(V)=0:06+0:09+0:06+0:07+0:03=0:31: (2.2)
Ajointensem bleXYisanensem bleinwhicheachoutcome isanordered
pairx;ywithx2AX=fa1;:::;aIgandy2AY=fb1;:::;bJg.
WecallP(x;y)thejointprobabilit yofxandy.
Commas areoptional when writing ordered pairs, soxy,x;y.
N.B.Inajointensem bleXYthetwovariables arenotnecessarily inde-
penden t.
22

<<<PAGE 35>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
2.1:Probabilities andensem bles 23
abcdefghijklmnopqrstuvwxyz{ya
bc
de
fg
hij
klmn
opq
r
s
tuvw
xy
z{x Figure 2.2.Theprobabilit y
distribution overthe2727
possible bigramsxyinanEnglish
language documen t,The
Frequently AskedQuestions
Manual forLinux.
Marginal probabilit y.Wecanobtain themarginal probabilit yP(x)from
thejointprobabilit yP(x;y)bysummation:
P(x=ai)X
y2AYP(x=ai;y): (2.3)
Similarly ,using briefer notation, themarginal probabilit yofyis:
P(y)X
x2AXP(x;y): (2.4)
Conditional probabilit y
P(x=aijy=bj)P(x=ai;y=bj)
P(y=bj)ifP(y=bj)6=0. (2.5)
[IfP(y=bj)=0thenP(x=aijy=bj)isundened.]
WepronounceP(x=aijy=bj)`theprobabilit ythatxequalsai,given
yequalsbj'.
Example 2.1.Anexample ofajointensem bleistheordered pairXYconsisting
oftwosuccessiv eletters inanEnglish documen t.Thepossible outcomes
areordered pairssuchasaa,ab,ac,andzz;ofthese, wemightexpect
abandactobemore probable thanaaandzz.Anestimate ofthe
jointprobabilit ydistribution fortwoneighbouring characters isshown
graphically ingure 2.2.
Thisjointensem blehasthespecialpropertythatitstwomarginal dis-
tributions, P(x)andP(y),areidentical. They arebothequal tothe
monogram distribution showningure 2.1.
Fromthisjointensem bleP(x;y)wecanobtain conditional distributions,
P(yjx)andP(xjy),bynormalizing therowsandcolumns, respectively
(gure 2.3). Theprobabilit yP(yjx=q)istheprobabilit ydistribution
ofthesecond letter giventhattherstletter isaq.Asyoucanseein
gure 2.3a,thetwomost probable values forthesecond letterygiven

<<<PAGE 36>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
24 2|Probabilit y,Entropy,andInference
abcdefghijklmnopqrstuvwxyz{yabcdefg
hijklmnopqrstuvwxyz{x
abcdefghijklmnopqrstuvwxyz{yabcdefg
hijklmnopqrstuvwxyz{x
(a)P(yjx) (b)P(xjy)Figure 2.3.Conditional
probabilit ydistributions. (a)
P(yjx):Eachrowshowsthe
conditional distribution ofthe
second letter,y,giventherst
letter,x,inabigramxy.(b)
P(xjy):Eachcolumn showsthe
conditional distribution ofthe
rstletter,x,giventhesecond
letter,y.
thattherstletterxisqareuand-.(The space iscommon afterq
because thesource documen tmakesheavyuseofthewordFAQ.)
Theprobabilit yP(xjy=u)istheprobabilit ydistribution oftherst
letterxgiventhatthesecond letteryisau.Asyoucanseeingure 2.3b
thetwomostprobable values forxgiveny=uarenando.
Rather thanwriting downthejointprobabilit ydirectly ,weoftendene an
ensem bleinterms ofacollection ofconditional probabilities. Thefollowing
rulesofprobabilit ytheory willbeuseful. (Hdenotes assumptions onwhich
theprobabilities arebased.)
Productrule{obtained fromthedenition ofconditional probabilit y:
P(x;yjH)=P(xjy;H)P(yjH)=P(yjx;H)P(xjH): (2.6)
Thisruleisalsoknownasthechainrule.
Sumrule{arewriting ofthemarginal probabilit ydenition:
P(xjH)=X
yP(x;yjH) (2.7)
=X
yP(xjy;H)P(yjH): (2.8)
Bayes'theorem {obtained fromtheproductrule:
P(yjx;H)=P(xjy;H)P(yjH)
P(xjH)(2.9)
=P(xjy;H)P(yjH)P
y0P(xjy0;H)P(y0jH): (2.10)
Indep endence .Tworandom variablesXandYareindependen t(sometimes
writtenX?Y)ifandonlyif
P(x;y)=P(x)P(y): (2.11)
Exercise 2.2.[1,p.40]Aretherandom variablesXandYinthejointensem ble
ofgure 2.2independen t?

<<<PAGE 37>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
2.2:Themeaning ofprobabilit y 25
Isaidthatweoften dene anensem bleinterms ofacollection ofcondi-
tional probabilities. Thefollowingexample illustrates thisidea.
Example 2.3.Johasatestforanastydisease. Wedenote Jo'sstateofhealth
bythevariableaandthetestresult byb.
a=1Johasthedisease
a=0Jodoesnothavethedisease.(2.12)
Theresult ofthetestiseither `positive'(b=1)or`negativ e'(b=0);
thetestis95%reliable: in95%ofcases ofpeople whoreally havethe
disease, apositiveresult isreturned, andin95%ofcases ofpeople who
donothavethedisease, anegativ eresult isobtained. Thenalpieceof
background information isthat1%ofpeople ofJo'sageandbackground
havethedisease.
OK{Johasthetest,andtheresult waspositive.What istheprobabilit y
thatJohasthedisease?
Solution. Wewrite downalltheprovided probabilities. Thetestreliabilit y
species theconditional probabilit yofbgivena:
P(b=1ja=1)=0:95P(b=1ja=0)=0:05
P(b=0ja=1)=0:05P(b=0ja=0)=0:95;(2.13)
andthedisease prevalence tellsusaboutthemarginal probabilit yofa:
P(a=1)=0:01P(a=0)=0:99: (2.14)
FromthemarginalP(a)andtheconditional probabilit yP(bja)wecandeduce
thejointprobabilit yP(a;b)=P(a)P(bja)andanyother probabilities weare
interested in.Forexample, bythesumrule,themarginal probabilit yofb=1
{theprobabilit yofgetting apositiveresult {is
P(b=1)=P(b=1ja=1)P(a=1)+P(b=1ja=0)P(a=0): (2.15)
Johasreceivedapositiveresultb=1andisinterested inhowplausible itis
thatshehasthedisease (i.e.,thata=1).Themaninthestreet mightbe
dupedbythestatemen t`thetestis95%reliable, soJo'spositiveresult implies
thatthere isa95%chance thatJohasthedisease', butthisisincorrect. The
correct solution toaninference problem isfound using Bayes'theorem.
P(a=1jb=1)=P(b=1ja=1)P(a=1)
P(b=1ja=1)P(a=1)+P(b=1ja=0)P(a=0)(2.16)
=0:950:01
0:950:01+0:050:99(2.17)
=0:16 (2.18)
Soinspiteofthepositiveresult, theprobabilit ythatJohasthedisease isonly
16%. 2
2.2Themeaning ofprobabilit y
Probabilities canbeusedintwoways.
Probabilities candescrib efrequencies ofoutcomes inrandom experimen ts,
butgiving noncircular denitions oftheterms `frequency' and`random' isa
challenge {what doesitmean tosaythatthefrequency ofatossed coin's

<<<PAGE 38>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
26 2|Probabilit y,Entropy,andInference
Box2.4.TheCoxaxioms.
Ifasetofbeliefssatisfy these
axioms thentheycanbemapped
ontoprobabilities satisfying
P(false)=0,P(true)=1,
0P(x)1,andtherulesof
probabilit y:
P(x)=1 P(x),
and
P(x;y)=P(xjy)P(y):Notation .Let`thedegree ofbeliefinpropositionx'bedenoted byB(x).The
negation ofx(not-x)iswrittenx.Thedegree ofbeliefinacondi-
tional proposition, `x,assuming propositionytobetrue', isrepresen ted
byB(xjy).
Axiom 1.Degrees ofbeliefcanbeordered; ifB(x)is`greater' thanB(y),and
B(y)is`greater' thanB(z),thenB(x)is`greater' thanB(z).
[Consequence: beliefscanbemappedontorealnumbers.]
Axiom 2.Thedegree ofbeliefinapropositionxanditsnegationxarerelated.
There isafunctionfsuchthat
B(x)=f[B(x)]:
Axiom 3.Thedegree ofbeliefinaconjunction ofpropositionsx;y(xandy)is
related tothedegree ofbeliefintheconditional propositionxjyandthe
degree ofbeliefinthepropositiony.There isafunctiongsuchthat
B(x;y)=g[B(xjy);B(y)]:
coming upheads is1/2?Ifwesaythatthisfrequency istheaverage fraction of
heads inlongsequences, wehavetodene `average'; anditishardtodene
`average' without using awordsynon ymous toprobabilit y!Iwillnotattempt
tocutthisphilosophical knot.
Probabilities canalsobeused, more generally ,todescrib edegrees ofbe-
liefinpropositions thatdonotinvolverandom variables {forexample `the
probabilit ythatMr.S.wasthemurderer ofMrs.S.,giventheevidence' (he
either wasorwasn't, andit'sthejury's jobtoassess howprobable itisthathe
was);`theprobabilit ythatThomas Jeerson hadachildbyoneofhisslaves';
`theprobabilit ythatShakespeare's playswerewritten byFrancis Bacon'; or,
topickamodern-da yexample, `theprobabilit ythataparticular signature on
aparticular cheque isgenuine'.
Themaninthestreet ishappytouseprobabilities inboththese ways,but
somebooksonprobabilit yrestrict probabilities toreferonlytofrequencies of
outcomes inrepeatable random experimen ts.
Nevertheless, degrees ofbeliefcanbemappedontoprobabilities iftheysat-
isfysimple consistency rulesknownastheCoxaxioms (Cox,1946) (gure 2.4).
Thusprobabilities canbeusedtodescrib eassumptions, andtodescrib ein-
ferences giventhose assumptions. Therulesofprobabilit yensure thatiftwo
people makethesame assumptions andreceivethesame datathentheywill
drawidenticalconclusions. Thismore general useofprobabilit ytoquantify
beliefsisknownastheBayesian viewp oint.Itisalsoknownasthesubjective
interpretation ofprobabilit y,since theprobabilities dependonassumptions.
Advocates ofaBayesian approac htodatamodelling andpattern recognition
donotviewthissubjectivit yasadefect, sinceintheirview,
youcannot doinference without making assumptions.
InthisbookitwillfromtimetotimebetakenforgrantedthataBayesian
approac hmakessense, butthereader iswarned thatthisisnotyetaglobally
heldview{theeldofstatistics wasdominated formostofthe20thcentury
bynon-Ba yesian metho dsinwhichprobabilities areallowedtodescrib eonly
random variables. Thebigdierence betweenthetwoapproac hesisthat

<<<PAGE 39>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
2.3:Forwardprobabilities andinverseprobabilities 27
Bayesians alsouseprobabilities todescrib einferences.
2.3Forwardprobabilities andinverseprobabilities
Probabilit ycalculations often fallintooneoftwocategories: forwardprob-
abilityandinverseprobabilit y.Hereisanexample ofaforwardprobabilit y
problem:
Exercise 2.4.[2,p.40]AnurncontainsKballs, ofwhichBareblackandW=
K Barewhite. Freddrawsaballatrandom fromtheurnandreplaces
it,Ntimes.
(a)What istheprobabilit ydistribution ofthenumberoftimes ablack
ballisdrawn,nB?
(b)What istheexpectation ofnB?What isthevariance ofnB?What
isthestandard deviation ofnB?Givenumerical answersforthe
casesN=5andN=400,whenB=2andK=10.
Forwardprobabilit yproblems involveagenerativ emodelthatdescrib esapro-
cessthatisassumed togiverisetosome data; thetaskistocompute the
probabilit ydistribution orexpectation ofsome quantitythatdependsonthe
data. Hereisanother example ofaforwardprobabilit yproblem:
Exercise 2.5.[2,p.40]AnurncontainsKballs, ofwhichBareblackandW=
K Barewhite. Wedene thefractionfBB=K.FreddrawsN
times fromtheurn,exactly asinexercise 2.4,obtainingnBblacks,and
computes thequantity
z=(nB fBN)2
NfB(1 fB): (2.19)
What istheexpectation ofz?InthecaseN=5andfB=1=5,what
istheprobabilit ydistribution ofz?What istheprobabilit ythatz<1?
[Hint:comparezwiththequantitiescomputed intheprevious exercise.]
Likeforwardprobabilit yproblems, inverse probability problems involvea
generativ emodelofaprocess,butinstead ofcomputing theprobabilit ydistri-
bution ofsomequantityproducedbytheprocess,wecompute theconditional
probabilit yofoneormore oftheunobserve dvariables intheprocess,given
theobserv edvariables. Thisinvariably requires theuseofBayes'theorem.
Example 2.6.There areelevenurnslabelledbyu2f0;1;2;:::;10g,eachcon-
taining tenballs. Urnucontainsublackballsand10 uwhite balls.
Fredselects anurnuatrandom anddrawsNtimes withreplacemen t
fromthaturn,obtainingnBblacksandN nBwhites. Fred'sfriend,
Bill,lookson.IfafterN=10drawsnB=3blackshavebeendrawn,
what istheprobabilit ythattheurnFredisusing isurnu,fromBill's
pointofview? (Billdoesn'tknowthevalueofu.)
Solution. Thejointprobabilit ydistribution oftherandom variablesuandnB
canbewritten
P(u;nBjN)=P(nBju;N)P(u): (2.20)
Fromthejointprobabilit yofuandnB,wecanobtain theconditional
distribution ofugivennB:
P(ujnB;N)=P(u;nBjN)
P(nBjN)(2.21)
=P(nBju;N)P(u)
P(nBjN): (2.22)

<<<PAGE 40>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
28 2|Probabilit y,Entropy,andInference
012345678910nB0
1
2
3
4
5
6
7
8
9
10uFigure 2.5.Jointprobabilit yofu
andnBforBillandFred'surn
problem, afterN=10draws.
Themarginal probabilit yofuisP(u)=1
11forallu.Youwrote downthe
probabilit yofnBgivenuandN,P(nBju;N),when yousolvedexercise 2.4
(p.27).[Youaredoing thehighly recommended exercises, aren't you?]Ifwe
denefuu=10then
P(nBju;N)= 
N
nB!
fnBu(1 fu)N nB: (2.23)
What aboutthedenominator, P(nBjN)?Thisisthemarginal probabilit yof
nB,whichwecanobtain using thesumrule:
P(nBjN)=X
uP(u;nBjN)=X
uP(u)P(nBju;N): (2.24)
Sotheconditional probabilit yofugivennBis
P(ujnB;N)=P(u)P(nBju;N)
P(nBjN)(2.25)
=1
P(nBjN)1
11 
N
nB!
fnBu(1 fu)N nB:(2.26)00.050.10.150.20.250.3
012345678910
u
uP(ujnB=3;N)
00
10.063
20.22
30.29
40.24
50.13
60.047
70.0099
80.00086
90.0000096
100
Figure 2.6.Conditional
probabilit yofugivennB=3and
N=10.This conditional distribution canbefound bynormalizing column3of
gure 2.5andisshowningure 2.6.Thenormalizing constan t,themarginal
probabilit yofnB,isP(nB=3jN=10)=0:083. Theposterior probabilit y
(2.26) iscorrect forallu,including theend-pointsu=0andu=10,where
fu=0andfu=1respectively.Theposterior probabilit ythatu=0given
nB=3isequal tozero,because ifFredweredrawingfromurn0itwouldbe
impossible foranyblackballstobedrawn.Theposterior probabilit ythat
u=10isalsozero,because there arenowhite ballsinthaturn.Theother
hypothesesu=1;u=2,:::u=9allhavenon-zero posterior probabilit y.2
Terminolo gyofinverse probability
Ininverseprobabilit yproblems itisconvenienttogivenames totheproba-
bilities appearing inBayes'theorem. Inequation (2.25), wecallthemarginal
probabilit yP(u)thepriorprobabilit yofu,andP(nBju;N)iscalled thelike-
lihoodofu.Itisimportanttonotethattheterms likelihoodandprobabilit y
arenotsynon yms. ThequantityP(nBju;N)isafunction ofbothnBand
u.Forxedu,P(nBju;N)denes aprobabilit yovernB.ForxednB,
P(nBju;N)denes thelikelihoodofu.

<<<PAGE 41>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
2.3:Forwardprobabilities andinverseprobabilities 29
Neversay`thelikelihoodofthedata'. Alwayssay`thelikelihood
oftheparameters'. Thelikelihoodfunction isnotaprobabilit y
distribution.
(Ifyouwanttomentionthedatathatalikelihoodfunction isassociated with,
youmaysay`thelikelihoodoftheparameters giventhedata'.)
Theconditional probabilit yP(ujnB;N)iscalled theposterior probabilit y
ofugivennB.Thenormalizing constan tP(nBjN)hasnou-dependence soits
valueisnotimportantifwesimply wishtoevaluate therelativ eprobablities
ofthealternativ ehypothesesu.However,inmost datamodelling problems
ofanycomplexit y,thisquantitybecomes important,anditisgivenvarious
names:P(nBjN)isknownastheevidence orthemarginal likelihood.
Ifdenotes theunkno wnparameters, Ddenotes thedata, andHdenotes
theoverallhypothesis space, thegeneral equation:
P(jD;H)=P(Dj;H)P(jH)
P(DjH)(2.27)
iswritten:
posterior =likelihoodprior
evidence: (2.28)
Inverse probability andprediction
Example 2.6(continued). Assuming again thatBillhasobserv ednB=3blacks
inN=10draws,letFreddrawanother ballfromthesame urn.What
istheprobabilit ythatthenextdrawnballisablack?[Youshould make
useoftheposterior probabilities ingure 2.6.]
Solution. Bythesumrule,
P(ballN+1isblackjnB;N)=X
uP(ballN+1isblackju;nB;N)P(ujnB;N):
(2.29)
Since theballsaredrawnwithreplacemen tfromthechosen urn,theproba-
bilityP(ballN+1isblackju;nB;N)isjustfu=u=10,whatev ernBandN
are.So
P(ballN+1isblackjnB;N)=X
ufuP(ujnB;N): (2.30)
Using thevalues ofP(ujnB;N)giveningure 2.6weobtain
P(ballN+1isblackjnB=3;N=10)=0:333:2(2.31)
Comment. Notice thedierence betweenthisprediction obtained using prob-
abilitytheory ,andthewidespread practice instatistics ofmaking predictions
byrstselecting themostplausible hypothesis (whichherewouldbethatthe
urnisurnu=3)andthenmaking thepredictions assuming thathypothesis
tobetrue(whichwouldgiveaprobabilit yof0.3thatthenextballisblack).
Thecorrect prediction istheonethattakesintoaccoun ttheuncertain tyby
marginalizing overthepossible values ofthehypothesisu.Marginalization
hereleads toslightlymore moderate, lessextreme predictions.

<<<PAGE 42>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
30 2|Probabilit y,Entropy,andInference
Inferenceasinverse probability
Nowconsider thefollowingexercise, whichhasthecharacter ofasimple sci-
enticinvestigation.
Example 2.7.Billtosses abentcoinNtimes, obtaining asequence ofheads
andtails. Weassume thatthecoinhasaprobabilit yfHofcoming up
heads; wedonotknowfH.IfnHheads haveoccurred inNtosses, what
istheprobabilit ydistribution offH?(Forexample,Nmightbe10,and
nHmightbe3;or,afteralotmoretossing, wemighthaveN=300and
nH=29.)What istheprobabilit ythattheN+1thoutcome willbea
head, givennHheads inNtosses?
Unlikeexample 2.6(p.27),thisproblem hasasubjectiveelemen t.Givena
restricted denition ofprobabilit ythatsays`probabilities arethefrequencies
ofrandom variables', thisexample isdieren tfromtheeleven-urns example.
Whereas theurnuwasarandom variable, thebiasfHofthecoinwouldnot
normally becalled arandom variable. Itisjustaxedbutunkno wnparameter
thatweareinterested in.Yetdon't thetwoexamples 2.6and2.7seemtohave
anessentialsimilarit y?[Especially whenN=10andnH=3!]
Tosolveexample 2.7,wehavetomakeanassumption aboutwhatthebias
ofthecoinfHmightbe.Thispriorprobabilit ydistribution overfH,P(fH), HereP(f)denotes aprobabilit yden-
sity,rather thanaprobabilit ydistri-
bution.corresp ondstotheprioroveruintheeleven-urns problem. Inthatexample,
thehelpful problem denition speciedP(u).Inreallife,wehavetomake
assumptions inorder toassign priors; these assumptions willbesubjective,
andouranswerswilldependonthem. Exactly thesame canbesaidforthe
other probabilities inourgenerativ emodeltoo.Weareassuming, forexample,
thattheballsaredrawnfromanurnindependen tly;butcould there notbe
correlations inthesequence because Fred'sball-dra wingaction isnotperfectly
random? Indeed therecould be,sothelikelihoodfunction thatweusedepends
onassumptions too.Inrealdatamodelling problems, priors aresubjectiveand
soarelikeliho ods.
Wearenowusing P()todenote probabilit ydensities overcontinuousvariables aswell
asprobabilities overdiscrete variables andprobabilities oflogical propositions. The
probabilit ythatacontinuousvariable vliesbetweenvalues aandb(where b>a)is
dened tobeRb
advP(v).P(v)dvisdimensionless. Thedensit yP(v)isadimensional
quantity,havingdimensions inversetothedimensions ofv{incontrasttodiscrete
probabilities, whicharedimensionless. Don't besurprised toseeprobabilit ydensities
greater than1.Thisisnormal, andnothing iswrong, aslongasRb
advP(v)<1for
anyinterval(a;b).
Conditional andjointprobabilit ydensities aredened injustthesame wayascondi-
tional andjointprobabilities.
.Exercise 2.8.[2]Assuming auniform prioronfH,P(fH)=1,solvetheproblem
posedinexample 2.7(p.30).Sketchtheposterior distribution offHand
compute theprobabilit ythattheN+1thoutcome willbeahead, for
(a)N=3andnH=0;
(b)N=3andnH=2;
(c)N=10andnH=3;
(d)N=300andnH=29.
Youwillndthebetaintegral useful:
Z1
0dpapFaa(1 pa)Fb= (Fa+1) (Fb+1)
 (Fa+Fb+2)=Fa!Fb!
(Fa+Fb+1)!:(2.32)

<<<PAGE 43>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
2.3:Forwardprobabilities andinverseprobabilities 31
Youmayalsonditinstructiv etolookbackatexample 2.6(p.27)and
equation (2.31).
Peoplesometimes confuse assigning apriordistribution toanunkno wnpa-
rameter suchasfHwithmaking aninitial guess ofthevalue oftheparameter.
ButtheprioroverfH,P(fH),isnotasimple statemen tlike`initially ,Iwould
guessfH=1/2'.Thepriorisaprobabilit ydensit yoverfHwhichspecies the
priordegree ofbeliefthatfHliesinanyinterval(f;f+f).Itmaywellbe
thecasethatourpriorforfHissymmetric about1/2,sothatthemeanoffH
under theprioris1/2.Inthiscase,thepredictiv edistribution forthersttoss
x1wouldindeed be
P(x1=head) =Z
dfHP(fH)P(x1=headjfH)=Z
dfHP(fH)fH=1/2:
(2.33)
Buttheprediction forsubsequen ttosses willdependonthewhole priordis-
tribution, notjustitsmean.
Datacompression andinverse probability
Consider thefollowingtask.
Example 2.9.Writeacomputer program capable ofcompressing binary les
likethisone:
00000000000000000000100100010 00000100000010000000000000000000000000000000000001010000000000000110000
10000000000100001000000000100 00000000000000000000100000000000000000100000000011000001000000011000100
00000000010010000000000100010 00000000000000011000000000000000000000000000010000000000000000100000000
Thestring showncontainsn1=291sandn0=2710s.
Intuitively,compression worksbytaking advantageofthepredictabilit yofa
le.Inthiscase, thesource oftheleappearsmore likelytoemit0sthan
1s.Adatacompression program thatcompresses thislemust,implicitly or
explicitly ,beaddressing thequestion `What istheprobabilit ythatthenext
character inthisleisa1?'
Doyouthink thisproblem issimilar incharacter toexample 2.7(p.30)?
Ido.Oneofthethemes ofthisbookisthatdatacompression anddata
modelling areoneandthesame, andthattheyshould bothbeaddressed, like
theurnofexample 2.6,using inverseprobabilit y.Example 2.9issolvedin
Chapter 6.
Thelikeliho odprinciple
Please solvethefollowingtwoexercises.
Example 2.10. UrnAcontains three balls: oneblack,andtwowhite; urnBA B
Figure 2.7.Urnsforexample 2.10.contains three balls: twoblack,andonewhite. Oneoftheurnsis
selected atrandom andoneballisdrawn.Theballisblack.What is
theprobabilit ythattheselected urnisurnA?
Example 2.11. UrnAcontainsveballs: oneblack,twowhite, onegreen and
......
...
gpc
ysg
p
Figure 2.8.Urnsforexample 2.11.onepink; urnBcontains vehundred balls: twohundred black,one
hundred white, 50yellow,40cyan,30sienna, 25green, 25silver,20
gold, and10purple. [OnefthofA'sballsareblack;two-fths ofB's
areblack.]Oneoftheurnsisselected atrandom andoneballisdrawn.
Theballisblack.What istheprobabilit ythattheurnisurnA?

<<<PAGE 44>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
32 2|Probabilit y,Entropy,andInference
What doyounotice aboutyoursolutions? Doeseachanswerdependonthe
detailed contentsofeachurn?
Thedetails oftheother possible outcomes andtheirprobabilities areir-
relevant.Allthatmatters istheprobabilit yoftheoutcome thatactually
happened(here, thattheballdrawnwasblack)giventhedieren thypothe-
ses.Weneedonlytoknowthelikeliho od,i.e.,howtheprobabilit yofthedata
thathappenedvarieswiththehypothesis. Thissimple ruleaboutinference is
knownasthelikelihoodprinciple .
Thelikelihoodprinciple: givenagenerativ emodelfordatadgiven
parameters ,P(dj),andhavingobserv edaparticular outcome
d1,allinferences andpredictions should dependonlyonthefunction
P(d1j).
Inspiteofthesimplicit yofthisprinciple, manyclassical statistical metho ds
violate it.
2.4Denition ofentropyandrelated functions
TheShannon information contentofanoutcomexisdened tobe
h(x)=log21
P(x): (2.34)
Itismeasured inbits.[Theword`bit'isalsousedtodenote avariable
whose valueis0or1;Ihopecontextwillalwaysmakeclearwhichofthe
twomeanings isintended.]
Inthenextfewchapters, wewillestablish thattheShannon information
contenth(ai)isindeed anatural measure oftheinformation content
oftheeventx=ai.Atthatpoint,wewillshorten thename ofthis
quantityto`theinformation content'.iaipih(pi)
1a.0575 4.1
2b.0128 6.3
3c.0263 5.2
4d.0285 5.1
5e.0913 3.5
6f.0173 5.9
7g.0133 6.2
8h.0313 5.0
9i.0599 4.1
10j.0006 10.7
11k.0084 6.9
12l.0335 4.9
13m.0235 5.4
14n.0596 4.1
15o.0689 3.9
16p.0192 5.7
17q.0008 10.3
18r.0508 4.3
19s.0567 4.1
20t.0706 3.8
21u.0334 4.9
22v.0069 7.2
23w.0119 6.4
24x.0073 7.1
25y.0164 5.9
26z.0007 10.4
27-.1928 2.4
X
ipilog21
pi4.1
Table2.9.Shannon information
contentsoftheoutcomes a{z.
Thefourth column intable 2.9showstheShannon information content
ofthe27possible outcomes when arandom character ispickedfrom
anEnglish documen t.Theoutcomex=zhasaShannon information
contentof10.4bits,andx=ehasaninformation contentof3.5bits.
Theentropyofanensem bleXisdened tobetheaverage Shannon in-
formation contentofanoutcome:
H(X)X
x2AXP(x)log1
P(x); (2.35)
with theconvention forP(x)=0that 0log1=00,since
lim!0+log1==0.
Liketheinformation content,entropyismeasured inbits.
When itisconvenient,wemayalsowriteH(X)asH(p),where pis
thevector (p1;p2;:::;pI).Another name fortheentropyofXisthe
uncertain tyofX.
Example 2.12. Theentropyofarandomly selected letter inanEnglish docu-
mentisabout4.11bits,assuming itsprobabilit yisasgivenintable2.9.
Weobtain thisnumberbyaveraging log1=pi(showninthefourth col-
umn) under theprobabilit ydistribution pi(showninthethirdcolumn).

<<<PAGE 45>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
2.5:Decomp osabilit yoftheentropy 33
Wenownotesome properties oftheentropyfunction.
H(X)0withequalit yipi=1foronei.[`i'means `ifandonlyif'.]
Entropyismaximized ifpisuniform:
H(X)log(jAXj)withequalit yipi=1=jXjforalli. (2.36)
Notation: thevertical bars`jj'havetwomeanings. IfAXisaset,jAXj
denotes thenumberofelemen tsinAX;ifxisanumber,thenjxjisthe
absolute valueofx.
Theredundancy measures thefractional dierence betweenH(X)anditsmax-
imumpossible value,log(jAXj).
Theredundancy ofXis:
1 H(X)
logjAXj: (2.37)
Wewon'tmakeuseof`redundancy' inthisbook,soIhavenotassigned
asymboltoit.
ThejointentropyofX;Yis:
H(X;Y)=X
xy2AXAYP(x;y)log1
P(x;y): (2.38)
Entropyisadditiv eforindependen trandom variables:
H(X;Y)=H(X)+H(Y)iP(x;y)=P(x)P(y): (2.39)
Ourdenitions forinformation contentsofarapply onlytodiscrete probabilit y
distributions overnite setsAX.Thedenitions canbeextended toinnite
sets,though theentropymaythenbeinnite. Thecaseofaprobabilit y
density overacontinuoussetisaddressed insection 11.3. Further important
denitions andexercises todowithentropywillcome along insection 8.1.
2.5Decomp osabilit yoftheentropy
Theentropyfunction satises arecursiv epropertythatcanbeveryuseful
when computing entropies. Forconvenience, we'llstretchournotation sothat
wecanwriteH(X)asH(p),wherepistheprobabilit yvectorassociated with
theensem bleX.
Let's illustrate thepropertybyanexample rst. Imagine thatarandom
variablex2f0;1;2giscreated byrstipping afaircointodetermine whether
x=0;then,ifxisnot0,ipping afaircoinasecond timetodetermine whether
xis1or2.Theprobabilit ydistribution ofxis
P(x=0)=1
2;P(x=1)=1
4;P(x=2)=1
4: (2.40)
What istheentropyofX?Wecaneither compute itbybrute force:
H(X)=1/2log2+1/4log4+1/4log4=1:5; (2.41)
orwecanusethefollowingdecomp osition, inwhichthevalueofxisrevealed
gradually .Imagine rstlearning whetherx=0,andthen,ifxisnot0,learning
whichnon-zero valueisthecase. Therevelation ofwhetherx=0ornotentails

<<<PAGE 46>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
34 2|Probabilit y,Entropy,andInference
revealing abinary variable whose probabilit ydistribution isf1/2;1/2g.This
revelation hasanentropyH(1/2;1/2)=1
2log2+1
2log2=1bit.Ifxisnot0,
welearnthevalueofthesecond coinip.Thistooisabinary variable whose
probabilit ydistribution isf1/2;1/2g,andwhose entropyis1bit.Weonlyget
toexperience thesecond revelation halfthetime, however,sotheentropycan
bewritten:
H(X)=H(1/2;1/2)+1/2H(1/2;1/2): (2.42)
Generalizing, theobserv ation wearemaking abouttheentropyofany
probabilit ydistribution p=fp1;p2;:::;pIgisthat
H(p)=H(p1;1 p1)+(1 p1)Hp2
1 p1;p3
1 p1;:::;pI
1 p1
: (2.43)
When it'swritten asaformula,thispropertylooksregretably ugly; nev-
ertheless itisasimple propertyandonethatyoushould makeuseof.
Generalizing further, theentropyhasthepropertyforanymthat
H(p)=H[(p1+p2++pm);(pm+1+pm+2++pI)]
+(p1++pm)Hp1
(p1++pm);:::;pm
(p1++pm)
+(pm+1++pI)Hpm+1
(pm+1++pI);:::;pI
(pm+1++pI)
:
(2.44)
Example 2.13. Asource produces acharacterxfrom thealphab etA=
f0;1;:::;9;a;b;:::;zg;withprobabilit y1/3,xisanumeral (0;:::;9);
withprobabilit y1/3,xisavowel(a;e;i;o;u);andwithprobabilit y1/3
it'soneofthe21consonan ts.Allnumerals areequiprobable, andthe
same goesforvowelsandconsonan ts.Estimate theentropyofX.
Solution. log3+1
3(log10+log5+log21)=log3+1
3log1050'log30bits.2
2.6Gibbs' inequalit y
The`ei'inLeiblerispronounced the
same asinheist.Therelativ eentropyorKullbac k{Leibler divergence between two
probabilit ydistributions P(x)andQ(x)thataredened overthesame
alphab etAXis
DKL(PjjQ)=X
xP(x)logP(x)
Q(x): (2.45)
Therelativ eentropysatises Gibbs' inequalit y
DKL(PjjQ)0 (2.46)
withequalit yonlyifP=Q.Notethatingeneral therelativ eentropy
isnotsymmetric under interchange ofthedistributions PandQ:in
generalDKL(PjjQ)6=DKL(QjjP),soDKL,although itissometimes
called the`KLdistance', isnotstrictly adistance. Therelativ eentropy
isimportantinpattern recognition andneural networks, aswellasin
information theory .
Gibbs' inequalit yisprobably themostimportantinequalit yinthisbook.It,
andmanyother inequalities, canbeprovedusing theconcept ofconvexity.

<<<PAGE 47>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
2.7:Jensen's inequalit yforconvexfunctions 35
2.7Jensen's inequalit yforconvexfunctions
Thewords`convex^'and`conca ve_'maybepronounced `convex-smile' and
`conca ve-frown'.Thisterminology hasuseful redundancy: while onemayforget which
wayup`convex'and`conca ve'are,itisharder toconfuse asmile withafrown.
Convex^functions .Afunctionf(x)isconvex^over(a;b)ifeverychord
x1 x2
x=x1+(1 )x2f(x)f(x1)+(1 )f(x2)
Figure 2.10.Denition of
convexity.ofthefunction liesabovethefunction, asshowningure 2.10;thatis,
forallx1;x22(a;b)and01,
f(x1+(1 )x2)f(x1)+(1 )f(x2): (2.47)
Afunctionfisstrictly convex^if,forallx1;x22(a;b),theequalit y
holds onlyfor=0and=1.
Similar denitions apply toconcave_andstrictly concave_functions.
Some strictly convex^functions are
x2,exande xforallx;
log(1=x)andxlogxforx>0.
x2
-10123e x
-1 0 1 2 3log1
x
0 1 2 3xlogx
0 1 2 3Figure 2.11.Convex^functions.
Jensen's inequalit y.Iffisaconvex^function andxisarandom variable
then:
E[f(x)]f(E[x]); (2.48)
whereEdenotes expectation. Iffisstrictly convex^andE[f(x)]=
f(E[x]),thentherandom variablexisaconstan t.
Jensen's inequalit ycanalsoberewritten foraconcave_function, with
thedirection oftheinequalit yreversed.
Aphysical version ofJensen's inequalit yrunsasfollows.Centre of gravity
Ifacollection ofmassespiareplaced onaconvex^curvef(x)
atlocations (xi;f(xi)),thenthecentreofgravityofthose masses,
whichisat(E[x];E[f(x)]),liesabovethecurve.
Ifthisfailstoconvince you,thenfeelfreetodothefollowingexercise.
Exercise 2.14.[2,p.41]ProveJensen's inequalit y.
Example 2.15. Three squares haveaverage areaA=100m2.Theaverage of
thelengths oftheirsidesisl=10m.What canbesaidaboutthesize
ofthelargest ofthethree squares? [UseJensen's inequalit y.]
Solution. Letxbethelength ofthesideofasquare, andlettheprobabilit y
ofxbe1/3;1/3;1/3overthethree lengthsl1;l2;l3.Then theinformation that
wehaveisthatE[x]=10andE[f(x)]=100,wheref(x)=x2isthefunction
mapping lengths toareas. Thisisastrictly convex^function. Wenotice
thattheequalit yE[f(x)]=f(E[x])holds, thereforexisaconstan t,andthe
three lengths mustallbeequal. Theareaofthelargest square is100m2.2

<<<PAGE 48>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
36 2|Probabilit y,Entropy,andInference
Convexity andconcavityalsorelatetomaximization
Iff(x)isconcave_andthere exists apointatwhich
@f
@xk=0forallk; (2.49)
thenf(x)hasitsmaxim umvalueatthatpoint.
Theconversedoesnothold: ifaconcave_f(x)ismaximized atsomex
itisnotnecessarily truethatthegradien trf(x)isequal tozerothere. For
example,f(x)= jxjismaximized atx=0where itsderivativeisundened;
andf(p)=log(p);foraprobabilit yp2(0;1),ismaximized ontheboundary
oftherange, atp=1,where thegradien tdf(p)=dp=1.
2.8Exercises
Sumsofrandom variables
Exercise 2.16.[3,p.41](a)Twoordinary dicewithfaceslabelled1;:::;6are
thrown.What istheprobabilit ydistribution ofthesumoftheval-
ues?What istheprobabilit ydistribution oftheabsolute dierence
betweenthevalues?
(b)Onehundred ordinary dicearethrown.What, roughly ,istheprob-
abilitydistribution ofthesumofthevalues? Sketchtheprobabilit y
distribution andestimate itsmean andstandard deviation.
(c)Howcantwocubical dice belabelled using thenumbers
f0;1;2;3;4;5;6gsothatwhen thetwodicearethrownthesum
hasauniform probabilit ydistribution overtheintegers 1{12?
(d)Isthere anywaythatonehundred dicecould belabelledwithinte-
gerssuchthattheprobabilit ydistribution ofthesumisuniform?
Inferenceproblems
Exercise 2.17.[2,p.41]Ifq=1 panda=lnp=q,showthat
p=1
1+exp( a): (2.50)
Sketchthisfunction andnditsrelationship tothehyperbolictangen t
function tanh(u)=eu e u
eu+e u.
Itwillbeuseful tobeuentinbase-2 logarithms also. Ifb=log2p=q,
whatisbasafunction ofp?
.Exercise 2.18.[2,p.42]Letxandybecorrelated random variables withxa
binary variable taking values inAX=f0;1g.UseBayes'theorem to
showthatthelogposterior probabilit yratioforxgivenyis
logP(x=1jy)
P(x=0jy)=logP(yjx=1)
P(yjx=0)+logP(x=1)
P(x=0): (2.51)
.Exercise 2.19.[2,p.42]Letx,d1andd2berandom variables suchthatd1and
d2areconditionally independen tgivenabinary variablex.UseBayes'
theorem toshowthattheposterior probabilit yratioforxgivenfdigis
P(x=1jfdig)
P(x=0jfdig)=P(d1jx=1)
P(d1jx=0)P(d2jx=1)
P(d2jx=0)P(x=1)
P(x=0): (2.52)

<<<PAGE 49>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
2.8:Exercises 37
Lifeinhigh-dimensional spaces
Probabilit ydistributions andvolumes havesome unexp ected properties in
high-dimensional spaces.
Exercise 2.20.[2,p.42]Consider asphere ofradiusrinanN-dimensional real
space. Showthatthefraction ofthevolume ofthesphere thatisinthe
surface shelllying atvalues oftheradius betweenr andr,where
0<<r,is:
f=1 
1 
rN
: (2.53)
EvaluatefforthecasesN=2,N=10andN=1000, with(a)=r=0:01;
(b)=r=0:5.
Implication: pointsthatareuniformly distributed inasphere inNdi-
mensions, whereNislarge, areverylikelytobeinathinshellnearthe
surface.
Expectations andentropies
Youareprobably familiar withtheideaofcomputing theexpectation ofa
function ofx,
E[f(x)]=hf(x)i=X
xP(x)f(x): (2.54)
Maybeyouarenotsocomfortable withcomputing thisexpectation incases
where thefunctionf(x)dependsontheprobabilit yP(x).Thenextfewex-
amples address thisconcern.
Exercise 2.21.[1,p.43]Letpa=0:1,pb=0:2,andpc=0:7.Letf(a)=10,
f(b)=5,andf(c)=10=7.What isE[f(x)]?What isE[1=P(x)]?
Exercise 2.22.[2,p.43]Foranarbitrary ensem ble,what isE[1=P(x)]?
.Exercise 2.23.[1,p.43]Letpa=0:1,pb=0:2,andpc=0:7.Letg(a)=0,g(b)=1,
andg(c)=0.What isE[g(x)]?
.Exercise 2.24.[1,p.43]Letpa=0:1,pb=0:2,andpc=0:7.What istheproba-
bilitythatP(x)2[0:15;0:5]?What is
PlogP(x)
0:2>0:05
?
Exercise 2.25.[3,p.43]Provetheassertion thatH(X)log(jXj)withequalit y
ipi=1=jXjforalli.(jXjdenotes thenumberofelemen tsintheset
AX.)[Hint:useJensen's inequalit y(2.48); ifyourrstattempt touse
Jensen doesnotsucceed, remem berthatJensen involvesbotharandom
variable andafunction, andyouhavequite alotoffreedom inchoosing
these; think aboutwhether yourchosen functionfshould beconvexor
concave.]
.Exercise 2.26.[3,p.44]Provethattherelativ eentropy(equation (2.45)) satises
DKL(PjjQ)0(Gibbs' inequalit y)withequalit yonlyifP=Q.
.Exercise 2.27.[2]Provethattheentropyisindeed decomp osable asdescrib ed
inequations (2.43{2.44).

<<<PAGE 50>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
38 2|Probabilit y,Entropy,andInference
.Exercise 2.28.[2]Arandom variablex2f0;1;2;3gisselected byipping a
bentcoinwithbiasftodetermine whether theoutcome isinf0;1gor
f2;3g;theneither ipping asecond bentcoinwithbiasgorathirdbent
f
1 f   
@
@
@R  
@@Rg
1 g
h
1 h  
@@R0
1
2
3coinwithbiashrespectively.Writedowntheprobabilit ydistribution
ofx.Usethedecomp osabilit yoftheentropy(2.44) tondtheentropy
ofX.[Notice howcompact anexpression results ifyoumakeuseofthe
binary entropyfunctionH2(x),compared withwriting outthefour-term
entropyexplicitly .]FindthederivativeofH(X)withrespecttof.[Hint:
dH2(x)=dx=log((1 x)=x).]
.Exercise 2.29.[2]Anunbiased coinisippeduntiloneheadisthrown.What is
theentropyoftherandom variablex2f1;2;3;:::g,thenumberofips?
Repeatthecalculation forthecaseofabiased coinwithprobabilit yfof
coming upheads. [Hint:solvetheproblem bothdirectly andbyusing
thedecomp osabilit yoftheentropy(2.43).]
2.9Further exercises
Forwardprobability
.Exercise 2.30.[1]Anurncontainswwhite ballsandbblackballs. Twoballs
aredrawn,oneaftertheother, without replacemen t.Provethatthe
probabilit ythattherstballiswhite isequal totheprobabilit ythatthe
second iswhite.
.Exercise 2.31.[2]Acircular coinofdiameteraisthrownontoasquare grid
whose squares arebb.(a<b)What istheprobabilit ythatthecoin
willlieentirely within onesquare? [Ans: (1 a=b)2]
.Exercise 2.32.[3]Buon's needle .Aneedle oflengthaisthrownontoaplane
coveredwithequally spaced parallel lineswithseparation b.What is
theprobabilit ythattheneedle willcross aline? [Ans, ifa<b:2a/b]
[Generalization {Buon's noodle:onaverage, arandom curveoflength
Aisexpected tointersect thelines2A/btimes.]
Exercise 2.33.[2]Twopointsareselected atrandom onastraigh tlinesegmen t
oflength 1.What istheprobabilit ythatatriangle canbeconstructed
outofthethree resulting segmen ts?
Exercise 2.34.[2,p.45]Anunbiased coinisippeduntilonehead isthrown.
What istheexpected numberoftailsandtheexpected numberofheads?
Fred,whodoesn'tknowthatthecoinisunbiased, estimates thebias
using ^fh=(h+t),wherehandtarethenumbersofheads andtails
tossed. Compute andsketchtheprobabilit ydistribution of^f.
NB,thisisaforwardprobabilit yproblem, asampling theory problem,
notaninference problem. Don't useBayes'theorem.
Exercise 2.35.[2,p.45]Fredrollsanunbiased six-sided dieoncepersecond, not-
ingtheoccasions when theoutcome isasix.
(a)What isthemean numberofrollsfromonesixtothenextsix?
(b)Betweentworolls,theclockstrikesone.What isthemean number
ofrollsuntilthenextsix?

<<<PAGE 51>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
2.9:Further exercises 39
(c)Nowthink backbeforetheclockstruck.What isthemean number
ofrolls,going backintime, untilthemostrecentsix?
(d)What isthemean numberofrollsfromthesixbefore theclock
strucktothenextsix?
(e)Isyouranswerto(d)dieren tfromyouranswerto(a)?Explain.
Another version ofthisexercise refers toFredwaiting forabusata
bus-stop inPoisson villewhere buses arriveindependen tlyatrandom (a
Poisson process), with, onaverage, onebuseverysixminutes. What is
theaverage waitforabus,afterFredarrivesatthestop? [6minutes.] So
what isthetimebetweenthetwobuses, theonethatFredjustmissed,
andtheonethathecatches?[12minutes.] Explain theapparen tpara-
dox.Notethecontrastwiththesituation inClockville, where thebuses
arespaced exactly 6minutesapart. There, asyoucanconrm, themean
waitatabus-stop is3minutes,andthetimebetweenthemissed bus
andthenextoneis6minutes.
Conditional probability
.Exercise 2.36.[2]YoumeetFred.Fredtellsyouhehastwobrothers, Alfand
Bob.
What istheprobabilit ythatFredisolderthanBob?
Fredtellsyouthatheisolder thanAlf.Now,what istheprobabilit y
thatFredisolderthanBob? (That is,whatistheconditional probabilit y
thatF>BgiventhatF>A?)
.Exercise 2.37.[2]Theinhabitan tsofanisland tellthetruth onethird ofthe
time. They liewithprobabilit y2/3.
Onanoccasion, afteroneofthem made astatemen t,youaskanother
`wasthatstatemen ttrue?' andhesays`yes'.
What istheprobabilit ythatthestatemen twasindeed true?
.Exercise 2.38.[2,p.46]Compare twowaysofcomputing theprobabilit yoferror
oftherepetition codeR3,assuming abinary symmetric channel (you
didthisonceforexercise 1.2(p.7))andconrm thattheygivethesame
answer.
Binomial distribution metho d.Addtheprobabilit yofallthreebits'
beingippedtotheprobabilit yofexactly twobits'beingipped.
Sumrulemetho d.Using thesumrule,compute themarginal prob-
abilitythatrtakesoneachoftheeightpossible values,P(r).
[P(r)=P
sP(s)P(rjs).]Then compute theposterior probabil-
ityofsforeachoftheeightvalues ofr.[Infact,bysymmetry ,
onlytwoexample casesr=(000)andr=(001)needbeconsid-
ered.] Notice thatsome oftheinferred bitsarebetterdetermined Equation (1.18) givestheposterior
probabilit yoftheinput s,giventhe
receiv edvectorr.thanothers. Fromtheposterior probabilit yP(sjr)youcanread
outthecase-b y-case errorprobabilit y,theprobabilit ythatthemore
probable hypothesis isnotcorrect,P(errorjr).Findtheaverage
errorprobabilit yusing thesumrule,
P(error) =X
rP(r)P(errorjr): (2.55)

<<<PAGE 52>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
40 2|Probabilit y,Entropy,andInference
.Exercise 2.39.[3C]Thefrequencypnofthenthmostfrequen twordinEnglish
isroughly approximated by
pn'(0:1
nforn21:::12367
0n>12367:(2.56)
[Thisremark able1=nlawisknownasZipf'slaw,andapplies totheword
frequencies ofmanylanguages (Zipf, 1949).] Ifweassume thatEnglish
isgenerated bypickingwordsatrandom according tothisdistribution,
whatistheentropyofEnglish (perword)? [Thiscalculation canbefound
in`Prediction andentropyofprintedEnglish', C.E.Shannon, BellSyst.
Tech.J.30,pp.50{64 (1950), but,inexplicably ,thegreat manmade
numerical errors init.]
2.10 Solutions
Solution toexercise 2.2(p.24).No,theyarenotindependen t.Iftheywere
thenalltheconditional distributions P(yjx)wouldbeidenticalfunctions of
y,regardless ofx(c.f.gure 2.3).
Solution toexercise 2.4(p.27).Wedene thefractionfBB=K.
(a)Thenumberofblackballshasabinomial distribution.
P(nBjfB;N)= 
N
nB!
fnB
B(1 fB)N nB: (2.57)
(b)Themean andvariance ofthisdistribution are:
E[nB]=NfB (2.58)
var[nB]=NfB(1 fB): (2.59)
These results werederivedinexample 1.1(p.1).Thestandard deviation
ofnBisp
var[nB]=p
NfB(1 fB).
WhenB=K=1=5andN=5,theexpectation andvariance ofnBare1
and4/5.Thestandard deviation is0.89.
WhenB=K=1=5andN=400,theexpectation andvariance ofnBare
80and64.Thestandard deviation is8.
Solution toexercise 2.5(p.27).Thenumerator ofthequantity
z=(nB fBN)2
NfB(1 fB)
canberecognized as(nB E[nB])2;thedenominator isequal tothevariance
ofnB(2.59), whichisbydenition theexpectation ofthenumerator. Sothe
expectation ofzis1.[Arandom variable likez,whichmeasures thedeviation
ofdatafromtheexpected value,issometimes called2(chi-squared).]
InthecaseN=5andfB=1=5,NfBis1,andvar[nB]is4/5. The
numerator hasvepossible values, onlyoneofwhichissmaller than1:(nB 
fBN)2=0hasprobabilit yP(nB=1)=0:4096; sotheprobabilit ythatz<1
is0.4096.

<<<PAGE 53>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
2.10: Solutions 41
Solution toexercise 2.14(p.35).Wewishtoprove,giventheproperty
f(x1+(1 )x2)f(x1)+(1 )f(x2); (2.60)
that,ifPpi=1andpi0,
IX
i=1pif(xi)f IX
i=1pixi!
: (2.61)
Weproceedbyrecursion, working fromtheright-hand side.(This proofdoes
nothandle cases where somepi=0;suchdetails arelefttothepedantic
reader.) Attherstlineweusethedenition ofconvexity(2.60) with=
p1PI
i=1pi=p1;atthesecond line,=p2PI
i=2pi.
f IX
i=1pixi!
=f 
p1x1+IX
i=2pixi!
p1f(x1)+"IX
i=2pi#"
f IX
i=2pixi,IX
i=2pi!#
(2.62)
p1f(x1)+"IX
i=2pi#"
p2PI
i=2pif(x2)+PI
i=3piPI
i=2pif IX
i=3pixi,IX
i=3pi!#
;
andsoforth. 2
Solution toexercise 2.16(p.36).
(a)Fortheoutcomesf2;3;4;5;6;7;8;9;10;11;12g,theprobabilities areP=
f1
36;2
36;3
36;4
36;5
36;6
36;5
36;4
36;3
36;2
36;1
36g.
(b)Thevalueofonediehasmean 3:5andvariance 35=12.Sothesumof
onehundred hasmean 350andvariance 3500=12'292,andbythe
centrallimittheorem theprobabilit ydistribution isroughly Gaussian
(butconned totheintegers), withthismean andvariance.
(c)Inorder toobtain asumthathasauniform distribution wehavetostart
fromrandom variables someofwhichhaveaspiky distribution withthe
probabilit ymassconcen trated attheextremes. Theunique solution is
tohaveoneordinary dieandonewithfaces6,6,6,0,0,0.
(d)Yes,auniform distribution canbecreated inseveralways,forexample
bylabelling therthdiewiththenumbersf0;1;2;3;4;5g6r.
Solution toexercise 2.17(p.36).
a=lnp
q)p
q=ea(2.63)
andq=1 pgives
p
1 p=ea(2.64)
)p=ea
ea+1=1
1+exp( a): (2.65)
Thehyperbolictangen tis
tanh(a)=ea e a
ea+e a(2.66)

<<<PAGE 54>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
42 2|Probabilit y,Entropy,andInference
so
f(a)1
1+exp( a)=1
2 
1 e a
1+e a+1!
=1
2 
ea=2 e a=2
ea=2+e a=2+1!
=1
2(tanh(a=2)+1): (2.67)
Inthecaseb=log2p=q,wecanrepeatsteps(2.63{2.63), replacingeby2,
toobtain
p=1
1+2 a: (2.68)
Solution toexercise 2.18(p.36).
P(xjy)=P(yjx)P(x)
P(y)(2.69)
)P(x=1jy)
P(x=0jy)=P(yjx=1)
P(yjx=0)P(x=1)
P(x=0)(2.70)
)logP(x=1jy)
P(x=0jy)=logP(yjx=1)
P(yjx=0)+logP(x=1)
P(x=0):(2.71)
Solution toexercise 2.19(p.36).Theconditional independence ofd1andd2
givenxmeans
P(x;d1;d2)=P(x)P(d1jx)P(d2jx): (2.72)
Thisgivesaseparation oftheposterior probabilit yratiointoaseries offactors,
oneforeachdatapoint,times thepriorprobabilit yratio.
P(x=1jfdig)
P(x=0jfdig)=P(fdigjx=1)
P(fdigjx=0)P(x=1)
P(x=0)(2.73)
=P(d1jx=1)
P(d1jx=0)P(d2jx=1)
P(d2jx=0)P(x=1)
P(x=0): (2.74)
Lifeinhigh-dimensional spaces
Solution toexercise 2.20(p.37).Thevolume ofahypersphere ofradiusrin
Ndimensions isinfact
V(r;N)=N=2
(N=2)!rN; (2.75)
butyoudon't needtoknowthis. Forthisquestion allthatweneedisthe
r-dependence,V(r;N)/rN:Sothefractional volume in(r ;r)is
rN (r )N
rN=1 
1 
rN
: (2.76)
Thefractional volumes intheshells fortherequired cases are:
N 2 10 1000
=r=0:010.02 0.096 0.99996
=r=0:50.75 0.999 1 2 1000
Notice thatnomatter howsmallis,forlarge enoughNessentially allthe
probabilit ymassisinthesurface shellofthickness.

<<<PAGE 55>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
2.10: Solutions 43
Solution toexercise 2.21(p.37).pa=0:1,pb=0:2,pc=0:7.f(a)=10,
f(b)=5,andf(c)=10=7.
E[f(x)]=0:110+0:25+0:710=7=3: (2.77)
Foreachx,f(x)=1=P(x),so
E[1=P(x)]=E[f(x)]=3: (2.78)
Solution toexercise 2.22(p.37).ForgeneralX,
E[1=P(x)]=X
x2AXP(x)1=P(x)=X
x2AX1=jAXj: (2.79)
Solution toexercise 2.23(p.37).pa=0:1,pb=0:2,pc=0:7.g(a)=0,g(b)=1,
andg(c)=0.
E[g(x)]=pb=0:2: (2.80)
Solution toexercise 2.24(p.37).
P(P(x)2[0:15;0:5])=pb=0:2: (2.81)
PlogP(x)
0:2>0:05
=pa+pc=0:8: (2.82)
Solution toexercise 2.25(p.37).Thistypeofquestion canbeapproac hedin
twoways:either bydieren tiating thefunction tobemaximized, nding the
maxim um,andprovingitisaglobal maxim um;thisstrategy issomewhat
riskysinceitispossible forthemaxim umofafunction tobeattheboundary
ofthespace, ataplace where thederivativeisnotzero. Alternativ ely,a
carefully chosen inequalit ycanestablish theanswer.Thesecond metho dis
muchneater.
Proofbydierentiation (nottherecommended metho d). Since itisslightly
easier todieren tiateln1=pthanlog21=p,wetemporarily deneH(X)tobe
measured using natural logarithms, thusscaling itdownbyafactor oflog2e.
H(X)=X
ipiln1
pi(2.83)
@H(X)
@pi=ln1
pi 1 (2.84)
wemaximize subjecttotheconstrain tP
ipi=1whichcanbeenforced with
aLagrange multiplier:
G(p)H(X)+ X
ipi 1!
(2.85)
@G(p)
@pi=ln1
pi 1+: (2.86)
Atamaxim um,
ln1
pi 1+=0 (2.87)
)ln1
pi=1 ; (2.88)
soallthepiareequal. Thatthisextrem umisindeed amaxim umisestablished
bynding thecurvature:
@2G(p)
@pi@pj= 1
piij; (2.89)
whichisnegativ edenite. 2

<<<PAGE 56>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
44 2|Probabilit y,Entropy,andInference
Proofusing Jensen's inequalit y(recommended metho d). First areminder of
theinequalit y.
Iffisaconvex^function andxisarandom variable then:
E[f(x)]f(E[x]):
Iffisstrictly convex^andE[f(x)]=f(E[x]),thentherandom
variablexisaconstan t(with probabilit y1).
Thesecret ofaproofusing Jensen's inequalit yistochoosetherightfunc-
tionandtherightrandom variable. Wecould dene
f(u)=log1
u= logu (2.90)
(whichisaconvexfunction) andthink ofH(X)=Ppilog1
piasthemean of
f(u)whereu=P(x),butthiswouldnotgetusthere {itwouldgiveusan
inequalit yinthewrong direction. Ifinstead wedene
u=1=P(x) (2.91)
thenwend:
H(X)= E[f(1=P(x))] f(E[1=P(x)]); (2.92)
nowweknowfromexercise 2.22(p.37)thatE[1=P(x)]=jAXj,so
H(X) f(jAXj)=logjAXj: (2.93)
Equalit yholds onlyiftherandom variableu=1=P(x)isaconstan t,which
meansP(x)isaconstan tforallx. 2
Solution toexercise 2.26(p.37).
DKL(PjjQ)=X
xP(x)logP(x)
Q(x): (2.94)
WeproveGibbs' inequalit yusing Jensen's inequalit y.Letf(u)=log1=uand
u=Q(x)
P(x).Then
DKL(PjjQ)=E[f(Q(x)=P(x))] (2.95)
f X
xP(x)Q(x)
P(x)!
=log1P
xQ(x)
=0;(2.96)
withequalit yonlyifu=Q(x)
P(x)isaconstan t,thatis,ifQ(x)=P(x). 2
Second solution. Intheaboveprooftheexpectations werewithrespectto
theprobabilit ydistribution P(x).Asecond solution metho dusesJensen's
inequalit ywithQ(x)instead. Wedenef(u)=uloguandletu=P(x)
Q(x).
Then
DKL(PjjQ)=X
xQ(x)P(x)
Q(x)logP(x)
Q(x)=X
xQ(x)fP(x)
Q(x)
(2.97)
f X
xQ(x)P(x)
Q(x)!
=f(1)=0; (2.98)
withequalit yonlyifu=P(x)
Q(x)isaconstan t,thatis,ifQ(x)=P(x). 2

<<<PAGE 57>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
2.10: Solutions 45
Solution toexercise 2.28(p.38).
H(X)=H2(f)+fH2(g)+(1 f)H2(h): (2.99)
Solution toexercise 2.29(p.38).Theprobabilit ythatthere arex 1tailsand
thenonehead(sowegettherstheadonthexthtoss)is
P(x)=(1 f)x 1f: (2.100)
Ifthersttossisatail,theprobabilit ydistribution forthefuture looksjust
likeitdidbeforewemade thersttoss.Thuswehavearecursiv eexpression
fortheentropy:
H(X)=H2(f)+(1 f)H(X): (2.101)
Rearranging,
H(X)=H2(f)=f: (2.102)
Solution toexercise 2.34(p.38).Theprobabilit yofthenumberoftailstis
P(t)=1
2t1
2fort0: (2.103)
Theexpected numberofheads is1,bydenition oftheproblem. Theexpected
numberoftailsis
E[t]=1X
t=0t1
2t1
2; (2.104)
whichmaybeshowntobe1inavarietyofways.Forexample, since the
situation afteronetailisthrownisequivalenttotheopening situation, wecan
write downtherecurrence relation
E[t]=1
2(1+E[t])+1
20)E[t]=1: (2.105)
Theprobabilit ydistribution ofthe`estimator' ^f=1=(1+t),giventhat
f=1=2,isplotted ingure 2.12.Theprobabilit yof^fissimply theprobabilit y
ofthecorresp onding valueoft.P(^f)
00.10.20.30.40.5
00.20.40.60.81
^f
Figure 2.12.Theprobabilit y
distribution oftheestimator
^f=1=(1+t),giventhatf=1=2.
Solution toexercise 2.35(p.38).
(a)Themean numberofrollsfromonesixtothenextsixissix(assuming
westartcountingrollsaftertherstofthetwosixes). Theprobabilit y
thatthenextsixoccurs ontherthrollistheprobabilit yofnotgetting
asixforr 1rollsmultiplied bytheprobabilit yofthengetting asix:
P(r1=r)=5
6r 11
6;forr2f1;2;3;:::g. (2.106)
Thisprobabilit ydistribution ofthenumberofrolls,r,maybecalled an
exponentialdistribution, since
P(r1=r)=e r=Z; (2.107)
where=ln(6=5),andZisanormalizing constan t.
(b)Themean numberofrollsfromtheclockuntilthenextsixissix.
(c)Themean numberofrolls,going backintime, untilthemostrecentsix
issix.

<<<PAGE 58>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
46 2|Probabilit y,Entropy,andInference
(d)Themean numberofrollsfromthesixbeforetheclockstrucktothesix
aftertheclockstruckisthesumoftheanswersto(b)and(c),lessone,
thatis,eleven.
(e)Rather thanexplaining thedierence between(a)and(d),letmegive
another hint.Imagine thatthebuses inPoisson villearriveindepen-
dentlyatrandom (aPoisson process), with, onaverage, onebusevery
sixminutes. Imagine thatpassengers turnupatbus-stops atauniform
rate,andarescoopedupbythebuswithout delay,sotheintervalbe-
tweentwobuses remains constan t.Buses thatfollowgapsbigger than
sixminutesbecome overcrowded. Thepassengers' represen tativecom-
plains thattwo-thirds ofallpassengers found themselv esonovercrowded
buses. Thebusoperator claims, `no,no{onlyonethird ofourbuses
areovercrowded'. Canboththese claims betrue?00.050.10.15
05101520
Figure 2.13.Theprobabilit y
distribution ofthenumberofrolls
r1fromone6tothenext(falling
solidline),
P(r1=r)=5
6r 11
6;
andtheprobabilit ydistribution
(dashed line)ofthenumberof
rollsfromthe6before1pmtothe
next6,rtot,
P(rtot=r)=r5
6r 11
62
:
Theprobabilit yP(r1>6)is
about1/3;theprobabilit y
P(rtot>6)isabout2/3.The
mean ofr1is6,andthemean of
rtotis11.Solution toexercise 2.38(p.39).
Binomial distribution metho d.Fromthesolution toexercise 1.2,pB=
3f2(1 f)+f3.
Sumrulemetho d.Themarginal probabilities oftheeightvalues ofrare
illustrated by:
P(r=000)=1/2(1 f)3+1/2f3; (2.108)
P(r=001)=1/2f(1 f)2+1/2f2(1 f)=1/2f(1 f):(2.109)
Theposterior probabilities arerepresen tedby
P(s=1jr=000)=f3
(1 f)3+f3(2.110)
and
P(s=1jr=001)=(1 f)f2
f(1 f)2+f2(1 f)=f: (2.111)
Theprobabilities oferrorinthese represen tativecases arethus
P(errorjr=000)=f3
(1 f)3+f3(2.112)
and
P(errorjr=001)=f: (2.113)
Notice thatwhile theaverage probabilit yoferrorofR3isabout3f2,the
probabilit y(givenr)thatanyparticular bitiswrong iseither aboutf3
orf.
Theaverage errorprobabilit y,using thesumrule,is
P(error) =X
rP(r)P(errorjr)
=2[1/2(1 f)3+1/2f3]f3
(1 f)3+f3+6[1/2f(1 f)]f:
SoThersttwoterms areforthecases
r=000and111;theremaining 6are
fortheother outcomes, whichshare
thesame probabilit yofoccuring and
identicalerrorprobabilit y,f.
P(error) =f3+3f2(1 f):
Solution toexercise 2.39(p.40).Theentropyis9.7bitsperword.

<<<PAGE 59>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
AboutChapter 3
Ifyouareeager togetontoinformation theory ,datacompression, andnoisy
channels, youcanskiptoChapter 4.Data compression anddatamodelling
areintimately connected, however,soyou'llprobably wanttocome backto
thischapter bythetimeyougettoChapter 6.Before reading Chapter 3,it
mightbegoodtolookatthefollowingexercises.
.Exercise 3.1.[2,p.59]Adieisselected atrandom fromtwotwenty-faced dice
onwhichthesymbols1{10arewritten withnonuniform frequency as
follows.
Symbol 12345678910
NumberoffacesofdieA6432111110
NumberoffacesofdieB3322222211
Therandomly chosen dieisrolled 7times, withthefollowingoutcomes:
5,3,9,3,8,4,7.
What istheprobabilit ythatthedieisdieA?
.Exercise 3.2.[2,p.59]Assume thatthere isathird twenty-faced die,dieC,on
whichthesymbols1{20arewritten onceeach.Asabove,oneofthe
three diceisselected atrandom androlled 7times, giving theoutcomes:
3,5,4,8,3,9,7.
What istheprobabilit ythatthedieis(a)dieA,(b)dieB,(c)dieC?
Exercise 3.3.[3,p.48]Inferring adecayconstant
Unstable particles areemitted fromasource anddecayatadistance
x,arealnumberthathasanexponentialprobabilit ydistribution with
characteristic length.Decayeventscanonlybeobserv ediftheyoccur
inawindo wextending fromx=1cmtox=20cm.Ndecaysare
observ edatlocationsfx1;:::;xNg.What is?
******** *
x
.Exercise 3.4.[3,p.55]Forensic evidence
Twopeoplehavelefttraces oftheirownbloodatthescene ofacrime. A
suspect,Oliver,istested andfound tohavetype`O'blood.Theblood
groups ofthetwotraces arefound tobeoftype`O'(acommon type
inthelocalpopulation, havingfrequency 60%) andoftype`AB'(arare
type,withfrequency 1%).Dothese data(type`O'and`AB'bloodwere
found atscene) giveevidence infavouroftheproposition thatOliver
wasoneofthetwopeople presen tatthecrime?
47

<<<PAGE 60>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
3
More aboutInference
Itisnotacontroversial statemen tthatBayes'theorem provides thecorrect
language fordescribing theinference ofamessage comm unicated overanoisy
channel, asweuseditinChapter 1(p.6).Butstrangely ,when itcomes to
other inference problems, theuseofBayes'theorem isnotsowidespread.
3.1Arstinference problem
When Iwasanundergraduate inCambridge, Iwasprivileged toreceivesu-
pervisions fromSteveGull. Sitting athisdeskinadishev elledoce inSt.
John's College, IaskedhimhowoneoughttoansweranoldTriposquestion
(exercise 3.3):
Unstable particles areemitted fromasource anddecayata
distancex,arealnumberthathasanexponentialprobabilit ydis-
tribution withcharacteristic length.Decayeventscanonlybe
observ ediftheyoccurinawindo wextending fromx=1cmto
x=20cm.Ndecaysareobserv edatlocationsfx1;:::;xNg.What
is?
******** *
x
Ihadscratc hedmyheadoverthisforsome time. Myeducation hadprovided
mewithacouple ofapproac hestosolving suchinference problems: contructing
`estimators' oftheunkno wnparameters; or`tting' themodeltothedata, or
aprocessed version ofthedata.
Since themean ofanunconstrained exponentialdistribution is,itseemed
reasonable toexamine thesample mean x=P
nxn=Nandseeifanestimator
^could beobtained fromit.Itwaseviden tthattheestimator ^=x 1would
beappropriate for20cm,butnotforcases where thetruncation ofthe
distribution attheright-hand sideissignican t;withalittleingenuityand
theintroduction ofadhocbins,promising estimators for20cmcould be
constructed. Butthere wasnoobvious estimator thatwouldworkunder all
conditions.
Norcould Indasatisfactory approac hbased ontting thedensit yP(xj)
toahistogram derivedfromthedata. Iwasstuck.
What isthegeneral solution tothisproblem andothers likeit?Isit
alwaysnecessary ,when confron tedbyanewinference problem, togropeinthe
darkforappropriate `estimators' andworryaboutnding the`best'estimator
(whatev erthatmeans)?
48

<<<PAGE 61>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
3.1:Arstinference problem 49
00.050.10.150.20.25
2468101214161820P(x|lambda=2)
P(x|lambda=5)
P(x|lambda=10)Figure 3.1.Theprobabilit y
densit yP(xj)asafunction ofx.
00.050.10.150.2
1 10 100P(x=3|lambda)
P(x=5|lambda)
P(x=12|lambda)Figure 3.2.Theprobabilit y
densit yP(xj)asafunction of,
forthree dieren tvaluesofx.
When plotted thiswayround, the
function isknownasthelikelihood
of.Themarks indicate the
three valuesof,=2;5;10,that
wereusedinthepreceding gure.
Stevewrote downtheprobabilit yofonedatapoint,given:
P(xj)=(1
e x==Z()1<x<20
0 otherwise(3.1)
where
Z()=Z20
1dx1
e x==
e 1= e 20=
: (3.2)
Thisseemed obvious enough. Then hewrote Bayes'theorem :
P(jfx1;:::;xNg)=P(fxgj)P()
P(fxg)(3.3)
/1
(Z())Nexp
 PN
1xn=
P():(3.4)
Suddenly ,thestraigh tforwarddistribution P(fx1;:::;xNgj),dening the
probabilit yofthedatagiventhehypothesis,wasbeingturned onitshead
soastodene theprobabilit yofahypothesis giventhedata. Asimple gure
showedtheprobabilit yofasingle datapointP(xj)asafamiliar function ofx,
fordieren tvalues of(gure 3.1).Eachcurvewasaninnocentexponential,
normalized tohavearea1.Plotting thesamefunction asafunction offora
xedvalueofx,something remark ablehappens:apeakemerges (gure 3.2).
Tohelpunderstand these twopointsofviewoftheonefunction, gure 3.3
showsasurface plotofP(xj)asafunction ofxand.x1
1.5
2
2.5110100123

Figure 3.3.Theprobabilit y
densit yP(xj)asafunction ofx
and.Figures 3.1and3.2are
vertical sections through this
surface.
Foradataset consisting ofseveralpoints,e.g.,thesixpointsfxgN
n=1=
f1:5;2;3;4;5;12g,thelikelihoodfunctionP(fxgj)istheproductoftheN
functions of,P(xnj)(gure 3.4).
02e-074e-076e-078e-071e-061.2e-061.4e-06
1 10 100Figure 3.4.Thelikelihoodfunction
inthecaseofasix-pointdataset,
P(fxg=f1:5;2;3;4;5;12gj),as
afunction of.

<<<PAGE 62>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
50 3|More aboutInference
Stevesummarized Bayes'theorem asembodying thefactthat
what youknowaboutafterthedataarriveiswhat youknew
before[P()],andwhat thedatatoldyou[P(fxgj)].
Probabilities areusedheretoquantifydegrees ofbelief. Tonippossible
confusion inthebud,itmustbeemphasized thatthehypothesisthatcor-
rectly describ esthesituation isnotastochastic variable, andthefactthatthe
Bayesian usesaprobabilit ydistribution Pdoesnotmean thathethinks of
theworldasstochastically changing itsnature betweenthestates describ ed
bythedieren thypotheses. Heusesthenotation ofprobabilities torepresen t
hisbeliefs aboutthemutually exclusiv emicro-h ypotheses (here, values of),
ofwhichonlyoneisactually true. That probabilities candenote degrees of
belief,givenassumptions, seemed reasonable tome.
Theposterior probabilit ydistribution (3.4)represen tstheunique andcom-
pletesolution totheproblem. There isnoneedtoinvent`estimators'; nordo
weneedtoinventcriteria forcomparing alternativ eestimators witheachother.
Whereas orthodoxstatisticians oertwentywaysofsolving aproblem, andan-
other twentydieren tcriteria fordeciding whichofthese solutions isthebest,
Bayesian statistics onlyoers oneanswertoawell-posedproblem. Ifyouhaveanydicult yunderstand-
ingthischapter Irecommend ensur-
ingyouarehappywithexercises 3.1
and3.2(p.47)thennoting theirsim-
ilaritytoexercise 3.3.Assumptions ininference
Ourinference isconditional onourassumptions [forexample, thepriorP()].
Critics viewsuchpriors asadicult ybecause theyare`subjective',butIdon't
seehowitcould beotherwise. Howcanoneperform inference without making
assumptions? Ibelievethatitisofgreat valuethatBayesian metho dsforce
onetomakethese tacitassumptions explicit.
First, onceassumptions aremade, theinferences areobjectiveandunique,
reproduceable withcomplete agreemen tbyanyonewhohasthesameinforma-
tionandmakesthesame assumptions. Forexample, giventheassumptions
listed above,H,andthedataD,everyonewillagree abouttheposterior prob-
abilityofthedecaylength:
P(jD;H)=P(Dj;H)P(jH)
P(DjH): (3.5)
Second, when theassumptions areexplicit, theyareeasier tocriticize, and
easier tomodify{indeed, wecanquantifythesensitivit yofourinferences to
thedetails oftheassumptions. Forexample, wecannotefromthelikelihood
curvesingure 3.2thatinthecaseofasingle datapointatx=5,the
likelihoodfunction islessstrongly peakedthaninthecasex=3;thedetails
ofthepriorP()become increasingly importantasthesample mean xgets
closer tothemiddle ofthewindo w,10.5. Inthecasex=12,thelikelihood
function doesn'thaveapeakatall{suchdatamerely ruleoutsmall values
of,anddon't giveanyinformation abouttherelativ eprobabilities oflarge
valuesof.Sointhiscase,thedetails oftheprioratthesmallendofthings
arenotimportant,butatthelargeend,thepriorisimportant.
Third, when wearenotsurewhichofvarious alternativ eassumptions is
themost appropriate foraproblem, wecantreat thisquestion asanother
inference task. Thus,givendataD,wecancompare alternativ eassumptions
Husing Bayes'theorem:
P(HjD;I)=P(DjH;I)P(HjI)
P(DjI); (3.6)

<<<PAGE 63>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
3.2:Thebentcoin 51
whereIdenotes thehighest assumptions, whichwearenotquestioning.
Fourth, wecantakeintoaccoun touruncertain tyregarding suchassump-
tionswhen wemakesubsequen tpredictions. Rather thanchoosing onepartic-
ularassumptionH,andworking outourpredictions aboutsome quantityt,
P(tjD;H;I),weobtain predictions thattakeintoaccoun touruncertain ty
aboutHbyusing thesumrule:
P(tjD;I)=X
HP(tjD;H;I)P(HjD;I): (3.7)
Thisisanother contrastwithorthodoxstatistics, inwhichitisconventional
to`test' adefault model,andthen, ifthetest`accepts themodel'atsome
`signicance level',touseexclusiv elythatmodeltomakepredictions.
Stevethuspersuaded methat
probabilit ytheory reachesparts thatadhocmetho dscannot reach.
Let'slookatafewmore examples ofsimple inference problems.
3.2Thebentcoin
AbentcoinistossedFtimes; weobserv easequence sofheads andtails
(whichwe'lldenote bythesymbolsaandb).Wewishtoknowthebiasof
thecoin,andpredict theprobabilit ythatthenexttosswillresult inahead.
Werstencoun tered thistaskinexample 2.7(p.30),andwewillencoun terit
again inChapter 6,when wediscuss adaptiv edatacompression. Itisalsothe
original inference problem studied byThomas Bayesinhisessaypublished in
1763.
Asinexercise 2.8(p.30),wewillassume auniform priordistribution and
obtain aposterior distribution bymultiplying bythelikelihood.Acriticmight
object,`where didthisprior come from?' Iwillnotclaim thattheuniform
prior isinanywayfundamen tal;indeed we'llgiveexamples ofnonuniform
priors later. Thepriorisasubjectiveassumption. Oneofthethemes ofthis
bookis:
youcan't doinference {ordatacompression {without making
assumptions.
WegivethenameH1toourassumptions. [We'llbeintroducing anal-
ternativ esetofassumptions inamomen t.]Theprobabilit y,givenpa,thatF
tosses result inasequence sthatcontainsfFa;Fbgcountsofthetwooutcomes
is
P(sjpa;F;H1)=pFaa(1 pa)Fb: (3.8)
[Forexample,P(s=aabajpa;F=4;H1)=papa(1 pa)pa:]Ourrstmodel
assumes auniform priordistribution forpa,
P(pajH1)=1;pa2[0;1] (3.9)
andpb1 pa.
Inferring unknown parameters
Givenastring oflengthFofwhichFaareasandFbarebs,weareinterested
in(a)inferring whatpamightbe;(b)predicting whether thenextcharacter is

<<<PAGE 64>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
52 3|More aboutInference
anaorab.[Predictions arealwaysexpressed asprobabilities. So`predicting
whether thenextcharacter isana'isthesame ascomputing theprobabilit y
thatthenextcharacter isana.]
AssumingH1tobetrue,theposterior probabilit yofpa,givenastring s
oflengthFthathascountsfFa;Fbg,is,byBayes'theorem,
P(pajs;F;H1)=P(sjpa;F;H1)P(pajH1)
P(sjF;H1): (3.10)
ThefactorP(sjpa;F;H1),which,asafunction ofpa,isknownasthelikeli-
hoodfunction, wasgiveninequation (3.8); thepriorP(pajH1)wasgivenin
equation (3.9). Ourinference ofpaisthus:
P(pajs;F;H1)=pFaa(1 pa)Fb
P(sjF;H1): (3.11)
Thenormalizing constan tisgivenbythebetaintegral
P(sjF;H1)=Z1
0dpapFaa(1 pa)Fb= (Fa+1) (Fb+1)
 (Fa+Fb+2)=Fa!Fb!
(Fa+Fb+1)!:
(3.12)
Exercise 3.5.[2,p.59]Sketchtheposterior probabilit yP(pajs=aba;F=3).
What isthemost probable valueofpa(i.e.,thevaluethatmaximizes
theposterior probabilit ydensit y)?What isthemean valueofpaunder
thisdistribution?
Answerthe same questions for the posterior probabilit y
P(pajs=bbb;F=3).
Frominferencestopredictions
Ourprediction aboutthenexttoss,theprobabilit ythatnexttossisaa,is
obtained byintegrating overpa.Thishastheeect oftaking intoaccoun tour
uncertain tyaboutpawhen making predictions. Bythesumrule,
P(ajs;F)=Z
dpaP(ajpa)P(pajs;F): (3.13)
Theprobabilit yofanagivenpaissimplypa,so
P(ajs;F)=Z
dpapapFaa(1 pa)Fb
P(sjF)(3.14)
=Z
dpapFa+1
a(1 pa)Fb
P(sjF)(3.15)
=(Fa+1)!Fb!
(Fa+Fb+2)!Fa!Fb!
(Fa+Fb+1)!
=Fa+1
Fa+Fb+2;(3.16)
whichisknownasLaplace's rule.
3.3Thebentcoinandmodelcomparison
Imagine thatascientistintroduces another theory forourdata. Heasserts
thatthesource isnotreally abentcoinbutisreally aperfectly formed diewith
onefacepaintedheads (`a')andtheother vepaintedtails(`b').Thusthe
parameterpa,whichintheoriginal model,H1,could takeanyvaluebetween
0and1,isaccording tothenewhypothesis,H0,notafreeparameter atall;
rather, itisequal to1=6.[This hypothesis istermedH0sothatthesux of
eachmodelindicates itsnumberoffreeparameters.]
Howcanwecompare these twomodelsinthelightofdata? Wewishto
inferhowprobableH1isrelativ etoH0.

<<<PAGE 65>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
3.3:Thebentcoinandmodelcomparison 53
Modelcomparison asinference
Inorder toperform modelcomparison, wewrite downBayes'theorem again,
butthistimewithadieren targumen tontheleft-hand side. Wewishto
knowhowprobableH1isgiventhedata. ByBayes'theorem,
P(H1js;F)=P(sjF;H1)P(H1)
P(sjF): (3.17)
Similarly ,theposterior probabilit yofH0is
P(H0js;F)=P(sjF;H0)P(H0)
P(sjF): (3.18)
Thenormalizing constan tinbothcases isP(sjF),whichisthetotalproba-
bilityofgetting theobserv eddata. IfH1andH0aretheonlymodelsunder
consideration, thisprobabilit yisgivenbythesumrule:
P(sjF)=P(sjF;H1)P(H1)+P(sjF;H0)P(H0): (3.19)
Toevaluate theposterior probabilities ofthehypotheses weneedtoassign
values totheprior probabilities P(H1)andP(H0);inthiscase, wemight
setthese to1/2each.Andweneedtoevaluate thedata-dep enden tterms
P(sjF;H1)andP(sjF;H0).Wecangivenames tothese quantities. The
quantityP(sjF;H1)isameasure ofhowmuchthedatafavourH1,andwe
callittheevidence formodelH1.Wealready encoun tered thisquantityin
equation (3.10) where itappeared asthenormalizing constan toftherst
inference wemade {theinference ofpagiventhedata.
Howmodelcomparison works: Theevidence foramodelis
usually thenormalizing constan tofanearlier Bayesian inference.
Weevaluated thenormalizing constan tformodelH1in(3.12). Theevi-
dence formodelH0isverysimple because thismodelhasnoparameters to
infer. Deningp0tobe1=6,wehave
P(sjF;H0)=pFa
0(1 p0)Fb: (3.20)
Thustheposterior probabilit yratioofmodelH1tomodelH0is
P(H1js;F)
P(H0js;F)=P(sjF;H1)P(H1)
P(sjF;H0)P(H0)(3.21)
=Fa!Fb!
(Fa+Fb+1)!
pFa
0(1 p0)Fb: (3.22)
Some valuesofthisposterior probabilit yratioareillustrated intable3.5.The
rstvelinesillustrate thatsomeoutcomes favouronemodel,andsomefavour
theother. Nooutcome iscompletely incompatible witheither model.With
small amoun tsofdata(sixtosses, say)itistypically notthecasethatoneof
thetwomodelsisoverwhelmingly more probable thantheother. Butwith
moredata, theevidence againstH0givenbyanydatasetwiththeratioFa:Fb
diering from1:5mountsup.Youcan't predict inadvancehowmuchdata
areneeded tobeprettysurewhichtheory istrue. Itdependswhatp0is.
Thesimpler model,H0,since ithasnoadjustable parameters, isableto
loseoutbythebiggest margin. Theoddsmaybehundreds tooneagainst it.
Themorecomplex modelcanneverloseoutbyalargemargin; there's nodata
setthatisactually unlikely givenmodelH1.

<<<PAGE 66>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
54 3|More aboutInference
FData (Fa;Fb)P(H1js;F)
P(H0js;F)
6 (5;1) 222.2
6 (3;3) 2.67
6 (2;4) 0.71 =1/1.4
6 (1;5) 0.356 =1/2.8
6 (0;6) 0.427 =1/2.3
20 (10;10) 96.5
20 (3;17) 0.2 =1/5
20 (0;20) 1.83Table3.5.Outcome ofmodel
comparison betweenmodelsH1
andH0forthe`bentcoin'. Model
H0states thatpa=1=6,pb=5=6.
H0istrue H1istrue
pa=1=6
-4-202468
05010015020010/1
1/10100/1
1/1001/11000/1pa=0:25
-4-202468
05010015020010/1
1/10100/1
1/1001/11000/1pa=0:5
-4-202468
05010015020010/1
1/10100/1
1/1001/11000/1
-4-202468
05010015020010/1
1/10100/1
1/1001/11000/1
-4-202468
05010015020010/1
1/10100/1
1/1001/11000/1
-4-202468
05010015020010/1
1/10100/1
1/1001/11000/1
-4-202468
05010015020010/1
1/10100/1
1/1001/11000/1
-4-202468
05010015020010/1
1/10100/1
1/1001/11000/1
-4-202468
05010015020010/1
1/10100/1
1/1001/11000/1Figure 3.6.Typical behaviour of
theevidence infavourofH1as
bentcointosses accum ulateunder
three dieren tconditions.
Horizon talaxisisthenumberof
tosses,F.Thevertical axisonthe
leftislnP(sjF;H1)
P(sjF;H0);theright-hand
vertical axisshowsthevaluesof
P(sjF;H1)
P(sjF;H0).
(Seealsogure 3.8,p.60.)
.Exercise 3.6.[2]ShowthatafterFtosses havetakenplace, thebiggest value
thatthelogevidence ratio
logP(sjF;H1)
P(sjF;H0)(3.23)
canhavescales linearlywithFifH1ismore probable, butthelog
evidence infavourofH0cangrowatmostaslogF.
.Exercise 3.7.[3,p.60]Putting yoursampling theory haton,assumingFahas
notyetbeenmeasured, compute aplausible range thatthelogevidence
ratiomightliein,asafunction ofFandthetruevalueofpa,andsketch
itasafunction ofFforpa=p0=1=6,pa=0:25,andpa=1=2.[Hint:
sketchthelogevidence asafunction oftherandom variableFaandwork
outthemean andstandard deviation ofFa.]
Typicalbehaviour oftheevidenc e
Figure 3.6showsthelogevidence ratioasafunction ofthenumberoftosses,
F,inanumberofsimulated experimen ts.Intheleft-hand experimen ts,H0
wastrue. Intheright-hand ones,H1wastrue,andthevalueofpaiseither
0.25or0.5.
Wewilldiscuss modelcomparison more inalaterchapter.

<<<PAGE 67>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
3.4:Anexample oflegalevidence 55
3.4Anexample oflegalevidence
Thefollowingexample illustrates thatthereismoretoBayesianinference than
thepriors.
Twopeople havelefttraces oftheirownbloodatthescene ofa
crime. Asuspect,Oliver,istested andfound tohavetype`O'
blood.Thebloodgroups ofthetwotraces arefound tobeoftype
`O'(acommon typeinthelocalpopulation, havingfrequency 60%)
andoftype`AB'(araretype,withfrequency 1%).Dothese data
(type`O'and`AB'bloodwerefound atscene) giveevidence in
favouroftheproposition thatOliverwasoneofthetwopeople
presen tatthecrime?
Acareless lawyermightclaim thatthefactthatthesuspect'sbloodtypewas
found atthescene ispositiveevidence forthetheory thathewaspresen t.But
thisisnotso.
Denote theproposition `thesuspectandoneunkno wnperson werepresen t'
byS.Thealternativ e,S,states `twounkno wnpeoplefromthepopulation were
presen t'.Thepriorinthisproblem isthepriorprobabilit yratiobetweenthe
propositionsSandS.Thisquantityisimportanttothenalverdict and
wouldbebased onallother available information inthecase. Ourtaskhereis
justtoevaluate thecontribution made bythedataD,thatis,thelikelihood
ratio,P(DjS;H)=P(DjS;H).Inmyview, ajury's taskshould generally beto
multiply together carefully evaluated likelihoodratios fromeachindependen t
piece ofadmissible evidence withanequally carefully reasoned prior proba-
bility.[This viewisshared bymanystatisticians butlearned British appeal
judges recentlydisagreed andactually overturned theverdict ofatrialbecause
thejurors hadbeentaughttouseBayes'theorem tohandle complicated DNA
evidence.]
Theprobabilit yofthedatagivenSistheprobabilit ythatoneunkno wn
person drawnfromthepopulation hasbloodtypeAB:
P(DjS;H)=pAB (3.24)
(since givenS,wealready knowthatonetrace willbeoftypeO).Theprob-
abilityofthedatagivenSistheprobabilit ythattwounkno wnpeople drawn
fromthepopulation havetypesOandAB:
P(DjS;H)=2pOpAB: (3.25)
Inthese equationsHdenotes theassumptions thattwopeople werepresen t
andleftbloodthere, andthattheprobabilit ydistribution ofthebloodgroups
ofunkno wnpeopleinanexplanation isthesameasthepopulation frequencies.
Dividing, weobtain thelikelihoodratio:
P(DjS;H)
P(DjS;H)=1
2pO=1
20:6=0:83: (3.26)
Thusthedatainfactprovideweakevidence against thesupposition that
Oliverwaspresen t.
Thisresult maybefound surprising, soletusexamine itfromvarious
pointsofview. Firstconsider thecaseofanother suspect,Alberto,whohas
typeAB.Intuitively,thedatadoprovideevidence infavourofthetheoryS0

<<<PAGE 68>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
56 3|More aboutInference
thatthissuspectwaspresen t,relativ etothenullhypothesis S.Andindeed
thelikelihoodratiointhiscaseis:
P(DjS0;H)
P(DjS;H)=1
2pAB=50: (3.27)
Nowletuschange thesituation slightly;imagine that99%ofpeople areof
bloodtypeO,andtherestareoftypeAB.Onlythese twobloodtypesexist
inthepopulation. Thedataatthescene arethesame asbefore. Consider
again howthese datainuence ourbeliefs aboutOliver,asuspectoftype
O,andAlberto,asuspectoftypeAB.Intuitively,westillbelievethatthe
presence oftherareABbloodprovides positiveevidence thatAlbertowas
there. ButdoesthefactthattypeObloodwasdetected atthescene favour
thehypothesis thatOliverwaspresen t?Ifthiswerethecase,thatwouldmean
thatregardless ofwhothesuspectis,thedatamakeitmoreprobable theywere
presen t;everyoneinthepopulation wouldbeunder greater suspicion, which
wouldbeabsurd. Thedatamaybecompatible withanysuspectofeither
bloodtypebeingpresen t,butiftheyprovideevidence forsometheories, they
mustalsoprovideevidence against other theories.
Hereisanother wayofthinking aboutthis:imagine thatinstead oftwo
people's bloodstains there areten,andthatintheentirelocalpopulation
ofonehundred, there areninetytypeOsuspectsandtentypeABsuspects.
Consider aparticular typeOsuspect,Oliver:without anyother information,
andbeforethebloodtestresults come in,there isaonein10chance thathe
wasatthescene, sinceweknowthat10outofthe100suspectswerepresen t.
Wenowgettheresults ofbloodtests, andndthatnineofthetenstains are
oftypeAB,andoneofthestains isoftypeO.Doesthismakeitmore likely
thatOliverwasthere? No,there isnowonlyaoneinninetychance thathe
wasthere, sinceweknowthatonlyoneperson presen twasoftypeO.
Maybetheintuition isaided nally bywriting downtheformulaeforthe
general casewherenObloodstains ofindividuals oftypeOarefound, and
nABoftypeAB,atotalofNindividuals inall,andunkno wnpeople come
fromalarge population withfractionspO;pAB.(There maybeother blood
typestoo.)Thetaskistoevaluate thelikelihoodratioforthetwohypotheses:
S,`thetypeOsuspect(Oliver)andN 1unkno wnothers leftNstains'; and
S,`Nunkno wnsleftNstains'. Theprobabilit yofthedataunder hypothesis
Sisjusttheprobabilit yofgettingnO;nABindividuals ofthetwotypeswhen
Nindividuals aredrawnatrandom fromthepopulation:
P(nO;nABjS)=N!
nO!nAB!pnO
OpnAB
AB: (3.28)
InthecaseofhypothesisS,weneedthedistribution oftheN 1other indi-
viduals:
P(nO;nABjS)=(N 1)!
(nO 1)!nAB!pnO 1
OpnAB
AB: (3.29)
Thelikelihoodratiois:
P(nO;nABjS)
P(nO;nABjS)=nO=N
pO: (3.30)
Thisisaninstructiv eresult. Thelikelihoodratio, i.e.thecontribution of
these datatothequestion ofwhether Oliverwaspresen t,dependssimply on
acomparison ofthefrequency ofhisbloodtypeintheobserv eddatawiththe
background frequency inthepopulation. There isnodependence onthecounts
oftheother typesfound atthescene, ortheirfrequencies inthepopulation.

<<<PAGE 69>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
3.5:Exercises 57
Ifthere aremore typeOstains thantheaverage numberexpected under
hypothesis S,thenthedatagiveevidence infavourofthepresence ofOliver.
Conversely,ifthere arefewertypeOstains thantheexpected numberunder
S,thenthedatareduce theprobabilit yofthehypothesis thathewasthere.
InthespecialcasenO=N=pO,thedatacontribute noevidence either way,
regardless ofthefactthatthedataarecompatible withthehypothesisS.
3.5Exercises
Exercise 3.8.[2,p.60]Thethreedoors,normalrules.
Onagame show,acontestan tistoldtherulesasfollows:
There arethree doors,labelled1,2,3.Asingle prize has
beenhidden behind oneofthem. Yougettoselect onedoor.
Initially yourchosen doorwillnotbeopened. Instead, the
gamesho whostwillopenoneoftheother twodoors,andhe
willdosoinsuchawayasnottorevealtheprize. Forexample,
ifyourstchoosedoor1,hewillthenopenoneofdoors2and
3,anditisguaran teedthathewillchoosewhichonetoopen
sothattheprizewillnotberevealed.
Atthispoint,youwillbegivenafresh choice ofdoor:you
caneither stickwithyourrstchoice, oryoucanswitchtothe
other closed door.Allthedoorswillthenbeopenedandyou
willreceivewhatev erisbehind yournalchoice ofdoor.
Imagine thatthecontestan tchoosesdoor1rst;thenthegamesho whost
opensdoor3,revealing nothing behind thedoor,aspromised. Should
thecontestan t(a)stickwithdoor1,or(b)switchtodoor2,or(c)does
itmakenodierence?
Exercise 3.9.[2,p.61]Thethreedoors,earthquak escenario.
Imagine thatthegame happensagain andjustasthegamesho whostis
abouttoopenoneofthedoorsaviolen tearthquak erattles thebuilding
andoneofthethree doorsiesopen.Ithappenstobedoor3,andit
happensnottohavetheprize behind it.Thecontestan thadinitially
chosen door1.
Repositioning histoupee,thehostsuggests, `OK,since youchosedoor
1initially ,door3isavaliddoorformetoopen,according totherules
ofthegame; I'llletdoor3stayopen.Let's carry onasifnothing
happened.'
Should thecontestan tstickwithdoor1,orswitchtodoor2,ordoesit
makenodierence? Assume thattheprizewasplaced randomly ,that
thegamesho whostdoesnotknowwhere itis,andthatthedoorew
openbecause itslatchwasbrokenbytheearthquak e.
[Asimilar alternativ escenario isagamesho wwhose confuse dhostfor-
getstherules, andwhere theprize is,andopensoneoftheunchosen
doorsatrandom. Heopensdoor3,andtheprizeisnotrevealed. Should
thecontestan tchoosewhat's behind door1ordoor2?Doestheopti-
maldecision forthecontestan tdependonthecontestan t'sbeliefsabout
whether thegamesho whostisconfused ornot?]
.Exercise 3.10.[2]Another example inwhich theemphasis isnotonpriors.You
visitafamily whose three children areallatthelocalschool.Youdon't

<<<PAGE 70>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
58 3|More aboutInference
knowanything aboutthesexes ofthechildren. While walking clum-
silyround thehome, youstumblethrough oneofthethree unlabelled
bedroomdoorsthatyouknowbelong, oneeach,tothethree children,
andndthatthebedroomcontainsgirliestuinsucien tquantities to
convince youthatthechildwholivesinthatbedroomisagirl.Later,
yousneak alookataletter addressed totheparents,whichreads `From
theHeadmaster: wearesending thisletter toallparentswhohavemale
children attheschooltoinform them aboutthefollowingboyishmat-
ters...'.
These twosources ofevidence establish thatatleastoneofthethree
children isagirl,andthatatleastoneofthechildren isaboy.What
aretheprobabilities thatthere are(a)twogirlsandoneboy;(b)two
boysandonegirl?
.Exercise 3.11.[2,p.61]MrsSisfound stabbedinherfamily garden. MrS
behavesstrangely afterherdeath andisconsidered asasuspect.On
investigation ofpoliceandsocialrecords itisfound thatMrShadbeaten
uphiswifeonatleastnineprevious occasions. Theprosecution advances
thisdataasevidence infavourofthehypothesis thatMrSisguiltyofthe
murder. `Ahno,'saysMrS'shighly paidlawyer,`statistic ally,onlyone
inathousand wife-b eaters actually goesontomurder hiswife.1Sothe
wife-b eating isnotstrong evidence atall.Infact,giventhewife-b eating
evidence alone, it'sextremely unlikelythathewouldbethemurderer of
hiswife{onlya1=1000chance. Youshould therefore ndhiminnocent.'
Isthelawyerrighttoimply thatthehistory ofwife-b eating doesnot
pointtoMrS'sbeingthemurderer? Oristhelawyeraslimytrickster?
Ifthelatter, what iswrong withhisargumen t?
[Havingreceivedanindignan tletter fromalawyeraboutthepreceding
paragraph, I'dliketoaddanextra inference exercise atthispoint:Does
mysuggestion thatMr.S.'slawyer mayhavebeenaslimytrickster imply
thatIbelieve alllawyers areslimytricksters? (Answ er:No.)]
.Exercise 3.12.[2]Abagcontains onecounter,knowntobeeither white or
black.Awhite counterisputin,thebagisshaken,andacounter
isdrawnout,whichprovestobewhite. What isnowthechance of
drawingawhite counter?[Notice thatthestate ofthebag,afterthe
operations, isexactly identicaltoitsstatebefore.]
.Exercise 3.13.[2,p.62]Youmoveintoanewhouse; thephone isconnected, and
you're prettysurethatthephone numberis740511 ,butnotassureas
youwouldliketobe.Asanexperimen t,youpickupthephone and
dial740511 ;youobtain a`busy' signal. Areyounowmore sureofyour
phone number?Ifso,howmuch?
.Exercise 3.14.[1]Inagame, twocoins aretossed. Ifeither ofthecoins comes
upheads, youhavewonaprize. Toclaim theprize, youmustpointto
oneofyourcoins thatisaheadandsay`look,thatcoin's ahead, I've
won'.YouwatchFredplaythegame. Hetosses thetwocoins, andhe
1IntheU.S.A., itisestimated that2million women areabused eachyearbytheirpartners.
In1994, 4739women werevictims ofhomicide; ofthose, 1326women (28%) wereslainby
husbands andboyfriends.
(Sources: http://www.umn.edu/mincava/p apers/factoid.htm,
http://www.gunfree.inter.net/vp c/womenfs.htm )

<<<PAGE 71>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
3.6:Solutions 59
pointstoacoinandsays`look,thatcoin's ahead, I'vewon'.What is
theprobabilit ythattheother coinisahead?
.Exercise 3.15.[2,p.63]Astatistical statemen tappeared inTheGuardianon
FridayJanuary4,2002:
When spunonedge250times, aBelgian one-euro coincame
upheads 140times andtails110. `Itlooksverysuspicious
tome',saidBarry Blight,astatistics lecturer attheLondon
SchoolofEconomics. `Ifthecoinwereunbiased thechance of
getting aresult asextreme asthatwouldbelessthan7%'.
Butdothese datagiveevidence thatthecoinisbiased rather thanfair?
[Hint:seeequation (3.22).]
3.6Solutions
Solution toexercise 3.1(p.47).LetthedatabeD.Assuming equal prior
probabilities,
P(AjD)
P(BjD)=1
23
21
13
21
22
21
2=9=32 (3.31)
andP(AjD)=9=41:
Solution toexercise 3.2(p.47).Theprobabilit yofthedatagiveneachhy-
pothesis is:
P(DjA)=3
201
202
201
203
201
201
20=18
207; (3.32)
P(DjB)=2
202
202
202
202
201
202
20=64
207; (3.33)
P(DjC)=1
201
201
201
201
201
201
20=1
207: (3.34)
So
P(AjD)=18
18+64+1=18
83;P(BjD)=64
83;P(CjD)=1
83:
(3.35)
(a) 00.20.40.60.81 (b) 00.20.40.60.81
P(pajs=aba;F=3)/p2
a(1 pa) P(pajs=bbb;F=3)/(1 pa)3Figure 3.7.Posterior probabilit y
forthebiaspaofabentcoin
giventwodieren tdatasets.
Solution toexercise 3.5(p.52).
(a)P(pajs=aba;F=3)/p2
a(1 pa).Themostprobable valueofpa(i.e.,
thevaluethatmaximizes theposterior probabilit ydensit y)is2=3.The
mean valueofpais3=5.
Seegure 3.7a.

<<<PAGE 72>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
60 3|More aboutInference
(b)P(pajs=bbb;F=3)/(1 pa)3.Themost probable valueofpa(i.e.,
thevaluethatmaximizes theposterior probabilit ydensit y)is0.The
mean valueofpais1=5.
Seegure 3.7b.
H0istrue H1istrue
pa=1=6
-4-202468
05010015020010/1
1/10100/1
1/1001/11000/1pa=0:25
-4-202468
05010015020010/1
1/10100/1
1/1001/11000/1pa=0:5
-4-202468
05010015020010/1
1/10100/1
1/1001/11000/1Figure 3.8.Range ofplausible
valuesofthelogevidence in
favourofH1asafunction ofF.
Thevertical axisontheleftis
logP(sjF;H1)
P(sjF;H0);theright-hand
vertical axisshowsthevaluesof
P(sjF;H1)
P(sjF;H0).
Thesolidlineshowsthelog
evidence iftherandom variable
Fatakesonitsmean value,
Fa=paF.Thedotted linesshow
(appro ximately) thelogevidence
ifFaisatits2.5th or97.5th
percentile.
(Seealsogure 3.6,p.54.)Solution toexercise 3.7(p.54).Thecurvesingure 3.8werefound bynding
themean andstandard deviation ofFa,thensettingFatothemeantwo
standard deviations togeta95%plausible range forFa,andcomputing the
three corresp onding values ofthelogevidence ratio.
Solution toexercise 3.8(p.57).LetHidenote thehypothesis thattheprizeis
behind doori.Wemakethefollowingassumptions: thethree hypothesesH1,
H2andH3areequiprobable apriori,i.e.,
P(H1)=P(H2)=P(H3)=1
3: (3.36)
Thedatum wereceive,afterchoosing door1,isoneofD=3andD=2(mean-
ingdoor3or2isopened, respectively). Weassume thatthese twopossible
outcomes havethefollowingprobabilities. Iftheprizeisbehind door1then
thehosthasafreechoice; inthiscaseweassume thatthehostselects at
random betweenD=2andD=3.Otherwise thechoice ofthehostisforced
andtheprobabilities are0and1.
P(D=2jH1)=1/2P(D=2jH2)=0P(D=2jH3)=1
P(D=3jH1)=1/2P(D=3jH2)=1P(D=3jH3)=0(3.37)
Now,using Bayes'theorem, weevaluate theposterior probabilities ofthe
hypotheses:
P(HijD=3)=P(D=3jHi)P(Hi)
P(D=3)(3.38)
P(H1jD=3)=(1=2)(1=3)
P(D=3)P(H2jD=3)=(1)(1=3)
P(D=3)P(H3jD=3)=(0)(1=3)
P(D=3)
(3.39)
Thedenominator P(D=3)is(1=2)because itisthenormalizing constan tfor
thisposterior distribution. So
P(H1jD=3)=1/3P(H2jD=3)=2/3P(H3jD=3)=0:
(3.40)
Sothecontestan tshould switchtodoor2inorder tohavethebiggest chance
ofgetting theprize.
Manypeople ndthisoutcome surprising. There aretwowaystomakeit
more intuitive.Oneistoplaythegame thirtytimes withafriend andkeep
trackofthefrequency withwhichswitchinggetstheprize. Alternativ ely,you
canperform athough texperimen tinwhichthegame isplayedwithamillion
doors.Therulesarenowthatthecontestan tchoosesonedoor,thenthegame

<<<PAGE 73>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
3.6:Solutions 61
showhostopens999,998 doorsinsuchawayasnottorevealtheprize, leaving
thecontestant's selected doorandoneotherdoorclosed. Thecontestan tmay
nowstickorswitch.Imagine thecontestan tconfron tedbyamillion doors,
ofwhichdoors1and234,598 havenotbeenopened, door1havingbeenthe
contestan t'sinitial guess. Where doyouthink theprizeis?
Solution toexercise 3.9(p.57).Ifdoor3isopenedbyanearthquak e,the
inference comes outdieren tly{eventhough visually thescene looksthe
same. Thenature ofthedata, andtheprobabilit yofthedata, areboth
nowdieren t.Thepossible dataoutcomes are,rstly,thatanynumberof
thedoorsmighthaveopened. Wecould labeltheeightpossible outcomes
d=(0;0;0);(0;0;1);(0;1;0);(1;0;0);(0;1;1);:::;(1;1;1).Secondly ,itmight
bethattheprizeisvisible aftertheearthquak ehasopenedoneormoredoors.
SothedataDconsists ofthevalueofd,andastatemen tofwhether the
prizewasrevealed. Itishardtosaywhattheprobabilities ofthese outcomes
are,sincetheydependonourbeliefsaboutthereliabilit yofthedoorlatches
andtheproperties ofearthquak es,butitispossible toextract thedesired
posterior probabilit ywithout naming thevalues ofP(djHi)foreachd.All
thatmatters aretherelativ evalues ofthequantitiesP(DjH1),P(DjH2),
P(DjH3),forthevalueofDthatactually occured. [This isthelikelihood
principle ,whichwemetinsection 2.3.]ThevalueofDthatactually occured is
d=(0;0;1),andnoprizevisible. First, itisclearthatP(DjH3)=0,sincethe
datum thatnoprizeisvisible isincompatible withH3.Now,assuming that
thecontestan tselected door1,howdoestheprobabilit yP(DjH1)compare
withP(DjH2)?Assuming thatearthquak esarenotsensitiv etodecisions of
game showcontestan ts,these twoquantities havetobeequal, bysymmetry .
Wedon't knowhowlikelyitisthatdoor3fallsoitshinges, buthowever
likelyitis,it'sjustaslikelytodosowhether theprize isbehind door1or
door2.So,ifP(DjH1)andP(DjH2)areequal, weobtain:
P(H1jD)=P(DjH1)(1/3)
P(D)P(H2jD)=P(DjH2)(1/3)
P(D)P(H3jD)=P(DjH3)(1/3)
P(D)
=1/2 =1/2 =0:
(3.41)
Thetwopossible hypotheses arenowequally likely.
Ifweassume thatthehostknowswhere theprizeisandmightbeacting
deceptiv ely,thentheanswermightbefurther modied, because wehaveto
viewthehost's wordsaspartofthedata.
Confused? It'swellworthmaking sureyouunderstand thesetwogamesho w
problems. Don't worry,Islippeduponthesecond problem, thersttimeI
metit.
There isageneral rulewhichhelps immensely inconfusing probabilit y
problems:
Alwayswrite downtheprobabilit yofeverything.
(Steve Gull)
Fromthisjointprobabilit y,anydesired inference canbemechanically ob-
tained (gure 3.9).Where theprizeis
doordoordoor
1 2 3
1,2,32,31,31,23pnone
3pnone
3pnone
3
p3
3p3
3p3
3
p1;2;3
3p1;2;3
3p1;2;3
321noneWhich doorsopenedbyearthquak e
Figure 3.9.Theprobabilit yof
everything, forthesecond
three-do orproblem, assuming an
earthquak ehasjustoccured.
Here,p3istheprobabilit ythat
door3alone isopenedbyan
earthquak e.
Solution toexercise 3.11(p.58).Thestatistic quoted bythelawyerindicates
theprobabilit ythatarandomly selected wife-b eaterwillalsomurder hiswife.
Theprobabilit ythatthehusband wasthemurderer, giventhatthewifehas
beenmurdered,isacompletely dieren tquantity.

<<<PAGE 74>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
62 3|More aboutInference
Todeduce thelatter, weneedtomakefurther assumptions aboutthe
probabilit yofthewife's beingmurdered bysomeone else. Ifshelivesina
neighbourho odwithfrequen trandom murders, thenthisprobabilit yislarge
andtheposterior probabilit ythatthehusband didit(intheabsence ofother
evidence) maynotbeverylarge. Butinmorepeaceful regions, itmaywellbe
thatthemostlikelyperson tohavemurdered you,ifyouarefound murdered,
isoneofyourclosest relativ es.
Let's workoutsome illustrativ enumberswiththehelpofthestatistics
onpage58.Letm=1denote theproposition thatawoman hasbeenmur-
dered;h=1,theproposition thatthehusband didit;andb=1,thepropo-
sition thathebeatherintheyearpreceding themurder. Thestatemen t
`someone elsedidit'isdenoted byh=0.WeneedtodeneP(hjm=1),
P(bjh=1;m=1),andP(b=1jh=0;m=1)inorder tocompute thepos-
terior probabilit yP(h=1jb=1;m=1).Fromthestatistics, wecanread
outP(h=1jm=1)=0:28.Andiftwomillion women outof100million
arebeaten, thenP(b=1jh=0;m=1)=0:02.Finally ,weneedavaluefor
P(bjh=1;m=1):ifamanmurders hiswife,howlikelyisitthatthisisthe
rsttimehelaidanger onher? Iexpectit'sprettyunlikely;somaybe
P(b=1jh=1;m=1)is0.9orlarger.
ByBayes'theorem, then,
P(h=1jb=1;m=1)=:9:28
:9:28+:02:72'95%: (3.42)
Onewaytomakeobvious thesliminess ofthelawyeronp.58istoconstruct
argumen ts,withthesame logical structure ashis,thatareclearly wrong.
Forexample, thelawyercould say`NotonlywasMrs.Smurdered, shewas
murdered between4.02pm and4.03pm. Statistic ally,onlyoneinamillion
wife-b eaters actually goesontomurder hiswifebetween4.02pm and4.03pm.
Sothewife-b eating isnotstrong evidence atall.Infact,giventhewife-b eating
evidence alone, it'sextremely unlikelythathewouldmurder hiswifeinthis
way{onlya1/1,000,000 chance.'
Solution toexercise 3.13(p.58).There aretwohypotheses.H0:yournumber
is740511 ;H1:itisanother number.Thedata,D,are`when Idialed740511 ,
Igotabusysignal'. What istheprobabilit yofD,giveneachhypothesis? If
yournumberis740511 ,thenweexpectabusysignal withcertain ty:
P(DjH0)=1:
Ontheother hand, ifH1istrue,thentheprobabilit ythatthenumberdialled
returns abusysignal issmaller than1,sincevarious other outcomes werealso
possible (aringing tone, oranumber-unobtainable signal, forexample). The
valueofthisprobabilit yP(DjH1)willdependontheprobabilit ythata
random phone numbersimilar toyourownphone numberwouldbeavalid
phone number,andontheprobabilit ythatyougetabusysignal when you
dialavalidphone number.
Iestimate from thesizeofmyphone bookthatCambridge hasabout
75000validphone numbers,alloflength sixdigits. Theprobabilit ythata
random six-digit numberisvalidistherefore about75000=106=0:075. If
weexclude numbersbeginning with0,1,and9fromtherandom choice, the
probabilit yisabout75000=700000'0:1.Ifweassume thattelephone
numbersareclustered thenamisremem berednumbermightbemore likely
tobevalidthanarandomly chosen number;sotheprobabilit y,,thatour
guessed numberwouldbevalid,assumingH1istrue,mightbebigger than

<<<PAGE 75>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
3.6:Solutions 63
0.1.Anyway,mustbesomewhere between0.1and1.Wecancarry forward
thisuncertain tyintheprobabilit yandseehowmuchitmatters attheend.
Theprobabilit ythatyougetabusysignal when youdialavalidphone
numberisequal tothefraction ofphones youthink areinuseoro-the-ho ok
when youmakeyourtentativecall.Thisfraction variesfromtowntotown
andwiththetimeofday.InCambridge, during theday,Iwouldguess that
about1%ofphones areinuse.At4am,maybe0.1%, orfewer.
Theprobabilit yP(DjH1)istheproductofand,thatis,about0:1
0:01=10 3.According toourestimates, there's aboutaone-in-a-thousand
chance ofgetting abusysignal when youdialarandom number;orone-in-a-
hundred, ifvalidnumbersarestrongly clustered; orone-in-104,ifyoudialin
theweehours.
Howdothedataaect yourbeliefsaboutyourphone number?Thepos-
terior probabilit yratioisthelikelihoodratiotimes thepriorprobabilit yratio:
P(H0jD)
P(H1jD)=P(DjH0)
P(DjH1)P(H0)
P(H1): (3.43)
Thelikelihoodratioisabout100-to-1 or1000-to-1, sotheposterior probabilit y
ratioisswung byafactor of100or1000infavourofH0.Ifthepriorprobabilit y
ofH0was0.5thentheposterior probabilit yis
P(H0jD)=1
1+P(H1jD)
P(H0jD)'0:99or0:999: (3.44)
Solution toexercise 3.15(p.59).Wecompare themodelsH0{thecoinisfair
{andH1{thecoinisbiased, withtheprior onitsbiassettotheuniform
distribution P(pjH1)=1.[Theuseofauniform prior seems reasonable
00.010.020.030.040.05
050100150200250140H0
H1
Figure 3.10.Theprobabilit y
distribution ofthenumberof
heads giventhetwohypotheses,
thatthecoinisfair,andthatitis
biased, withthepriordistribution
ofthebiasbeinguniform. The
outcome (D=140heads) gives
weakevidence infavourofH0,the
hypothesis thatthecoinisfair.tome,since Iknowthatsome coins, suchasAmerican pennies, havesevere
biases when spunonedge; sothesituationsp=0:01orp=0:1orp=0:95
wouldnotsurprise me.]
When ImentionH0{thecoinisfair{apedantwouldsay,`howabsurd toeven
consider thatthecoinisfair{anycoinissurely biased tosome extent'.Andof
course Iwouldagree. Sowillpedantskindly understand H0asmeaning `thecoinis
fairtowithin onepartinathousand, i.e.,p20:50:001'.
Thelikelihoodratiois:
P(DjH1)
P(DjH0)=140!110!
251!
1=2250=0:48: (3.45)
Thusthedatagivescarcely anyevidence either way;infacttheygiveweak
evidence (twotoone)infavourofH0!
`No,no',objectsthebelieverinbias, `yoursillyuniform prior doesn't
represen tmypriorbeliefsaboutthebiasofbiased coins{Iwasexpectingonly
asmall bias'. Tobeasgenerous aspossible totheH1,let'sseehowwellit
could fareifthepriorwereprescien tlyset.Letusallowaprioroftheform
P(pjH1;)=1
Z()p 1(1 p) 1;whereZ()= ()2= (2)(3.46)
(aBeta distribution, withtheoriginal uniform prior reproduced bysetting
=1).Bytweaking,thelikelihoodratioforH1overH0,
P(DjH1;)
P(DjH0)= (140 +) (110 +) (2)2250
 (250 +2) ()2; (3.47)

<<<PAGE 76>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
64 3|More aboutInference
canbeincreased alittle. Itisshownforseveralvalues ofingure 3.11.
P(DjH1;)
P(DjH0)
.37 .25
1.0 .48
2.7 .82
7.4 1.3
20 1.8
55 1.9
148 1.7
403 1.3
1096 1.1
Figure 3.11.Likelihoodratiofor
various choices oftheprior
distribution's hyperparameter .Eventhemostfavourable choice of('50)canyieldalikelihoodratioof
onlytwotooneinfavourofH1.
Inconclusion, thedataarenot`verysuspicious'. They canbeconstrued
asgiving atmost two-to-one evidence infavourofoneorother ofthetwo
hypotheses.
Arethese wimpylikelihoodratios thefaultofover-restrictiv epriors? Isthere any
wayofproducing a`verysuspicious' conclusion? Theprior thatisbest-matc hedto
thedata, interms oflikelihood,istheprior thatsetsptof140=250withprob-
abilityone.Let's callthismodelH.ThelikelihoodratioisP(DjH)=P(DjH0)=
2250f140(1 f)110=6:1.Sothestrongest evidence thatthese datacanpossibly
muster against thehypothesis thatthere isnobiasissix-to-one.
While wearenoticing theabsurdly misleading answersthat`sampling the-
ory'statistics produces, suchasthep-valueof7%intheexercise wejustsolved,
let'sstickthebootin.Ifwemakeatinychange tothedataset,increasing
thenumberofheads in250tosses from140to141,wendthatthep-value
goesbelowthemystical valueof0.05(thep-valueis0.0497). Thesampling
theory statistician wouldhappily squeak `theprobabilit yofgetting aresult as
extreme as141heads issmaller than0.05{wethusreject thenullhypothesis
atasignicance levelof5%'.Thecorrect answerisshownforseveralvalues
ofingure 3.12. Thevalues worthhighligh tingfromthistable are,rst,
thelikelihoodratiowhenH1usesthestandard uniform prior, whichis1:0.61
infavourofthenullhypothesisH0.Second, themostfavourable choice of,
fromthepointofviewofH1,canonlyyieldalikelihoodratioofabout2.3:1
infavourofH1.
Bewarned! Ap-valueof0.05isofteninterpreted asimplying thattheodds
arestackedabouttwenty-to-one against thenullhypothesis. Butthetruth
inthiscaseisthattheevidence either slightlyfavours thenullhypothesis, or
disfavoursitbyatmost2.3toone,depending onthechoice ofprior.P(D0jH1;)
P(D0jH0)
.37 .32
1.0 .61
2.7 1.0
7.4 1.6
20 2.2
55 2.3
148 1.9
403 1.4
1096 1.2
Figure 3.12.Likelihoodratiofor
various choices oftheprior
distribution's hyperparameter ,
when thedataareD0=141heads
in250trials.Thep-valuesand`signicance levels'ofclassical statistics should betreated
withextremecaution .Shunthem! Hereendsthesermon.

<<<PAGE 77>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
PartI
DataCompression

<<<PAGE 78>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
AboutChapter 4
Inthischapter wediscuss howtomeasure theinformation contentofthe
outcome ofarandom experimen t.
Thischapter hassome tough bits. Ifyoundthemathematical details
hard, skimthrough them andkeepgoing {you'llbeabletoenjoyChapters 5
and6without thischapter's tools.
Notation
x2A xisamemberofthe
setA
SA Sisasubset ofthe
setA
SA Sisasubset of,or
equal to,thesetA
V=B[AVistheunion ofthe
setsBandA
V=B\AVistheintersection
ofthesetsBandA
jAj numberofelemen ts
insetABefore reading Chapter 4,youshould havereadChapter 2andworkedon
exercises 2.21{2.25 and2.16(pp.37{36) ,andexercise 4.1below.
Thefollowingexercise isintended tohelpyouthink abouthowtomeasure
information content.
Exercise 4.1.[2,p.69]{Pleaseworkonthisproblem beforereading Chapter 4.
Youaregiven12balls, allequal inweightexcept foronethatiseither
heavierorlighter.Youarealsogivenatwo-pan balance touse.Ineach
useofthebalance youmayputanynumberofthe12ballsontheleft
pan,andthesamenumberontherightpan,andpushabutton toinitiate
theweighing; there arethree possible outcomes: either theweightsare
equal, ortheballsontheleftareheavier,ortheballsontheleftare
lighter.Yourtaskistodesign astrategy todetermine whichistheodd
ballandwhether itisheavierorlighterthantheothers inasfewuses
ofthebalanceaspossible .
While thinking aboutthisproblem, youmayndithelpful toconsider
thefollowingquestions:
(a)Howcanonemeasure information ?
(b)When youhaveidentied theoddballandwhether itisheavyor
light,howmuchinformation haveyougained?
(c)Once youhavedesigned astrategy ,drawatreeshowing, foreach
ofthepossible outcomes ofaweighing, whatweighing youperform
next. Ateachnodeinthetree,howmuchinformation havethe
outcomes sofargivenyou,andhowmuchinformation remains to
begained?
(d)Howmuchinformation isgained when youlearn (i)thestateofa
ippedcoin;(ii)thestates oftwoippedcoins; (iii)theoutcome
when afour-sided dieisrolled?
(e)Howmuchinformation isgained ontherststepoftheweighing
problem if6ballsareweighed against theother 6?Howmuchis
gained if4areweighed against 4ontherststep, leavingout4
balls?
66

<<<PAGE 79>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
4
TheSource CodingTheorem
4.1Howtomeasure theinformation contentofarandom variable?
Inthenextfewchapters, we'llbetalking aboutprobabilit ydistributions and
random variables. Most ofthetimewecangetbywithsloppynotation,
butoccasionally ,wewillneedprecise notation. Hereisthenotation thatwe
established inChapter 2.
Anensem bleXisatriple (x;AX;PX),where theoutcomexisthevalue
ofarandom variable, whichtakesononeofasetofpossible values,
AX=fa1;a2;:::;ai;:::;aIg,havingprobabilitiesPX=fp1;p2;:::;pIg,
withP(x=ai)=pi,pi0andP
ai2AXP(x=ai)=1.
Howcanwemeasure theinformation contentofanoutcomex=aifromsuch
anensem ble?Inthischapter weexamine theassertions
1.thattheShannon information content,
h(x=ai)log21
pi; (4.1)
isasensible measure oftheinformation contentoftheoutcomex=ai,
and
2.thattheentropyoftheensem ble,
H(X)=X
ipilog21
pi; (4.2)
isasensible measure oftheensem ble'saverage information content.
0246810
00.20.40.60.81ph(p)=log21
pph(p)H2(p)
0.001 10.0 0.011
0.01 6.6 0.081
0.1 3.3 0.47
0.2 2.3 0.72
0.5 1.0 1.0H2(p)
00.20.40.60.81
00.20.40.60.81pFigure 4.1.TheShannon
information contenth(p)=log21
p
andthebinary entropyfunction
H2(p)=H(p;1 p)=
plog21
p+(1 p)log21
(1 p)asa
function ofp.
Figure 4.1showstheShannon information contentofanoutcome withprob-
abilityp,asafunction ofp.Thelessprobable anoutcome is,thegreater
itsShannon information content.Figure 4.1alsoshowsthebinary entropy
function,
H2(p)=H(p;1 p)=plog21
p+(1 p)log21
(1 p); (4.3)
whichistheentropyoftheensem bleXwhose alphab etandprobabilit ydis-
tribution areAX=fa;bg;PX=fp;(1 p)g.
67

<<<PAGE 80>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
68 4|TheSource CodingTheorem
Information content ofindependent random variables
Whyshould log1=pihaveanything todowiththeinformation content?Why
notsome other function ofpi?We'llexplore thisquestion indetail shortly ,
butrst,notice anicepropertyofthisparticular functionh(x)=log1=p(x).
Imagine learning thevalueoftwoindependent random variables,xandy.
Thedenition ofindependence isthattheprobabilit ydistribution isseparable
intoaproduct:
P(x;y)=P(x)P(y): (4.4)
Intuitively,wemightwantanymeasure ofthe`amoun tofinformation gained'
tohavethepropertyofadditivity {thatis,forindependen trandom variables
xandy,theinformation gained when welearnxandyshould equal thesum
oftheinformation gained ifxalone werelearned andtheinformation gained
ifyalone werelearned.
TheShannon information contentoftheoutcomex;yis
h(x;y)=log1
P(x;y)=log1
P(x)P(y)=log1
P(x)+log1
P(y)(4.5)
soitdoesindeed satisfy
h(x;y)=h(x)+h(y);ifxandyareindependen t. (4.6)
Exercise 4.2.[1]Showthat, ifxandyareindependen t,theentropyofthe
outcomex;ysatises
H(X;Y)=H(X)+H(Y): (4.7)
Inwords,entropyisadditiv eforindependen tvariables.
Wenowexplore these ideas withsome examples; then, insection 4.4and
inChapters 5and6,weprovethattheShannon information contentandthe
entropyarerelated tothenumberofbitsneeded todescrib etheoutcome of
anexperimen t.
Theweighing problem: designing informative experiments
Haveyousolvedtheweighing problem (exercise 4.1,p.66)yet?Areyousure?
Notice thatinthree usesofthebalance {whichreads either `leftheavier',
`rightheavier', or`balanced' {thenumberofconceiv ableoutcomes is33=27,
whereas thenumberofpossible states oftheworldis24:theoddballcould
beanyoftwelveballs, anditcould beheavyorlight.Soinprinciple, the
problem mightbesolvableinthree weighings {butnotintwo,since32<24.
Ifyouknowhowyoucandetermine theoddweightandwhether itis
heavyorlightinthreeweighings, thenyoumayreadon.Ifyouhaven'tfound
astrategy thatalwaysgetsthere inthree weighings, Iencourage youtothink
aboutexercise 4.1some more.
Whyisyourstrategy optimal? What isitaboutyourseries ofweighings
thatallowsuseful information tobegained asquicklyaspossible? Theanswer
isthatateachstepofanoptimal procedure, thethreeoutcomes (`leftheavier',
`rightheavier', and`balance') areascloseaspossible toequiprobable.An
optimal solution isshowningure 4.2.
Suboptimal strategies, suchasweighing balls1{6against 7{12ontherst
step,donotachievealloutcomes withequal probabilit y:thesetwosetsofballs
canneverbalance, sotheonlypossible outcomes are`leftheavy'and`right
heavy'.Suchabinary outcome onlyrulesouthalfofthepossible hypotheses,

<<<PAGE 81>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
4.1:Howtomeasure theinformation contentofarandom variable? 69
Figure 4.2.Anoptimal solution totheweighing problem. Ateachstepthere aretwoboxes:theleft
boxshowswhichhypotheses arestillpossible; therightboxshowstheballsinvolvedinthe
nextweighing. The24hypotheses arewritten 1+;:::;12 ,with, e.g.,1+denoting that
1istheoddballanditisheavy.Weighings arewritten bylisting thenames oftheballs
onthetwopans, separated byaline;forexample, intherstweighing, balls1,2,3,and
4areputontheleft-hand sideand5,6,7,and8ontheright.Ineachtriplet ofarrows
theupperarrowleads tothesituation when theleftsideisheavier,themiddle arrowto
thesituation when therightsideisheavier,andthelowerarrowtothesituation when the
outcome isbalanced. Thethree pointslabelled?corresp ondtoimpossible outcomes.1+
2+
3+
4+
5+
6+
7+
8+
9+
10+
11+
12+
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 1234
5678weigh

B
B
B
B
B
B
B
B
B
B
B
B
B
BBN-1+
2+
3+
4+
5 
6 
7 
8 126
345weigh
1 
2 
3 
4 
5+
6+
7+
8+126
345weigh
9+
10+
11+
12+
9 
10 
11 
12 91011
123weigh
A
A
A
AAU-

A
A
A
AAU-

A
A
A
AAU-1+2+5  1
2
3+4+6  3
4
7 8  1
7
6+3 4  3
4
1 2 5+ 1
2
7+8+ 7
1
9+10+11+ 9
10
9 10 11  9
10
12+12  12
1  
@@R-
  
@@R-
  
@@R-  
@@R-
  
@@R-
  
@@R-
  
@@R-
  
@@R-
  
@@R-1+
2+
5 
3+
4+
6 
7 
8 
?
4 
3 
6+
2 
1 
5+
7+
8+
?
9+
10+
11+
10 
9 
11 
12+
12 
?

<<<PAGE 82>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
70 4|TheSource CodingTheorem
soastrategy thatusessuchoutcomes mustsometimes takelonger tondthe
rightanswer.
Theinsigh tthattheoutcomes should beasnearaspossible toequiprobable
makesiteasier tosearchforanoptimal strategy .Therstweighing must
divide the24possible hypotheses intothree groups ofeight.Then thesecond
weighing mustbechosen sothatthere isa3:3:2splitofthehypotheses.
Thuswemightconclude:
theoutcome ofarandom experimen tisguaran teedtobemostin-
formativ eiftheprobabilit ydistribution overoutcomes isuniform.
Thisconclusion agrees withthepropertyoftheentropythatyouproved
when yousolvedexercise 2.25(p.37):theentropyofanensem bleXisbiggest
ifalltheoutcomes haveequal probabilit ypi=1=jAXj.
Guessing games
Inthegame oftwentyquestions, oneplayerthinks ofanobject,andthe
other playerattempts toguess what theobjectisbyasking questions that
haveyes/no answers,forexample, `isitalive?',or`isithuman?' Theaim
istoidentifytheobjectwithasfewquestions aspossible. What isthebest
strategy forplayingthisgame? Forsimplicit y,imagine thatweareplaying
therather dullversion oftwentyquestions called `sixty-three'.
Example 4.3.Thegame `sixty-three' .What's thesmallest numberofyes/no
questions needed toidentifyanintegerxbetween0and63?
Intuitively,thebestquestions successiv elydivide the64possibilities intoequal
sized sets. Sixquestions suce. Onereasonable strategy asksthefollowing
questions:
1:isx32?
2:isxmod3216?
3:isxmod168?
4:isxmod84?
5:isxmod42?
6:isxmod2=1?
[Thenotationxmod32,pronounced `xmodulo32',denotes theremainder
whenxisdivided by32;forexample, 35mod32=3and32mod32=0.]
Theanswerstothese questions, iftranslated fromfyes;nogtof1;0g,give
thebinary expansion ofx,forexample 35)100011 . 2
What aretheShannon information contentsoftheoutcomes inthisex-
ample? Ifweassume thatallvalues ofxareequally likely,thentheanswers
tothequestions areindependen tandeachhasShannon information content
log2(1=0:5)=1bit;thetotalShannon information gained isalwayssixbits.
Furthermore, thenumberxthatwelearnfromthese questions isasix-bit bi-
narynumber.Ourquestioning strategy denes awayofencodingtherandom
variablexasabinary le.
Sofar,theShannon information contentmakessense: itmeasures the
length ofabinary lethatencodesx.However,wehavenotyetstudied
ensem bleswhere theoutcomes haveunequal probabilities. DoestheShannon
information contentmakesense there too?

<<<PAGE 83>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
4.1:Howtomeasure theinformation contentofarandom variable? 71
A
B
C
D
E
F
G
H
87654321j j
j





j



jS
move# 1 2 32 48 49
question G3 B1 E5 F3 H3
outcome x=n x=n x=n x=n x=y
P(x)63
6462
6332
3316
171
16
h(x) 0.0227 0.0230 0.0443 0.0874 4.0
Totalinfo. 0.0227 0.0458 1.0 2.0 6.0
Figure 4.3.Agame ofsubmarine .
Thesubmarine ishitonthe49th
attempt.Thegameofsubmarine: howmany bitscanonebitconvey?
Inthegame ofbattleships, eachplayerhidesaeetofshipsinasearepresen ted
byasquare grid.Oneachturn,oneplayerattempts tohittheother's shipsby
ring atonesquare intheopponent'ssea.Theresponsetoaselected square
suchas`G3'iseither `miss', `hit',or`hitanddestro yed'.
Inaboring version ofbattleships calledsubmarine ,eachplayerhides just
onesubmarine inonesquare ofaneight-by-eigh tgrid.Figure 4.3showsafew
pictures ofthisgame inprogress: thecircle represen tsthesquare thatisbeing
redat,andthesshowsquares inwhichtheoutcome wasamiss,x=n;the
submarine ishit(outcomex=yshownbythesymbols)onthe49thattempt.
Eachshotmade byaplayerdenes anensem ble.Thetwopossible out-
comes arefy;ng,corresp onding toahitandamiss, andtheir probabili-
tiesdependonthestate oftheboard. Atthebeginning,P(y)=1=64and
P(n)=63=64.Atthesecond shot,iftherstshotmissed,P(y)=1=63and
P(n)=62=63.Atthethird shot, ifthersttwoshots missed,P(y)=1=62
andP(n)=61=62.
TheShannon information gained fromanoutcomexish(x)=log(1=P(x)).
Ifwearelucky,andhitthesubmarine ontherstshot,then
h(x)=h(1)(y)=log264=6bits: (4.8)
Now,itmightseem alittlestrange thatonebinary outcome canconveysix
bits. Butwehavelearntthehiding place, whichcould havebeenanyof64
squares; sowehave,byoneluckybinary question, indeed learntsixbits.
What iftherstshotmisses? TheShannon information thatwegainfrom
thisoutcome is
h(x)=h(1)(n)=log264
63=0:0227bits: (4.9)
Doesthismakesense? Itisnotsoobvious. Let's keepgoing. Ifoursecond
shotalsomisses, theShannon information contentofthesecond outcome is
h(2)(n)=log263
62=0:0230bits: (4.10)
Ifwemissthirty-twotimes (ring atanewsquare eachtime), thetotalShan-
noninformation gained is
log264
63+log263
62++log233
32
=0:0227+0:0230++0:0430 =1:0bits: (4.11)

<<<PAGE 84>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
72 4|TheSource CodingTheorem
Whythisround number?Well,whathavewelearnt?Wenowknowthatthe
submarine isnotinanyofthe32squares weredat;learning thatfactisjust
likeplayingagame ofsixty-three (p.70),asking asourrstquestion `isx
oneofthethirty-twonumberscorresp onding tothese squares Iredat?',and
receiving theanswer`no'.Thisanswerrulesouthalfofthehypotheses, soit
givesusonebit.
After 48unsuccessful shots, theinformation gained is2bits:theunkno wn
location hasbeennarroweddowntoonequarter oftheoriginal hypothesis
space.
What ifwehitthesubmarine onthe49thshot,when therewere16squares
left?TheShannon information contentofthisoutcome is
h(49)(y)=log216=4:0bits: (4.12)
ThetotalShannon information contentofalltheoutcomes is
log264
63+log263
62++log217
16+log216
1
=0:0227+0:0230++0:0874+4:0=6:0bits: (4.13)
Soonceweknowwhere thesubmarine is,thetotalShannon information con-
tentgained is6bits.
Thisresult holds regardless ofwhen wehitthesubmarine. Ifwehitit
when there arensquares lefttochoosefrom{nwas16inequation (4.13) {
thenthetotalinformation gained is:
log264
63+log263
62++log2n+1
n+log2n
1
=log264
6363
62n+1
nn
1
=log264
1=6bits:(4.14)
What havewelearned fromtheexamples sofar?Ithink thesubmarine
example makesquite aconvincing casefortheclaim thattheShannon infor-
mation contentisasensible measure ofinformation content.Andthegame of
sixty-three showsthattheShannon information contentcanbeintimately
connected tothesizeofalethatencodestheoutcomes ofarandom experi-
ment,thussuggesting apossible connection todatacompression.
Incaseyou're notconvinced, let'slookatonemore example.
TheWenglish language
Wenglish isalanguage similar toEnglish. Wenglish sentences consist ofwords
drawnatrandom fromtheWenglish dictionary ,whichcontains215=32,768
words,alloflength 5characters. EachwordintheWenglish dictionary was
constructed atrandom bypickingveletters fromtheprobabilit ydistribution
overa:::zdepicted ingure 2.1.1aaail
2aaaiu
3aaald
...
129abati
...
2047azpan
2048aztdn
...
...
16384odrcr
...
...
32737zatnt
...
32768zxast
Figure 4.4.TheWenglish
dictionary .
Some entriesfromthedictionary areshowninalphab etical order ing-
ure4.4. Notice thatthenumberofwordsinthedictionary (32,768) is
muchsmaller thanthetotal numberofpossible wordsoflength 5letters,
265'12,000,000.
Because theprobabilit yoftheletterzisabout1=1000, only32ofthe
wordsinthedictionary beginwiththeletterz.Incontrast, theprobabilit y
oftheletteraisabout0:0625, and2048ofthewordsbeginwiththelettera.
Ofthose 2048words,twostartaz,and128startaa.
Let'simagine thatwearereading aWenglish documen t,andlet'sdiscuss
theShannon information contentofthecharacters asweacquire them. Ifwe

<<<PAGE 85>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
4.2:Data compression 73
aregiventhetextonewordatatime, theShannon information contentof
eachve-character wordislog32,768 =15bits,since Wenglish usesallits
wordswithequal probabilit y.Theaverage information contentpercharacter
istherefore 3bits.
Nowlet'slookattheinformation contentifwereadthedocumen tone
character atatime. If,say,therstletter ofawordisa,theShannon
information contentislog1=0:0625'4bits.Iftherstletter isz,theShannon
information contentislog1=0:001'10bits.Theinformation contentisthus
highly variable attherstcharacter. Thetotalinformation contentofthe5
characters inaword,however,isexactly 15bits;sotheletters thatfollowan
initialzhaveloweraverage information contentpercharacter thantheletters
thatfollowaninitiala.Arareinitial letter suchaszindeed conveysmore
information aboutwhat thewordisthanacommon initial letter.
Similarly ,inEnglish, ifrarecharacters occuratthestartoftheword(e.g.
xyl... ),thenoften wecanidentifythewhole wordimmediately; whereas
wordsthatstartwithcommon characters (e.g.pro... )require more charac-
tersbeforewecanidentifythem.
4.2Datacompression
Thepreceding examples justify theideathattheShannon information content
ofanoutcome isanatural measure ofitsinformation content.Improbable out-
comes doconveymore information thanprobable outcomes. Wenowdiscuss
theinformation contentofasource byconsidering howmanybitsareneeded
todescrib etheoutcome ofanexperimen t.
Ifwecanshowthatwecancompress datafromaparticular source into
aleofLbitspersource symbolandrecoverthedatareliably ,thenwewill
saythattheaverage information contentofthatsource isatmostLbitsper
symbol.
Example: compression oftextles
Aleiscomposedofasequence ofbytes. Abyteiscomposedof8bitsand Here weusetheword`bit'withits
meaning, `asymbolwithtwovalues',
nottobeconfused withtheunitof
information content.canhaveadecimal valuebetween0and255.Atypical textleiscomposed
oftheASCIIcharacter set(decimal values 0to127). Thischaracter setuses
onlysevenoftheeightbitsinabyte.
.Exercise 4.4.[1,p.86]Byhowmuchcould thesizeofalebereduced given
thatitisanASCIIle?Howwouldyouachievethisreduction?
Intuitively,itseems reasonable toassert thatanASCIIlecontains 7=8as
muchinformation asanarbitrary leofthesame size,sincewealready know
oneoutofeveryeightbitsbeforeweevenlookatthele.Thisisasimple ex-
ample ofredundancy .Most sources ofdatahavefurther redundancy: English
textlesusetheASCIIcharacters withnon-equal frequency; certain pairsof
letters aremoreprobable thanothers; andentirewordscanbepredicted given
thecontextandaseman ticunderstanding ofthetext.
Somesimple datacompression methodsthatdene measuresofinforma-
tioncontent
Onewayofmeasuring theinformation contentofarandom variable issimply
tocountthenumberofpossible outcomes,jAXj.(Thenumberofelemen tsin
asetAisdenoted byjAj.)Ifwegaveabinary name toeachoutcome, the

<<<PAGE 86>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
74 4|TheSource CodingTheorem
length ofeachname wouldbelog2jAXjbits,ifjAXjhappenedtobeapower
of2.Wethusmakethefollowingdenition.
TherawbitcontentofXis
H0(X)=log2jAXj: (4.15)
H0(X)isalowerbound forthenumberofbinary questions thatarealways
guaran teedtoidentifyanoutcome fromtheensem bleX.Itisanadditiv e
quantity:therawbitcontentofanordered pairx;y,havingjAXjjAYjpossible
outcomes, satises
H0(X;Y)=H0(X)+H0(Y): (4.16)
Thismeasure ofinformation contentdoesnotinclude anyprobabilistic
elemen t,andtheencodingruleitcorresp ondstodoesnot`compress' thesource
data, itsimply maps eachoutcome toaconstan t-length binary string.
Exercise 4.5.[2,p.86]Could there beacompressor thatmaps anoutcomexto
abinary codec(x),andadecompressor thatmapscbacktox,such
thatevery possible outcomeiscompressed intoabinary codeoflength
shorter thanH0(X)bits?
Eventhough asimple countingargumen tshowsthatitisimpossible tomake
areversible compression program thatreduces thesizeofallles, ama-
teurcompression enthusiasts frequen tlyannounce thattheyhaveinvented
aprogram thatcandothis{indeed thattheycanfurther compress com-
pressed lesbyputting themthrough theircompressor severaltimes. Stranger
yet,patentshavebeengrantedtothese modern-da yalchemists. Seethe
comp.compression frequen tlyaskedquestions forfurther reading.1
There areonlytwowaysinwhicha`compressor' canactually compress
les:
1.Alossy compressor compresses some les,butmaps some lestothe
same encoding. We'llassume thattheuserrequires perfect recoveryof
thesource le,sotheoccurrence ofoneofthese confusable lesleads
toafailure (though inapplications suchasimage compression, lossy
compression isviewedassatisfactory). We'lldenote bytheprobabilit y
thatthesource string isoneoftheconfusable les,soalossycompressor
hasaprobabilit yoffailure. Ifcanbemade verysmall thenalossy
compressor maybepractically useful.
2.Alossless compressor maps alllestodieren tencodings; ifitshortens
some les,itnecessarily makes others longer .Wetrytodesign the
compressor sothattheprobabilit ythataleislengthened isverysmall,
andtheprobabilit ythatitisshortened islarge.
Inthischapter wediscuss asimple lossycompressor. Insubsequen tchapters
wediscuss lossless compression metho ds.
4.3Information contentdened interms oflossycompression
Whichevertypeofcompressor weconstruct, weneedsomeho wtotakeinto
accoun ttheprobabilities ofthedieren toutcomes. Imagine comparing the
information contentsoftwotextles{oneinwhichall128ASCIIcharacters
1http://sunsite.org.uk/public/u senet/news-faqs/comp.compression/

<<<PAGE 87>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
4.3:Information contentdened interms oflossycompression 75
areusedwithequal probabilit y,andoneinwhichthecharacters areusedwith
their frequencies inEnglish text. Canwedene ameasure ofinformation
contentthatdistinguishes betweenthese twoles? Intuitively,thelatter le
containslessinformation percharacter because itismore predictable.
Onesimple waytouseourknowledge thatsome symbolshaveasmaller
probabilit yistoimagine recodingtheobserv ations intoasmaller alphab et
{thuslosing theabilitytoencodesome ofthemore improbable symbols{
andthenmeasuring therawbitcontentofthenewalphab et.Forexample,
wemighttakeariskwhen compressing English text,guessing thatthemost
infrequen tcharacters won'toccur,andmakeareduced ASCIIcodethatomits
thecharactersf!,@,#,%,^,*,~,<,>,/,\,_,{,},[,],|g,thereb yreducing
thesizeofthealphab etbyseventeen. Thelarger theriskwearewilling to
take,thesmaller ournalalphab etbecomes.
Weintroduceaparameterthatdescrib estheriskwearetaking when
using thiscompression metho d:istheprobabilit ythatthere willbeno
name foranoutcomex.
Example 4.6.Let
AX=fa;b;c;d;e;f;g;hg;
andPX=f1
4;1
4;1
4;3
16;1
64;1
64;1
64;1
64g:(4.17)
Therawbitcontentofthisensem bleis3bits,corresp onding to8binary
names. Butnotice thatP(x2fa;b;c;dg)=15=16.Soifwearewilling
torunariskof=1=16ofnothavinganame forx,thenwecanget
bywithfournames {halfasmanynames asareneeded ifeveryx2AX
hasaname.
Table4.5showsbinary names thatcould begiventothedieren tout-
comes inthecases=0and=1=16.When=0weneed3bitsto
encodetheoutcome; when=1=16weonlyneed2bits.=0
xc(x)
a000
b001
c010
d011
e100
f101
g110
h111=1=16
xc(x)
a00
b01
c10
d11
e 
f 
g 
h 
Table4.5.Binary names forthe
outcomes, fortwofailure
probabilities .
Letusnowformalize thisidea. Tomakeacompression strategy withrisk
,wemakethesmallest possible subsetSsuchthattheprobabilit ythatxis
notinSislessthanorequal to,i.e.,P(x62S).Foreachvalueof
wecanthendene anewmeasure ofinformation content{thelogofthesize
ofthissmallest subsetS.[Inensem blesinwhichseveralelemen tshavethe
same probabilit y,there maybeseveralsmallest subsets thatcontaindieren t
elemen ts,butallthatmatters istheirsizes(whichareequal), sowewillnot
dwellonthisambiguit y.]
Thesmallest-sucien tsubsetSisthesmallest subset ofAXsatisfying
P(x2S)1 : (4.18)
ThesubsetScanbeconstructed byranking theelemen tsofAXinorder of
decreasing probabilit yandadding successiv eelemen tsstarting fromthemost
probable elemen tsuntilthetotalprobabilit yis(1 ).
Wecanmakeadatacompression codebyassigning abinary name toeach
elemen tofthesmallest sucien tsubset. Thiscompression scheme motivates
thefollowingmeasure ofinformation content:
TheessentialbitcontentofXis:
H(X)=log2jSj (4.19)
NotethatH0(X)isthespecialcaseofH(X)with=0(ifP(x)>0forall
x2AX).[Caution: donotconfuseH0(X)andH(X)withthefunctionH2(p)
displa yedingure 4.1.]
Figure 4.6showsH(X)fortheensem bleofexample 4.6asafunction of
.

<<<PAGE 88>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
76 4|TheSource CodingTheorem
(a)-log2P(x) 2  2:4  4  6
S0S1
16
a,b,cd e,f,g,h66 6
(b)H(X)
00.511.522.53
00.10.20.30.40.50.60.70.80.9{a,b}{a,b,c}{a,b,c,d}{a,b,c,d,e}{a,b,c,d,e,f}
{a}{a,b,c,d,e,f,g}{a,b,c,d,e,f,g,h}
Figure 4.6.(a)Theoutcomes ofX
(from example 4.6(p.75)),ranked
bytheirprobabilit y.(b)The
essentialbitcontentH(X).The
labelsonthegraph showthe
smallest sucien tsetasa
function of.NoteH0(X)=3
bitsandH1=16(X)=2bits.
Extende densembles
Isthiscompression metho danymore useful ifwecompress blocksofsymbols
fromasource?
Wenowturntoexamples where theoutcome x=(x1;x2;:::;xN)isa
string ofNindependen tidentically distributed random variables fromasingle
ensem bleX.Wewilldenote byXNtheensem ble(X1;X2;:::;XN).Remem-
berthatentropyisadditiv eforindependen tvariables (exercise 4.2(p.68)),so
H(XN)=NH(X).
Example 4.7.Consider astring ofNipsofabentcoin,x=(x1;x2;:::;xN),
wherexn2f0;1g,withprobabilities p0=0:9;p1=0:1.Themostprob-
ablestrings xarethose withmost0s.Ifr(x)isthenumberof1sinx
then
P(x)=pN r(x)
0pr(x)
1: (4.20)
ToevaluateH(XN)wemustndthesmallest sucien tsubsetS.This
subset willcontainallxwithr(x)=0;1;2;:::;uptosomermax() 1,
andsomeofthexwithr(x)=rmax().Figures 4.7and4.8showgraphs
ofH(XN)againstforthecasesN=4andN=10.Thestepsarethe
values ofatwhichjSjchanges by1,andthecusps where theslopeof
thestaircase changes arethepointswherermaxchanges by1.
Exercise 4.8.[2,p.86]What arethemathematical shapesofthecurvesbetween
thecusps?
Fortheexamples showningures 4.6{4.8,H(XN)dependsstrongly on
thevalueof,soitmightnotseem afundamen taloruseful denition of
information content.Butwewillconsider what happensasN,thenumber
ofindependen tvariables inXN,increases. Wewillndtheremark ableresult
thatH(XN)becomes almost independen tof{andforallitisveryclose
toNH(X),whereH(X)istheentropyofoneoftherandom variables.
Figure 4.9illustrates thisasymptotic tendency forthebinary ensem bleof
example 4.7.AsNincreases,1
NH(XN)becomes anincreasingly atfunction,

<<<PAGE 89>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
4.3:Information contentdened interms oflossycompression 77
(a)-log2P(x)
0 2 4 6 8 10 12 14
S0:01 S0:1
0000 0010;0001;::: 0110;1010;::: 1101;1011;::: 111166666
(b)H(X4)
00.511.522.533.54
00.050.10.150.20.250.30.350.4N=4
Figure 4.7.(a)Thesixteen
outcomes oftheensem bleX4with
p1=0:1,rankedbyprobabilit y.
(b)Theessentialbitcontent
H(X4).Theupperschematic
diagram indicates thestrings's
probabilities bythevertical lines's
lengths (nottoscale).
H(X10)
0246810
0 0.2 0.4 0.6 0.8 1N=10
Figure 4.8.H(XN)forN=10
binary variables withp1=0:1.
1
NH(XN)
00.20.40.60.81
0 0.2 0.4 0.6 0.8 1N=10
N=210
N=410
N=610
N=810
N=1010
Figure 4.9.1
NH(XN)for
N=10;210;:::;1010binary
variables withp1=0:1.

<<<PAGE 90>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
78 4|TheSource CodingTheorem
x log2(P(x))
...1.......... .........1.....1....1.1.......1........1...........1.....................1.......11... 50.1
.............. ........1.....1.....1.......1....1.........1.....................................1.... 37.3
........1....1 ..1...1....11..1.1.........11.........................1...1.1..1...1................1. 65.9
1.1...1....... .........1.......................11.1..1............................1.....1..1.11..... 56.4
...11......... ..1...1.....1.1......1..........1....1...1.....1............1......................... 53.2
.............. 1......1.........1.1.......1..........1............1...1......................1....... 43.7
.....1........ 1.......1...1............1............1...........1......1..11........................ 46.8
.....1..1..1.. .............111...................1...............1.........1.1...1...1.............1 56.4
.........1.... ......1.....1......1..........1....1..............................................1... 37.3
......1....... .................1..............1.....1..1.1.1..1...................................1. 43.7
1............. ..........1..........1...1...................1....1....1........1..11..1.1...1........ 56.4
...........11. 1.........1................1......1.....................1............................. 37.3
.1..........1. ..1.1.............1.......11...........1.1...1..............1.............11.......... 56.4
......1...1..1 .....1..11.1.1.1...1.....................1............1.............1..1.............. 59.5
............11 .1......1....1..1............................1.......1..............1.......1......... 46.8
.............. ...................................................................................... 15.2
11111111111111 11111111111111111111111111111111111111111111111111111111111111111111111111111111111111 332.1Figure 4.10.Thetop15strings
aresamples fromX100,where
p1=0:1andp0=0:9.The
bottom twoarethemostand
leastprobable strings inthis
ensem ble.Thenalcolumn shows
thelog-probabilities ofthe
random strings, whichmaybe
compared withtheentropy
H(X100)=46:9bits.
except fortailsclose to=0and1.Aslongasweareallowedatiny
probabilit yoferror,compression downtoNHbitsispossible. Evenifwe
areallowedalarge probabilit yoferror, westillcancompress onlydownto
NHbits.Thisisthesource codingtheorem.
Theorem 4.1Shannon's source codingtheorem.LetXbeanensemble with
entropyH(X)=Hbits.Given>0and0<<1,thereexists apositive
integerN0suchthatforN>N0,
1
NH(XN) H<: (4.21)
4.4Typicalit y
WhydoesincreasingNhelp? Let'sexamine longstrings fromXN.Table4.10
showsfteen samples fromXNforN=100andp1=0:1.Theprobabilit y
ofastringxthatcontainsr1sandN r0sis
P(x)=pr
1(1 p1)N r: (4.22)
Thenumberofstrings thatcontainr1sis
n(r)= 
N
r!
: (4.23)
Sothenumberof1s,r,hasabinomial distribution:
P(r)= 
N
r!
pr
1(1 p1)N r: (4.24)
These functions areshowningure 4.11. Themean ofrisNp1,andits
standard deviation isp
Np1(1 p1)(p.1).IfNis100then
rNp1q
Np1(1 p1)'103: (4.25)

<<<PAGE 91>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
4.4:Typicalit y 79
Figure 4.11.Anatom yofthetypical setT.Forp1=0:1andN=100andN=1000, these graphs
shown(r),thenumberofstrings containingr1s;theprobabilit yP(x)ofasingle string
thatcontainsr1s;thesameprobabilit yonalogscale; andthetotalprobabilit yn(r)P(x)of
allstrings thatcontainr1s.Thenumberrisonthehorizon talaxis.Theplotoflog2P(x)
alsoshowsbyadotted linethemean valueoflog2P(x)= NH2(p1)whichequals 46:9
whenN=100and 469whenN=1000. Thetypical setincludes onlythestrings that
havelog2P(x)closetothisvalue. Therange markedTshowsthesetTN(asdened in
section 4.4)forN=100and=0:29(left) andN=1000,=0:09(right).N=100 N=1000
n(r)= N
r
02e+284e+286e+288e+281e+291.2e+29
010203040506070809010005e+2981e+2991.5e+2992e+2992.5e+2993e+299
01002003004005006007008009001000
P(x)=pr
1(1 p1)N r
01e-052e-05
010203040506070809010001e-052e-05
012345
log2P(x)
-350-300-250-200-150-100-500
0102030405060708090100T
-3500-3000-2500-2000-1500-1000-5000
01002003004005006007008009001000T
n(r)P(x)= N
r
pr
1(1 p1)N r
00.020.040.060.080.10.120.14
010203040506070809010000.0050.010.0150.020.0250.030.0350.040.045
01002003004005006007008009001000
r r

<<<PAGE 92>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
80 4|TheSource CodingTheorem
IfN=1000then
r10010: (4.26)
Notice thatasNgetsbigger, theprobabilit ydistribution ofrbecomes more
concen trated, inthesense thatwhile therange ofpossible values ofrgrows
asN,thestandard deviation ofronlygrowsasp
N.Thatrismostlikelyto
fallinasmall range ofvaluesimplies thattheoutcome xisalsomostlikelyto
fallinacorresp onding small subset ofoutcomes thatwewillcallthetypical
set.
Denition ofthetypicalset
Letusdene typicalit yforanarbitrary ensem bleXwithalphab etAX.Our
denition ofatypical string willinvolvethestring's probabilit y.Alongstring
ofNsymbolswillusually containaboutp1Noccurrences oftherstsymbol,
p2Noccurrences ofthesecond, etc.Hence theprobabilit yofthisstring is
roughly
P(x)typ=P(x1)P(x2)P(x3):::P(xN)'p(p1N)
1p(p2N)
2:::p(pIN)
I (4.27)
sothattheinformation contentofatypical string is
log21
P(x)'NX
ipilog21
pi'NH: (4.28)
Sotherandom variable log21/P(x),whichistheinformation contentofx,is
verylikelytobecloseinvaluetoNH.Webuild ourdenition oftypicalit y
onthisobserv ation.
Wedene thetypical elemen tsofAN
Xtobethose elemen tsthathaveprob-
abilitycloseto2 NH.(Note thatthetypical set,unlikethesmallest sucien t
subset, doesnotinclude themostprobable elemen tsofAN
X,butwewillshow
thatthese mostprobable elemen tscontribute negligible probabilit y.)
Weintroduceaparameterthatdenes howclosetheprobabilit yhasto
beto2 NHforanelemen ttobe`typical'. Wecallthesetoftypical elemen ts
thetypical set,TN:
TN
x2AN
X:1
Nlog21
P(x) H<
: (4.29)
Wewillshowthatwhatev ervalueofwechoose,thetypical setcontains
almost alltheprobabilit yasNincreases.
Thisimportantresult issometimes called the`asymptotic equipartition'
principle .
`Asymptotic equipartition' principle .Foranensem bleofNindependen t
identically distributed (i.i.d.) random variablesXN(X1;X2;:::;XN),
withNsucien tlylarge, theoutcome x=(x1;x2;:::;xN)isalmost
certain tobelong toasubset ofAN
Xhavingonly2NH(X)members,each
havingprobabilit y`close to'2 NH(X).
Notice thatifH(X)<H0(X)then2NH(X)isatinyfraction ofthenumber
ofpossible outcomesjAN
Xj=jAXjN=2NH0(X):
Thetermequipartition ischosen todescrib etheideathatthemembersofthetypical
sethaveroughly equalprobabilit y.[This should notbetakentooliterally ,hence my
useofquotes around `asymptotic equipartition'; seepage83.]
Asecond meaning forequipartition, inthermal physics, istheideathateachdegree
offreedom ofaclassical system hasequal average energy ,1
2kT.Thissecond meaning
isnotintended here.

<<<PAGE 93>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
4.5:Proofs 81
-log2P(x)
 NH(X)
TN
6666 6
0000000000000 ...000000000000001000000000 ...000000000000100000001000 ...000100000000000100000010 ...000010000101111111111110 ...11111110111
Figure 4.12.Schematic diagram
showingallstrings intheensem ble
XNrankedbytheirprobabilit y,
andthetypical setTN.The`asymptotic equipartition' principle isequivalentto:
Shannon's source codingtheorem (verbal statemen t).Ni.i.d. ran-
domvariables eachwithentropyH(X)canbecompressed intomore
thanNH(X)bitswithnegligible riskofinformation loss,asN!1;
conversely iftheyarecompressed intofewerthanNH(X)bitsitisvir-
tually certain thatinformation willbelost.
These twotheorems areequivalentbecause wecandene acompression algo-
rithm thatgivesadistinct name oflengthNH(X)bitstoeachxinthetypical
set.
4.5Proofs
Thissection maybeskippediffound tough going.
Thelawoflargenumbers
Ourproofofthesource codingtheorem usesthelawoflargenumbers.
Mean andvariance ofarealrandom variable areE[u]=u=P
uP(u)u
andvar(u)=2
u=E[(u u)2]=P
uP(u)(u u)2:
Technical note: strictly Iamassuming herethatuisafunction u(x)ofasample
xfromanitediscrete ensem bleX.Then thesummationsP
uP(u)f(u)should
bewrittenP
xP(x)f(u(x)).Thismeans thatP(u)isanite sumofdelta
functions. Thisrestriction guaran teesthatthemean andvariance ofudoexist,
whichisnotnecessarily thecaseforgeneral P(u).
Chebyshev's inequalit y1.Lettbeanon-negativ erealrandom variable,
andletbeapositiverealnumber.Then
P(t)t
: (4.30)
Proof:P(t)=P
tP(t).Wemultiply eachtermbyt=1and
obtain:P(t)P
tP(t)t=:Weaddthe(non-negativ e)missing
terms andobtain:P(t)P
tP(t)t==t=. 2

<<<PAGE 94>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
82 4|TheSource CodingTheorem
Chebyshev's inequalit y2.Letxbearandom variable, andletbea
positiverealnumber.Then
P
(x x)2
2
x=: (4.31)
Proof:Taket=(x x)2andapply theprevious proposition. 2
Weaklawoflargenumbers.Takextobetheaverage ofNindependen t
random variablesh1;:::;hN,havingcommon mean handcommon vari-
ance2
h:x=1
NPN
n=1hn.Then
P((x h)2)2
h=N: (4.32)
Proof:obtained byshowingthatx=handthat2
x=2
h=N. 2
Weareinterested inxbeingveryclosetothemean (verysmall). Nomatter
howlarge2
his,andnomatter howsmall therequiredis,andnomatter
howsmall thedesired probabilit ythat(x h)2,wecanalwaysachieveit
bytakingNlargeenough.
Proofoftheorem4.1(p.78)
Weapply thelawoflargenumberstotherandom variable1
Nlog21
P(x)dened
forxdrawnfromtheensem bleXN.Thisrandom variable canbewritten as
theaverage ofNinformation contentshn=log2(1=P(xn)),eachofwhichisa
random variable withmeanH=H(X)andvariance2var[log2(1=P(xn))].
(EachtermhnistheShannon information contentofthenthoutcome.)
Weagain dene thetypical setwithparameters Nandthus:
TN=(
x2AN
X:1
Nlog21
P(x) H2
<2)
: (4.33)
Forallx2TN,theprobabilit yofxsatises
2 N(H+)<P(x)<2 N(H ): (4.34)
Andbythelawoflargenumbers,
P(x2TN)1 2
2N: (4.35)
Wehavethusprovedthe`asymptotic equipartition' principle. AsNincreases,
theprobabilit ythatxfallsinTNapproac hes1,forany.Howdoesthis
result relate tosource coding?
WemustrelateTNtoH(XN).Wewillshowthatforanygiventhere
isasucien tlybigNsuchthatH(XN)'NH.
Part1:1
NH(XN)<H+.
ThesetTNisnotthebestsubset forcompression. SothesizeofTNgives
anupperboundonH.WeshowhowsmallH(XN)mustbebycalculating
howbigTNcould possibly be.Wearefreetosettoanyconvenientvalue.
Thesmallest possible probabilit ythatamemberofTNcanhaveis2 N(H+),
andthetotalprobabilit ythatTNcontainscan't beanybigger than1.So
jTNj2 N(H+)<1; (4.36)
thatis,thesizeofthetypical setisbounded by
jTNj<2N(H+): (4.37)
Ifweset=andN0suchthat2
2N,thenP(TN)1 ,andtheset
TNbecomes awitness tothefactthatH(XN)log2jTNj<N(H+).1
NH(XN)
H0(X)
0 1H HH+
Figure 4.13.Schematic illustration
ofthetwoparts ofthetheorem.
Givenanyand,weshowthat
forlargeenoughN,1
NH(XN)
lies(1)belowthelineH+and
(2)abovethelineH .

<<<PAGE 95>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
4.6:Commen ts 83
Part2:1
NH(XN)>H .
Imagine thatsomeone claims thissecond partisnotso{that, foranyN,
thesmallest-sucien tsubsetSissmaller thantheaboveinequalit ywould
allow.Wecanmakeuseofourtypical settoshowthattheymustbemistak en.
Remem berthatwearefreetosettoanyvaluewechoose.Wewillset
==2,sothatourtaskistoprovethatasubsetS0havingjS0j2N(H 2)
andachievingP(x2S0)1 cannot exist(forNgreater thananN0that
wewillspecify).
So,letusconsider theprobabilit yoffalling inthisrivalsmaller subsetS0.
Theprobabilit yofthesubsetS0isTN S0
&%'$
&%'$
CCCO
S0\TN@@IS0\TNP(x2S0)=P(x2S0\TN)+P(x2S0\TN); (4.38)
whereTNdenotes thecomplemen tfx62TNg.Themaxim umvalueof
thersttermisfound ifS0\TNcontains 2N(H 2)outcomes allwiththe
maxim umprobabilit y,2 N(H ).Themaxim umvaluethesecond termcan
haveisP(x62TN).So:
P(x2S0)2N(H 2)2 N(H )+2
2N=2 N+2
2N: (4.39)
Wecannowset==2andN0suchthatP(x2S0)<1 ,whichshows
thatS0cannot satisfy thedenition ofasucien tsubsetS.Thusanysubset
S0withsizejS0j2N(H )hasprobabilit ylessthan1 ,sobythedenition
ofH,H(XN)>N(H ).
ThusforlargeenoughN,thefunction1
NH(XN)isessentially aconstan t
function of,for0<<1,asillustrated ingures 4.9and4.13. 2
4.6Commen ts
Thesource codingtheorem (p.78)hastwoparts,1
NH(XN)<H+,and
1
NH(XN)>H .Bothresults areinteresting.
Therstparttellsusthateveniftheprobabilit yoferrorisextremely
small, thenumberofbitspersymbol1
NH(XN)needed tospecifyalong
N-symbolstring xwithvanishingly small errorprobabilit ydoesnothaveto
exceedH+bits.Weneedtohaveonlyatinytolerance forerror, andthe
numberofbitsrequired drops signican tlyfromH0(X)to(H+).
What happensifweareyetmore toleran ttocompression errors? Part2
tellsusthatevenifisverycloseto1,sothaterrors aremade mostofthe
time, theaverage numberofbitspersymbolneeded tospecifyxmuststillbe
atleastH bits.These twoextremes tellusthatregardless ofourspecic
allowanceforerror, thenumberofbitspersymbolneeded tospecifyxisH
bits;nomore andnoless.
Caveatregarding`asymptotic equipartition '
Iputthewords`asymptotic equipartition' inquotes because itisimportant
nottothink thattheelemen tsofthetypical setTNreally dohaveroughly
thesame probabilit yaseachother. They aresimilar inprobabilit yonlyin
thesense thattheirvaluesoflog21
P(x)arewithin 2Nofeachother. Now,as
isdecreased, howdoesNhavetoincrease, ifwearetokeepourbound on
themassofthetypical set,P(x2TN)1 2
2N,constan t?Nmustgrow
as1=2,so,ifwewriteinterms ofNas=p
N,forsome constan t,then

<<<PAGE 96>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
84 4|TheSource CodingTheorem
themostprobable string inthetypical setwillbeoforder 2p
Ntimes greater
thantheleastprobable string inthetypical set.Asdecreases,Nincreases,
andthisratio2p
Ngrowsexponentially.Thuswehave`equipartition' onlyin
aweaksense!
Whydidweintroducethetypicalset?
Thebestchoice ofsubset forblockcompression is(bydenition) S,nota
typical set.Sowhydidwebother introducing thetypical set?Theansweris,
wecancountthetypicalset.Weknowthatallitselemen tshave`almost iden-
tical' probabilit y(2 NH),andweknowthewhole sethasprobabilit yalmost
1,sothetypical setmusthaveroughly 2NHelemen ts.Without thehelpof
thetypical set(whichisverysimilar toS)itwouldhavebeenhardtocount
howmanyelemen tsthere areinS.
4.7Exercises
Weighing problems
.Exercise 4.9.[1]While some people, when theyrstencoun tertheweighing
problem with 12balls andthethree-outcome balance (exercise 4.1
(p.66)),think thatweighing sixballsagainst sixballsisagoodrst
weighing, others say`no,weighing sixagainst sixconveysnoinforma-
tionatall'.Explain tothesecond group whytheyarebothrightand
wrong. Compute theinformation gained aboutwhich istheoddball,
andtheinformation gained aboutwhich istheoddballandwhether itis
heavyorlight.
.Exercise 4.10.[2]Solvetheweighing problem forthecasewhere there are39
ballsofwhichoneisknowntobeodd.
.Exercise 4.11.[2]Youaregiven16balls, allofwhichareequal inweightexcept
foronethatiseither heavierorlighter.Youarealsogivenabizarre two-
panbalance thatcanreportonlytwooutcomes: `thetwosidesbalance'
or`thetwosidesdonotbalance'. Design astrategy todetermine which
istheoddballinasfewusesofthebalance aspossible.
.Exercise 4.12.[2]Youhaveatwo-pan balance; yourjobistoweighoutbagsof
ourwithinteger weights1to40pounds inclusiv e.Howmanyweights
doyouneed? [Youareallowedtoputweightsoneither pan.You'reonly
allowedtoputoneourbagonthebalance atatime.]
Exercise 4.13.[4,p.86](a)Isitpossible tosolveexercise 4.1(p.66)(theweigh-
ingproblem with12ballsandthethree-outcome balance) using a
sequence ofthreexedweighings, suchthattheballschosen forthe
second weighing donotdependontheoutcome oftherst,andthe
thirdweighing doesnotdependontherstorsecond?
(b)Findasolution tothegeneralN-ballweighing problem inwhich
exactly oneofNballsisodd.ShowthatinWweighings, anodd
ballcanbeidentied fromamongN=(3W 3)=2balls.
Exercise 4.14.[3]Youaregiven12ballsandthethree-outcome balance ofexer-
cise4.1;thistime,twooftheballsareodd;eachoddballmaybeheavy
orlight,andwedon't knowwhich.Wewanttoidentifytheoddballs
andinwhichdirection theyareodd.

<<<PAGE 97>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
4.7:Exercises 85
(a)Estimate howmanyweighings arerequired bytheoptimal strategy .
Andwhat ifthere arethree oddballs?
(b)Howdoyouranswerschange ifitisknownthatalltheregular balls
weigh100g,thatlightballsweigh99g,andheavyonesweigh110g?
Sourcecodingwithalossycompressor, withloss
.Exercise 4.15.[2,p.87]LetPX=f0:2;0:8g.Sketch1
NH(XN)asafunction of
forN=1;2and1000.
.Exercise 4.16.[2]LetPY=f0:5;0:5g.Sketch1
NH(YN)asafunction offor
N=1;2;3and100.
.Exercise 4.17.[2,p.87](Forphysicsstuden ts.)Discuss therelationship between
theproofofthe`asymptotic equipartition' principle andtheequivalence
(forlargesystems) oftheBoltzmann entropyandtheGibbs entropy.
Distributions thatdon'tobeythelawoflargenumbers
Thelawoflargenumbers,whichweusedinthischapter, showsthatthemean
ofasetofNi.i.d.random variables hasaprobabilit ydistribution thatbecomes
narrower,withwidth/1=p
N,asNincreases. However,wehaveproved
thispropertyonlyfordiscrete random variables, thatis,forrealnumbers
taking onanite setofpossible values. While manyrandom variables with
continuousprobabilit ydistributions alsosatisfy thelawoflargenumbers,there
areimportantdistributions thatdonot.Some continuousdistributions donot
haveamean orvariance.
.Exercise 4.18.[3,p.88]SketchtheCauchydistribution
P(x)=1
Z1
x2+1;x2( 1;1): (4.40)
What isitsnormalizing constan tZ?Canyouevaluate itsmean or
variance?
Consider thesumz=x1+x2,wherex1andx2areindependen trandom
variables fromaCauchydistribution. What isP(z)?What istheprob-
abilitydistribution ofthemean ofx1andx2,x=(x1+x2)=2?What is
theprobabilit ydistribution ofthemean ofNsamples fromthisCauchy
distribution?
Other asymptotic properties
Exercise 4.19.[3]Cherno bound. Wederivedtheweaklawoflarge numbers
fromChebyshev's inequalit y(4.30) byletting therandom variabletin
theinequalit yP(t)t=beafunction,t=(x x)2,oftherandom
variablexwewereinterested in.
Other useful inequalities canbeobtained byusing other functions. The
Cherno bound, whichisuseful forbounding thetailsofadistribution,
isobtained bylettingt=exp(sx).
Showthat
P(xa)e sag(s);foranys>0 (4.41)
and
P(xa)e sag(s);foranys<0 (4.42)

<<<PAGE 98>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
86 4|TheSource CodingTheorem
whereg(s)isthemomen t-generating function ofx,
g(s)=X
xP(x)esx: (4.43)
Curious functions relatedtoplog1=p
Exercise 4.20.[4,p.89]Thisexercise hasnopurposeatall;it'sincluded forthe
enjoymentofthose wholikemathematical curiousities.
Sketchthefunction
f(x)=xxxxx
(4.44)
forx0.Hint:Workouttheinversefunction tof{thatis,thefunction
g(y)suchthatifx=g(y)theny=f(x){it'sclosely related toplog1=p.
4.8Solutions
Solution toexercise 4.2(p.68).LetP(x;y)=P(x)P(y).Then
H(X;Y)=X
xyP(x)P(y)log1
P(x)P(y)(4.45)
=X
xyP(x)P(y)log1
P(x)+X
xyP(x)P(y)log1
P(y)(4.46)
=X
xP(x)log1
P(x)+X
yP(y)log1
P(y)(4.47)
=H(X)+H(Y): (4.48)
Solution toexercise 4.4(p.73).AnASCIIlecanbereduced insizebya
factor of7/8.Thisreduction could beachievedbyablockcodethatmaps
8-byteblocksinto7-byteblocksbycopyingthe56information-carrying bits
into7bytes,andignoring thelastbitofeverycharacter.
Solution toexercise 4.5(p.74).Thepigeon-hole principle states: youcan't
put16pigeons into15holes without using oneoftheholes twice.
Similarly ,youcan'tgiveAXoutcomes unique binary names ofsomelength
lshorter thanlog2jAXjbits,because there areonly2lsuchbinary names,
andl<log2jAXjimplies 2l<jAXj,soatleasttwodieren tinputs tothe
compressor wouldcompress tothesame output le.
Solution toexercise 4.8(p.76).Betweenthecusps, allthechanges inproba-
bilityareequal, andthenumberofelemen tsinTchanges byoneateachstep.
SoHvarieslogarithmically with( ).
Solution toexercise 4.13(p.84).Thissolution wasfound byDyson andLyness
in1946andpresen tedinthefollowingelegan tformbyJohnConwayin1999.
Bewarned: thesymbolsA,B,andCareusedtoname theballs, toname the
pansofthebalance, toname theoutcomes, andtoname thepossible states
oftheoddball!
(a)Labelthe12ballsbythesequences
AABABAABBABCBBCBCABCBBCCCAACABCACCCA
andinthe

<<<PAGE 99>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
4.8:Solutions 87
1st AABABAABBABC BBCBCABCBBCC
2ndweighings putAABCAACABCACinpanA,ABAABBABCBBCinpanB.
3rd ABABCACAACCA AABABBBCBCAB
Nowinagivenweighing, apanwilleither endupinthe
Canonical position (C)thatitassumes when thepansarebalanced,
or
Abovethatposition (A),or
Belowit(B),
sothethree weighings determine foreachpanasequence ofthree of
these letters.
Ifbothsequences areCCC,thenthere's nooddball.Otherwise, forjust
oneofthetwopans, thesequence isamong the12above,andnames
theoddball,whose weightisAboveorBelowtheproperoneaccording
asthepanisAorB.
(b)InWweighings theoddballcanbeidentied fromamong
N=(3W 3)=2 (4.49)
ballsinthesame way,bylabelling them withallthenon-constan tse-
quences ofWletters fromA,B,Cwhose rstchange isA-to-B orB-to-C
orC-to-A, andatthewthweighing putting those whosewthletter isA
inpanAandthose whosewthletter isBinpanB.
Solution toexercise 4.15(p.85).Thecurves1
NH(XN)asafunction offor
N=1;2and1000areshowningure 4.14. NotethatH2(0:2)=0:72bits.
00.20.40.60.81
0 0.2 0.4 0.6 0.8 1N=1
N=2
N=1000N=1
1
NH(X)2H(X)
0{0.2 1 2
0.2{1 0 1N=2
1
NH(X)2H(X)
0{0.04 1 4
0.04{0.2 0.79248 3
0.2{0.36 0.5 2
0.36{1 0 1
Figure 4.14.1
NH(X)(vertical
axis)against(horizon tal),for
N=1;2;100binary variables
withp1=0:4.Solution toexercise 4.17(p.85).TheGibbs entropyiskBP
ipiln1
pi,wherei
runsoverallstates ofthesystem. Thisentropyisequivalent(apart fromthe
factor ofkB)totheShannon entropyoftheensem ble.
Whereas theGibbs entropycanbedened foranyensem ble,theBoltz-
mann entropyisonlydened formicrocanonical ensem bles,whichhavea
probabilit ydistribution thatisuniform overasetofaccessible states. The
Boltzmann entropyisdened tobeSB=kBln
where 
isthenumberofac-
cessible states ofthemicrocanonical ensem ble.Thisisequivalent(apart from
thefactor ofkB)totheperfect information contentH0ofthatconstrained
ensem ble.TheGibbs entropyofamicrocanonical ensem bleistrivially equal
totheBoltzmann entropy.

<<<PAGE 100>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
88 4|TheSource CodingTheorem
Wenowconsider athermal distribution (thecanonical ensem ble),where
theprobabilit yofastatexis
P(x)=1
Zexp
 E(x)
kBT
: (4.50)
With thiscanonical ensem blewecanassociateacorresp onding microcanonical
ensem ble,anensem blewithtotal energy xed tothemean energy ofthe
canonical ensem ble(xed towithin some precision).Now,xing thetotal
energy toaprecisionisequivalenttoxing thevalueofln1/P(x)towithin
kBT.Ourdenition ofthetypical setTNwasprecisely thatitconsisted
ofallelemen tsthathaveavalueoflogP(x)veryclosetothemean valueof
logP(x)under thecanonical ensem ble, NH(X).Thusthemicrocanonical
ensem bleisequivalenttoauniform distribution overthetypical setofthe
canonical ensem ble.
Ourproofofthe`asymptotic equipartition' principle thusproves{forthe
caseofasystem whose energy isseparable intoasumofindependen tterms
{thattheBoltzmann entropyofthemicrocanonical ensem bleisveryclose
(forlargeN)totheGibbs entropyofthecanonical ensem ble,iftheenergy of
themicrocanonical ensem bleisconstrained toequal themean energy ofthe
canonical ensem ble.
Solution toexercise 4.18(p.85).Thenormalizing constan toftheCauchydis-
tribution
P(x)=1
Z1
x2+1
is
Z=Z1
 1dx1
x2+1=h
tan 1xi1
 1=
2  
2=: (4.51)
Themean andvariance ofthisdistribution arebothundened. (Thedistribu-
tionissymmetrical aboutzero,butthisdoesnotimply thatitsmean iszero.
Themean isthevalueofadivergentintegral.) Thesumz=x1+x2,where
x1andx2bothhaveCauchydistributions, hasprobabilit ydensit ygivenby
theconvolution
P(z)=1
2Z1
 1dx11
x2
1+11
(z x1)2+1; (4.52)
whichafteraconsiderable labourusing standard metho dsgives
P(z)=1
22
z2+4=2
1
z2+22; (4.53)
whichwerecognize asaCauchydistribution withwidth parameter 2(where
theoriginal distribution haswidth parameter 1).Thisimplies thatthemean
ofthetwopoints,x=(x1+x2)=2=z=2,hasaCauchydistribution with
width parameter 1.Generalizing, themean ofNsamples from aCauchy
distribution isCauchy-distributed withthesameparameters astheindividual
samples. Theprobabilit ydistribution ofthemean doesnotbecome narrower
as1=p
N.
ThecentrallimittheoremdoesnotapplytotheCauchy distribution, because
itdoesnothaveanitevarianc e.
Analternativ eneatmetho dforgetting toequation (4.53) makesuseofthe
Fourier transform oftheCauchydistribution, whichisabiexponentiale j!j.
Convolution inrealspace corresp ondstomultiplication inFourier space, so
theFourier transform ofzissimplye j2!j.Reversing thetransform, weobtain
equation (4.53).

<<<PAGE 101>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
4.8:Solutions 89
Solution toexercise 4.20(p.86).Thefunctionf(x)hasinversefunction
01020304050
00.20.40.60.811.21.4
012345
00.20.40.60.811.21.4
00.10.20.30.40.5
0 0.2
Figure 4.15.f(x)=xxxxx
; shown
atthree dieren tscales.g(y)=y1=y: (4.54)
Note
logg(y)=1=ylogy: (4.55)
Iobtained atentativegraph off(x)byplottingg(y)withyalong thevertical
axisandg(y)along thehorizon talaxis. Theresulting graph suggests that
f(x)issingle valued forx2(0;1),andlookssurprisingly well-behavedand
ordinary; forx2(1;e1=e),f(x)istwo-valued.f(p
2)isequal bothto2and
4.Forx>e1=e(whichisabout1.44),f(x)isinnite. However,itmightbe
argued thatthisapproac htosketchingf(x)isonlypartly valid,ifwedenef
asthelimitofthesequence offunctionsx,xx,xxx;:::;thissequence doesnot
havealimitfor0x(1=e)e'0:07onaccoun tofapitchfork bifurcation
atx=(1=e)e;andforx2(1;e1=e),thesequence's limitissingle-v alued {the
lowerofthetwovalues sketchedinthegure.

<<<PAGE 102>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
AboutChapter 5
Inthelastchapter, wesawaproofofthefundamen talstatus oftheentropy
asameasure ofaverage information content.Wedened adatacompression
scheme using xedlength blockcodes,andprovedthatasNincreases, itis
possible toencodeNi.i.d.variables x=(x1;:::;xN)intoablockofN(H(X)+
)bitswithvanishing probabilit yoferror, whereas ifweattempt toencode
XNintoN(H(X) )bits,theprobabilit yoferrorisvirtually 1.
Wethusveried thepossibility ofdatacompression, buttheblockcoding
dened intheproofdidnotgiveapractical algorithm. Inthischapter and
thenext, westudy practical datacompression algorithms. Whereas thelast
chapter's compression scheme usedlarge blocksofxedsizeandwaslossy,
inthenextchapter wediscuss variable-length compression schemes thatare
practical forsmall blocksizesandthatarenotlossy.
Imagine arubberglovelledwithwater.Ifwecompress twongers ofthe
glove,some other partoftheglovehastoexpand, because thetotalvolume
ofwaterisconstan t.(Waterisessentially incompressible.) Similarly ,when
weshorten thecodewordsforsome outcomes, there mustbeother codewords
thatgetlonger, ifthescheme isnotlossy.Inthischapter wewilldiscoverthe
information-theoretic equivalentofwatervolume.
Before reading Chapter 5,youshould haveworkedonexercise 2.26(p.37).
Wewillusethefollowingnotation forintervals:
x2[1;2)means thatx1andx<2;
x2(1;2]means thatx>1andx2.
90

<<<PAGE 103>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
5
SymbolCodes
Inthischapter, wediscuss variable-length symbolcodes,whichencodeone
source symbolatatime, instead ofencodinghugestrings ofNsource sym-
bols.These codesarelossless: unlikethelastchapter's blockcodes,theyare
guaran teedtocompress anddecompress without anyerrors; butthere isa
chance thatthecodesmaysometimes produceencodedstrings longer than
theoriginal source string.
Theideaisthatwecanachievecompression, onaverage, byassigning
shorter encodings tothemoreprobable outcomes andlonger encodings tothe
lessprobable.
Thekeyissues are:
What aretheimplications ifasymbolcodeislossless ?Ifsomecode-
wordsareshortened, byhowmuchdoother codewordshavetobelength-
ened?
Making compression practical .Howcanweensure thatasymbolcodeis
easytodecode?
Optimal symbolcodes.Howshould weassign codelengths toachievethe
bestcompression, andwhat isthebestachievablecompression?
Weagain verifythefundamen talstatus oftheShannon information content
andtheentropy,proving:
Source codingtheorem (symbolcodes).There exists avariable-length
encodingCofanensem bleXsuchthattheaverage length ofanen-
codedsymbol,L(C;X),satisesL(C;X)2[H(X);H(X)+1).
Theaverage length isequal totheentropyH(X)onlyifthecodelength
foreachoutcome isequal toitsShannon information content.
Wewillalsodene aconstructiv eprocedure, theHuman codingalgorithm,
thatproduces optimal symbolcodes.
Notation foralphab ets.ANdenotes thesetoforderedN-tuples ofele-
mentsfromthesetA,i.e.,allstrings oflengthN.ThesymbolA+will
denote thesetofallstrings ofnite length composedofelemen tsfrom
thesetA.
Example 5.1.f0;1g3=f000;001;010;011;100;101;110;111g.
Example 5.2.f0;1g+=f0;1;00;01;10;11;000;001;:::g.
91

<<<PAGE 104>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
92 5|SymbolCodes
5.1Symbolcodes
A(binary) symbolcodeCforanensem bleXisamapping fromtherange
ofx,AX=fa1;:::;aIg,tof0;1g+.c(x)willdenote thecodewordcor-
responding tox,andl(x)willdenote itslength, withli=l(ai).
Theextended codeC+isamapping fromA+
Xtof0;1g+obtained by
concatenation, without punctutation, ofthecorresp onding codewords:
c+(x1x2:::xN)=c(x1)c(x2):::c(xN): (5.1)
[Theterm`mapping' hereisasynon ymfor`function'.]
Example 5.3.Asymbolcodefortheensem bleXdened by
AX=fa;b;c;dg;
PX=f1/2;1/4;1/8;1/8g;(5.2)
isC0,showninthemargin. C0:aic(ai)li
a1000 4
b0100 4
c0010 4
d0001 4Using theextended code,wemayencodeacdbac as
c+(acdbac )=100000100001010010000010 (5.3)
There arebasic requiremen tsforauseful symbolcode.First, anyencoded
string musthaveaunique decoding. Second, thesymbolcodemustbeeasyto
decode.Andthird, thecodeshould achieveasmuchcompression aspossible.
Anyencodedstring musthaveaunique decoding
AcodeC(X)isuniquely decodeable if,under theextended codeC+,no
twodistinct strings havethesame encoding,i.e.,
8x;y2A+
X;x6=y)c+(x)6=c+(y): (5.4)
ThecodeC0dened aboveisanexample ofauniquely decodeable code.
Thesymbolcodemustbeeasytodecode
Asymbolcodeiseasiest todecodeifitispossible toidentifytheendofa
codewordassoonasitarrives,whichmeans thatnocodewordcanbeaprex
ofanother codeword.[Awordcisaprex ofanother worddifthere exists a
tailstringtsuchthattheconcatenation ctisidenticaltod.Forexample, 1is
aprex of101,andsois10.]
Wewillshowlaterthatwedon't loseanyperformance ifweconstrain our
symbolcodetobeaprex code.
Asymbolcodeiscalled aprex codeifnocodewordisaprex ofany
other codeword.
Aprex codeisalsoknownasaninstan taneous orself-punctuating code,
because anencodedstring canbedecodedfromlefttorightwithout
looking ahead tosubsequen tcodewords. Theendofacodewordisim-
mediately recognizable. Aprex codeisuniquely decodeable.
Prex codesarealsoknownas`prex-free codes'or`prex condition codes'.
Prex codescorresp ondtotrees.

<<<PAGE 105>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
5.1:Symbolcodes 93
C10
10
0
1101Example 5.4.ThecodeC1=f0;101gisaprex codebecause0isnotaprex
of101,noris101aprex of0.
Example 5.5.LetC2=f1;101g.Thiscodeisnotaprex codebecause1isa
prex of101.
Example 5.6.ThecodeC3=f0;10;110;111gisaprex code.
C30
10
100
1
11110110
C40
10
100
0
101
10
11
Prex codescanberepresen ted
onbinary trees. Complete prex
codescorresp ondtobinary trees
withnounusedbranches.C1isan
incomplete code.Example 5.7.ThecodeC4=f00;01;10;11gisaprex code.
Exercise 5.8.[1,p.104]IsC2uniquely decodeable?
Example 5.9.Consider exercise 4.1(p.66)andgure 4.2(p.69).Anyweighing
strategy thatidentiestheoddballandwhether itisheavyorlightcan
beviewedasassigning aternary codetoeachofthe24possible states.
Thiscodeisaprex code.
Thecodeshould achieve asmuchcompression aspossible
Theexpected lengthL(C;X)ofasymbolcodeCforensem bleXis
L(C;X)=X
x2AXP(x)l(x): (5.5)
Wemayalsowrite thisquantityas
L(C;X)=IX
i=1pili (5.6)
whereI=jAXj.
C3:
aic(ai)pih(pi)li
a01/21.0 1
b101/42.0 2
c1101/83.0 3
d1111/83.0 3Example 5.10. Let
AX=fa;b;c;dg;
andPX=f1/2;1/4;1/8;1/8g;(5.7)
andconsider thecodeC3.TheentropyofXis1.75bits,andtheexpected
lengthL(C3;X)ofthiscodeisalso1.75bits.Thesequence ofsymbols
x=(acdbac )isencodedasc+(x)=0110111100110 .C3isaprex code
andistherefore uniquely decodeable. Notice thatthecodewordlengths
satisfyli=log2(1=pi),orequivalently,pi=2 li.
Example 5.11. Consider thexedlength codeforthesame ensem bleX,C4.
Theexpected lengthL(C4;X)is2bits.C4C5
a000
b011
c1000
d1111 Example 5.12. ConsiderC5.Theexpected lengthL(C5;X)is1.25bits,which
islessthanH(X).Butthecodeisnotuniquely decodeable. These-
quence x=(acdbac )encodesas000111000 ,whichcanalsobedecoded
as(cabdca ).
Example 5.13. Consider thecodeC6.Theexpected lengthL(C6;X)ofthisC6:
aic(ai)pih(pi)li
a01/21.0 1
b011/42.0 2
c0111/83.0 3
d1111/83.0 3codeis1.75bits.Thesequence ofsymbolsx=(acdbac )isencodedas
c+(x)=0011111010011 .
IsC6aprex code?Itisnot,becausec(a)=0isaprex ofc(b)and
c(c).

<<<PAGE 106>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
94 5|SymbolCodes
IsC6uniquely decodeable? Thisisnotsoobvious. Ifyouthink thatit
mightnotbeuniquely decodeable, trytoproveitsobynding apairof
strings xandythathavethesame encoding. [Thedenition ofunique
decodeabilit yisgiveninequation (5.4).]
C6certainly isn'teasytodecode.When wereceive`00',itispossible
thatxcould start`aa',`ab'or`ac'.Once wehavereceived`001111 ',
thesecond symbolisstillambiguous, asxcould be`abd...'or`acd...'.
Buteventually aunique decodingcrystallizes, oncethenext0appears
intheencodedstream.
C6isinfactuniquely decodeable. Comparing withtheprex codeC3,
weseethatthecodewordsofC6arethereverseofC3's.ThatC3is
uniquely decodeable provesthatC6istoo,since anystring fromC6is
identicaltoastring fromC3readbackwards.
5.2What limitisimposedbyunique decodeabilit y?
Wenowask,givenalistofpositiveintegersflig,doesthere existauniquely
decodeable codewiththose integers asitscodewordlengths? Atthisstage, we
ignore theprobabilities ofthedieren tsymbols;onceweunderstand unique
decodeabilit ybetter, we'llreintroducetheprobabilities anddiscuss howto
makeanoptimal uniquely decodeable symbolcode.
Intheexamples above,wehaveobserv edthatifwetakeacodesuchas
f00;01;10;11g,andshorten oneofitscodewords, forexample 00!0,then
wecanretain unique decodeabilit yonlyifwelengthen other codewords. Thus
there seems tobeaconstrained budget thatwecanspendoncodewords,with
shorter codewordsbeingmore expensive.
Letusexplore thenature ofthisbudget. Ifwebuild acodepurely from
codewordsoflengthlequal tothree, howmanycodewordscanwehaveand
retain unique decodeabilit y?Theansweris2l=8.Once wehavechosen all
eightofthese codewords,isthere anywaywecould addtothecodeanother
codewordofsomeother length andretain unique decodeabilit y?Itwould
seemnot.
What ifwemakeacodethatincludes alength-one codeword,`0',withthe
other codewordsbeingoflength three? Howmanylength-three codewordscan
wehave?Ifwerestrict attentiontoprex codes,thenwecanhaveonlyfour
codewordsoflength three, namelyf100;101;110;111g.What aboutother
codes?Isthere anyother wayofchoosing codewordsoflength 3thatcangive
more codewords? Intuitively,wethink thisunlikely.Acodewordoflength 3
appearstohaveacostthatis22times smaller thanacodewordoflength 1.
Let'sdene atotalbudget ofsize1,whichwecanspendoncodewords. If
wesetthecostofacodewordwhose length islto2 l,thenwehaveapricing
system thattstheexamples discussed above.Codewordsoflength 3cost
1/8each;codewordsoflength 1cost1=2each.Wecanspendourbudget on
anycodewords. Ifwegooverourbudget thenthecodewillcertainly notbe
uniquely decodeable. If,ontheother hand,
X
i2 li1; (5.8)
thenthecodemaybeuniquely decodeable. Thisinequalit yistheKraft in-
equalit y.
Kraft inequalit y.Foranyuniquely decodeable codeCoverthebinary al-

<<<PAGE 107>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
5.2:What limitisimposedbyunique decodeabilit y? 95
phabetf0;1g,thecodewordlengths mustsatisfy:
IX
i=12 li1; (5.9)
whereI=jAXj.
Completeness .Ifauniquely decodeable codesatises theKraft inequalit y
withequalit ythenitiscalled acomplete code.
Wewantcodesthatareuniquely decodeable; prex codesareuniquely de-
codeable, andareeasytodecode.Solifewouldbesimpler forusifwecould
restrict attentiontoprex codes.Fortunately ,foranysource thereisanop-
timal symbolcodethatisalsoaprex code.
Kraft inequalit yandprex codes.Givenasetofcodewordlengths that
satisfy theKraft inequalit y,there exists auniquely decodeable prex
codewiththese codewordlengths.
TheKraft inequalit ymightbemore accurately referred toastheKraft{McMillan
inequalit y:Kraft provedthatiftheinequalit yissatised, thenaprex codeexists with
thegivenlengths. McMillan (1956) provedtheconverse,thatunique decodeabilit y
implies thattheinequalit yholds.
ProofoftheKraft inequalit y.DeneS=P
i2 li.Consider thequantity
SN="X
i2 li#N
=IX
i1=1IX
i2=1IX
iN=12 (li1+li2+liN)(5.10)
Thequantityintheexponent,(li1+li2++liN),isthelength ofthe
encodingofthestringx=ai1ai2:::aiN.ForeverystringxoflengthN,
there isonetermintheabovesum. IntroduceanarrayAlthatcounts
howmanystrings xhaveencodedlengthl.Then, deninglmin=minili
andlmax=max ili:
SN=NlmaxX
l=Nlmin2 lAl: (5.11)
NowassumeCisuniquely decodeable, sothatforallx6=y,c+(x)6=
c+(y).Concen trate onthexthathaveencodedlengthl.There area
totalof2ldistinct bitstrings oflengthl,soitmustbethecasethat
Al2l.So
SN=NlmaxX
l=Nlmin2 lAlNlmaxX
l=Nlmin1Nlmax: (5.12)
ThusSNlmaxNforallN.NowifSweregreater than1,thenasN
increases,SNwouldbeanexponentially growingfunction, andforlarge
enoughN,anexponentialalwaysexceeds apolynomial suchaslmaxN.
Butourresult (SNlmaxN)istrueforanyN.ThereforeS1. 2
.Exercise 5.14.[3,p.104]Provetheresult stated above,thatforanysetofcode-
wordlengthsfligsatisfying theKraft inequalit y,there isaprex code
havingthose lengths.

<<<PAGE 108>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
96 5|SymbolCodes
1111111011011100101110101001100001110110010101000011001000010000
111110101100011010001000
11100100
0
1
The total symbol code budgetFigure 5.1.Thesymbolcoding
budget. The`cost' 2 lofeach
codeword(with lengthl)is
indicated bythesizeoftheboxit
iswritten in.Thetotalbudget
available when making auniquely
decodeable codeis1.
Youcanthink ofthisdiagram as
showingacodewordsupermark et,
withthecodewordsarranged in
aisles bytheirlength, andthecost
ofeachcodewordindicated bythe
sizeofitsboxontheshelf. Ifthe
costofthecodewordsthatyou
takeexceeds thebudget thenyour
codewillnotbeuniquely
decodeable.
C0 C3 C4 C6                                        







1111111011011100101110101001100001110110010101000011001000010000
111110101100011010001000
11100100
0
1
																																																																																																																																																																																																																																																																																																																																																																																																																



















































































































































































































































































































































































































1111111011011100101110101001100001110110010101000011001000010000
111110101100011010001000
11100100
0
1







1111111011011100101110101001100001110110010101000011001000010000
111110101100011010001000
11100100
0
1







1111111011011100101110101001100001110110010101000011001000010000
111110101100011010001000
11100100
0
1
Figure 5.2.Selections of
codewordsmade bycodes
C0;C3;C4andC6fromsection
5.1.

<<<PAGE 109>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
5.3:What's themostcompression thatwecanhopefor? 97
Apictorial viewoftheKraft inequalit ymayhelpyousolvethisexercise.
Imagine thatwearechoosing thecodewordstomakeasymbolcode.Wecan
drawthesetofallcandidate codewordsinasupermark etthatdispla ysthe
`cost' ofthecodewordbytheareaofabox(gure 5.1). Thetotal budget
available {the`1'ontheright-hand sideoftheKraft inequalit y{isshownat
oneside. Some ofthecodesdiscussed insection 5.1areillustrated ingure
5.2.Notice thatthecodesthatareprex codes,C0,C3,andC4,havethe
propertythattotherightofanyselected codeword,therearenoother selected
codewords{because prex codescorresp ondtotrees. Notice thatacomplete
prex codecorresp ondstoacomplete treehavingnounusedbranches.
Wearenowready toputbackthesymbols'sprobabilitiesfpig.Givena
setofsymbolprobabilities (theEnglish language probabilities ofgure 2.1,
forexample), howdowemakethebestsymbolcode{onewiththesmallest
possible expected lengthL(C;X)?Andwhatisthatsmallest possible expected
length? It'snotobvious howtoassign thecodewordlengths. Ifwegiveshort
codewordstothemore probable symbolsthentheexpected length mightbe
reduced; ontheother hand, shortening some codewordsnecessarily causes
others tolengthen, bytheKraft inqualit y.
5.3What's themostcompression thatwecanhopefor?
Wewishtominimize theexpected length ofacode,
L(C;X)=X
ipili: (5.13)
Asyoumighthaveguessed, theentropyappearsasthelowerboundonthe
expected length ofacode.
Lowerbound onexpected length .Theexpected lengthL(C;X)ofa
uniquely decodeable codeisbounded belowbyH(X).
Proof.Wedene theimplicit probabilities qi2 li=z,wherez=P
i02 li0,so
thatli=log1=qi logz.WethenuseGibbs' inequalit y,P
ipilog1=qiP
ipilog1=pi,withequalit yifqi=pi,andtheKraft inequalit yz1:
L(C;X)=X
ipili=X
ipilog1=qi logz (5.14)
X
ipilog1=pi logz (5.15)
H(X): (5.16)
Theequalit yL(C;X)=H(X)isachievedonlyiftheKraft equalit yz=1
issatised, andifthecodelengths satisfyli=log(1=pi). 2
Thisisanimportantresult solet'ssayitagain:
Optimal source codelengths .Theexpected length isminimized andis
equal toH(X)onlyifthecodelengths areequal totheShannon in-
formation contents:
li=log2(1=pi): (5.17)
Implicit probabilities dened bycodelengths .Conversely,anychoice
ofcodelengthsfligimplicitly denes aprobabilit ydistributionfqig,
qi2 li=z; (5.18)
forwhichthose codelengths wouldbetheoptimal codelengths. Ifthe
codeiscomplete thenz=1andtheimplicit probabilities aregivenby
qi=2 li.

<<<PAGE 110>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
98 5|SymbolCodes
5.4Howmuchcanwecompress?
So,wecan't compress belowtheentropy.Howclosecanweexpecttogetto
theentropy?
Theorem 5.1Source codingtheoremforsymbolcodes.ForanensembleX
thereexists aprexcodeCwithexpectedlength satisfying
H(X)L(C;X)<H(X)+1: (5.19)
Proof.Wesetthecodelengths tointegers slightlylarger thantheoptim um
lengths:
li=dlog2(1=pi)e (5.20)
wheredledenotes thesmallest integer greater thanorequal tol.[We
arenotasserting thattheoptimal codenecessarily usesthese lengths,
wearesimply choosing these lengths because wecanusethem toprove
thetheorem.]
Wecheckthatthereisaprex codewiththese lengths byconrming
thattheKraft inequalit yissatised.
X
i2 li=X
i2 dlog2(1=pi)eX
i2 log2(1=pi)=X
ipi=1:(5.21)
Then weconrm
L(C;X)=X
ipidlog(1=pi)e<X
ipi(log(1=pi)+1)=H(X)+1:(5.22)
2
Thecostofusingthewrongcodelengths
Ifweuseacodewhose lengths arenotequal totheoptimal codelengths, the
average message length willbelarger thantheentropy.
Ifthetrueprobabilities arefpigandweuseacomplete codewithlengths
li,wecanviewthose lengths asdening implicit probabilities qi=2 li.Con-
tinuingfromequation (5.14), theaverage length is
L(C;X)=H(X)+X
ipilogpi=qi; (5.23)
i.e.,itexceeds theentropybytherelativ eentropyDKL(pjjq)(asdened on
p.34).
5.5Optimal source codingwithsymbolcodes:Human coding
GivenasetofprobabilitiesP,howcanwedesign anoptimal prex code?
Forexample, whatisthebestsymbolcodefortheEnglish language ensem ble
showningure 5.3? When wesay`optimal', let'sassume ouraimistoxP(x)
a 0.0575
b 0.0128
c 0.0263
d 0.0285
e 0.0913
f 0.0173g 0.0133
h 0.0313
i 0.0599
j 0.0006
k 0.0084
l 0.0335
m 0.0235
n 0.0596
o 0.0689p 0.0192q 0.0008
r 0.0508
s 0.0567
t 0.0706
u 0.0334
v 0.0069
w 0.0119
x 0.0073y 0.0164
z 0.0007
  0.1928
Figure 5.3.Anensem bleinneedof
asymbolcode.
minimize theexpected lengthL(C;X).
Hownottodoit
Onemighttrytoroughly splitthesetAXintwo,andcontinuebisecting the
subsets soastodene abinary treefromtheroot.Thisconstruction hasthe
rightspirit, asintheweighing problem, butitisnotnecessarily optimal; it
achievesL(C;X)H(X)+2.

<<<PAGE 111>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
5.5:Optimal source codingwithsymbolcodes:Human coding 99
TheHuman codingalgorithm
Wenowpresen tabeautifully simple algorithm fornding anoptimal prex
code.Thetrickistoconstruct thecodebackwar dsstarting fromthetailsof
thecodewords;webuildthebinary treefromitsleaves.
Algorithm 5.4.Human coding
algorithm. 1.Takethetwoleastprobable symbolsinthealphab et.These two
symbolswillbegiventhelongest codewords,whichwillhaveequal
length, anddier onlyinthelastdigit.
2.Combinethese twosymbolsintoasingle symbol,andrepeat.
Since eachstepreduces thesizeofthealphab etbyone,thisalgorithm will
haveassigned strings toallthesymbolsafterjAXj 1steps.
Example 5.15. LetAX=fa,b,c,d,eg
andPX=f0.25,0.25,0.2,0.15,0.15g.
0.25
0.25
0.2
0.15
0.150.25
0.25
0.2
0.30.25
0.45
0.30.55
0.451.0 a
b
c
d
e0
10
10
10
1
    



  x step1step2step3step4
Thecodewordsarethenobtained byconcatenating thebinary digits in
reverseorder:C=f00;10;11;010;011g.Thecodelengths selectedaipih(pi)lic(ai)
a0.25 2.0200
b0.25 2.0210
c0.2 2.3211
d0.15 2.73010
e0.15 2.73011
Table5.5.Codecreated bythe
Human algorithm.
bytheHuman algorithm (column 4oftable 5.5)areinsome cases
longer andinsomecasesshorter thantheidealcodelengths, theShannon
information contentslog21/pi(column 3).Theexpected length ofthe
codeisL=2:30bits,whereas theentropyisH=2:2855bits. 2
Ifatanypointthere ismore thanonewayofselecting thetwoleastprobable
symbolsthenthechoice maybemade inanymanner {theexpected length of
thecodewillnotdependonthechoice.
Exercise 5.16.[3,p.105]Provethatthere isnobettersymbolcodeforasource
thantheHuman code.
Example 5.17. WecanmakeaHuman codefortheprobabilit ydistribution
overthealphab etintroduced ingure 2.1.Theresult isshowning-
ure5.6.Thiscodehasanexpected length of4.15bits;theentropyof
theensem bleis4.11bits.Observ ethedisparities betweentheassigned
codelengths andtheidealcodelengths log21/pi.
Constructing abinary treetop-down issuboptimal
Inprevious chapters westudied weighing problems inwhichwebuiltternary
orbinary trees. Wenoticed thatbalanced trees{onesinwhich,ateverystep,
thetwopossible outcomes wereascloseaspossible toequiprobable {appeared
todescrib ethemostecien texperimen ts.Thisgaveanintuitivemotivation
forentropyasameasure ofinformation content.

<<<PAGE 112>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
100 5|SymbolCodes
aipi log21
pilic(ai)
a0.0575 4.1 40000
b0.0128 6.3 6001000
c0.0263 5.2 500101
d0.0285 5.1 510000
e0.0913 3.5 41100
f0.0173 5.9 6111000
g0.0133 6.2 6001001
h0.0313 5.0 510001
i0.0599 4.1 41001
j0.0006 10.7 101101000000
k0.0084 6.9 71010000
l0.0335 4.9 511101
m0.0235 5.4 6110101
n0.0596 4.1 40001
o0.0689 3.9 41011
p0.0192 5.7 6111001
q0.0008 10.3 9110100001
r0.0508 4.3 511011
s0.0567 4.1 40011
t0.0706 3.8 41111
u0.0334 4.9 510101
v0.0069 7.2 811010001
w0.0119 6.4 71101001
x0.0073 7.1 71010001
y0.0164 5.9 6101001
z0.0007 10.4 101101000001
{0.1928 2.4 201pfi
o
e
tuy
r
ls
−
wvq
ma
n
c
d
hgb
k
x
j
zFigure 5.6.Human codeforthe
English language ensem ble
(monogram statistics).
Itisnotthecase,however,thatoptimal codescanalways beconstructed
byagreedy top-do wnmetho dinwhichthealphab etissuccessiv elydivided
intosubsets thatareasnearaspossible toequiprobable.
Example 5.18. Findtheoptimal binary symbolcodefortheensem ble:
AX=fa;b;c;d;e;f;gg
PX=f0:01;0:24;0:05;0:20;0:47;0:01;0:02g: (5.24)
Notice thatagreedy top-do wnmetho dcansplitthissetintotwosub-
setsfa;b;c;dgandfe;f;ggwhichbothhaveprobabilit y1=2,andthat
fa;b;c;dgcanbedivided intosubsetsfa;bgandfc;dg,whichhaveprob-
ability1=4;soagreedy top-do wnmetho dgivesthecodeshowninthe
thirdcolumn oftable5.7,whichhasexpected length 2.53. TheHumanaipiGreedy Human
a.01000 000000
b.24001 01
c.05010 0001
d.20011 001
e.4710 1
f.01110 000001
g.02111 00001
Table5.7.Agreedily-constructed
codecompared withtheHuman
code.
codingalgorithm yields thecodeshowninthefourth column, whichhas
expected length 1.97. 2
5.6Disadv antages oftheHuman code
TheHuman algorithm produces anoptimal symbolcodeforanensem ble,
butthisisnottheendofthestory.Boththeword`ensem ble'andthephrase
`symbolcode'needcareful attention.
Changing ensemble
Ifwewishtocomm unicate asequence ofoutcomes fromoneunchanging en-
semble,thenaHuman codemaybeconvenient.Butoften theappropriate

<<<PAGE 113>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
5.6:Disadv antages oftheHuman code 101
ensem blechanges. Ifforexample wearecompressing text,thenthesymbol
frequencies willvarywithcontext:inEnglish theletteruismuchmoreprob-
ableafteraqthanafterane(gure 2.3).Andfurthermore, ourknowledge of
these context-dep enden tsymbolfrequencies willalsochange aswelearn the
statistical properties ofthetextsource.
Human codesdonothandle changing ensem bleprobabilities withany
elegance. Onebrute-force approac hwouldbetorecompute theHuman code
everytimetheprobabilit yoversymbolschanges. Another attitude istodeny
theoption ofadaptation, andinstead runthrough theentireleinadvance
andcompute agoodprobabilit ydistribution, whichwillthenremain xed
throughout transmission. Thecodeitselfmustalsobecomm unicated inthis
scenario. Suchatechnique isnotonlycumbersome andrestrictiv e,itisalso
suboptimal, since theinitial message specifying thecodeandthedocumen t
itselfarepartially redundan t.Thistechnique therefore wastesbits.
Theextrabit
Anequally serious problem withHuman codesistheinnocuous-lo oking `ex-
trabit'relativ etotheidealaverage length ofH(X){aHuman codeachieves
alength thatsatisesH(X)L(C;X)<H(X)+1;asprovedintheorem 5.1.
AHuman codethusincurs anoverhead ofbetween0and1bitspersymbol.
IfH(X)werelarge, thenthisoverhead wouldbeanunimp ortantfractional
increase. Butformanyapplications, theentropymaybeaslowasonebit
persymbol,orevensmaller, sotheoverheadL(C;X) H(X)maydomi-
natetheencodedlelength. Consider English text: insome contexts, long
strings ofcharacters maybehighly predictable. Forexample, inthecontext
`strings_of_ch ',onemightpredict thenextninesymbolstobe`aracters_ '
withaprobabilit yof0.99each.Atraditional Human codewouldbeobliged
touseatleastonebitpercharacter, making atotalcostofninebitswhere
virtually noinformation isbeingconveyed(0.13 bitsintotal, tobeprecise).
TheentropyofEnglish, givenagoodmodel,isaboutonebitpercharacter
(Shannon, 1948), soaHuman codeislikelytobehighly inecien t.
Atraditional patch-upofHuman codesusesthem tocompress blocksof
symbols,forexample the`extended sources'XNwediscussed inChapter 4.
Theoverhead perblockisatmost1bitsotheoverhead persymbolisatmost
1=Nbits. Forsucien tlylarge blocks,theproblem oftheextra bitmaybe
removed{butonlyattheexpenses of(a)losing theelegan tinstan taneous
decodeabilit yofsimple Human coding;and(b)havingtocompute theprob-
abilities ofallrelevantstrings andbuildtheassociated Human tree.Onewill
endupexplicitly computing theprobabilities andcodesforahugenumberof
strings, mostofwhichwillneveractually occur. (Seeexercise 5.29(p.103).)
Beyond symbolcodes
Human codes,therefore, although widely trump etedas`optimal', havemany
defects forpractical purposes.Theyareoptimal symbolcodes,butforpracti-
calpurposeswedon'twantasymbolcode.
Thedefects ofHuman codesarerectied byarithmetic coding,which
dispenses withtherestriction thateachsymbolmusttranslate intoaninteger
numberofbits.Arithmetic codingisthemaintopic ofthenextchapter.

<<<PAGE 114>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
102 5|SymbolCodes
5.7Summary
Kraft inequalit y.Ifacodeisuniquely decodeable itslengths mustsatisfy
X
i2 li1: (5.25)
Foranylengths satisfying theKraft inequalit y,there exists aprex code
withthose lengths.
Optimal source codelengths foranensem bleareequal totheShannon
information contents
li=log21
pi; (5.26)
andconversely,anychoice ofcodelengths denes implicit probabilities
qi=2 li
z: (5.27)
Therelativ eentropyDKL(pjjq)measures howmanybitspersymbolare
wasted byusing acodewhose implicit probabilities areq,when the
ensem ble'strueprobabilit ydistribution isp.
Source codingtheorem forsymbolcodes.Foranensem bleX,there ex-
istsaprex codewhose expected length satises
H(X)L(C;X)<H(X)+1: (5.28)
TheHuman codingalgorithm generates anoptimal symbolcodeitera-
tively.Ateachiteration, thetwoleastprobable symbolsarecombined.
5.8Exercises
.Exercise 5.19.[2]Isthecodef00;11;0101;111;1010;100100;0110guniquely
decodeable?
.Exercise 5.20.[2]Istheternary codef00;012;0110;0112;100;201;212;22g
uniquely decodeable?
Exercise 5.21.[3,p.106]MakeHuman codesforX2,X3andX4whereAX=
f0;1gandPX=f0:9;0:1g.Compute theirexpected lengths andcom-
parethem withtheentropiesH(X2),H(X3)andH(X4).
Repeatthisexercise forX2andX4wherePX=f0:6;0:4g.
Exercise 5.22.[2,p.106]Findaprobabilit ydistributionfp1;p2;p3;p4gsuchthat
there aretwooptimal codesthatassign dieren tlengthsfligtothefour
symbols.
Exercise 5.23.[3](Continuation ofexercise 5.22.) Assume thatthefourproba-
bilitiesfp1;p2;p3;p4gareordered suchthatp1p2p3p40.Let
Qbethesetofallprobabilit yvectors psuchthatthere aretwooptimal
codeswithdieren tlengths. Giveacomplete description ofQ.Find
three probabilit yvectors q(1),q(2),q(3),whicharetheconvexhullofQ,
i.e.,suchthatanyp2Qcanbewritten as
p=1q(1)+2q(2)+3q(3); (5.29)
wherefigarepositive.

<<<PAGE 115>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
5.8:Exercises 103
.Exercise 5.24.[1]Writeashortessaydiscussing howtoplaythegame oftwenty
questions optimally .[Intwentyquestions, oneplayerthinks ofanobject,
andtheother playerhastoguess theobjectusing asfewbinary questions
aspossible, preferably fewerthantwenty.]
.Exercise 5.25.[2]Showthat,ifeachprobabilit ypiisequal toanintegerpower
of2thenthere exists asource codewhose expected length equals the
entropy.
.Exercise 5.26.[2,p.106]Makeensem blesforwhichthedierence betweenthe
entropyandtheexpected length oftheHuman codeisasbigaspossible.
.Exercise 5.27.[2,p.106]Abinary sourceXhasanalphab etofelevencharacters
fa;b;c;d;e;f;g;h;i;j;kg;
allofwhichhaveequal probabilit y,1=11.
Findanoptimal uniquely decodeable symbolcodeforthissource. How
muchgreater istheexpected length ofthisoptimal codethantheentropy
ofX?
.Exercise 5.28.[2]Consider theoptimal symbolcodeforanensem bleXwith
alphab etsizeIfromwhichallsymbolshaveidenticalprobabilit yp=
1=I.Iisnotapowerof2.
Showthatthefractionf+oftheIsymbolsthatareassigned codelengths
equal to
l+dlog2Ie (5.30)
satises
f+=2 2l+
I(5.31)
andthattheexpected length oftheoptimal symbolcodeis
L=l+ 1+f+: (5.32)
Bydieren tiating theexcess length LL H(X)withrespecttoI,
showthattheexcess length isbounded by
L1 ln(ln2)
ln2 1
ln2=0:086: (5.33)
Exercise 5.29.[2]Consider asparse binary source withPX=f0:99;0:01g.Dis-
cusshowHuman codescould beusedtocompress thissource eciently .
Estimate howmanycodewordsyourproposedsolutions require.
.Exercise 5.30.[2]Scientic Americancarried thefollowingpuzzle in1975.
Thepoisoned glass.`Mathematicians arecurious birds', thepolice
commissioner saidtohiswife. `Yousee,wehadallthose partly
lledglasses linedupinrowsonatableinthehotel kitchen.Only
onecontained poison, andwewantedtoknowwhichonebefore
searchingthatglassforngerprin ts.Ourlabcould testtheliquid
ineachglass, buttheteststaketimeandmoney ,sowewantedto
makeasfewofthemaspossible bysimultaneously testing mixtures
ofsmall samples fromgroups ofglasses. Theuniversitysentovera

<<<PAGE 116>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
104 5|SymbolCodes
mathematics professor tohelpus.Hecountedtheglasses, smiled
andsaid:
`\Pickanyglassyouwant,Commissioner. We'lltestitrst."
`\Butwon'tthatwasteatest?" Iasked.
`\No," hesaid,\it'spartofthebestprocedure. Wecantestone
glassrst.Itdoesn'tmatter whichone."'
`Howmanyglasses werethere tostartwith?' thecommissioner's
wifeasked.
`Idon't remem ber.Somewhere between100and200.'
What wastheexact numberofglasses?
Solvethispuzzle andthenexplain whytheprofessor wasinfactwrong
andthecommissioner wasright.What isinfacttheoptimal procedure
foridentifying theonepoisoned glass? What istheexpected waste
relativ etothisoptim umifonefollowedtheprofessor's strategy? Explain
therelationship tosymbolcoding.
Exercise 5.31.[2,p.106]Assume thatasequence ofsymbolsfromtheensem ble
Xintroduced atthebeginning ofthischapter iscompressed using the
codeC3.Imagine pickingonebitatrandom fromthebinary encodedC3:
aic(ai)pih(pi)li
a01/21.0 1
b101/42.0 2
c1101/83.0 3
d1111/83.0 3sequence c=c(x1)c(x2)c(x3):::.What istheprobabilit ythatthisbit
isa1?
.Exercise 5.32.[2,p.107]Howshould thebinary Human encodingscheme be
modied tomakeoptimal symbolcodesinanencodingalphab etwithq
symbols?(Also knownas`radixq'.)
Mixtur ecodes
Itisatempting ideatoconstruct a`metaco de'fromseveralsymbolcodesthat
assign dieren t-length codewordstothealternativ esymbols,thenswitchfrom
onecodetoanother, choosing whicheverassigns theshortest codewordtothe
curren tsymbol.Clearly wecannot dothisforfree. Ifonewishes tochoose
betweentwocodes,thenitisnecessary tolengthen themessage inawaythat
indicates whichofthetwocodesisbeingused. Ifweindicate thischoice by
asingle leading bit,itwillbefound thattheresulting codeissuboptimal
because itisincomplete (that is,itfailstheKraft equalit y).
Exercise 5.33.[3,p.108]Provethatthismetaco deisincomplete, andexplain
whythiscombined codeissuboptimal.
5.9Solutions
Solution toexercise 5.8(p.93).Yes,C2=f1;101gisuniquely decodeable,
eventhough itisnotaprex code,because notwodieren tstrings canmap
ontothesame string; onlythecodewordc(a2)=101containsthesymbol0.
Solution toexercise 5.14(p.95).Wewishtoprovethatforanysetofcodeword
lengthsfligsatisfying theKraft inequalit y,there isaprex codehavingthose
lengths. Thisisreadily provedbythinking ofthecodewordsillustrated in
gure 5.8asbeingina`codewordsupermark et',withsizeindicating cost.
Weimagine purchasing codewordsoneatatime, starting fromtheshortest
codewords(i.e.,thebiggest purchases), using thebudget shownattheright
ofgure 5.8.Westart atonesideofthecodewordsupermark et,saythe

<<<PAGE 117>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
5.9:Solutions 105
1111111011011100101110101001100001110110010101000011001000010000
111110101100011010001000
11100100
0
1
The total symbol code budgetFigure 5.8.Thecodeword
supermark etandthesymbol
codingbudget. The`cost' 2 lof
eachcodeword(with lengthl)is
indicated bythesizeoftheboxit
iswritten in.Thetotalbudget
available when making auniquely
decodeable codeis1.
symbolprobabilit yHuman Rivalcode's Modied rival
codewords codewords code
apa cH(a)cR(a) cR(c)
bpb cH(b)cR(b)cR(b)
cpc cH(c)cR(c)cR(a)Figure 5.9.ProofthatHuman
codingmakesanoptimal symbol
code.Weassume thattherival
code,whichissaidtobeoptimal,
assigns unequallength codewords
tothetwosymbolswithsmallest
probabilit y,aandb.By
interchanging codewordsaandc
oftherivalcode,wherecisa
symbolwithrivalcodelength as
longasb's,wecanmakeacode
betterthantherivalcode.This
showsthattherivalcodewasnot
optimal.top,andpurchasetherstcodewordoftherequired length. Weadvance
downthesupermark etadistance 2 l,andpurchasethenextcodewordofthe
nextrequired length, andsoforth. Because thecodewordlengths aregetting
longer, andthecorresp onding intervalsaregetting shorter, wecanalways
buyanadjacen tcodewordtothelatest purchase, sothere isnowasting of
thebudget. ThusattheIthcodewordwehaveadvanced adistancePI
i=12 li
downthesupermark et;ifP2 li1,wewillhavepurchased allthecodewords
without running outofbudget.
Solution toexercise 5.16(p.99).TheproofthatHuman codingisoptimal
dependsonprovingthatthekeystepinthealgorithm {thedecision togive
thetwosymbolswithsmallest probabilit yequal encodedlengths {cannot
leadtoalarger expected length thananyother code.Wecanprovethisby
contradiction.
Assume thatthetwosymbolswithsmallest probabilit y,calledaandb,
towhichtheHuman algorithm wouldassign equal length codewords,donot
haveequal lengths inanyoptimal symbolcode.Theoptimal symbolcode
issome other rivalcodeinwhichthese twocodewordshaveunequal lengths
laandlbwithla<lb.Without lossofgeneralit ywecanassume thatthis
other codeisacomplete prex code,because anycodelengths ofauniquely
decodeable codecanberealized byaprex code.
Inthisrivalcode,there mustbesome other symbolcwhose probabilit y
pcisgreater thanpaandwhose length intherivalcodeisgreater thanor
equal tolb,because thecodeforbmusthaveanadjacen tcodewordofequal
orgreater length {acomplete prex codeneverhasasolocodewordofthe
maxim umlength.
Consider exchanging thecodewordsofaandc(gure 5.9),sothatais

<<<PAGE 118>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
106 5|SymbolCodes
encodedwiththelonger codewordthatwasc's,andc,whichismoreprobable
thana,getstheshorter codeword.Clearly thisreduces theexpected length
ofthecode.Thechange inexpected length is(pa pc)(lc la).Thuswehave
contradicted theassumption thattherivalcodeisoptimal. Therefore itis
validtogivethetwosymbolswithsmallest probabilit yequal encodedlengths.
Human codingproduces optimal symbolcodes. 2
Solution toexercise 5.21(p.102).AHuman codeforX2whereAX=f0;1g
andPX=f0:9;0:1gisf00;01;10;11g!f1;01;000;001g.Thiscodehas
L(C;X2)=1:29,whereas theentropyH(X2)is0.938.
AHuman codeforX3is
f000;100;010;001;101;011;110;111g!
f1;011;010;001;00000;00001;00010;00011g:
Thishasexpected lengthL(C;X3)=1:598whereas theentropyH(X3)is
1.4069.
AHuman codeforX4maps thesixteen source strings tothefollowing
codelengths:
f0000;1000;0100;0010;0001;1100;0110;0011;0101;1010;1001;1110;1101;
1011;0111;1111g!f1;3;3;3;4;6;7;7;7;7;7;9;9;9;10;10g:
Thishasexpected lengthL(C;X4)=1:9702whereas theentropyH(X4)is
1.876.
WhenPX=f0:6;0:4g,theHuman codeforX2haslengthsf2;2;2;2g;
theexpected length is2bits,andtheentropyis1.94bits.AHuman codefor
X4isshownintable 5.10. Theexpected length is3.92bits,andtheentropy
is3.88bits.aipi lic(ai)
0000 0.1296 3000
0001 0.0864 40100
0010 0.0864 40110
0100 0.0864 40111
1000 0.0864 3100
1100 0.0576 41010
1010 0.0576 41100
1001 0.0576 41101
0110 0.0576 41110
0101 0.0576 41111
0011 0.0576 40010
1110 0.0384 500110
1101 0.0384 501010
1011 0.0384 501011
0111 0.0384 41011
1111 0.0256 500111
Table5.10.Human codeforX4
whenp0=0:6.Column 3shows
theassigned codelengths and
column 4thecodewords. Some
strings whose probabilities are
identical, e.g.,thefourth and
fth,receiv edieren tcodelengths.
Solution toexercise 5.22(p.102).Thesetofprobabilitiesfp1;p2;p3;p4g=
f1/6;1/6;1/3;1/3ggivesrisetotwodieren toptimal setsofcodelengths, because
atthesecond stepoftheHuman codingalgorithm wecanchooseanyofthe
three possible pairings. Wemayeither putthem inaconstan tlength code
f00;01;10;11gorthecodef000;001;01;1g.Bothcodeshaveexpected length
2.
Another solution isfp1;p2;p3;p4g=f1/5;1/5;1/5;2/5g.
Andathirdisfp1;p2;p3;p4g=f1/3;1/3;1/3;0g.
Solution toexercise 5.26(p.103).Letpmaxbethelargest probabilit yin
p1;p2;:::;pI.Thedierence betweentheexpected lengthLandtheentropy
Hcanbenobigger thanmax(pmax;0:086)(Gallager, 1978).
Seeexercises 5.27{5.28 tounderstand where thecurious 0.086 comes from.
Solution toexercise 5.27(p.103).Length entropy=0.086.
Solution toexercise 5.31(p.104).There aretwowaystoanswerthisproblem
correctly ,andonepopular waytoansweritincorrectly .Let'sgivetheincorrect
answerrst:
Erroneous answer.\Wecanpickarandom bitbyrstpickingarandom
source symbolxiwithprobabilit ypi,thenpickingarandom bitfrom
c(xi).Ifwedenefitobethefraction ofthebitsofc(xi)thatare1s,
wend C3:aic(ai)pili
a01/21
b101/42
c1101/83
d1111/83P(bitis1)=X
ipifi (5.34)
=1/20+1/41/2+1/82/3+1/81=1/3."(5.35)

<<<PAGE 119>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
5.9:Solutions 107
Thisansweriswrong because itfallsforthebus-stop fallacy, whichwasintro-
duced inexercise 2.35(p.38):ifbuses arriveatrandom, andweareinterested
in`theaverage timefromonebusuntilthenext', wemustdistinguish two
possible averages: (a)theaverage timefromarandomly chosen busuntilthe
next; (b)theaverage timebetweenthebusyoujustmissed andthenextbus.
Thesecond `average' istwiceasbigastherstbecause, bywaiting forabus
atarandom time, youbiasyourselection ofabusinfavourofbuses that
followalargegap.You'reunlikelytocatchabusthatcomes 10seconds after
apreceding bus!Similarly ,thesymbolscanddgetencodedintolonger-length
binary strings thana,sowhen wepickabitfromthecompressed string at
random, wearemorelikelytolandinabitbelonging toacoradthanwould
begivenbytheprobabilities piintheexpectation (5.34). Alltheprobabilities
needtobescaled upbyli,andrenormalized.
Correct answerinthesame style.Everytimesymbolxiisencoded,li
bitsareadded tothebinary string, ofwhichfiliare1s.Theexpected
numberof1sadded persymbolis
X
ipifili; (5.36)
andtheexpected totalnumberofbitsadded persymbolis
X
ipili: (5.37)
Sothefraction of1sinthetransmitted string is
P(bitis1)=P
ipifiliP
ipili(5.38)
=1/20+1/41+1/82+1/83
7/4=7/8
7/4=1=2:
Forageneral symbolcodeandageneral ensem ble,theexpectation (5.38) is
thecorrect answer.Butinthiscase,there isamore powerfulargumen twe
canuse.
Information-theoretic answer.Theencodedstring cistheoutput ofan
optimal compressor thatcompresses samples fromXdowntoanex-
pected length ofH(X)bits.Wecan't expecttocompress thisdataany
further. Butiftheprobabilit yP(bitis1)werenotequal to1/2thenit
would bepossible tocompress thebinary string further (using ablock
compression code,say).ThereforeP(bitis1)mustbeequal to1/2;in-
deedtheprobabilit yofanysequence oflbitsinthecompressed stream
taking onanyparticular valuemustbe2 l.Theoutput ofaperfect
compressor isalwaysperfectly random bits.
Toputitanother way,iftheprobabilit yP(bitis1)werenotequal to
1/2,thentheinformation contentperbitofthecompressed string would
beatmostH2(P(1)),whichwouldbelessthan1;butthiscontradicts
thefactthatwecanrecovertheoriginal datafromc,sotheinformation
contentperbitofthecompressed string mustbeH(X)=L(C;X)=1.
Solution toexercise 5.32(p.104).Thegeneral Human codingalgorithm for
anencodingalphab etwithqsymbolshasonedierence fromthebinary case.
Theprocessofcombiningqsymbolsinto1symbolreduces thenumberof
symbolsbyq 1.Soifwestart withAsymbols,we'llonlyendupwitha

<<<PAGE 120>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
108 5|SymbolCodes
completeq-arytreeifAmod(q 1)isequal to1.Otherwise, weknowthat
whatev erprex codewemake,itmustbeanincomplete treewithanumber
ofmissing leavesequal, modulo(q 1),toAmod(q 1) 1.Forexample, if
aternary treeisbuiltforeightsymbols,thenthere willunavoidably beone
missing leafinthetree.
Theoptimalq-arycodeismade byputting theseextra leavesinthelongest
branchofthetree. Thiscanbeachievedbyadding theappropriate number
ofsymbolstotheoriginal source symbolset,allofthese extra symbolshaving
probabilit yzero. Thetotalnumberofleavesisthenequal tor(q 1)+1,for
some integerr.Thesymbolsarethenrepeatedly combined bytaking theq
symbolswithsmallest probabilit yandreplacing them byasingle symbol,as
inthebinary Human codingalgorithm.
Solution toexercise 5.33(p.104).Wewishtoshowthatagreedy metaco de,
whichpicksthecodewhichgivestheshortest encoding,isactually suboptimal,
because itviolates theKraft inequalit y.
We'llassume thateachsymbolxisassigned lengthslk(x)byeachofthe
candidate codesCk.Letusassume there areKalternativ ecodesandthatwe
canencodewhichcodeisbeingusedwithaheader oflength logKbits.Then
themetaco deassigns lengthsl0(x)thataregivenby
l0(x)=log2K+min
klk(x): (5.39)
Wecompute theKraft sum:
S=X
x2 l0(x)=1
KX
x2 minklk(x): (5.40)
Let'sdivide thesetAXintonon-overlapping subsetsfAkgK
k=1suchthatsubset
Akcontainsallthesymbolsxthatthemetaco desends viacodek.Then
S=1
KX
kX
x2Ak2 lk(x): (5.41)
Nowifonesub-co deksatises theKraft equalit yP
x2AX2 lk(x)=1,thenit
mustbethecasethatX
x2Ak2 lk(x)1; (5.42)
withequalit yonlyifallthesymbolsxareinAk,whichwouldmean thatwe
areonlyusing oneoftheKcodes.So
S1
KKX
k=11=1; (5.43)
withequalit yonlyifequation (5.42) isanequalit yforallcodesk.Butit's
impossible forallthesymbolstobeinallthenon-overlapping subsetsfAkgK
k=1,
sowecan't haveequalit y(5.42) holding forallk.SoS<1.
Another wayofseeing thatamixture codeissuboptimal istoconsider
thebinary treethatitdenes. Think ofthespecialcaseoftwocodes.The
rstbitwesendidentieswhichcodeweareusing. Now,inacomplete code,
anysubsequen tbinary string isavalidstring. Butonceweknowthatwe
areusing, say,codeA,weknowthatwhat followscanonlybeacodeword
corresp onding toasymbolxwhose encodingisshorter under codeAthan
codeB.Sosome strings areinvalidcontinuations, andthemixture codeis
incomplete andsuboptimal.
Forfurther discussion ofthisissue anditsrelationship toprobabilistic
modelling readabout`bitsbackcoding' insection 28.3andinFrey(1998).

<<<PAGE 121>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
AboutChapter 6
Before reading Chapter 6,youshould havereadtheprevious chapter and
workedonmostoftheexercises init.
We'llalsomakeuseofsome Bayesian modelling ideas thatarrivedinthe
vicinit yofexercise 2.8(p.30).
109

<<<PAGE 122>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
6
Stream Codes
Inthischapter wediscuss twodatacompression schemes.
Arithmetic codingisabeautiful metho dthatgoeshandinhandwiththe
philosoph ythatcompression ofdatafromasource entailsprobabilistic mod-
elling ofthatsource. Asof1999, thebestcompression metho dsfortextles
usearithmetic coding,andseveralstate-of-the-art image compression systems
useittoo.
Lempel{Ziv codingisa`universal' metho d,designed under thephilosoph y
thatwewouldlikeasingle compression algorithm thatwilldoareasonable job
foranysource. Infact,formanyreallifesources, thisalgorithm's universal
properties holdonlyinthelimitofunfeasibly largeamoun tsofdata, but,all
thesame, Lempel{Ziv compression iswidely usedandoften eectiv e.
6.1Theguessing game
Asamotivation forthese twocompression metho ds,consider theredundancy
inatypical English textle.Suchleshaveredundancy atseverallevels:for
example, theycontaintheASCIIcharacters withnon-equal frequency; certain
consecutiv epairs ofletters aremore probable thanothers; andentirewords
canbepredicted giventhecontextandaseman ticunderstanding ofthetext.
Toillustrate theredundancy ofEnglish, andacurious wayinwhichit
could becompressed, wecanimagine aguessing game inwhichanEnglish
speakerrepeatedly attempts topredict thenextcharacter inatextle.
Forsimplicit y,letusassume thattheallowedalphab etconsists ofthe26
uppercaselettersA,B,C,..., Zandaspace `-'.Thegame involvesasking
thesubjecttoguess thenextcharacter repeatedly ,theonlyfeedbac kbeing
whether theguess iscorrect ornot,untilthecharacter iscorrectly guessed.
After acorrect guess, wenotethenumberofguesses thatweremade when
thecharacter wasidentied, andaskthesubjecttoguess thenextcharacter
inthesame way.
Onesentence gavethefollowingresult when ahuman wasaskedtoguess
asentence. Thenumbersofguesses arelisted beloweachcharacter.
THERE-IS-NO-REVERSE-ON-A-MOTORCYCLE-
11151121121115117111213212271111411111
Notice thatinmanycases, thenextletter isguessed immediately ,inone
guess. Inother cases, particularly atthestartofsyllables, more guesses are
needed.
What dothisgame andthese results oerus?First, theydemonstrate the
redundancy ofEnglish fromthepointofviewofanEnglish speaker.Second,
thisgame mightbeusedinadatacompression scheme, asfollows.
110

<<<PAGE 123>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
6.2:Arithmetic codes 111
Thestring ofnumbers`1,1,1,5,1,...',listed above,wasobtained by
presen tingthetexttothesubject.Themaxim umnumberofguesses thatthe
subjectwillmakeforagivenletter istwenty-seven,sowhat thesubjectis
doing forusisperforming atime-v arying mapping ofthetwenty-sevenletters
fA;B;C;:::;Z; gontothetwenty-sevennumbersf1;2;3;:::;27g,whichwe
canviewassymbolsinanewalphab et.Thetotalnumberofsymbolshasnot
beenreduced, butsinceheusessome ofthese symbolsmuchmore frequen tly
thanothers {forexample, 1and2{itshould beeasytocompress thisnew
string ofsymbols.
Howwouldtheuncompression ofthesequence ofnumbers`1,1,1,5,1,...'
work?Atuncompression time, wedonothavetheoriginal string `THERE...',
wehaveonlytheencodedsequence. Imagine thatoursubjecthasanabsolutely
identicaltwinwhoalsoplaystheguessing game withus,asifweknew the
source text.Ifwestophimwhenev erhehasmade anumberofguesses equal to
thegivennumber,thenhewillhavejustguessed thecorrect letter, andwecan
thensay`yes,that's right',andmovetothenextcharacter. Alternativ ely,if
theidenticaltwinisnotavailable, wecould design acompression system with
thehelpofjustonehuman asfollows.Wechooseawindo wlengthL,thatis,
anumberofcharacters ofcontexttoshowthehuman. Foreveryoneofthe
27Lpossible strings oflengthL,weaskthem, `What wouldyoupredict isthe
nextcharacter?', and`Ifthatprediction werewrong, what wouldyournext
guesses be?'.After tabulating theiranswerstothese 2627Lquestions, we
could usetwocopies ofthese enormous tables attheencoderandthedecoder
inplace ofthetwohuman twins. Suchalanguage modeliscalled anLthorder
Markovmodel.
These systems areclearly unrealistic forpractical compression, butthey
illustrate severalprinciples thatwewillmakeuseofnow.
6.2Arithmetic codes
When wediscussed variable-length symbolcodes,andtheoptimal Human
algorithm forconstructing them, weconcluded bypointingouttwopractical
andtheoretical problems withHuman codes(section 5.6).
These defects arerectied byarithmetic codes,whichwereinventedby
Elias, byRissanen andbyPasco,andsubsequen tlymade practical byWitten
etal.(1987). Inanarithmetic code,theprobabilistic modelling isclearly
separated fromtheencodingoperation. Thesystem israther similar tothe
guessing game. Thehuman predictor isreplaced byaprobabilistic modelof
thesource. Aseachsymbolisproduced bythesource, theprobabilistic model
supplies apredictiv edistribution overallpossible values ofthenextsymbol,
thatis,alistofpositivenumbersfpigthatsumtoone.Ifwechoosetomodel
thesource asproducing i.i.d.symbolswithsomeknowndistribution, thenthe
predictiv edistribution isthesameeverytime; butarithmetic codingcanwith
equal easehandle complex adaptiv emodelsthatproducecontext-dep enden t
predictiv edistributions. Thepredictiv emodelisusually implemen tedina
computer program.
Theencodermakesuseofthemodel'spredictions tocreate abinary string.
Thedecodermakesuseofanidenticaltwinofthemodel(justasintheguessing
game) tointerpret thebinary string.
Letthesource alphab etbeAX=fa1;:::;aIg,andlettheIthsymbolaI
havethespecialmeaning `endoftransmission'. Thesource spitsoutasequence
x1;x2;:::;xn;::::Thesource doesnotnecessarily producei.i.d.symbols.We
willassume thatacomputer program isprovided totheencoderthatassigns a

<<<PAGE 124>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
112 6|Stream Codes
predictiv eprobabilit ydistribution overaigiventhesequence thathasoccurred
thusfar,P(xn=aijx1;:::;xn 1).Thereceiverhasanidenticalprogram that
produces thesamepredictiv eprobabilit ydistribution P(xn=aijx1;:::;xn 1).
0.00
0.25
0.50
0.75
1.006
?0
6
?16
?0101101Figure 6.1.Binary strings dene
realintervalswithin therealline
[0,1). Werstencoun tered a
picture likethiswhen we
discussed thesymbol-code
supermark etinChapter 5.
Conceptsforunderstanding arithmetic coding
Notation forintervals. Theinterval[0:01;0:10)isallnumbersbetween0:01and0:10,
including 0:01_00:01000 :::butnot0:10_00:10000 ::::
Abinary transmission denes anintervalwithin thereallinefrom0to1.
Forexample, thestring01isinterpreted asabinary realnumber0.01...,which
corresp ondstotheinterval[0:01;0:10)inbinary ,i.e.,theinterval[0:25;0:50)
inbaseten.
Thelonger string01101 corresp onds toasmaller interval[0:01101;
0:01110). Because 01101 hastherststring,01,asaprex, thenewin-
tervalisasub-in tervaloftheinterval[0:01;0:10).Aone-megab ytebinary le
(223bits)isthusviewedasspecifying anumberbetween0and1toaprecision
ofabouttwomillion decimal places {twomillion decimal digits, because each
bytetranslates intoalittlemore thantwodecimal digits.
Now,wecanalsodivide therealline[0,1)intoIintervalsoflengths equal
totheprobabilities P(x1=ai),asshowningure 6.2.
0.00
P(x1=a1)
P(x1=a1)+P(x1=a2)
P(x1=a1)+:::+P(x1=aI 1)
1.0...6?a1
6
?a2
6?aI...a2a5a2a1Figure 6.2.Aprobabilistic model
denes realintervalswithin the
realline[0,1).
Wemaythentakeeachintervalaiandsubdivide itintointervalsde-
notedaia1;aia2;:::;aiaI,suchthatthelength ofaiajisproportional to
P(x2=ajjx1=ai).Indeed thelength oftheintervalaiajwillbeprecisely
thejointprobabilit y
P(x1=ai;x2=aj)=P(x1=ai)P(x2=ajjx1=ai): (6.1)
Iterating thisprocedure, theinterval[0;1)canbedivided intoasequence
ofintervalscorresp onding toallpossible nite length stringsx1x2:::xN,such
thatthelength ofanintervalisequal totheprobabilit yofthestring given
ourmodel.

<<<PAGE 125>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
6.2:Arithmetic codes 113
Algorithm 6.3.Arithmetic coding.
Iterativ eprocedure tondthe
interval[u;v)forthestring
x1x2:::xN.u:=0.0
v:=1.0
p:=v u
forn=1toN{
Compute thecumulativeprobabilities QnandRn(6.2,6.3)
v:=u+pRn(xnjx1;:::;xn 1)
u:=u+pQn(xnjx1;:::;xn 1)
p:=v u
}
Formulae describing arithmetic coding
Theprocessdepicted ingure 6.2canbewritten explicitly asfollows.Theintervals
aredened interms oftheloweranduppercumulativ eprobabilities
Qn(aijx1;:::;xn 1)i 1X
i0=1P(xn=ai0jx1;:::;xn 1); (6.2)
Rn(aijx1;:::;xn 1)iX
i0=1P(xn=ai0jx1;:::;xn 1): (6.3)
Asthenthsymbolarrives,wesubdivide then 1thintervalatthepointsdened by
QnandRn.Forexample, starting withtherstsymbol,theintervals`a1',`a2',and
`aI'are
a1$[Q1(a1);R1(a1))=[0;P(x1=a1)); (6.4)
a2$[Q1(a2);R1(a2))=[P(x=a1);P(x=a1)+P(x=a2)); (6.5)
and
aI$[Q1(aI);R1(aI))=[P(x1=a1)+:::+P(x1=aI 1);1:0): (6.6)
Algorithm 6.3describ esthegeneral procedure.
Toencodeastringx1x2:::xN,welocatetheintervalcorresp onding to
x1x2:::xN,andsendabinary string whose intervallieswithin thatinterval.
Thisencodingcanbeperformed onthey,aswenowillustrate.
Example: compressing thetosses ofabentcoin
Imagine thatwewatchasabentcoinistossed some numberoftimes (c.f.
example 2.7(p.30)andsection 3.2(p.51)).Thetwooutcomes when thecoin
istossed aredenotedaandb.Athird possibilit yisthattheexperimen tis
halted, aneventdenoted bythe`endofle'symbol,`2'.Because thecoinis
bent,weexpectthattheprobabilities oftheoutcomes aandbarenotequal,
though beforehand wedon't knowwhichisthemore probable outcome.
Encoding
Letthesource string be`bbba2'.Wepassalong thestring onesymbolata
timeanduseourmodeltocompute theprobabilit ydistribution ofthenext

<<<PAGE 126>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
114 6|Stream Codes
symbolgiventhestring thusfar.Letthese probabilities be:
Context
(sequence thusfar) Probabilit yofnextsymbol
P(a)=0:425 P(b)=0:425 P(2)=0:15
b P(ajb)=0:28P(bjb)=0:57P(2jb)=0:15
bb P(ajbb)=0:21P(bjbb)=0:64P(2jbb)=0:15
bbb P(ajbbb)=0:17P(bjbbb)=0:68P(2jbbb)=0:15
bbba P(ajbbba)=0:28P(bjbbba)=0:57P(2jbbba)=0:15
Figure 6.4showsthecorresp onding intervals.Theintervalbisthemiddle
0.425 of[0;1).Theintervalbbisthemiddle 0.567 ofb,andsoforth.
a
b
2ba
bb
b2bba
bbb
bb2bbba
bbbb
bbb20
100
01000
0010000
000100000
00001
00010
00011
0010
001100100
00101
00110
00111
010
0110100
010101000
01001
01010
01011
0110
011101100
01101
01110
01111
10
11100
1011000
100110000
10001
10010
10011
1010
101110100
10101
10110
10111
110
1111100
110111000
11001
11010
11011
1110
111111100
11101
11110
11111


BBBB
100111101CCCCObbbabbbaa
bbbab
bbba21001110010111
10011000
10011001
10011010
10011011
10011100
10011101
10011110
10011111
10100000Figure 6.4.Illustration ofthe
arithmetic codingprocessasthe
sequence bbba2istransmitted.
When therstsymbol`b'isobserv ed,theencoderknowsthattheencoded
string willstart `01',`10',or`11',butdoesnotknowwhich.Theencoder
writes nothing forthetimebeing,andexamines thenextsymbol,whichis`b'.
Theinterval`bb'lieswholly within interval`1',sotheencodercanwrite the
rstbit:`1'.Thethirdsymbol`b'narrowsdowntheintervalalittle, butnot
quite enough forittoliewholly within interval`10'.Onlywhen thenext`a'
isreadfromthesource canwetransmit some more bits.Interval`bbba'lies
wholly within theinterval`1001',sotheencoderadds`001'tothe`1'ithas
written. Finally when the`2'arrives,weneedaprocedure forterminating the
encoding. Magnifying theinterval`bbba2'(gure 6.4,right)wenotethatthe
markedinterval`100111101 'iswholly contained bybbba2,sotheencoding
canbecompleted byappending `11101 '.

<<<PAGE 127>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
6.2:Arithmetic codes 115
Exercise 6.1.[2,p.127]Showthattheoverhead required toterminate amessage
isnevermore than2bits,relativ etotheidealmessage length giventhe
probabilistic modelH,h(xjH)=log[1=P(xjH)].
Thisisanimportantresult. Arithmetic codingisverynearly optimal. The
message length isalwayswithin twobitsoftheShannon information content
oftheentiresource string, sotheexpected message length iswithin twobits
oftheentropyoftheentiremessage.
Decoding
Thedecoderreceivesthestring `100111101 'andpasses along itonesymbol
atatime. First, theprobabilities P(a);P(b);P(2)arecomputed using the
identicalprogram thattheencoderusedandtheintervals`a',`b'and`2'are
deduced. Once thersttwobits`10'havebeenexamined, itiscertain that
theoriginal string musthavebeenstarted witha`b',sincetheinterval`10'lies
wholly within interval`b'.Thedecodercanthenusethemodeltocompute
P(ajb);P(bjb);P(2jb)anddeduce theboundaries oftheintervals`ba',`bb'
and`b2'.Continuing,wedecodethesecondboncewereach`1001',thethird
boncewereach`100111 ',andsoforth, withtheunambiguous identication
of`bbba2'oncethewhole binary string hasbeenread. With theconvention
that`2'denotes theendofthemessage, thedecoderknowstostopdecoding.
Transmission ofmultiple les
Howmightoneusearithmetic codingtocomm unicate severaldistinct lesover
thebinary channel? Once the2character hasbeentransmitted, weimagine
thatthedecoderisresetintoitsinitial state. There isnotransfer ofthelearnt
statistics oftherstletothesecond le.If,however,wedidbelievethat
there isarelationship among thelesthatwearegoing tocompress, wecould
dene ouralphab etdieren tly,introducing asecond end-of-le character that
marks theendofthelebutinstructs theencoderanddecodertocontinue
using thesame probabilistic model.
Thebigpicture
Notice thattocomm unicate astring ofNletters boththeencoderandthe
decoderneeded tocompute onlyNjAjconditional probabilities {theproba-
bilities ofeachpossible letter ineachcontextactually encoun tered {justasin
theguessing game. Thiscostcanbecontrasted withthealternativ eofusing
aHuman codewithalargeblocksize(inorder toreduce thepossible one-
bit-per-sym boloverhead discussed insection 5.6),where allblocksequences
thatcould occurmustbeconsidered andtheirprobabilities evaluated.
Notice howexible arithmetic codingis:itcanbeusedwithanysource
alphab etandanyencodedalphab et.Thesizeofthesource alphab etandthe
encodedalphab etcanchange withtime. Arithmetic codingcanbeusedwith
anyprobabilit ydistribution, whichcanchange utterly fromcontexttocontext.
Furthermore, ifwewouldlikethesymbolsoftheencodingalphab et(say,
0and1)tobeusedwithunequalfrequency ,thatcaneasily bearranged by
subdividing theright-hand intervalinproportion totherequired frequencies.
Howtheprobabilistic modelmight makeitspredictions
Thetechnique ofarithmetic codingdoesnotforceonetoproducethepredic-
tiveprobabilit yinanyparticular way,butthepredictiv edistributions might

<<<PAGE 128>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
116 6|Stream Codes
a
b
2aa
ab
a2aaa
aab
aa2aaaa
aaab
aaba
aabb
aba
abb
ab2abaa
abab
abba
abbb
ba
bb
b2baa
bab
ba2baaa
baab
baba
babb
bba
bbb
bb2bbaa
bbab
bbba
bbbb0
100
01000
0010000
000100000
00001
00010
00011
0010
001100100
00101
00110
00111
010
0110100
010101000
01001
01010
01011
0110
011101100
01101
01110
01111
10
11100
1011000
100110000
10001
10010
10011
1010
101110100
10101
10110
10111
110
1111100
110111000
11001
11010
11011
1110
111111100
11101
11110
11111Figure 6.5.Illustration ofthe
intervalsdened byasimple
Bayesianprobabilistic model.The
sizeofanintervalsisproportional
totheprobabilit yofthestring.
Thismodelanticipates thatthe
source islikelytobebiased
towardsoneofaandb,so
sequences havinglotsofasorlots
ofbshavelarger intervalsthan
sequences ofthesamelength that
are50:50asandbs.
naturally beproduced byaBayesian model.
Figure 6.4wasgenerated using asimple modelthatalwaysassigns aprob-
abilityof0.15to2,andassigns theremaining 0.85toaandb,divided in
proportion toprobabilities givenbyLaplace's rule,
PL(ajx1;:::;xn 1)=Fa+1
Fa+Fb+2; (6.7)
whereFa(x1;:::;xn 1)isthenumberoftimes thatahasoccurred sofar,and
Fbisthecountofbs.These predictions corresp ondstoasimple Bayesian
modelthatexpectsandadapts toanon-equal frequency ofuseofthesource
symbolsaandbwithin ale.
Figure 6.5displa ystheintervalscorresp onding toanumberofstrings of
length uptove.Notethatifthestring sofarhascontained alargenumberof
bsthentheprobabilit yofbrelativ etoaisincreased, andconversely ifmany
asoccurthenasaremade moreprobable. Larger intervals,remem ber,require
fewerbitstoencode.
Details oftheBayesian model
Havingemphasized thatanymodelcould beused{arithmetic codingisnotwedded
toanyparticular setofprobabilities {letmeexplain thesimple adaptiv eprobabilistic

<<<PAGE 129>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
6.2:Arithmetic codes 117
modelusedinthepreceding example; werstencoun tered thismodelinexercise 2.8
(p.30).
Assumptions
Themodelwillbedescrib edusing parameters p2,paandpb,dened below,which
should notbeconfused withthepredictiv eprobabilities inaparticular context ,for
example, P(ajs=baa).Abentcoinlabelledaandbistossed somenumberoftimes l,
whichwedon't knowbeforehand. Thecoin's probabilit yofcoming upawhen tossed
ispa,andpb=1 pa;theparameters pa;pbarenotknownbeforehand. Thesource
strings=baaba 2indicates thatlwas5andthesequence ofoutcomes wasbaaba .
1.Itisassumed thatthelength ofthestring lhasanexponentialprobabilit y
distribution
P(l)=(1 p2)lp2: (6.8)
Thisdistribution corresp ondstoassuming aconstan tprobabilit yp2forthe
termination symbol`2'ateachcharacter.
2.Itisassumed thatthenon-terminal characters inthestring areselected inde-
penden tlyatrandom from anensem blewithprobabilities P=fpa;pbg;the
probabilit ypaisxedthroughout thestring tosome unkno wnvaluethatcould
beanywhere between0and1.Theprobabilit yofanaoccurring asthenext
symbol,givenpa(ifonlyweknew it),is(1 p2)pa.Theprobabilit y,given
pa,thatanunterminated string oflength Fisagivenstring sthatcontains
fFa;Fbgcountsofthetwooutcomes istheBernoulli distribution
P(sjpa;F)=pFa
a(1 pa)Fb: (6.9)
3.Weassume auniform priordistribution forpa,
P(pa)=1;pa2[0;1] (6.10)
andpb1 pa.Itwouldbeeasytoassume other priors onpa,withbeta
distributions beingthemostconvenienttohandle.
Thismodelwasstudied insection 3.2.Thekeyresult werequire isthepredictiv e
distribution forthenextsymbol,giventhestring sofar,s.Thisprobabilit yofanaor
bbeingthenextcharacter (assuming thatitisnot`2')wasderivedinequation (3.16)
andisprecisely Laplace's rule(6.7).
.Exercise 6.2.[3]Compare theexpected message length when anASCIIleis
compressed bythefollowingthree metho ds.
Human-with-header .Read thewhole le,ndtheempirical fre-
quency ofeachsymbol,construct aHuman codeforthose frequen-
cies,transmit thecodebytransmitting thelengths oftheHuman
codewords, thentransmit theleusing theHuman code.(The
actual codewordsdon't needtobetransmitted, sincewecanusea
deterministic metho dforbuilding thetreegiventhecodelengths.)
Arithmetic codeusing theLaplace model.
PL(ajx1;:::;xn 1)=Fa+1P
a0(Fa0+1): (6.11)
Arithmetic codeusing aDirichletmodel.This model's predic-
tionsare:
PL(ajx1;:::;xn 1)=Fa+P
a0(Fa0+); (6.12)
whereisxedtoanumbersuchas0.01. Thiscorresp ondstoa
more responsiveversion oftheLaplace model;theprobabilit yover
characters isexpected tobemore nonuniform;=1reproduces
theLaplace model.
Takecarethattheheader ofyourHuman message isself-delimiting.
Specialcasesworthconsidering are(a)shortleswithjustafewhundred
characters; (b)largelesinwhichsome characters areneverused.

<<<PAGE 130>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
118 6|Stream Codes
6.3Further applications ofarithmetic coding
Ecient generationofrandom samples
Arithmetic codingnotonlyoers awaytocompress strings believedtocome
fromagivenmodel;italsooers awaytogenerate random strings froma
model.Imagine stickingapinintotheunitintervalatrandom, thatline
havingbeendivided intosubintervalsinproportion toprobabilities pi;the
probabilit ythatyourpinwilllieinintervaliispi.
Sotogenerate asample fromamodel,allweneedtodoisfeedordinary
random bitsintoanarithmetic decoderforthatmodel.Aninnite random
bitsequence corresp ondstotheselection ofapointatrandom fromtheline
[0;1),sothedecoderwillthenselect astring atrandom fromtheassumed
distribution. Thisarithmetic metho disguaran teedtouseverynearly the
smallest numberofrandom bitspossible tomaketheselection {animportant
pointincomm unities where random numbersareexpensive![Thisisnotajoke.
Large amoun tsofmoney arespentongenerating random bitsinsoftwareand
hardw are.Random numbersarevaluable.]
Asimple example oftheuseofthistechnique isinthegeneration ofrandom
bitswithanonuniform distributionfp0;p1g.
Exercise 6.3.[2,p.127]Compare thefollowing twotechniques forgenerating
random symbolsfromanonuniform distributionfp0;p1g=f0:99;0:01g:
(a)Thestandard metho d:useastandard random numbergenerator
togenerate aninteger between1and232.Rescale theinteger to
(0;1).Testwhether thisuniformly distributed random variable is
lessthan0:99,andemita0or1accordingly .
(b)Arithmetic codingusing thecorrect model,fedwithstandard ran-
dombits.
Roughly howmanyrandom bitswilleachmetho dusetogenerate a
thousand samples fromthissparse distribution?
Ecient data-entry devices
When weentertextintoacomputer, wemakegestures ofsome sort{maybe
wetapakeyboard, orscribble withapointer,orclickwithamouse; an
ecient textentrysystem isonewhere thenumberofgestures required to
enteragiventextstring issmall.
Writing canbeviewedasaninverseprocesstodatacompression. IndataCompression:
text!bits
Writing:
text gestures
compression, theaimistomapagiventextstring intoasmallnumberofbits.
Intextentry,wewantasmall sequence ofgestures toproduceourintended
text.
Byinverting anarithmetic coder,wecanobtain aninformation-ecien t
textentrydevice thatisdrivenbycontinuouspointinggestures (Wardetal.,
2000). Inthissystem, called Dasher, theuserzoomsinontheunitintervalto
locatetheintervalcorresp onding totheirintended string, inthesamestyleas
gure 6.4.Alanguage model(exactly asusedintextcompression) controls
thesizesoftheintervalssuchthatprobable strings arequickandeasyto
identify.After anhour's practice, anoviceusercanwrite withonenger
driving Dasher atabout25wordsperminute{that's abouthalftheirnormal
ten-nger typing speedonaregular keyboard. It'sevenpossible towriteat25

<<<PAGE 131>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
6.4:Lempel{Ziv coding 119
wordsperminute,hands-fr ee,using gazedirection todriveDasher (Wardand
MacKa y,2002). Dasher isavailable asfreesoftwareforvarious platforms.1
6.4Lempel{Ziv coding
TheLempel{Ziv algorithms, whicharewidely usedfordatacompression (e.g.,
thecompress andgzipcommands), aredieren tinphilosoph ytoarithmetic
coding. There isnoseparation betweenmodelling andcoding, andnooppor-
tunityforexplicit modelling.
Basic Lempel{Ziv algorithm
Themetho dofcompression istoreplace asubstring with apointerto
anearlier occurrence ofthesame substring. Forexample ifthestring is
1011010100010 ...,weparse itintoanordered dictionary ofsubstrings that
havenotappeared beforeasfollows:,1,0,11,01,010,00,10,....Wein-
clude theemptysubstringastherstsubstring inthedictionary andorder
thesubstrings inthedictionary bytheorder inwhichtheyemerged fromthe
source. After everycomma, welookalong thenextpartoftheinput sequence
untilwehavereadasubstring thathasnotbeenmarkedobefore. Amo-
ment'sreection willconrm thatthissubstring islonger byonebitthana
substring thathasoccurred earlier inthedictionary .Thismeans thatwecan
encodeeachsubstring bygiving apointertotheearlier occurrence ofthatpre-
xandthensending theextra bitbywhichthenewsubstring inthedictionary
diers fromtheearlier substring. If,atthenthbit,wehaveenumerateds(n)
substrings, thenwecangivethevalueofthepointerindlog2s(n)ebits.The
codefortheabovesequence isthenasshowninthefourth lineofthefollowing
table (with punctuation included forclarity),theupperlinesindicating the
source string andthevalueofs(n):
source substrings 101101010 00 10
s(n) 01 2 3 4 5 6 7
s(n)binary 000001010011100101 110 111
(pointer;bit) (;1)(0;0)(01;1)(10;1)(100;0)(010;0)(001;0)
Notice thattherstpointerwesendisempty,because, giventhatthere is
onlyonesubstring inthedictionary {thestring{nobitsareneeded to
conveythe`choice' ofthatsubstring astheprex. Theencodedstring is
100011101100001000010 .Theencoding, inthissimple case, isactually a
longer string thanthesource string, because there wasnoobvious redundancy
inthesource string.
.Exercise 6.4.[2]Provethatanyuniquely decodeable codefromf0;1g+to
f0;1g+necessarily makessome strings longer ifitmakessome strings
shorter.
Onereason whythealgorithm describ edabovelengthens alotofstrings is
because itisinecien t{ittransmits unnecessary bits;toputitanother way,
itscodeisnotcomplete. Once asubstring inthedictionary hasbeenjoined
there bybothofitschildren, thenwecanbesurethatitwillnotbeneeded
(except possibly aspartofourprotocolforterminating amessage); soatthat
pointwecould dropitfrom ourdictionary ofsubstrings andshuethem
allalong one,thereb yreducing thelength ofsubsequen tpointermessages.
1www.inference.phy.cam.ac.uk/da sher/

<<<PAGE 132>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
120 6|Stream Codes
Equiv alently,wecould write thesecond prex intothedictionary atthepoint
previously occupied bytheparent.Asecond unnecessary overhead isthe
transmission ofthenewbitinthese cases {thesecond timeaprex isused,
wecanbesureoftheidentityofthenextbit.
Decoding
Thedecoderagain involvesanidenticaltwinatthedecodingendwhocon-
structs thedictionary ofsubstrings asthedataaredecoded.
.Exercise 6.5.[2,p.128]Encodethestring000000000000100000000000 using
thebasic Lempel{Ziv algorithm describ edabove.
.Exercise 6.6.[2,p.128]Decodethestring
00101011101100100100011010 101000011
thatwasencodedusing thebasic Lempel{Ziv algorithm.
Practicalities
Inthisdescription Ihavenotdiscussed themetho dforterminating astring.
There aremanyvariations ontheLempel{Ziv algorithm, allexploiting the
sameideabutusing dieren tprocedures fordictionary managemen t,etc.The
resulting programs arefast,buttheirperformance oncompression ofEnglish
text, although useful, doesnotmatchthestandards setinthearithmetic
codingliterature.
Theoreticalproperties
Incontrasttotheblockcode,Human code,andarithmetic codingmetho ds
wediscussed inthelastthree chapters, theLempel{Ziv algorithm isdened
without making anymentionofaprobabilistic modelforthesource. Yet,given
anyergodicsource (i.e.,onethatismemoryless onsucien tlylongtimescales),
theLempel{Ziv algorithm canbeprovenasymptotic allytocompress downto
theentropyofthesource. Thisiswhyitiscalled a`universal' compression
algorithm. Foraproofofthisproperty,seeCoverandThomas (1991).
Itachievesitscompression, however,onlybymemorizing substrings that
havehappenedsothatithasashort name forthem thenexttimetheyoccur.
Theasymptotic timescale onwhichthisuniversalperformance isachievedmay,
formanysources, beunfeasibly long,because thenumberoftypical substrings
thatneedmemorizing maybeenormous. Theuseful performance oftheal-
gorithm inpractice isareection ofthefactthatmanylescontainmultiple
repetitions ofparticular short sequences ofcharacters, aformofredundancy
towhichthealgorithm iswellsuited.
Common ground
Ihaveemphasized thedierence inphilosoph ybehind arithmetic codingand
Lempel{Ziv coding. There iscommon ground betweenthem, though: inprin-
ciple, onecandesign adaptiv eprobabilistic models,andthence arithmetic
codes,thatare`universal', thatis,modelsthatwillasymptotically compress
anysourceinsomeclass towithin some factor (preferably 1)ofitsentropy.
However,forpractical purposes,Ithink suchuniversalmodelscanonlybe
constructed iftheclassofsources isseverely restricted. Ageneral purpose
compressor thatcandiscovertheprobabilit ydistribution ofanysource would

<<<PAGE 133>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
6.5:Demonstration 121
beageneral purposearticial intelligence! Ageneral purposearticial intelli-
gence doesnotyetexist.
6.5Demonstration
Aninteractiv eaidforexploring arithmetic coding,dasher.tcl ,isavailable.2
Ademonstration arithmetic-co dingsoftwarepackagewritten byRadford
Neal3consists ofencodinganddecodingmodules towhichtheuseraddsa
moduledening theprobabilistic model.Itshould beemphasized thatthere
isnosingle general-purp osearithmetic-co dingcompressor; anewmodelhasto
bewritten foreachtypeofsource. Radford Neal's packageincludes asimple
adaptiv emodelsimilar totheBayesian modeldemonstrated insection 6.2.
Theresults using thisLaplace modelshould beviewedasabasic benchmark
since itisthesimplest possible probabilistic model{itsimply assumes the
characters inthelecome independen tlyfromaxedensem ble.Thecounts
fFigofthesymbolsfaigarerescaled androunded astheleisreadsuchthat
allthecountsliebetween1and256.
Astate-of-the-art compressor fordocumen tscontaining textandimages,
DjVu,usesarithmetic coding.4Itusesacarefully designed approximate arith-
metic coderforbinary alphab etscalled theZ-coder(Bottou etal.,1998), which
ismuchfaster thanthearithmetic codingsoftwaredescrib edabove.Oneof
theneattrickstheZ-coderusesisthis:theadaptiv emodeladapts onlyocca-
sionally (tosaveoncomputer time), withthedecision aboutwhen toadapt
beingpseudo-randomly controlled bywhether thearithmetic encoderemitted
abit.
TheJBIG image compression standard forbinary images usesarithmetic
codingwithacontext-dep enden tmodel,whichadapts using arulesimilar to
Laplace's rule.PPM (Teahan, 1995) isaleading metho dfortextcompression,
anditusesarithmetic coding.
There aremanyLempel{Ziv-based programs. gzipisbased onaversion
ofLempel{Ziv called `LZ77'.compress isbased on`LZW'.Inmyexperience
thebestisgzip,withcompress beinginferior onmostles.
bzip isablock-sorting lecompressor ,whichmakesuseofaneathack
called theBurro ws{Wheeler transform (Burro wsandWheeler, 1994). This
metho disnotbased onanexplicit probabilistic model,anditonlyworkswell
forleslarger thanseveralthousand characters; butinpractice itisavery
eectiv ecompressor forlesinwhichthecontextofacharacter isagood
predictor forthatcharacter.5
Compr ession ofatextle
Table6.6givesthecomputer timeinseconds takenandthecompression
achievedwhen these programs areapplied totheLATEXlecontaining the
textofthischapter, ofsize20,942 bytes.
Compr ession ofasparsele
Interestingly ,gzipdoesnotalwaysdosowell.Table6.7givesthecompres-
sionachievedwhen these programs areapplied toatextlecontaining 106
2http://www.inference.phy.cam.a c.uk/mackay/itprnn/softwareI.html
3ftp://ftp.cs.toronto.edu/pub/r adford/www/ac.software.html
4http://www.djvuzone.org/
5There isalotofinformation abouttheBurro ws{Wheeler transform onthenet.
http://dogma.net/DataCompressio n/BWT.shtml

<<<PAGE 134>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
122 6|Stream Codes
Metho d Compression Compressed size Uncompression
time=sec (%age of20,942) time=sec
Laplace model 0.28 12974(61%) 0.32
gzip 0.10 8177(39%) 0.01
compress 0.05 10816(51%) 0.05
bzip 7495(36%)
bzip2 7640(36%)
ppmz 6800(32%)Table6.6.Comparison of
compression algorithms applied to
atextle.
characters, eachofwhichiseither0and1withprobabilities 0.99and0.01.
TheLaplace modelisquite wellmatchedtothissource, andthebenchmark
arithmetic codergivesgoodperformance, followedclosely bycompress ;gzip,
interestingly ,isworst. Anideal modelforthissource wouldcompress the
leintoabout106H2(0:01)=8'10100bytes. TheLaplace modelcompressor
fallsshort ofthisperformance because itisimplemen tedusing onlyeight-bit
precision. Theppmzcompressor compresses thebestofall,buttakesmuch
more computer time.
Metho d Compression Compressed size Uncompression
time=sec =bytes time=sec
Laplace model 0.45 14143(1.4%) 0.57
gzip 0.22 20646(2.1%) 0.04
gzip--best+ 1.63 15553(1.6%) 0.05
compress 0.13 14785(1.5%) 0.03
bzip 0.30 10903(1.09%) 0.17
bzip2 0.19 11260(1.12%) 0.05
ppmz 533 10447(1.04%) 535Table6.7.Comparison of
compression algorithms applied to
arandom leof106characters,
99%0sand1%1s.
6.6Summary
Inthelastthree chapters wehavestudied three classes ofdatacompression
codes.
Fixed-length blockcodes(Chapter 4).These aremappings fromaxed
numberofsource symbolstoaxedlength binary message. Onlyatiny
fraction ofthesource strings aregivenanencoding. These codeswere
funforidentifying theentropyasthemeasure ofcompressibilit ybutthey
areoflittlepractical use.
Symbolcodes(Chapter 5).Symbolcodesemplo yavariable length codefor
eachsymbolinthesource alphab et,thecodelengths beingintegerlengths
determined bytheprobabilities ofthesymbols.Human's algorithm
constructs anoptimal symbolcodeforagivensetofsymbolprobabilities.
Everysource string hasauniquely decodeable encoding,andifthesource
symbolscome fromtheassumed distribution thenthesymbolcodewill
compress toanexpected lengthLlyingintheinterval[H;H+1).Sta-
tistical uctuations inthesource maymaketheactual length longer or
shorter thanthismean length.

<<<PAGE 135>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
6.7:Exercises onstream codes 123
Ifthesource isnotwellmatchedtotheassumed distribution thenthe
mean length isincreased bytherelativ eentropyDKLbetweenthesource
distribution andthecode'simplicit distribution. Forsources withsmall
entropy,thesymbolhastoemitatleast onebitpersource symbol;
compression belowonebitpersource symbolcanonlybeachievedby
thecumbersome procedure ofputting thesource dataintoblocks.
Stream codes.Thedistinctiv epropertyofstream codes,compared with
symbolcodes,isthattheyarenotconstrained toemitatleastonebitfor
everysymbolreadfromthesource stream. Solargenumbersofsource
symbolsmaybecodedintoasmaller numberofbits. Thisproperty
could onlybeobtained using asymbolcodeifthesource stream were
someho wchoppedintoblocks.
Arithmetic codescombineaprobabilistic modelwithanencoding
algorithm thatidentieseachstring withasub-in tervalof[0;1)of
sizeequal totheprobabilit yofthatstring under themodel.This
codeisalmost optimal inthesense thatthecompressed length ofa
stringxclosely matchestheShannon information contentofxgiven
theprobabilistic model.Arithmetic codestwiththephilosoph y
thatgoodcompression requires datamodelling ,intheformofan
adaptiv eBayesian model.
Lempel{Ziv codesareadaptiv einthesense thattheymemorize
strings thathavealready occurred. They arebuiltonthephiloso-
phythatwedon't knowanything atallaboutwhattheprobabilit y
distribution ofthesource willbe,andwewantacompression algo-
rithm thatwillperform reasonably wellwhatev erthatdistribution
is.
Both arithmetic codesandLempel{Ziv codeswillfailtodecodecorrectly
ifanyofthebitsofthecompressed learealtered. Soifcompressed lesare
tobestored ortransmitted overnoisy media, error-correcting codeswillbe
essential.Reliable comm unication overunreliable channels isthetopic ofthe
nextfewchapters.
6.7Exercises onstream codes
Exercise 6.7.[2]Describ eanarithmetic codingalgorithm toencoderandom bit
strings oflengthNandweightK(i.e.,KonesandN Kzeroes)where
NandKaregiven.
ForthecaseN=5,K=2showindetail theintervalscorresp onding to
allsource substrings oflengths 1{5.
.Exercise 6.8.[2,p.128]Howmanybitsareneeded tospecifyaselection ofK
objectsfromNobjects? (NandKareassumed tobeknownandthe
selection ofKobjectsisunordered.) Howmightsuchaselection be
made atrandom without beingwasteful ofrandom bits?
.Exercise 6.9.[2]Abinary sourceXemits independen tidentically distributed
symbolswithprobabilit ydistributionff0;f1g,wheref1=0:01.Find
anoptimal uniquely-deco deable symbolcodeforastringx=x1x2x3of
three successiv esamples fromthissource.
Estimate (toonedecimal place) thefactor bywhichtheexpected length
ofthisoptimal codeisgreater thantheentropyofthethree-bit stringx.

<<<PAGE 136>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
124 6|Stream Codes
[H2(0:01)'0:08,whereH2(x)=xlog2(1=x)+(1 x)log2(1=(1 x)).]
Anarithmetic codeisusedtocompress astring of1000samples from
thesourceX.Estimate themean andstandard deviation ofthelength
ofthecompressed le.
.Exercise 6.10.[2]Describ eanarithmetic codingalgorithm togenerate random
bitstrings oflengthNwithdensit yf(i.e.,eachbithasprobabilit yfof
beingaone)whereNisgiven.
Exercise 6.11.[2]Useamodied Lempel{Ziv algorithm inwhich,asdiscussed
onp.119,thedictionary ofprexes ispruned bywriting newprexes
intothespace occupied byprexes thatwillnotbeneeded again.
Suchprexes canbeidentied when boththeir children havebeen
added tothedictionary ofprexes. (Youmayneglect theissue of
termination ofencoding.) Usethisalgorithm toencodethestring
010000100010001010100000 1.Highligh tthebitsthatfollowaprex
onthesecond occasion thatthatprex isused. (Asdiscussed earlier,
these bitscould beomitted.)
Exercise 6.12.[2,p.128]Showthatthismodied Lempel{Ziv codeisstillnot
`complete', thatis,there arebinary strings thatarenotencodings of
anystring.
.Exercise 6.13.[3,p.128]Giveexamples ofsimple sources thathavelowentropy
butwouldnotbecompressed wellbytheLempel{Ziv algorithm.
6.8Further exercises ondatacompression
Thefollowingexercises maybeskippedbythereader whoiseager tolearn
aboutnoisy channels.
Exercise 6.14.[3,p.130]Consider aGaussian distribution inNdimensions,
P(x)=1
(22)N=2exp 
 P
nx2
n
22!
: (6.13)
Dene theradius ofapointxtober= P
nx2
n1=2.Estimate themean
andvariance ofthesquare oftheradius,r2= P
nx2
n.
Youmayndhelpful theintegral
Z
dx1
(22)1=2x4exp
 x2
22
=34; (6.14)
though youshould beabletoestimate therequired quantitieswithout it.probabilit ydensit y
ismaximized here
almost all
probabilit ymassisherep
N
Figure 6.8.Schematic
represen tation ofthetypical setof
anN-dimensional Gaussian
distribution.
Assuming thatNislarge, showthatnearly alltheprobabilit yofa
Gaussian iscontained inathinshellofradiusp
N.Findthethickness
oftheshell.
Evaluate theprobabilit ydensit y(6.13) atapointinthatthinshelland
attheoriginx=0andcompare. UsethecaseN=1000asanexample.
Notice thatnearly alltheprobabilit ymassislocated inadieren tpart
ofthespace fromtheregion ofhighest probabilit ydensit y.

<<<PAGE 137>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
6.8:Further exercises ondatacompression 125
Exercise 6.15.[2]Explain whatismeantbyanoptimal binary symbolcode.
Findanoptimal binary symbolcodefortheensem ble:
A=fa;b;c;d;e;f;g;h;i;jg;
P=1
100;2
100;4
100;5
100;6
100;8
100;9
100;10
100;25
100;30
100
;
andcompute theexpected length ofthecode.
Exercise 6.16.[2]Astringy=x1x2consists oftwoindependen tsamples from
anensem ble
X:AX=fa;b;cg;PX=1
10;3
10;6
10
:
What istheentropyofy?Construct anoptimal binary symbolcodefor
thestringy,andnditsexpected length.
Exercise 6.17.[2]Strings ofNindependen tsamples from anensem blewith
P=f0:1;0:9garecompressed using anarithmetic codethatismatched
tothatensem ble.Estimate themean andstandard deviation ofthe
compressed strings' lengths forthecaseN=1000. [H2(0:1)'0:47]
Exercise 6.18.[3]Source codingwithvariable-length symbols.
Inthechapters onsource coding, weassumed thatwewere
encodingintoabinary alphab etf0;1ginwhichbothsymbols
should beusedwithequal frequency .Inthisquestion weex-
plorehowtheencodingalphab etshould beusedifthesymbols
takedieren ttimes totransmit.
Apoverty-stric kenstuden tcomm unicates forfreewithafriend using a
telephone byselecting anintegern2f1;2;3:::g,making thefriend's
phone ringntimes, thenhanging upinthemiddle ofthenthring.This
processisrepeated sothatastring ofsymbolsn1n2n3:::isreceived.
What istheoptimal waytocomm unicate? Iflargeintegersnareselected
thenthemessage takeslonger tocomm unicate. Ifonlysmall integersn
areusedthentheinformation contentpersymbolissmall. Weaimto
maximize therateofinformation transfer, perunittime.
Assume thatthetimetakentotransmit anumberofringsnandto
redial islnseconds. Consider aprobabilit ydistribution overn,fpng.
Dening theaverage duration persymboltobe
L(p)=X
npnln (6.15)
andtheentropypersymboltobe
H(p)=X
npnlog21
pn; (6.16)
showthatfortheaverage information ratepersecondtobemaximized,
thesymbolsmustbeusedwithprobabilties oftheform
pn=1
Z2 ln(6.17)

<<<PAGE 138>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
126 6|Stream Codes
whereZ=P
n2 lnandsatises theimplicit equation
=H(p)
L(p); (6.18)
thatis,istherateofcomm unication. Showthatthese twoequations
(6.17, 6.18) imply thatmustbesetsuchthat
logZ=0: (6.19)
Assuming thatthechannel hastheproperty
ln=nseconds; (6.20)
ndtheoptimal distribution pandshowthatthemaximal information
rateis1bitpersecond.
Howdoesthiscompare withtheinformation ratepersecond achievedif
pissetto(1=2;1=2;0;0;0;0;:::)|thatis,onlythesymbolsn=1and
n=2areselected, andtheyhaveequal probabilit y?
Discuss therelationship betweentheresults (6.17, 6.19) derivedabove,
andtheKraft inequalit yfromsource codingtheory .
Howmightarandom binary source beecien tlyencodedintoase-
quence ofsymbolsn1n2n3:::fortransmission overthechannel dened
inequation (6.20)?
.Exercise 6.19.[1]Howmanybitsdoesittaketoshueapackofcards?
.Exercise 6.20.[2]Inthecardgame Bridge, thefourplayersreceive13cards
eachfromthedeckof52andstarteachgame bylooking attheirown
handandbidding. Thelegalbidsare,inascending order 1|;1};1~;1;
1NT;2|;2};:::7~;7;7NT,andsuccessiv ebidsmustfollowthis
order; abidof,say,2~mayonlybefollowedbyhigher bidssuchas2
or3|or7NT.(Letusneglect the`double' bid.)
Theplayershaveseveralaimswhen bidding. Oneoftheaimsisfortwo
partners tocomm unicate toeachother asmuchaspossible aboutwhat
cards areintheirhands.
Letusconcen trateonthistask.
(a)After thecards havebeendealt, howmanybitsareneeded forNorth
toconveytoSouth whatherhandis?
(b)Assuming thatEandWdonotbidatall,what isthemaxim um
total information thatNandScanconveytoeachother while
bidding? Assume thatNstarts thebidding, andthatonceeither
NorSstops bidding, thebidding stops.
.Exercise 6.21.[2]Myold`arabic' microwaveovenhad11buttons forentering
cooking times, andmynew`roman' microwavehasjustve.Thebut-
tonsoftheroman microwavearelabelled`10minutes', `1minute',`10
seconds', `1second', and`Start'; I'llabbreviate these vestrings tothe
symbolsM,C,X,I,2.Toenteroneminuteandtwenty-three secondsArabic
123
456
789
02Roman
MX
CI2
Figure 6.9.Alternativ ekeypads
formicrowaveovens.
(1:23), thearabic sequence is
1232; (6.21)

<<<PAGE 139>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
6.9:Solutions 127
andtheroman sequence is
CXXIII 2: (6.22)
Eachofthese keypads denes acodemapping the3599cooking times
from0:01to59:59 intoastring ofsymbols.
(a)Whichtimes canbeproduced withtwoorthree symbols?(For
example, 0:20canbeproduced bythree symbolsineither code:
XX2and202.)
(b)Arethetwocodescomplete? Giveadetailed answer.
(c)Foreachcode,name acooking timethatitcanproduceinfour
symbolsthattheother codecannot.
(d)Discuss theimplicit probabilit ydistributions overtimes towhich
eachofthese codesisbestmatched.
(e)Conco ctaplausible probabilit ydistribution overtimes thatareal
usermightuse,andevaluate roughly theexpected numberofsym-
bols,andmaxim umnumberofsymbols,thateachcoderequires.
Discuss thewaysinwhicheachcodeisinecien torecien t.
(f)Inventamore ecien tcooking-time-enco dingsystem forami-
crowaveoven.
Exercise 6.22.[2,p.132]Isthestandard binary represen tation forpositiveinte-
gers(e.g.cb(5)=101)auniquely decodeable code?
Design abinary codeforthepositiveintegers, i.e.,amapping from
n2f1;2;3;:::gtoc(n)2f0;1g+,thatisuniquely decodeable. Try
todesign codesthatareprex codesandthatsatisfy theKraft equalit yP
n2 ln=1.
Motiv ations: anydataleterminated byaspecialendoflecharacter canbe
mappedontoaninteger, soaprex codeforintegers canbeusedasaself-
delimiting encodingoflestoo.Large lescorresp ondtolargeintegers. Also,
oneofthebuilding blocksofa`universal' codingscheme {thatis,acoding
scheme thatwillworkOKforalargevarietyofsources {istheabilitytoencode
integers. Finally ,inmicrowaveovens,cooking times arepositiveintegers!
Discuss criteria bywhichonemightcompare alternativ ecodesforinte-
gers(or,equivalently,alternativ eself-delimiting codesforles).
6.9Solutions
Solution toexercise 6.1(p.115).Theworst-case situation iswhen theinterval
toberepresen tedliesjustinside abinary interval.Inthiscase,wemaychoose
either oftwobinary intervalsasshowningure 6.10. These binary intervals
arenosmaller thanP(xjH)=4,sothebinary encodinghasalength nogreater
thanlog21=P(xjH)+log24,whichistwobitsmore thantheidealmessage
length.
Solution toexercise 6.3(p.118).Thestandard metho duses32random bits
pergenerated symbolandsorequires 32000bitstogenerate onethousand
samples.
Arithmetic codingusesonaverage aboutH2(0:01)=0:081bitspergener-
atedsymbol,andsorequires about83bitstogenerate onethousand samples
(assuming anoverhead ofroughly twobitsassociated withtermination).
Fluctuations inthenumberof1swouldproducevariations around this
mean withstandard deviation 21.

<<<PAGE 140>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
128 6|Stream Codes
Source string's interval
P(xjH)Binary intervals
?6
6
?
6
?Figure 6.10.Termination of
arithmetic codingintheworst
case,where there isatwobit
overhead. Either ofthetwo
binary intervalsmarkedonthe
right-hand sidemaybechosen.
These binary intervalsareno
smaller thanP(xjH)=4.
Solution toexercise 6.5(p.120).Theencodingis010100110010110001100 ,
whichcomes fromtheparsing
0;00;000;0000;001;00000;000000 (6.23)
whichisencodedthus
(;0);(1;0);(10;0);(11;0);(010;1);(100;0);(110;0): (6.24)
Solution toexercise 6.6(p.120).Thedecodingis
010000100010001010100000 1.
Solution toexercise 6.8(p.123).Thisproblem isequivalenttoexercise 6.7
(p.123).
Theselection ofKobjects fromNobjects requiresdlog2 N
Kebits'
NH2(K=N)bits.Thisselection could bemade using arithmetic coding. The
selection corresp ondstoabinary string oflengthNinwhichthe1bitsrep-
resentwhichobjectsareselected. Initially theprobabilit yofa1isK=Nand
theprobabilit yofa0is(N K)=N.Thereafter, giventhattheemitted string
thusfar,oflengthn,containsk1s,theprobabilit yofa1is(K k)=(N n)
andtheprobabilit yofa0is1 (K k)=(N n).
Solution toexercise 6.12(p.124).Thismodied Lempel{Ziv codeisstillnot
`complete', because, forexample, afterveprexes havebeencollected, the
pointercould beanyofthestrings000,001,010,011,100,butitcannot be
101,110or111.Thusthere aresomebinary strings thatcannot beproduced
asencodings.
Solution toexercise 6.13(p.124).Sources withlowentropythatarenotwell
compressed byLempel{Ziv include:
(a)Sources withsome symbolsthathavelongrange correlations andinter-
vening random junk. Anidealmodelshould capture what's correlated
andcompress it.Lempel{Ziv canonlycompress thecorrelated features
bymemorizing allcases oftheintervening junk. Asasimple example,
consider atelephone bookinwhicheverylinecontainsan(oldnumber,
newnumber)pair:
285-3820:572-5892 2
258-8302:593-2010 2
Thenumberofcharacters perlineis18,drawnfromthe13-character
alphab etf0;1;:::;9; ;:;2g.Thecharacters `-',`:'and`2'occurina
predictable sequence, sothetrueinformation contentperline,assuming
allthephone numbersaresevendigits long,andassuming thattheyare

<<<PAGE 141>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
6.9:Solutions 129
random sequences, isabout14bans. (Abanistheinformation contentof
arandom integerbetween0and9.)Anite statelanguage modelcould
easily capture theregularities inthese data. ALempel{Ziv algorithm
willtakealongtimebeforeitcompresses suchaledownto14bans
perline,however,because inorder foritto`learn' thatthestring:ddd
isalwaysfollowedby-,foranythree digitsddd,itwillhavetoseeall
those strings. Sonear-optimal compression willonlybeachievedafter
thousands oflinesofthelehavebeenread.
Figure 6.11.Asource withlowentropythatisnotwellcompressed byLempel{Ziv. Thebitsequence
isreadfromlefttoright.Eachlinediers fromthelineaboveinf=5%ofitsbits.The
image width is400pixels.
(b)Sources withlongrange correlations, forexample two-dimensional im-
agesthatarerepresen tedbyasequence ofpixels, rowbyrow,sothat
vertically adjacen tpixels areadistancewapart inthesource stream,
wherewistheimage width. Consider, forexample, afaxtransmission in
whicheachlineisverysimilar totheprevious line(gure 6.11). Thetrue
entropyisonlyH2(f)perpixel, wherefistheprobablit ythatapixel
diers fromitsparent.Lempel{Ziv algorithms willonlycompress down
totheentropyonceallstrings oflength 2w=2400haveoccurred and
theirsuccessors havebeenmemorized. There areonlyabout2300par-
ticles intheuniverse,sowecanconden tlysaythatLempel{Ziv codes
willnever capture theredundancy ofsuchanimage.
Figure 6.12.Atexture consisting ofhorizon talandvertical pinsdropp edatrandom ontheplane.
Another highly redundan ttexture isshowningure 6.12.Theimage was
made bydropping horizon talandvertical pinsrandomly ontheplane. It
containsbothlong-range vertical correlations andlong-range horizon tal
correlations. There isnopractical waythatLempel{Ziv, fedwitha
pixel-b y-pixel scanofthisimage, could capture boththese correlations.
Biological computational systems canreadily identifytheredundancy in
these images andinimages thataremuchmorecomplex; thuswemight
anticipate thatthebestdatacompression algorithms willresult fromthe
developmen tofarticial intelligence metho ds.

<<<PAGE 142>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
130 6|Stream Codes
(c)Sources withintricate redundancy ,suchaslesgenerated bycomputers.
Forexample, aLATEXlefollowedbyitsencodingintoaPostScript
le.Theinformation contentofthispairoflesisroughly equal tothe
information contentoftheLATEXlealone.
(d)Apicture oftheMandelbrot set.Thepicture hasaninformation content
equal tothenumberofbitsrequired tospecifytherange ofthecomplex
plane studied, thepixelsizes, andthecolouring ruleused.
(e)Apicture ofaground stateofafrustrated antiferromagnetic Isingmodel
(gure 6.13), whichwewilldiscuss inChapter 31.Likegure 6.12,this
binary image hasinteresting correlations intwodirections.
Figure 6.13.Frustrated triangular
Isingmodelinoneofitsground
states.
(f)Cellular automata {gure 6.14showsthestate history of100steps of
acellular automaton with400cells. Theupdaterule,inwhicheach
cell'snewstatedependsonthestateofvepreceding cells,wasselected
atrandom. Theinformation contentisequal totheinformation inthe
boundary (400bits), andthepropagation rule,whichherecanbede-
scribedin32bits.Anoptimal compressor willthusgiveacompressed le
length whichisessentially constan t,independen tofthevertical heightof
theimage. Lempel{Ziv wouldonlygivethiszero-cost compression once
thecellular automaton hasentered aperiodiclimitcycle, whichcould
easily takeabout2100iterations.
Incontrast, theJBIG compression metho d,whichmodelstheprobabilit y
ofapixelgivenitslocalcontextandusesarithmetic coding,woulddoa
goodjobonthese images.
Solution toexercise 6.14(p.124).Foraone-dimensional Gaussian, thevari-
anceofx,E[x2],is2.Sothemean valueofr2inNdimensions, since the
componentsofxareindependen trandom variables, is
E[r2]=N2: (6.25)
Figure 6.14.The100-step time-history ofacellular automaton with400cells.


<<<PAGE 143>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
6.9:Solutions 131
Thevariance ofr2,similarly ,isNtimes thevariance ofx2,wherexisa
one-dimensional Gaussian variable.
var(x2)=Z
dx1
(22)1=2x4exp 
 x2
22!
 4: (6.26)
Theintegral isfound tobe34(equation (6.14)), sovar(x2)=24.Thusthe
variance ofr2is2N4.
ForlargeN,thecentrallimittheorem indicates thatr2hasaGaussian
distribution withmeanN2andstandard deviationp
2N2,sotheprobabilit y
densit yofrmustsimilarly beconcen trated aboutr'p
N.
Thethickness ofthisshellisgivenbyturning thestandard deviation
ofr2intoastandard deviation onr:forsmallr=r,logr=r=r=
(1/2)logr2=(1/2)(r2)=r2,sosetting(r2)=p
2N2,rhasstandard de-
viationr=(1/2)r(r2)=r2==p
2.
Theprobabilit ydensit yoftheGaussian atapointxshellwherer=p
N
is
P(xshell)=1
(22)N=2exp 
 N2
22!
=1
(22)N=2exp
 N
2
:(6.27)
Whereas theprobabilit ydensit yattheorigin is
P(x=0)=1
(22)N=2: (6.28)
ThusP(xshell)=P(x=0)=exp( N=2):Theprobabilit ydensit yatthetypical
radius ise N=2times smaller thanthedensit yattheorigin. IfN=1000, then
theprobabilit ydensit yattheorigin ise500times greater.

<<<PAGE 144>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
7
CodesforIntegers
Thischapter isanaside, whichmaysafely beskipped.
Solution toexercise 6.22(p.127)
Todiscuss thecodingofintegers weneedsome denitions.
Thestandard binary represen tation ofapositiveintegernwillbe
denoted bycb(n),e.g.,cb(5)=101,cb(45)=101101 .
Thestandard binary length ofapositiveintegern,lb(n),isthe
length ofthestringcb(n).Forexample,lb(5)=3,lb(45)=6.
Thestandard binary represen tationcb(n)isnotauniquely decodeable code
forintegers sincethere isnowayofknowingwhen aninteger hasended. For
example,cb(5)cb(5)isidenticaltocb(45). Itwouldbeuniquely decodeable if
weknew thestandard binary length ofeachinteger beforeitwasreceived.
Noticing thatallpositiveintegers haveastandard binary represen tation
thatstarts witha1,wemightdene another represen tation:
Theheadless binary represen tation ofapositiveintegernwillbede-
noted bycB(n),e.g.,cB(5)=01,cB(45)=01101 andcB(1)=(where
denotes thenullstring).
Thisrepresen tation wouldbeuniquely decodeable ifweknew thelengthlb(n)
oftheinteger.
So,howcanwemakeauniquely decodeable codeforintegers? Twostrate-
giescanbedistinguished.
1.Self-delimiting codes.Werstcomm unicate someho wthelength of
theinteger,lb(n),whichisalsoapositiveinteger; thencomm unicate the
original integernitselfusingcB(n).
2.Codeswith`endofle'characters .Wecodetheintegerintoblocks
oflengthbbits,andreserv eoneofthe2bsymbolstohavethespecial
meaning `endofle'.Thecodingofintegers intoblocksisarranged so
thatthisreserv edsymbolisnotneeded foranyother purpose.
Thesimplest uniquely decodeable codeforintegers istheunary code,which
canbeviewedasacodewithanendoflecharacter.
132

<<<PAGE 145>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
7|CodesforIntegers 133
Unary code.Anintegernisencodedbysending astring ofn 10sfollowed
bya1.
ncU(n)
11
201
3001
40001
500001
...
45000000000000000000000000 000000000000000000001
Theunary codehaslengthlU(n)=n.
Theunary codeistheoptimal codeforintegers iftheprobabilit ydistri-
bution overnispU(n)=2 n.
Self-delimiting codes
Wecanusetheunary codetoencodethelength ofthebinary encodingofn
andmakeaself-delimiting code:
CodeC.Wesendtheunary codeforlb(n),followedbytheheadless binary
represen tation ofn.
c(n)=cU[lb(n)]cB(n) (7.1)
Table7.1showsthecodesforsome integers. Theoverlining indicates
thedivision ofeachstring intothepartscU[lb(n)]andcB(n).Wemightncb(n)lb(n)c(n)
11 11
210 2010
311 2011
4100 300100
5101 300101
6110 300110
...
45101101 600000101101
Table7.1.C.
equivalentlyviewc(n)asconsisting ofastring of(lb(n) 1)zeroes
followedbythestandard binary represen tation ofn,cb(n).
Thecodewordc(n)haslengthl(n)=2lb(n) 1.
Theimplicit probabilit ydistribution overnforthecodeCisseparable
intotheproductofaprobabilit ydistribution overthelengthl,
P(l)=2 l; (7.2)
andauniform distribution overintegers havingthatlength,
P(njl)=(
2 l+1lb(n)=l
0otherwise:(7.3)
Now,fortheabovecode,theheader thatcomm unicates thelength always
occupies thesame numberofbitsasthestandard binary represen tation of
theinteger (giveortakeone). Ifweareexpecting toencoun terlargeintegers
(large les)thenthisrepresen tation seems suboptimal, sinceitleadstoallles
occupyingasizethatisdouble theiroriginal uncodedsize.Instead ofusing
theunary codetoencodethelengthlb(n),wecould useC.nc(n) c(n)
11 1
20100 01000
30101 01001
401100 010100
501101 010101
601110 010110
...
4500110011010111001101
Table7.2.CandC.
CodeC.Wesendthelengthlb(n)usingC,followedbytheheadless binary
represen tation ofn.
c(n)=c[lb(n)]cB(n) (7.4)
Iterating thisprocedure, wecandene asequence ofcodes.
CodeC.
c(n)=c[lb(n)]cB(n) (7.5)
CodeC.
c(n)=c[lb(n)]cB(n) (7.6)

<<<PAGE 146>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
134 7|CodesforIntegers
Codeswithend-of-le symbols
Wecanalsomakebyte-based represen tations. (Let's usethetermbyteexibly
here, todenote anyxed-length string ofbits,notjustastring oflength 8
bits.) Ifweencodethenumberinsome base, forexample decimal, thenwe
canrepresen teachdigitinabyte.Inorder torepresen tadigitfrom0to9ina
byteweneedfourbits.Because 24=16,thisleaves6extra four-bit symbols,
f1010,1011,1100,1101,1110,1111g,thatcorresp ondtonodecimal digit.
Wecanusethese asend-of-le symbolstoindicate theendofourpositive
integer.
Clearly itisredundan ttohavemorethanoneend-of-le symbol,soamore
ecien tcodewouldencodetheintegerintobase15,andusejustthesixteen th
symbol,1111,asthepunctuation character. Generalizing thisidea,wecan
makesimilar byte-based codesforintegers inbases 3and7,andinanybase
oftheform2n 1.nc3(n) c7(n)
10111 001111
21011 010111
3010011 011111
...
450110000011110011111
Table7.3.Twocodeswith
end-of-le symbols,C3andC7.
Spaces havebeenincluded to
showthebyteboundaries.
These codesarealmost complete. (Recall thatacode'sbeing`complete'
means thatitsatises theKraft inequalit ywithequalit y.)Thecodes'sremain-
ingineciency isthattheyprovidetheabilitytoencodetheinteger zeroand
theemptystring, neither ofwhichwasrequired.
.Exercise 7.1.[2,p.136]Consider theimplicit probabilit ydistribution overinte-
gerscorresp onding tothecodewithanend-of-le character.
(a)Ifthecodehaseight-bitblocks(i.e.,theinteger iscodedinbase
255), what isthemean length inbitsoftheinteger, under the
implicit distribution?
(b)Ifonewishes toencodebinary lesofexpected sizeaboutonehun-
dredkilobytesusing acodewithanend-of-le character, what is
theoptimal blocksize?
Encodingatinyle
Toillustrate thecodeswehavediscussed, wenowuseeachcodetoencodea
small leconsisting ofjust14characters,
ClaudeShannon:
IfwemaptheASCIIcharacters ontoseven-bit symbols(e.g.,indecimal,
C=67,l=108,etc.), this14character lecorresp ondstotheinteger
n=167987786364950891085602469870(decimal):
Theunary codefornconsists ofthismany(lessone)zeroes,followedby
aone.Ifalltheoceans wereturned intoink,andifwewrote ahundred
bitswitheverycubic millimeter, there mightbeenough inktowrite
cU(n).
Thestandard binary represen tation ofnisthislength 98sequence of
bits:
cb(n)=1000011110110011000011110 101110010011001010100000
1010011110100011000011101 110110111011011111101110:
.Exercise 7.2.[2]Writedownordescrib ethefollowingself-delimiting represen-
tations oftheabovenumbern:c(n),c(n),c(n),c(n),c3(n),c7(n),
andc15(n).Whichofthese encodings istheshortest? [Answer:c15.]

<<<PAGE 147>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
7|CodesforIntegers 135
Comparingthecodes
Onecould answerthequestion `whichoftwocodesissuperior?' byasentence
oftheform`Forn>k,code1issuperior, forn<k,code2issuperior' butI
contendthatsuchananswermisses thepoint:anycomplete codecorresp onds
toapriorforwhichitisoptimal; youshould notsaythatanyother codeis
superiortoit.Other codesareoptimal forother priors. These implicit priors
should bethough taboutsoastoachievethebestcodeforone's application.
Notice thatonecannot, forfree,switchfromonecodetoanother, choosing
whicheverisshorter. Ifoneweretodothis,thenitwouldbenecessary to
lengthen themessage insome waythatindicates whichofthetwocodesis
beingused. Ifthisisdonebyasingle leading bit,itwillbefound thatthe
resulting codeissuboptimal because itfailstheKraft equalit y,aswasdiscussed
inexercise 5.33(p.104).
Another waytocompare codesforintegers istoconsider asequence of
probabilit ydistributions, suchasmonotonic probabilit ydistributions overn
1,andrankthecodesastohowwelltheyencodeanyofthese distributions.
Acodeiscalled a`universal' codeifforanydistribution inagivenclass, it
encodesintoanaverage length thatiswithin some factor oftheidealaverage
length.
Letmesaythisagain. Wearemeeting analternativ eworldview{rather
thanguring outagoodprior overintegers, asadvocated above,manythe-
orists havestudied theproblem ofcreating codesthatarereasonably good
codesforanypriors inabroad class. Heretheclassofpriors convention-
allyconsidered isthesetofpriors that(a)assign amonotonically decreasing
probabilit yoverintegers and(b)havenite entropy.
Severalofthecodeswehavediscussed aboveareuniversal. Another code
whichelegan tlytranscends thesequence ofself-delimiting codesisElias's `uni-
versalcodeforintegers' (Elias, 1975), whicheectiv elychoosesfromallthe
codesC;C;::::Itworksbysending asequence ofmessages eachofwhich
encodesthelength ofthenextmessage, andindicates byasingle bitwhether
ornotthatmessage isthenalinteger(initsstandard binary represen tation).
Because alength isapositiveinteger andallpositiveintegers beginwith`1',
alltheleading1scanbeomitted.
Write`0'
Loopf
Ifblognc=0halt
Prependcb(n)tothewritten string
n:=blognc
gAlgorithm 7.4.Elias's encoderfor
anintegern.
TheencoderofC!isshowninalgorithm 7.4.Theencodingisgenerated
fromrighttoleft.Table7.5showstheresulting codewords.
.Exercise 7.3.[2]ShowthattheElias codeisnotactually thebestcodefora
priordistribution thatexpectsverylargeintegers. (Dothisbyconstruct-
inganother codeandspecifying howlargenmustbeforyourcodeto
giveashorter length thanElias's.)

<<<PAGE 148>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
136 7|CodesforIntegers
nc!(n)nc!(n) nc!(n) nc!(n)
10 3110100111110 91110010 2561110001000000000
2100 32101011000000 101110100 3651110001011011010
3110 45101011011010 111110110 5111110001111111110
4101000 63101011111110 121111000 51211100110000000000
5101010 641011010000000 131111010 71911100110110011110
6101100 1271011011111110 141111100 102311100111111111110
7101110 12810111100000000 151111110 1024111010100000000000
81110000 25510111111111110 1610100100000 1025111010100000000010
Table7.5.Elias's `universal' code
forintegers. Examples from1to
1025.Solutions
Solution toexercise 7.1(p.134).Theuseoftheend-of-le symbolinacode
thatrepresen tstheintegerinsomebaseqcorresp ondstoabeliefthatthere is
aprobabilit yof(1=(q+1))thatthecurren tcharacter isthelastcharacter of
thenumber.Thusthepriortowhichthiscodeismatchedputsanexponential
priordistribution overthelength oftheinteger.
(a)Theexpected numberofcharacters isq+1=256,sotheexpected length
oftheinteger is2568'2000bits.
(b)Wewishtondqsuchthatqlogq'800000bits.Avalueofqbetween
215and216satises thisconstrain t,so16-bit blocksareroughly the
optimal size,assuming there isoneend-of-le character.

<<<PAGE 149>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
PartII
Noisy-Channel Coding

<<<PAGE 150>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
8
Correlated Random Variables
Inthelastthree chapters ondatacompression weconcen trated onrandom
vectors xcoming fromanextremely simple probabilit ydistribution, namely
theseparable distribution inwhicheachcomponentxnisindependen tofthe
others.
Inthischapter, weconsider jointensem blesinwhichtherandom variables
arecorrelated. Thismaterial hastwomotivations. First, datafromthereal
worldhaveinteresting correlations, sotododatacompression well,weneed
toknowhowtoworkwithmodelsthatinclude correlations. Second, anoisy
channel withinputxandoutputydenes ajointensem bleinwhichxandyare
correlated {iftheywereindependen t,itwouldbeimpossible tocomm unicate
overthechannel {socomm unication overnoisy channels (thetopicofchapters
9{11) isdescrib edinterms oftheentropyofjointensem bles.
8.1More aboutentropy
Thissection givesdenitions andexercises todowithentropy,carrying on
fromsection 2.4.
ThejointentropyofX;Yis:
H(X;Y)=X
xy2AXAYP(x;y)log1
P(x;y): (8.1)
Entropyisadditiv eforindependen trandom variables:
H(X;Y)=H(X)+H(Y)iP(x;y)=P(x)P(y): (8.2)
Theconditional entropyofXgiveny=bkistheentropyoftheproba-
bilitydistribution P(xjy=bk).
H(Xjy=bk)X
x2AXP(xjy=bk)log1
P(xjy=bk): (8.3)
Theconditional entropyofXgivenYistheaverage, overy,ofthecon-
ditional entropyofXgiveny.
H(XjY)X
y2AYP(y)2
4X
x2AXP(xjy)log1
P(xjy)3
5
=X
xy2AXAYP(x;y)log1
P(xjy): (8.4)
Thismeasures theaverage uncertain tythatremains aboutxwhenyis
known.
138

<<<PAGE 151>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
8.1:More aboutentropy 139
Themarginal entropyofXisanother name fortheentropyofX,H(X),
usedtocontrastitwiththeconditional entropies listed above.
Chain ruleforinformation content.Fromtheproductruleforprobabil-
ities,equation (2.6), weobtain:
log1
P(x;y)=log1
P(x)+log1
P(yjx)(8.5)
so
h(x;y)=h(x)+h(yjx): (8.6)
Inwords,thissaysthattheinformation contentofxandyistheinfor-
mation contentofxplustheinformation contentofygivenx.
Chain ruleforentropy.The jointentropy,conditional entropyand
marginal entropyarerelated by:
H(X;Y)=H(X)+H(YjX)=H(Y)+H(XjY): (8.7)
Inwords,thissaysthattheuncertain tyofXandYistheuncertain ty
ofXplustheuncertain tyofYgivenX.
Themutual information betweenXandYis
I(X;Y)H(X) H(XjY); (8.8)
andsatisesI(X;Y)=I(Y;X),andI(X;Y)0.Itmeasures the
average reduction inuncertain tyaboutxthatresults fromlearning the
valueofy;orviceversa,theaverage amoun tofinformation thatx
conveysabouty.
Theconditional mutual information betweenXandYgivenz=ck
isthemutual information betweentherandom variablesXandYin
thejointensem bleP(x;yjz=ck),
I(X;Yjz=ck)=H(Xjz=ck) H(XjY;z=ck): (8.9)
Theconditional mutual information betweenXandYgivenZis
theaverage overzoftheaboveconditional mutual information.
I(X;YjZ)=H(XjZ) H(XjY;Z): (8.10)
Noother `three-term entropies' willbedened. Forexample, expres-
sions suchasI(X;Y;Z)andI(XjY;Z)areillegal. Butyoumayput
conjunctions ofarbitrary numbersofvariables ineachofthethree spots
intheexpressionI(X;YjZ){forexample,I(A;B;C;DjE;F)isne:
itmeasures howmuchinformation onaveragecanddconveyabouta
andb,assumingeandfareknown.
Figure 8.1showshowthetotalentropyH(X;Y)ofajointensem blecanbe
brokendown.Thisgure isimportant. 

<<<PAGE 152>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
140 8|Correlated Random Variables
H(X;Y)
H(X)
H(Y)
I(X;Y) H(XjY) H(YjX)Figure 8.1.Therelationship
betweenjointinformation,
marginal entropy,conditional
entropyandmutual entropy.
8.2Exercises
.Exercise 8.1.[1]Consider three independen trandom variablesu;v;wwithen-
tropiesHu;Hv;Hw.LetX(U;V)andY(V;W).What isH(X;Y)?
What isH(XjY)?What isI(X;Y)?
.Exercise 8.2.[3,p.142]Referring tothedenitions ofconditional entropy(8.3{
8.4),conrm (with anexample) thatitispossible forH(Xjy=bk)to
exceedH(X),butthattheaverage,H(XjY)islessthanH(X).So
dataarehelpful {theydonotincrease uncertain ty,onaverage.
.Exercise 8.3.[2,p.142]Provethechain ruleforentropy,equation (8.7).
[H(X;Y)=H(X)+H(YjX)].
Exercise 8.4.[2,p.143]Provethatthemutual information I(X;Y)H(X) 
H(XjY)satisesI(X;Y)=I(Y;X)andI(X;Y)0.
[Hint:seeexercise 2.26(p.37)andnotethat
I(X;Y)=DKL(P(x;y)jjP(x)P(y)):] (8.11)
Exercise 8.5.[4]The`entropydistance' betweentworandom variables canbe
dened tobethedierence betweentheirjointentropyandtheirmutual
information:
DH(X;Y)H(X;Y) I(X;Y): (8.12)
Provethattheentropydistance satises theaxioms foradistance {
DH(X;Y)0,DH(X;X)=0,DH(X;Y)=DH(Y;X),andDH(X;Z)
DH(X;Y)+DH(Y;Z).[Inciden tally,weareunlikelytoseeDH(X;Y)
again butitisagoodfunction onwhichtopractise inequalit y-proving.]
Exercise 8.6.[2]Ajointensem bleXYhasthefollowingjointdistribution.
P(x;y) x
1234
11/81/161/321/32
y21/161/81/321/32
31/161/161/161/16
41/400043211234
What isthejointentropyH(X;Y)?What arethemarginal entropies
H(X)andH(Y)?Foreachvalueofy,what istheconditional entropy
H(Xjy)?What istheconditional entropyH(XjY)?What isthe
conditional entropyofYgivenX?What isthemutual information
betweenXandY?

<<<PAGE 153>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
8.3:Further exercises 141
Exercise 8.7.[2,p.143]Consider theensem bleXYZinwhichAX=AY=
AZ=f0;1g,xandyareindependen twithPX=fp;1 pgand
PY=fq;1 qgand
z=(x+y)mod2: (8.13)
(a)Ifq=1/2,whatisPZ?What isI(Z;X)?
(b)Forgeneralpandq,what isPZ?What isI(Z;X)?Notice that
thisensem bleisrelated tothebinary symmetric channel, withx=
input,y=noise, andz=output.
H(X|Y) H(Y|X)I(X;Y)
H(X)H(Y)
H(X,Y)Figure 8.2.Amisleading
represen tation ofentropies
(contrastwithgure 8.1).
Threetermentropies
Exercise 8.8.[3,p.143]Manytextsdrawgure 8.1intheformofaVenndiagram
(gure 8.2). Discuss whythisdiagram isamisleading represen tation
ofentropies. Hint:consider thethree-v ariable ensem bleXYZinwhich
x2f0;1gandy2f0;1gareindependen tbinary variables andz2f0;1g
isdened tobez=x+ymod2.
8.3Further exercises
Thedata-pr ocessing theorem
Thedataprocessing theorem states thatdataprocessing canonlydestro y
information.
Exercise 8.9.[3,p.144]Provethistheorem byconsidering anensem bleWDR
inwhichwisthestate oftheworld,disdatagathered, andristhe
processed data, sothatthese three variables formaMarkovchain
w!d!r; (8.14)
thatis,theprobabilit yP(w;d;r)canbewritten as
P(w;d;r)=P(w)P(djw)P(rjd): (8.15)
Showthattheaverage information thatRconveysaboutW,I(W;R),is
lessthanorequal totheaverage information thatDconveysaboutW,
I(W;D).
Thistheorem isasmuchacaution aboutourdenition of`information' asit
isacaution aboutdataprocessing!
Inferenceandinformation measures
Exercise 8.10.[2]Thethreecards.

<<<PAGE 154>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
142 8|Correlated Random Variables
(a)Onecardiswhite onbothfaces; oneisblackonbothfaces; andone
iswhite ononesideandblackontheother. Thethree cards are
shued andtheirorientations randomized. Onecardisdrawnand
placed onthetable. Theupperfaceisblack.What isthecolour of
itslowerface? (Solvetheinference problem.)
(b)Doesseeing thetopfaceconveyinformation aboutthecolour of
thebottom face? Discuss theinformation contents andentropies
inthissituation. Letthevalueoftheupperface's colour beuand
thevalueofthelowerface's colour bel.Imagine thatwedraw
arandom cardandlearn bothuandl.What istheentropyof
u,H(U)?What istheentropyofl,H(L)?What isthemutual
information betweenUandL,I(U;L)?
EntropiesofMarkov processes
.Exercise 8.11.[3]Intheguessing game, weimagined predicting thenextletter
inadocumen tstarting fromthebeginning andworking towardstheend.
Consider thetaskofpredicting thereversedtext,thatis,predicting the
letter thatprecedes those already known.Mostpeoplendthisaharder
task. Assuming thatwemodelthelanguage using anN-gram model
(whichsaystheprobabilit yofthenextcharacter dependsonlyonthe
N 1preceding characters), isthere anydierence betweentheaverage
information contentsofthereversedlanguage andtheforwardlanguage?
8.4Solutions
Solution toexercise 8.2(p.140).Seeexercise 8.6(p.140)foranexample where
H(Xjy)exceedsH(X)(sety=3).
Wecanprovetheinequalit yH(XjY)H(X)byturning theexpression
intoarelativ eentropy(using Bayes'theorem) andinvoking Gibbs' inequalit y
(exercise 2.26(p.37)):
H(XjY)X
y2AYP(y)2
4X
x2AXP(xjy)log1
P(xjy)3
5
=X
xy2AXAYP(x;y)log1
P(xjy)(8.16)
=X
xyP(x)P(yjx)logP(y)
P(yjx)P(x)(8.17)
=X
xP(x)log1
P(x)+X
xP(x)X
yP(yjx)logP(y)
P(yjx):(8.18)
Thelastexpression isasumofrelativ eentropies betweenthedistributions
P(yjx)andP(y).So
H(XjY)H(X)+0; (8.19)
withequalit yonlyifP(yjx)=P(y)forallxandy(that is,onlyifXandY
areindependen t).
Solution toexercise 8.3(p.140).Thechainruleforentropyfollowsfromthe
decomp osition ofajointprobabilit y:
H(X;Y)=X
xyP(x;y)log1
P(x;y)(8.20)

<<<PAGE 155>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
8.4:Solutions 143
=X
xyP(x)P(yjx)
log1
P(x)+log1
P(yjx)
(8.21)
=X
xP(x)log1
P(x)+X
xP(x)X
yP(yjx)log1
P(yjx)(8.22)
=H(X)+H(YjX): (8.23)
Solution toexercise 8.4(p.140).Symmetry ofmutual information:
I(X;Y)=H(X) H(XjY) (8.24)
=X
xP(x)log1
P(x) X
xyP(x;y)log1
P(xjy)(8.25)
=X
xyP(x;y)logP(xjy)
P(x)(8.26)
=X
xyP(x;y)logP(x;y)
P(x)P(y): (8.27)
Thisexpression issymmetric inxandyso
I(X;Y)=H(X) H(XjY)=H(Y) H(YjX): (8.28)
Wecanprovethatmutual information ispositivetwoways.Oneistocontinue
from
I(X;Y)=X
x;yP(x;y)logP(x;y)
P(x)P(y)(8.29)
whichisarelativ eentropyanduseGibbs' inequalit y(provedonp.44),which
asserts thatthisrelativ eentropyis0,withequalit yonlyifP(x;y)=
P(x)P(y),thatis,ifXandYareindependen t.
Theother istouseJensen's inequalit yon
 X
x;yP(x;y)logP(x)P(y)
P(x;y) logX
x;yP(x;y)
P(x;y)P(x)P(y)=log1=0:(8.30)
Solution toexercise 8.7(p.141).z=x+ymod2:
(a)Ifq=1/2,PZ=f1/2;1/2gandI(Z;X)=H(Z) H(ZjX)=1 1=0.
(b)Forgeneralqandp,PZ=fpq+(1 p)(1 q);p(1 q)+q(1 p)g.
Themutual information isI(Z;X)=H(Z) H(ZjX)=H2(pq+(1 
p)(1 q)) H2(q).
Threetermentropies
Solution toexercise 8.8(p.141).Thedepiction ofentropies interms ofVenn
diagrams ismisleading foratleasttworeasons.
First, oneisusedtothinking ofVenndiagrams asdepicting sets;butwhat
arethe`sets'H(X)andH(Y)depicted ingure 8.2,andwhataretheobjects
thataremembersofthose sets? Ithink thisdiagram encourages thenovice
studen ttomakeinappropriate analogies. Forexample, somestuden tsimagine
thattherandom outcome (x;y)mightcorresp ondtoapointinthediagram,
andthusconfuse entropies withprobabilities.
Secondly ,thedepiction interms ofVenndiagrams encourages onetobe-
lievethatalltheareas corresp ondtopositivequantities. Inthespecialcaseof
tworandom variables itisindeed truethatH(XjY),I(X;Y)andH(YjX)
arepositivequantities. Butassoonasweprogress tothree-v ariable ensem bles,

<<<PAGE 156>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
144 8|Correlated Random Variables                                                                                     




H(Y|X,Z)
H(X)
H(Z)I(X;Y)
H(Z|X) H(Z|X,Y)I(X;Y|Z)A
H(Z|Y)H(X|Y,Z)H(Y)
H(X,Y|Z)Figure 8.3.Amisleading
represen tation ofentropies,
continued.
weobtain adiagram withpositive-looking areas thatmayactually corresp ond
tonegativ equantities. Figure 8.3correctly showsrelationships suchas
H(X)+H(ZjX)+H(YjX;Z)=H(X;Y;Z): (8.31)
Butitgivesthemisleading impression thattheconditional mutual information
I(X;YjZ)islessthanthemutual information I(X;Y).Infactthearea
labelledAcancorresp ondtoanegative quantity.Consider thejointensem ble
(X;Y;Z)inwhichx2f0;1gandy2f0;1gareindependen tbinary variables
andz2f0;1gisdened tobez=x+ymod2.Then clearlyH(X)=
H(Y)=1bit.AlsoH(Z)=1bit.AndH(YjX)=H(Y)=1sincethetwo
variables areindependen t.Sothemutual information betweenXandYis
zero.I(X;Y)=0.However,ifzisobserv ed,XandYbecome correlated |
knowingx,givenz,tellsyouwhatyis:y=z xmod2.SoI(X;YjZ)=1
bit.ThusthearealabelledAmustcorresp ondto 1bitsforthegure togive
thecorrect answers.
Theaboveexample isnotatallacapricious orexceptional illustration. The
binary symmetric channel withinputX,noiseY,andoutputZisasituation
inwhichI(X;Y)=0(input andnoise areuncorrelated) butI(X;YjZ)>0
(once youseetheoutput, theunkno wninput andtheunkno wnnoise are
intimately related!).
TheVenndiagram represen tation istherefore validonlyifoneisaware
thatpositiveareas mayrepresen tnegativ equantities. With thisprovisokept
inmind, theinterpretation ofentropies interms ofsetscanbehelpful (Yeung,
1991).
Solution toexercise 8.9(p.141).Foranyjointensem bleXYZ,thefollowing
chainruleformutual information holds.
I(X;Y;Z)=I(X;Y)+I(X;ZjY): (8.32)
Now,inthecasew!d!r,wandrareindependen tgivend,so
I(W;RjD)=0.Using thechainruletwice, wehave:
I(W;D;R)=I(W;D) (8.33)
and
I(W;D;R)=I(W;R)+I(W;DjR); (8.34)
so
I(W;R) I(W;D)0: (8.35)

<<<PAGE 157>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
AboutChapter 9
Before reading Chapter 9,youshould havereadChapter 1andworkedon
exercise 2.26(p.37),andexercises 8.2{8.7 (pp.140{141) .
145

<<<PAGE 158>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
9
Comm unication overaNoisy Channel
9.1Thebigpicture
Noisy
channelEncoder DecoderCompressor DecompressorSource
coding
Channel
codingSource
-666
??
InChapters 4{6,wediscussed source codingwithblockcodes,symbolcodes
andstream codes.Weimplicitly assumed thatthechannel fromthecompres-
sortothedecompressor wasnoise-free. Realchannels arenoisy.Wewillnow
spendtwochapters onthesubjectofnoisy-c hannel coding{thefundamen-
talpossibilities andlimitations oferror-free comm unication through anoisy
channel. Theaimofchannel codingistomakethenoisy channel behavelike
anoiseless channel. Wewillassume thatthedatatobetransmitted hasbeen
through agoodcompressor, sothebitstream hasnoobvious redundancy .The
channel code,whichmakesthetransmission, willputbackredundancy ofa
specialsort,designed tomakethenoisy receivedsignal decodeable.
Supposewetransmit 1000 bitspersecond withp0=p1=1/2overa
noisy channel thatipsbitswithprobabilit yf=0:1.What istherateof
transmission ofinformation? Wemightguess thattherateis900bitsper
second bysubtracting theexpected numberoferrors persecond. Butthisis
notcorrect, because therecipien tdoesnotknowwhere theerrors occurred.
Consider thecasewhere thenoise issogreat thatthereceivedsymbolsare
independen tofthetransmitted symbols.Thiscorresp ondstoanoise levelof
f=0:5,since halfofthereceivedsymbolsarecorrect duetochance alone.
Butwhenf=0:5,noinformation istransmitted atall.
Givenwhatwehavelearntaboutentropy,itseems reasonable thatamea-
sureoftheinformation transmitted isgivenbythemutual information between
thesource andthereceivedsignal, thatis,theentropyofthesource minusthe
conditional entropyofthesource giventhereceivedsignal.
Wewillnowreview thedenition ofconditional entropyandmutual in-
formation. Then wewillexamine whether itispossible tousesuchanoisy
channel tocomm unicate reliably .WewillshowthatforanychannelQthere
isanon-zero rate,thecapacit yC(Q),uptowhichinformation canbesent
146

<<<PAGE 159>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
9.2:Review ofprobabilit yandinformation 147
witharbitrarily small probabilit yoferror.
9.2Review ofprobabilit yandinformation
Asanexample, wetakethejointdistribution XYfromexercise 8.6(p.140).
Themarginal distributions P(x)andP(y)areshowninthemargins.
P(x;y) x P(y)
1234
11/81/161/321/321/4
y21/161/81/321/321/4
31/161/161/161/161/4
41/40001/4
P(x)1/21/41/81/8
ThejointentropyisH(X;Y)=27=8bits.Themarginal entropies areH(X)=
7=4bitsandH(Y)=2bits.
Wecancompute theconditional distribution ofxforeachvalueofy,and
theentropyofeachofthose conditional distributions:
P(xjy)xH(Xjy)=bits
1234
11/21/41/81/87/4
y21/41/21/81/87/4
31/41/41/41/4 2
41000 0
H(XjY)=11/8
NotethatwhereasH(Xjy=4)=0islessthanH(X),H(Xjy=3)isgreater
thanH(X).Soinsome cases, learningycanincreaseouruncertain tyabout
x.NotealsothatalthoughP(xjy=2)isadieren tdistribution fromP(x),
theconditional entropyH(Xjy=2)isequal toH(X).Solearning thaty
is2changes ourknowledge aboutxbutdoesnotreduce theuncertain tyof
x,asmeasured bytheentropy.Onaverage though, learningydoesconvey
information aboutx,sinceH(XjY)<H(X).
OnemayalsoevaluateH(YjX)=13=8bits.Themutual information is
I(X;Y)=H(X) H(XjY)=3=8bits.
9.3Noisy channels
Adiscrete memoryless channelQischaracterized byaninput alphab et
AX,anoutput alphab etAY,andasetofconditional probabilit ydistri-
butionsP(yjx),oneforeachx2AX.
These transition probabilities maybewritten inamatrix
Qjji=P(y=bjjx=ai): (9.1)
Iusually orientthismatrix withtheoutput variable jindexing therowsandthe
input variable iindexing thecolumns, sothateachcolumn ofQisaprobabilit y
vector. With thisconvention,wecanobtain theprobabilit yoftheoutput, pY,
fromaprobabilit ydistribution overtheinput, pX,byright-multiplication:
pY=QpX: (9.2)

<<<PAGE 160>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
148 9|Comm unication overaNoisy Channel
Some useful modelchannels are:
Binary symmetric channel .AX=f0;1g.AY=f0;1g.
x--
  @@R10
10yP(y=0jx=0)=1 f;
P(y=1jx=0)=f;P(y=0jx=1)=f;
P(y=1jx=1)=1 f:1001
Binary erasure channel .AX=f0;1g.AY=f0;?;1g.
x
--
  @@R
10
10
?yP(y=0jx=0)=1 f;
P(y=?jx=0)=f;
P(y=1jx=0)=0;P(y=0jx=1)=0;
P(y=?jx=1)=f;
P(y=1jx=1)=1 f:1?001
Noisy typewriter .AX=AY=the27lettersfA,B,...,Z,-g.Theletters
arearranged inacircle, andwhen thetypist attempts totypeB,what
comes outiseitherA,BorC,withprobabilit y1/3each;when theinput is
C,theoutput isB,CorD;andsoforth, withthenalletter `-'adjacen t
totherstletterA.
--
-
1PPPq
-ZY
-ZY1
PPPq-1...PPPq1-1
PPPq-1
PPPq-1
PPPq-1
PPPq-1
PPPq-1
PPPq-1
PPPq-PPPq
C
C
C
C
C
C
C
C
C
C
C
CWHHGGFFEEDDCCBBAA
...
P(y=Fjx=G)=1=3;
P(y=Gjx=G)=1=3;
P(y=Hjx=G)=1=3;
...
-ZYXWVUTSRQPONMLKJIHGFEDCBAABCDEFGHIJKLMNOPQRSTUVWXYZ-
Zchannel .AX=f0;1g.AY=f0;1g.
x--
  
10
10yP(y=0jx=0)=1;
P(y=1jx=0)=0;P(y=0jx=1)=f;
P(y=1jx=1)=1 f:1001
9.4Inferring theinput giventheoutput
Ifweassume thattheinputxtoachannel comes fromanensem bleX,then
weobtain ajointensem bleXYinwhichtherandom variablesxandyhave
thejointdistribution:
P(x;y)=P(yjx)P(x): (9.3)
Nowifwereceiveaparticular symboly,what wastheinput symbolx?We
typically won'tknowforcertain. Wecanwritedowntheposterior distribution
oftheinput using Bayes'theorem:
P(xjy)=P(yjx)P(x)
P(y)=P(yjx)P(x)P
x0P(yjx0)P(x0): (9.4)
Example 9.1.Consider abinary symmetric channel withprobabilit yoferror
f=0:15.Lettheinput ensem blebePX:fp0=0:9;p1=0:1g.Assume
weobserv ey=1.
P(x=1jy=1)=P(y=1jx=1)P(x=1)P
x0P(yjx0)P(x0)
=0:850:1
0:850:1+0:150:9
=0:085
0:22=0:39: (9.5)

<<<PAGE 161>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
9.5:Information conveyedbyachannel 149
Thus`x=1'isstilllessprobable than`x=0',although itisnotasim-
probable asitwasbefore.
Exercise 9.2.[1,p.157]Nowassume weobserv ey=0.Compute theprobabilit y
ofx=1giveny=0.
Example 9.3.Consider aZchannel withprobabilit yoferrorf=0:15.Letthe
input ensem blebePX:fp0=0:9;p1=0:1g.Assume weobserv ey=1.
P(x=1jy=1)=0:850:1
0:850:1+00:9
=0:085
0:085=1:0: (9.6)
Sogiventheoutputy=1webecome certain oftheinput.
Exercise 9.4.[1,p.157]Alternativ ely,assume weobserv ey=0.Compute
P(x=1jy=0).
9.5Information conveyedbyachannel
Wenowconsider howmuchinformation canbecomm unicated through achan-
nel.Inoperational terms, weareinterested innding waysofusing thechan-
nelsuchthatallthebitsthatarecomm unicated arerecoveredwithnegligible
probabilit yoferror. Inmathematical terms, assuming aparticular input en-
sembleX,wecanmeasure howmuchinformation theoutput conveysabout
theinput bythemutual information:
I(X;Y)H(X) H(XjY)=H(Y) H(YjX): (9.7)
Ouraimistoestablish theconnection betweenthesetwoideas. Letusevaluate
I(X;Y)forsome ofthechannels above.
Hintforcomputing mutual information
Wewilltendtothink ofI(X;Y)asH(X) H(XjY),i.e.,howmuchthe
uncertain tyoftheinputXisreduced when welookattheoutputY.Butfor
computational purposesitisoftenhandy toevaluateH(Y) H(YjX)instead.
H(X;Y)
H(X)
H(Y)
I(X;Y) H(XjY) H(YjX)Figure 9.1.Therelationship
betweenjointinformation,
marginal entropy,conditional
entropyandmutual entropy.
Thisgure isimportant,soI'm
showingittwice.
Example 9.5.Consider thebinary symmetric channel again, withf=0:15and
PX:fp0=0:9;p1=0:1g.Wealready evaluated themarginal probabil-
itiesP(y)implicitly above:P(y=0)=0:78;P(y=1)=0:22.The
mutual information is:
I(X;Y)=H(Y) H(YjX):

<<<PAGE 162>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
150 9|Comm unication overaNoisy Channel
What isH(YjX)?Itisdened tobetheweightedsumoverxofH(Yjx);
butH(Yjx)isthesame foreachvalueofx:H(Yjx=0)isH2(0:15),
andH(Yjx=1)isH2(0:15).So
I(X;Y)=H(Y) H(YjX)
=H2(0:22) H2(0:15)
=0:76 0:61=0:15bits: (9.8)
This maybecontrasted with theentropyofthesourceH(X)=
H2(0:1)=0:47bits.
Note: herewehaveusedthebinary entropyfunctionH2(p)H(p;1 
p)=plog21
p+(1 p)log21
(1 p).
Example 9.6.AndnowtheZchannel, withPXasabove.P(y=1)=0:085.
I(X;Y)=H(Y) H(YjX)
=H2(0:085) [0:9H2(0)+0:1H2(0:15)]
=0:42 (0:10:61)=0:36bits: (9.9)
Theentropyofthesource, asabove,isH(X)=0:47bits.Notice that
themutual information I(X;Y)fortheZchannel isbigger thanthe
mutual information forthebinary symmetric channel withthesamef.
TheZchannel isamore reliable channel.
Exercise 9.7.[1,p.157]Compute themutual information betweenXandYfor
thebinary symmetric channel withf=0:15when theinput distribution
isPX=fp0=0:5;p1=0:5g.
Exercise 9.8.[2,p.158]Compute themutual information betweenXandYfor
theZchannel withf=0:15when theinput distribution isPX:
fp0=0:5;p1=0:5g.
Maximizing themutual information
Wehaveobserv edintheaboveexamples thatthemutual information between
theinput andtheoutput dependsonthechosen input ensem ble.
Letusassume thatwewishtomaximize themutual information conveyed
bythechannel bychoosing thebestpossible input ensem ble.Wedene the
capacit yofthechannel tobeitsmaxim ummutual information.
Thecapacit yofachannelQis:
C(Q)=max
PXI(X;Y): (9.10)
ThedistributionPXthatachievesthemaxim umiscalled theoptimal
input distribution ,denoted byP
X.[There maybemultiple optimal
input distributions achieving thesame valueofI(X;Y).]
InChapter 10wewillshowthatthecapacit ydoesindeed measure themaxi-
mumamoun toferror-free information thatcanbetransmitted overthechan-
nelperunittime.
Example 9.9.Consider thebinary symmetric channel withf=0:15.Above,
weconsideredPX=fp0=0:9;p1=0:1g,andfoundI(X;Y)=0:15bits.
Howmuchbettercanwedo?Bysymmetry ,theoptimal input distribu-I(X;Y)
00.10.20.30.4
00.25 0.50.75 1
p1
Figure 9.2.Themutual
information I(X;Y)forabinary
symmetric channel withf=0:15
asafunction oftheinput
distribution.

<<<PAGE 163>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
9.6:Thenoisy-c hannel codingtheorem 151
tionisf0:5;0:5gandthecapacit yis
C(QBSC)=H2(0:5) H2(0:15)=1:0 0:61=0:39bits:(9.11)
We'lljustify thesymmetry argumen tlater. Ifthere's anydoubt about
thesymmetry argumen t,wecanalwaysresort toexplicit maximization
ofthemutual information I(X;Y),
I(X;Y)=H2((1 f)p1+(1 p1)f) H2(f)(gure 9.2). (9.12)
Example 9.10. Thenoisy typewriter. Theoptimal input distribution isauni-
formdistribution overx,andgivesC=log29bits.
Example 9.11. Consider theZchannel withf=0:15.Identifying theoptimal
input distribution isnotsostraigh tforward.WeevaluateI(X;Y)explic-
itlyforPX=fp0;p1g.First, weneedtocomputeP(y).Theprobabilit y
ofy=1iseasiest towrite down:
P(y=1)=p1(1 f): (9.13)
Then themutual information is:
I(X;Y)=H(Y) H(YjX)
=H2(p1(1 f)) (p0H2(0)+p1H2(f))
=H2(p1(1 f)) p1H2(f): (9.14)
Thisisanon-trivial function ofp1,showningure 9.3.Itismaximized
forf=0:15byp
1=0:445. WendC(QZ)=0:685. Notice that
theoptimal input distribution isnotf0:5;0:5g.Wecancomm unicate
slightlymoreinformation byusing input symbol0morefrequen tlythan
1.I(X;Y)
00.10.20.30.40.50.60.7
00.25 0.50.75 1
p1
Figure 9.3.Themutual
information I(X;Y)foraZ
channel withf=0:15asa
function oftheinput distribution.
Exercise 9.12.[1,p.158]What isthecapacit yofthebinary symmetric channel
forgeneralf?
Exercise 9.13.[2,p.158]Showthatthecapacit yofthebinary erasure channel
withf=0:15isCBEC=0:85.What isitscapacit yforgeneralf?
Commen t.
9.6Thenoisy-c hannel codingtheorem
Itseems plausible thatthe`capacit y'wehavedened maybeameasure of
information conveyedbyachannel; what isnotobvious, andwhat wewill
proveinthenextchapter, isthatthecapacit yindeed measures therateat
whichblocksofdatacanbecomm unicated overthechannel witharbitrarily
smallprobability oferror.
Wemakethefollowingdenitions.
An(N;K)blockcodeforachannelQisalistofS=2Kcodewords
fx(1);x(2);:::;x(2K)g;x(s)2AN
X;
eachoflengthN.Using thiscodewecanencodeasignals2
f1;2;3;:::;2Kgasx(s).[ThenumberofcodewordsSisaninteger,
butthenumberofbitsspecied bychoosing acodeword,Klog2S,is
notnecessarily aninteger.]

<<<PAGE 164>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
152 9|Comm unication overaNoisy Channel
TherateofthecodeisR=K=Nbitsperchannel use.
[Wewillusethisdenition oftherateforanychannel, notonlychan-
nelswithbinary inputs; notehoweverthatitissometimes conventional
todene therateofacodeforachannel withqinput symbolstobe
K=(Nlogq).]
Adecoderforan(N;K)blockcodeisamapping fromthesetoflength-N
strings ofchannel outputs,AN
Ytoacodewordlabel^s2f0;1;2;:::;2Kg.
Theextra symbol^s=0canbeusedtoindicate a`failure'.
Theprobabilit yofblockerror ofacodeanddecoder,foragivenchannel,
andforagivenprobabilit ydistribution overtheencodedsignalP(sin),
is:
pB=X
sinP(sin)P(sout6=sinjsin) (9.15)
Themaximal probabilit yofblockerror is
pBM=maxsinP(sout6=sinjsin) (9.16)
Theoptimal decoderforachannel codeistheonethatminimizes theprob-
abilityofblockerror. Itdecodesanoutput yastheinputsthathas
maxim umposterior probabilit yP(sjy).
P(sjy)=P(yjs)P(s)P
s0P(yjs0)P(s0)(9.17)
^soptimal =argmaxP(sjy): (9.18)
Auniform priordistribution onsisusually assumed, inwhichcasethe
optimal decoderisalsothemaxim umlikelihooddecoder,i.e.,thedecoder
thatmaps anoutput ytotheinputsthathasmaxim umlikelihood
P(yjs).
Theprobabilit yofbiterrorpbisdened assuming thatthecodeword
numbersisrepresen tedbyabinary vectorsoflengthKbits;itisthe
average probabilit ythatabitofsoutisnotequal tothecorresp onding
bitofsin(averaging overallKbits).
Shannon's noisy-c hannel codingtheorem (part one).Associated with
eachdiscrete memoryless channel,there isanon-negativ enumberC-6
CRpBM
achievable
Figure 9.4.Portion oftheR;pBM
plane asserted tobeachievableby
therstpartofShannon's noisy
channel codingtheorem.
(called thechannel capacit y)withthefollowingproperty.Forany>0
andR<C,forlargeenoughN,thereexists ablockcodeoflengthNand
rateRandadecodingalgorithm, suchthatthemaximal probabilit y
ofblockerroris<.
Conrmation ofthetheoremforthenoisytypewriter channel
Inthecaseofthenoisy typewriter, wecaneasily conrm thetheorem, because
wecancreate acompletely error-free comm unication strategy using ablock
codeoflengthN=1:weuseonlythelettersB,E,H,...,Z,i.e.,everythird
letter. These letters formanon-confusable subset oftheinput alphab et(see
gure 9.5). Anyoutput canbeuniquely decoded. Thenumberofinputs
inthenon-confusable subset is9,sotheerror-free information rateofthis
system islog29bits,whichisequal tothecapacit yC,whichweevaluated in
example 9.10(p.151).

<<<PAGE 165>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
9.7:Intuitivepreview ofproof 153
-Z
-ZY1
PPPq...-1
PPPq-1
PPPq-1
PPPq
IHHGFEEDCBBA
-ZYXWVUTSRQPONMLKJIHGFEDCBAABCDEFGHIJKLMNOPQRSTUVWXYZ-Figure 9.5.Anon-confusable
subset ofinputs forthenoisy
typewriter.
1001
11011000
00100111
1111011110110011110101011001000111100110101000101100010010000000
0000100001001100001010100110111000011001010111010011101101111111
N=1N=2 N=4Figure 9.6.Extended channels
obtained fromabinary symmetric
channel withtransition
probabilit y0.15.
Howdoesthistranslate intotheterms ofthetheorem? Thefollowingtable
explains.
Thetheorem Howitapplies tothenoisytypewriter
Associatedwitheachdiscrete
memoryless channel, thereisa
non-ne gative numberC.Thecapacit yCislog29.
Forany>0andR<C,forlarge
enoughN,Nomatter whatandRare,wesettheblocklengthNto1.
thereexists ablockcodeoflengthNand
rateRTheblockcodeisfB;E;:::;Zg.ThevalueofKisgivenby
2K=9,soK=log29,andthiscodehasratelog29,whichis
greater thantherequested valueofR.
andadecodingalgorithm, Thedecodingalgorithm maps thereceivedletter tothenearest
letter inthecode;
suchthatthemaximal probability of
blockerroris<.themaximal probabilit yofblockerror iszero, whichisless
thanthegiven.
9.7Intuitivepreview ofproof
Extende dchannels
Toprovethetheorem foragivenchannel, weconsider theextended channel
corresp onding toNusesofthegivenchannel. Theextended channel has
jAXjNpossible inputs xandjAYjNpossible outputs. Extended channels
obtained fromabinary symmetric channel andfromaZchannel areshownin
gures 9.6and9.7,withN=2andN=4.

<<<PAGE 166>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
154 9|Comm unication overaNoisy Channel
1001
11011000
00100111
1111011110110011110101011001000111100110101000101100010010000000
0000100001001100001010100110111000011001010111010011101101111111
N=1N=2 N=4Figure 9.7.Extended channels
obtained fromaZchannel with
transition probabilit y0.15.Each
column corresp ondstoaninput,
andeachrowisadieren toutput.
AN
Y
'
&$
%Typical y

 




























 

 

 



6
Typical yforagiventypical xAN
Y
'
&$
%Typical y


 

 

 


(a) (b)Figure 9.8.(a)Some typical
outputs inAN
Ycorresp onding to
typical inputs x.(b)Asubset of
thetypical setsshownin(a)that
donotoverlapeachother. This
picture canbecompared withthe
solution tothenoisy typewriter in
gure 9.5.
Exercise 9.14.[2,p.159]Findthetransition probabilit ymatrices Qfortheex-
tended channel, withN=2,derivedfromthebinary erasure channel
havingerasure probabilit y0.15.
Byselecting twocolumns ofthistransition probabilit ymatrix, wecan
dene arate-1/2codeforthischannel withblocklengthN=2.What is
thebestchoice oftwocolumns? What isthedecodingalgorithm?
Toprovethenoisy-c hannel codingtheorem, wemakeuseoflarge block
lengthsN.Theintuitiveideaisthat,ifNislarge, anextende dchannel looks
alotlikethenoisytypewriter. Anyparticular inputxisverylikelytoproduce
anoutput inasmall subspace oftheoutput alphab et{thetypical output set,
giventhatinput. Sowecanndanon-confusable subset oftheinputs that
produceessentially disjoin toutput sequences. ForagivenN,letusconsider
awayofgenerating suchanon-confusable subset oftheinputs, andcountup
howmanydistinct inputs itcontains.
Imagine making aninput sequence xfortheextended channel bydrawing
itfromanensem bleXN,whereXisanarbitrary ensem bleovertheinput
alphab et.Recall thesource codingtheorem ofChapter 4,andconsider the
numberofprobable output sequences y.Thetotalnumberoftypical output
sequences yis2NH(Y),allhavingsimilar probabilit y.Foranyparticular typical
input sequence x,thereareabout2NH(YjX)probable sequences. Some ofthese
subsets ofAN
Yaredepicted bycircles ingure 9.8a.
Wenowimagine restricting ourselv estoasubset ofthetypical inputs
xsuchthatthecorresp onding typical output setsdonotoverlap, asshown
ingure 9.8b. Wecanthenbound thenumberofnon-confusable inputs by

<<<PAGE 167>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
9.8:Further exercises 155
dividing thesizeofthetypical yset,2NH(Y),bythesizeofeachtypical- y-
given-typical- xset,2NH(YjX).Sothenumberofnon-confusable inputs, ifthey
areselected fromthesetoftypical inputs xXN,is2NH(Y) NH(YjX)=
2NI(X;Y).
Themaxim umvalueofthisbound isachievedifXistheensem blethat
maximizes I(X;Y),inwhichcasethenumberofnon-confusable inputs is
2NC.Thusasymptotically uptoCbitspercycle, andnomore, canbe
comm unicated withvanishing errorprobabilit y. 2
Thissketchhasnotrigorously provedthatreliable comm unication really
ispossible {that's ourtaskforthenextchapter.
9.8Further exercises
Exercise 9.15.[3,p.159]Refer backtothecomputation ofthecapacit yoftheZ
channel withf=0:15.
(a)Whyisp
1lessthan0.5?Onecould argue thatitisgoodtofavour
the0input, sinceitistransmitted without error{andalsoargue
thatitisgoodtofavourthe1input, sinceitoften givesrisetothe
highly prized1output, whichallowscertain identication ofthe
input! Trytomakeaconvincing argumen t.
(b)Inthecaseofgeneralf,showthattheoptimal input distribution
is
p
1=1=(1 f)
1+2(H2(f)=(1 f)): (9.19)
(c)What happenstop
1ifthenoise levelfisverycloseto1?
Exercise 9.16.[2,p.159]Sketchgraphs ofthecapacit yoftheZchannel, the
binary symmetric channel andthebinary erasure channel asafunction
off.
.Exercise 9.17.[2]What isthecapacit yoftheve-input, ten-output channel
whose transition probabilit ymatrix is
2
6666666666666640:25 0 0 00:25
0:25 0 0 00:25
0:250:25 0 0 0
0:250:25 0 0 0
00:250:25 0 0
00:250:25 0 0
0 00:250:25 0
0 00:250:25 0
0 0 00:250:25
0 0 00:250:253
777777777777775987654321001234
? (9.20)
Exercise 9.18.[2,p.159]Consider aGaussian channel withbinary inputx2
f 1;+1gandrealoutput alphab etAY,withtransition probabilit yden-
sity
Q(yjx;;)=1p
22e (y x)2
22; (9.21)
whereisthesignal amplitude.

<<<PAGE 168>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
156 9|Comm unication overaNoisy Channel
(a)Compute theposterior probabilit yofxgiveny,assuming thatthe
twoinputs areequiprobable. Putyouranswerintheform
P(x=1jy;;)=1
1+e a(y): (9.22)
SketchthevalueofP(x=1jy;;)asafunction ofy.
(b)Assume thatasingle bitistobetransmitted. What istheoptimal
decoder,andwhatisitsprobabilit yoferror? Express youranswer
interms ofthesignal tonoise ratio2=2andtheerror function
(thecumulativeprobabilit yfunction oftheGaussian distribution),
(z)Zz
 11p
2e z2
2dz: (9.23)
[Note thatthisdenition oftheerrorfunction (z)maynotcorre-
spondtoother people's.]
Pattern recognition asanoisychannel
Wemaythink ofmanypattern recognition problems interms ofcomm uni-
cation channels. Consider thecaseofrecognizing handwritten digits (such
aspostcodesonenvelopes).Theauthor ofthedigitwishes tocomm unicate
amessage fromthesetAX=f0;1;2;3;:::;9g;thisselected message isthe
input tothechannel. What comes outofthechannel isapattern ofinkon
paper.Iftheinkpattern isrepresen tedusing 256binary pixels, thechannel
Qhasasitsoutput arandom variabley2AY=f0;1g256.Anexample ofan
elemen tfromthisalphab etisshowninthemargin.
Exercise 9.19.[2]Estimate howmanypatterns inAYarerecognizable asthe
character `2'.[Theaimofthisproblem istotrytodemonstrate the
existence ofasmany patterns aspossible thatarerecognizable as2s.]
Figure 9.9.Some more2s.Discuss howonemightmodelthechannelP(yjx=2).Estimate the
entropyoftheprobabilit ydistribution P(yjx=2).
Onestrategy fordoing pattern recognition istocreate amodelfor
P(yjx)foreachvalueoftheinputx=f0;1;2;3;:::;9g,thenuseBayes'
theorem toinferxgiveny.
P(xjy)=P(yjx)P(x)P
x0P(yjx0)P(x0): (9.24)
Thisstrategy isknownasfullprobabilistic modelling orgenerativ e
modelling .Thisisessentially howcurren tspeechrecognition systems
work.Inaddition tothechannel model,P(yjx),oneusesapriorproba-
bilitydistribution P(x),whichinthecaseofbothcharacter recognition
andspeechrecognition isalanguage modelthatspecies theprobabilit y
ofthenextcharacter/w ordgiventhecontextandtheknowngrammar
andstatistics ofthelanguage.
Random coding
Exercise 9.20.[2,p.160]Giventwenty-four people inaroom,whatistheprob-
abilitythatthere areatleasttwopeople presen twhohavethesame
birthda y(i.e.,dayandmonthofbirth)? What istheexpected number
ofpairsofpeople withthesamebirthda y?Whichofthese twoquestions
iseasiest tosolve?Whichanswergivesmost insigh t?Youmayndit
helpful tosolvethese problems andthose thatfollowusing notation such
asA=numberofdaysinyear=365andS=numberofpeople =24.

<<<PAGE 169>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
9.9:Solutions 157
.Exercise 9.21.[2]Thebirthda yproblem mayberelated toacodingscheme.
Assume wewishtoconveyamessage toanoutsider identifying oneof
thetwenty-four people. Wecould simply comm unicate anumbersfrom
AS=f1;2;:::;24g,havingagreed amapping ofpeople ontonumbers;
alternativ ely,wecould conveyanumberfromAX=f1;2;:::;365g,
identifying thedayoftheyearthatistheselected person's birthda y
(with apologies toleapyearians). [Thereceiverisassumed toknowall
thepeople's birthda ys.]What, roughly ,istheprobabilit yoferrorofthis
comm unication scheme, assuming itisusedforasingle transmission?
What isthecapacit yofthecomm unication channel, andwhat isthe
rateofcomm unication attempted bythisscheme?
.Exercise 9.22.[2]Nowimagine thatthere areKroomsinabuilding, each
containingqpeople. (Youmightthink ofK=2andq=24asan
example.) Theaimistocomm unicate aselection ofoneperson fromeach
roombytransmitting anordered listofKdays(fromAX).Compare
theprobabilit yoferrorofthefollowingtwoschemes.
(a)Asbefore, where eachroomtransmits thebirthda yoftheselected
person.
(b)ToeachK-tuple ofpeople, onedrawnfromeachroom,anordered
K-tuple ofrandomly selected daysfromAXisassigned (thisK-
tuple hasnothing todowiththeirbirthda ys).Thisenormous list
ofS=qKstrings isknowntothereceiver.When thebuilding has
selected aparticular person fromeachroom,theordered string of
dayscorresp onding tothatK-tuple ofpeople istransmitted.
What istheprobabilit yoferrorwhenq=364andK=1?What isthe
probabilit yoferrorwhenq=364andKislarge, e.g.K=6000?
9.9Solutions
Solution toexercise 9.2(p.149).Ifweassume weobserv ey=0,
P(x=1jy=0)=P(y=0jx=1)P(x=1)P
x0P(yjx0)P(x0)(9.25)
=0:150:1
0:150:1+0:850:9(9.26)
=0:015
0:78=0:019: (9.27)
Solution toexercise 9.4(p.149).Ifweobserv ey=0,
P(x=1jy=0)=0:150:1
0:150:1+1:00:9(9.28)
=0:015
0:915=0:016: (9.29)
Solution toexercise 9.7(p.150).Theprobabilit ythaty=1is0:5,sothe
mutual information is:
I(X;Y)=H(Y) H(YjX) (9.30)
=H2(0:5) H2(0:15) (9.31)
=1 0:61=0:39bits: (9.32)

<<<PAGE 170>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
158 9|Comm unication overaNoisy Channel
Solution toexercise 9.8(p.150).Weagain compute themutual information
usingI(X;Y)=H(Y) H(YjX).Theprobabilit ythaty=0is0:575,and
H(YjX)=P
xP(x)H(Yjx)=P(x=1)H(Yjx=1)+P(x=0)H(Yjx=0)
sothemutual information is:
I(X;Y)=H(Y) H(YjX) (9.33)
=H2(0:575) [0:5H2(0:15)+0:50] (9.34)
=0:98 0:30=0:679bits: (9.35)
Solution toexercise 9.12(p.151).Bysymmetry ,theoptimal input distribution
isf0:5;0:5g.Then thecapacit yis
C=I(X;Y)=H(Y) H(YjX) (9.36)
=H2(0:5) H2(f) (9.37)
=1 H2(f): (9.38)
Wouldyouliketondtheoptimal input distribution without invoking sym-
metry? Wecandothisbycomputing themutual information inthegeneral
casewhere theinput ensem bleisfp0;p1g:
I(X;Y)=H(Y) H(YjX) (9.39)
=H2(p0f+p1(1 f)) H2(f): (9.40)
Theonlyp-dependence isinthersttermH2(p0f+p1(1 f)),whichis
maximized bysetting theargumen tto0.5.Thisvalueisgivenbysetting
p0=1=2.
Solution toexercise 9.13(p.151).Answer1.Bysymmetry ,theoptimal input
distribution isf0:5;0:5g.Thecapacit yismosteasily evaluated bywriting the
mutual information asI(X;Y)=H(X) H(XjY).Theconditional entropy
H(XjY)isP
yP(y)H(Xjy);whenyisknown,xisonlyuncertain ify=?,
whichoccurs withprobabilit yf=2+f=2,sotheconditional entropyH(XjY)
isfH2(0:5).
C=I(X;Y)=H(X) H(XjY) (9.41)
=H2(0:5) fH2(0:5) (9.42)
=1 f: (9.43)
Thebinary erasure channel failsafractionfofthetime. Itscapacit yis
precisely 1 f,whichisthefraction ofthetimethatthechannel isreliable.
Thisresult seems veryreasonable, butitisfarfromobvious howtoencode
information soastocomm unicate reliably overthischannel.
Answer2.Alternativ ely,without invoking thesymmetry assumed above,we
canstart fromtheinput ensem blefp0;p1g.Theprobabilit ythaty=?is
p0f+p1f=f,andwhen wereceivey=?,theposterior probabilit yofxis
thesame asthepriorprobabilit y,so:
I(X;Y)=H(X) H(XjY) (9.44)
=H2(p1) fH2(p1) (9.45)
=(1 f)H2(p1): (9.46)
Thismutual information achievesitsmaxim umvalueof(1 f)whenp1=1=2.

<<<PAGE 171>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
9.9:Solutions 159
Q1?001
(a)11?1011???0?10?000
00100111
(b)11?1011???0?10?000
00100111x(1)x(2)
(c)11?1011???0?10?000
00100111x(1)x(2)
-------
^m=2^m=2^m=2^m=1^m=1^m=1
^m=0
N=1N=2Figure 9.10.(a)Theextended
channel (N=2)obtained froma
binary erasure channel with
erasure probabilit y0.15.(b)A
blockcodeconsisting ofthetwo
codewords00and11.(c)The
optimal decoderforthiscode.
Solution toexercise 9.14(p.153).Theextended channel isshowning-
ure9.10. Thebestcodeforthischannel withN=2isobtained bychoosing
twocolumns thathaveminimal overlap, forexample, columns00and11.The
decodingalgorithm returns `00'iftheextended channel output isamong the
topfourand`11'ifit'samong thebottom four,andgivesupiftheoutput is
`??'.
Solution toexercise 9.15(p.155).Inexample 9.11(p.151)weshowedthatthe
mutual information betweeninput andoutput oftheZchannel is
I(X;Y)=H(Y) H(YjX)
=H2(p1(1 f)) p1H2(f): (9.47)
Wedieren tiatethisexpression withrespecttop1,taking carenottoconfuse
log2withloge:
d
dp1I(X;Y)=(1 f)log21 p1(1 f)
p1(1 f) H2(f): (9.48)
Setting thisderivativetozeroandrearranging using skills developedinexer-
cise2.17(p.36),weobtain:
p
1(1 f)=1
1+2H2(f)=(1 f); (9.49)
sotheoptimal input distribution is
p
1=1=(1 f)
1+2(H2(f)=(1 f)): (9.50)
Asthenoiselevelftends to1,thisexpression tends to1=e(asyoucanprove
using L'H^opital's rule).
Forallvaluesoff,p
1issmaller than1=2.Arough intuition forwhyinput1
isusedlessthaninput0isthatwhen input1isused, thenoisy channel injects
entropyintothereceivedstring; whereas when input0isused, thenoise has
zeroentropy.Thusstarting fromp1=1=2,aperturbation towardssmaller
p1willreduce theconditional entropyH(YjX)linearly while leavingH(Y)
unchanged, torstorder.H(Y)decreases onlyquadratically in(p1 1/2).
Solution toexercise 9.16(p.155).Thecapacities ofthethree channels are
showningure 9.11. Foranyf<0:5,theBECisthechannel withhighest00.10.20.30.40.50.60.70.80.91
00.10.20.30.40.50.60.70.80.91Z
BSC
BEC
Figure 9.11.Capacities oftheZ
channel, binary symmetric
channel, andbinary erasure
channel.
capacit yandtheBSCthelowest.
Solution toexercise 9.18(p.155).Thelogarithm oftheposterior probabilit y
ratio, giveny,is
a(y)=logP(x=1jy;;)
P(x= 1jy;;)=logQ(yjx=1;;)
Q(yjx= 1;;)=2y
2:(9.51)

<<<PAGE 172>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
160 9|Comm unication overaNoisy Channel
Using ourskills pickedupfromexercise 2.17(p.36),werewrite thisinthe
form
P(x=1jy;;)=1
1+e a(y): (9.52)
Theoptimal decoderselects themost probable hypothesis; thiscanbedone
simply bylooking atthesignofa(y).Ifa(y)>0thendecodeas^x=1.
Theprobabilit yoferroris
pb=Z0
 1dyQ(yjx=1;;)=Z x
 1dy1p
22e y2
22=
 x

:(9.53)
Random coding
Solution toexercise 9.20(p.156).Theprobabilit ythatS=24people whose
birthda ysaredrawnatrandom fromA=365daysallhavedistinct birthda ys
is
A(A 1)(A 2):::(A S+1)
Aq: (9.54)
Theprobabilit ythattwo(ormore) people share abirthda yisoneminusthis
quantity,which,forS=24andA=365,isabout0.5.Thisexact wayof
answering thequestion isnotveryinformativ esince itisnotclearforwhat
valueofStheprobabilit ychanges frombeingcloseto0tobeingcloseto1.
ThenumberofpairsisS(S 1)=2,andtheprobabilit ythataparticular
pairshares abirthda yis1=A,sotheexpectednumberofcollisions is
S(S 1)
21
A: (9.55)
Thisanswerismore instructiv e.Theexpected numberofcollisions istinyif
Sp
AandbigifSp
A.
Wecanalsoapproximate theprobabilit ythatallbirthda ysaredistinct,
forsmallS,thus:
A(A 1)(A 2):::(A S+1)
AS=(1)(1 1/A)(1 2/A):::(1 (S 1)/A)
'exp(0) exp( 1=A)exp( 2=A):::exp( (S 1)=A) (9.56)
'exp 
 1
AS 1X
i=1i!
=exp
 S(S 1)=2
A
: (9.57)

<<<PAGE 173>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
AboutChapter 10
Before reading Chapter 10,youshould havereadChapters 4and9.Exer-
cise9.14(p.153)isespecially recommended.
Castofcharacters
Q thenoisy channel
C thecapacit yofthechannel
XNanensem bleusedtocreate arandom code
C arandom code
N thelength ofthecodewords
x(s)acodeword,thesthinthecode
s thenumberofachosen codeword(mnemonic: thesource
selectss)
S=2Kthetotalnumberofcodewordsinthecode
K=log2Sthenumberofbitsconveyedbythechoice ofonecodeword
fromS,assuming itischosen withuniform probabilit y
s abinary represen tation ofthenumbers
R=K=N therateofthecode,inbitsperchannel use(sometimes called
R0instead)
^s thedecoder'sguess ofs
161

<<<PAGE 174>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
10
TheNoisy-Channel CodingTheorem
10.1 Thetheorem
Thetheorem hasthreeparts, twopositiveandonenegativ e.Themainpositive
result istherst.
-6

C RR(pb)pb
12
3
Figure 10.1.Portion oftheR;pb
plane tobeprovedachievable
(1,2)andnotachievable(3).1.Foreverydiscrete memoryless channel, thechannel capacit y
C=max
PXI(X;Y) (10.1)
hasthefollowingproperty.Forany>0andR<C,forlargeenoughN,
there exists acodeoflengthNandrateRandadecodingalgorithm,
suchthatthemaximal probabilit yofblockerroris<.
2.Ifaprobabilit yofbiterrorpbisacceptable, ratesuptoR(pb)areachiev-
able,where
R(pb)=C
1 H2(pb): (10.2)
3.Foranypb,ratesgreater thanR(pb)arenotachievable.
10.2 Jointlytypical sequences
Weformalize theintuitivepreview ofthelastchapter.
Wewilldene codewordsx(s)ascoming fromanensem bleXN,andcon-
sidertherandom selection ofonecodewordandacorresp onding channel out-
puty,thusdening ajointensem ble(XY)N.Wewilluseatypical setdecoder,
whichdecodesareceivedsignalyassifx(s)andyarejointlytypical ,aterm
tobedened shortly .
Theproofwillthencentreondetermining theprobabilities (a)thatthe
trueinput codewordisnotjointlytypical withtheoutput sequence; and(b)
thatafalseinput codewordisjointlytypical withtheoutput. Wewillshow
that,forlargeN,bothprobabilities gotozeroaslongasthere arefewerthan
2NCcodewords,andtheensem bleXistheoptimal input distribution.
Jointtypicalit y.Apairofsequences x;yoflengthNaredened tobe
jointlytypical (totolerance)withrespecttothedistribution P(x;y)
if
xistypical ofP(x),i.e.,1
Nlog1
P(x) H(X)<;
yistypical ofP(y),i.e.,1
Nlog1
P(y) H(Y)<;
andx;yistypical ofP(x;y),i.e.,1
Nlog1
P(x;y) H(X;Y)<:
162

<<<PAGE 175>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
10.2: Jointlytypical sequences 163
Thejointlytypical setJNisthesetofalljointlytypical sequence pairs
oflengthN.
Example. Hereisajointlytypical pairoflengthN=100fortheensem ble
P(x;y)inwhichP(x)has(p0;p1)=(0:9;0:1)andP(yjx)corresp ondstoa
binary symmetric channel withnoise level0:2.
x11111111110000000000000000000 00000000000000000000000000000000000000000000000000000000000000000000000
y00111111110000000000000000000 00000000000000000000000000000000000000000000000000000111111111111111111
Notice thatxhas101s,andsoistypical oftheprobabilit yP(x)(atany
tolerance);andyhas261s,soitistypical ofP(y)(becauseP(y=1)=0:26);
andxandydier in20bits,whichisthetypical numberofipsforthis
channel.
Jointtypicalit ytheorem .Letx;ybedrawnfrom theensem ble(XY)N
dened by
P(x;y)=NY
n=1P(xn;yn):
Then
1.theprobabilit ythatx;yarejointlytypical (totolerance)tends
to1asN!1;
2.thenumberofjointlytypical sequencesjJNjiscloseto2NH(X;Y).
Tobeprecise,
jJNj2N(H(X;Y)+); (10.3)
3.ifx0XNandy0YN,i.e.,x0andy0areindependent samples
withthesamemarginal distribution asP(x;y),thentheprobabilit y
that(x0;y0)lands inthejointlytypical setisabout2 NI(X;Y).To
beprecise,
P((x0;y0)2JN)2 N(I(X;Y) 3): (10.4)
Proof.Theproofofparts 1and2bythelawoflarge numbersfollowsthat
ofthesource codingtheorem inChapter 4.Forpart2,letthepairx;y
playtheroleofxinthesource codingtheorem, replacingP(x)there by
theprobabilit ydistribution P(x;y).
Forthethirdpart,
P((x0;y0)2JN)=X
(x;y)2JNP(x)P(y) (10.5)
jJNj2 N(H(X) )2 N(H(Y) )(10.6)
2N(H(X;Y)+) N(H(X)+H(Y) 2)(10.7)
=2 N(I(X;Y) 3): 2(10.8)
Acartoonofthejointlytypical setisshowningure 10.2. Twoindependen t
typical vectors arejointlytypical withprobabilit y
P((x0;y0)2JN)'2 N(I(X;Y))(10.9)
because thetotalnumberofindependen ttypical pairsistheareaofthedashed
rectangle, 2NH(X)2NH(Y),andthenumberofjointlytypical pairs isroughly
2NH(X;Y),sotheprobabilit yofhitting atypical pairisroughly
2NH(X;Y)=2NH(X)+NH(Y)=2 NI(X;Y): (10.10)

<<<PAGE 176>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
164 10|TheNoisy-Channel CodingTheorem
 -
?6- 
2NH(X)6
?2NH(Y)-
-
2NH(XjY)6
?6
?2NH(YjX)q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
qq
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
qq
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
qq
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
qq
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
qq
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q2NH(X;Y)dotsAN
X
AN
YFigure 10.2.Thejointlytypical
set.Thehorizon taldirection
represen tsAN
X,thesetofallinput
strings oflengthN.Thevertical
direction represen tsAN
Y,thesetof
alloutput strings oflengthN.
Theouter boxcontainsall
conceiv ableinput{output pairs.
Eachdotrepresen tsajointly
typical pairofsequences (x;y).
Thetotalnumberofjointly
typical sequences isabout
2NH(X;Y).
10.3 Proofofthenoisy-c hannel codingtheorem
Analogy
Imagine thatwewishtoprovethatthere isababyinaclassofonehundred
babies whoweighslessthan10kg.Individual babies aredicult tocatchand
weigh. Shannon's metho dofsolving thetaskistoscoopupallthebabies
Figure 10.3.Shannon's metho dfor
provingonebabyweighslessthan
10kg.andweighthem allatonceonabigweighing machine. Ifwendthattheir
averageweightissmaller than10kg,there mustexistatleastonebabywho
weighs lessthan10kg{indeed there mustbemany!Shannon's metho disn't
guaran teedtorevealtheexistence ofanunderw eightchild,since itrelies on
there beingatinynumberofelephan tsintheclass. Butifweusehismetho d
andgetatotalweightsmaller than1000kgthenourtaskissolved.
Fromskinny childrentofantastic codes
Wewishtoshowthatthere exists acodeandadecoderhavingsmall prob-
abilityoferror. Evaluating theprobabilit yoferror ofanyparticular coding
anddecodingsystem isnoteasy.Shannon's innovation wasthis:instead of
constructing agoodcodinganddecodingsystem andevaluating itserrorprob-
ability,Shannon calculated theaverage probabilit yofblockerrorofallcodes,
andprovedthatthisaverage issmall. There mustthenexistindividual codes
thathavesmall probabilit yofblockerror.
Random codingandtypicalsetdecoding
Consider thefollowingencoding{deco dingsystem, whose rateisR0.
1.WexP(x)andgenerate theS=2NR0codewordsofa(N;NR0)=

<<<PAGE 177>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
10.3: Proofofthenoisy-c hannel codingtheorem 165
x(3)x(1)x(2)x(4)
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
qq
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
qq
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
qq
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
qq
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
qq
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
qx(3)x(1)x(2)x(4)
ycydybya
----
^s(yc)=4^s(yd)=0^s(yb)=3^s(ya)=0q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
qq
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
qq
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
qq
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
qq
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
qq
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
q
(a) (b)Figure 10.4.(a)Arandom code.
(b)Example decodings bythe
typical setdecoder.Asequence
thatisnotjointlytypical withany
ofthecodewords,suchasya,is
decodedas^s=0.Asequence that
isjointlytypical withcodeword
x(3)alone,yb,isdecodedas^s=3.
Similarly ,ycisdecodedas^s=4.
Asequence thatisjointlytypical
withmorethanonecodeword,
suchasyd,isdecodedas^s=0.
(N;K)codeCatrandom according to
P(x)=NY
n=1P(xn): (10.11)
Arandom codeisshownschematically ingure 10.4a.
2.Thecodeisknowntobothsender andreceiver.
3.Amessagesischosen fromf1;2;:::;2NR0g,andx(s)istransmitted. The
receivedsignal isy,with
P(yjx(s))=NY
n=1P(ynjx(s)
n): (10.12)
4.Thesignal isdecodedbytypical setdecoding.
Typical setdecoding.Decodeyas^sif(x(^s);y)arejointlytypical and
there isnoothers0suchthat(x(s0);y)arejointlytypical;
otherwise declare afailure (^s=0).
Thisisnottheoptimal decodingalgorithm, butitwillbegoodenough,
andeasier toanalyze. Thetypical setdecoderisillustrated ing-
ure10.4b.
5.Adecodingerroroccurs if^s6=s.
There arethree probabilities oferrorthatwecandistinguish. First, there
istheprobabilit yofblockerrorforaparticular codeC,thatis,
pB(C)P(^s6=sjC): (10.13)
Thisisadicult quantitytoevaluate foranygivencode.
Second, there istheaverage overallcodesofthisblockerrorprobabilit y,
hpBiX
CP(^s6=sjC)P(C): (10.14)

<<<PAGE 178>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
166 10|TheNoisy-Channel CodingTheorem
Fortunately ,thisquantityismucheasier toevaluate thantherstquantity
P(^s6=sjC).
Third, themaximal blockerrorprobabilit yofacodeC,
pBM(C)maxsP(^s6=sjs;C); (10.15)
isthequantitywearemostinterested in:wewishtoshowthatthere exists a
codeCwiththerequired ratewhose maximal blockerrorprobabilit yissmall.
Wewillgettothisresult byrstnding theaverage blockerrorprobabilit y,
hpBi.Once wehaveshownthatthiscanbemade smaller thanadesired small
number,weimmediately deduce thatthere mustexistatleastonecodeC
whose blockerror probabilit yisalsolessthanthissmall number.Finally ,
weshowthatthiscode,whose blockerror probabilit yissatisfactorily small
butwhose maximal blockerrorprobabilit yisunkno wn(andcould conceiv ably
beenormous), canbemodied tomakeacodeofslightlysmaller ratewhose
maximal blockerror probabilit yisalsoguaran teedtobesmall. Wemodify
thecodebythrowingawaytheworst50%ofitscodewords.
Wetherefore nowembarkonnding theaverage probabilit yofblockerror.
Probability oferroroftypicalsetdecoder
There aretwosources oferrorwhen weusetypical setdecoding. Either (a)
theoutput yisnotjointlytypical withthetransmitted codewordx(s),or(b)
there issome other codewordinCthatisjointlytypical withy.
Bythesymmetry ofthecodeconstruction, theaverage probabilit yoferror
averaged overallcodesdoesnotdependontheselected valueofs;wecan
assume without lossofgeneralit ythats=1.
(a)Theprobabilit ythattheinputx(1)andtheoutput yarenotjointly
typical vanishes, bythejointtypicalit ytheorem's rstpart(p.163).Wegivea
name,,totheupperboundonthisprobabilit y,satisfying!0asN!1;
foranydesired,wecanndablocklengthN()suchthattheP((x(1);y)62
JN).
(b)Theprobabilit ythatx(s0)andyarejointlytypical, foragivens06=1
is2 N(I(X;Y) 3),bypart3.Andthere are(2NR0 1)rivalvalues ofs0to
worryabout.
Thustheaverage probabilit yoferrorhpBisatises:
hpBi+2NR0
X
s0=22 N(I(X;Y) 3)(10.16)
+2 N(I(X;Y) R0 3): (10.17)
Theinequalit y(10.16) thatbounds thetotalprobabilit yoferror PERRbythesumof
theprobabilities Ps0ofallsortsofeventss0eachofwhichissucien ttocause error,
PTOTP1+P2+;
iscalled a`union bound'. Itisonlyanequalit yifthedieren teventsthatcause error
neveroccuratthesame timeaseachother.
Theaverage probabilit yoferrorcanbemade<2byincreasingNif
R0<I(X;Y) 3: (10.18)
Wearealmost there. Wemakethree modications:
1.WechooseP(x)intheprooftobetheoptimal input distribution ofthe
channel. Then theconditionR0<I(X;Y) 3becomesR0<C 3.

<<<PAGE 179>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
10.4: Comm unication (with errors) abovecapacit y 167
)
(a)Arandom code::: (b)expurgatedFigure 10.5.Howexpurgation
works. (a)Inatypical random
code,asmall fraction ofthe
codewordsareinvolvedin
collisions {pairsofcodewordsare
sucien tlyclosetoeachother
thattheprobabilit yoferrorwhen
either codewordistransmitted is
nottiny.Weobtain anewcode
fromarandom codebydeleting
allthese confusable codewords.
(b)Theresulting codehasslightly
fewercodewords,sohasaslightly
lowerrate,anditsmaximal
probabilit yoferrorisgreatly
reduced.
2.Since theaverage probabilit yoferroroverallcodesis<2,there must
existacodewithmean probabilit yofblockerrorpB(C)<2.
3.Toshowthatnotonlytheaverage butalsothemaximal probabilit yof
error,pBM,canbemade small, wemodifythiscodebythrowingaway
theworsthalfofthecodewords{theonesmostlikelytoproduceerrors.
Those thatremain mustallhaveconditional probabilit yoferror less
than4.Weusethese remaining codewordstodene anewcode.This
newcodehas2NR0 1codewords,i.e.,wehavereduced theratefromR0
toR0 1/N(anegligible reduction, ifNislarge), andachievedpBM<4.
Thistrickiscalled expurgation (gure 10.5). Theresulting codemay
notbethebestcodeofitsrateandlength, butitisstillgoodenough to
provethenoisy-c hannel codingtheorem, whichiswhatwearetrying to
dohere.
Inconclusion, wecan`construct' acodeofrateR0 1/N,whereR0<C 3,
withmaximal probabilit yoferror<4.Weobtain thetheorem asstated by
settingR0=(R+C)=2,==4,<(C R0)=3,andNsucien tlylargefor
theremaining conditions tohold. Thetheorem's rstpartisthusproved.2
10.4 Comm unication (with errors) abovecapacit y-6
CRpb
achievable
Figure 10.6.Portion oftheR;pb
plane provedachievableinthe
rstpartofthetheorem. [We've
provedthatthemaximal
probabilit yofblockerrorpBMcan
bemade arbitrarily small, sothe
samegoesforthebiterror
probabilit ypb,whichmustbe
smaller thanpBM.]
Wehaveproved,foranydiscrete memoryless channel, theachievabilityofa
portion oftheR;pbplane showningure 10.6. Wehaveshownthatwecan
turnanynoisy channel intoanessentially noiseless binary channel withrate
uptoCbitspercycle. Wenowextend theright-hand boundary oftheregion
ofachievabilityatnon-zero errorprobabilities. [This iscalled rate-distortion
theory .]
Wedothiswithanewtrick.Since weknowwecanmakethenoisy channel
intoaperfect channel withasmaller rate,itissucien ttoconsider comm u-
nication witherrors overanoiseless channel. Howfastcanwecomm unicate
overanoiseless channel, ifweareallowedtomakeerrors?
Consider anoiseless binary channel, andassume thatweforcecomm uni-
cation atarategreater thanitscapacit yof1bit.Forexample, ifwerequire
thesender toattempt tocomm unicate atR=2bitspercycle thenhemust
eectiv elythrowawayhalfoftheinformation. What isthebestwaytodo
thisiftheaimistoachievethesmallest possible probabilit yofbiterror? One
simple strategy istocomm unicate afraction 1=Rofthesource bits,andignore
therest. Thereceiverguesses themissing fraction 1 1=Ratrandom, and

<<<PAGE 180>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
168 10|TheNoisy-Channel CodingTheorem
theaverage probabilit yofbiterroris
pb=1
2(1 1=R): (10.19)
Thecurvecorresp onding tothisstrategy isshownbythedashed lineing-
ure10.7.
Wecandobetterthanthis(interms ofminimizing pb)byspreading out
theriskofcorruption evenlyamong allthebits. Infact,wecanachieve
pb=H 1
2(1 1=R),whichisshownbythesolidcurveingure 10.7. So,howpb
00.050.10.150.20.250.3
0 0.5 1 1.5 2 2.5
Optimum
Simple
R
Figure 10.7.Asimple bound on
achievablepoints(R;pb),and
Shannon's bound.canthisoptim umbeachieved?
Wereuse atoolthatwejustdeveloped,namely the(N;K)codefora
noisy channel, andweturnitonitshead, using thedecodertodene alossy
compressor. Specically ,wetakeanexcellen t(N;K)codeforthebinary
symmetric channel. Assume thatsuchacodehasarateR0=K=N,andthat
itiscapable ofcorrecting errors introduced byabinary symmetric channel
whose transition probabilit yisq.Asymptotically ,rateR0codesexistthat
haveR0'1 H2(q).Recall that,ifweattachoneofthese capacit y-achieving
codesoflengthNtoabinary symmetric channel then(a)theprobabilit y
distribution overtheoutputs isclose touniform, since theentropyofthe
output isequal totheentropyofthesource (NR0)plustheentropyofthe
noise (NH2(q)),and(b)theoptimal decoderofthecode,inthissituation,
typically maps areceivedvector oflengthNtoatransmitted vector diering
inqNbitsfromthereceivedvector.
Wetakethesignal thatwewishtosend,andchopitintoblocksoflengthN
(yes,N,notK).Wepasseachblockthrough thedecoder,andobtain ashorter
signal oflengthKbits,whichwecomm unicate overthenoiseless channel. To
decodethetransmission, wepasstheKbitmessage totheencoderofthe
original code.Thereconstituted message willnowdier fromtheoriginal
message insome ofitsbits{typicallyqNofthem. Sotheprobabilit yofbit
errorwillbepb=q.Therateofthislossycompressor isR=N=K=1=R0=
1=(1 H2(pb)).
Now,attachingthislossycompressor toourcapacit y-Cerror-free comm u-
nicator, wehaveprovedtheachievabilityofcomm unication uptothecurve
(pb;R)dened by:
R=C
1 H2(pb): 2 (10.20)
Forfurther reading aboutrate-distortion theory ,seeGallager (1968), p.451,
orMcEliece (2002), p.75.
10.5 Thenon-ac hievableregion (part 3ofthetheorem)
Thesource, encoder,noisy channel anddecoderdene aMarkovchain: s!x!y!^s
P(s;x;y;^s)=P(s)P(xjs)P(yjx)P(^sjy): (10.21)
Thedataprocessing inequalit y(exercise 8.9,p.141)mustapply tothischain:
I(s;^s)I(x;y):Furthermore, bythedenition ofchannel capacit y,I(x;y)
NC,soI(s;^s)NC.
Assume thatasystem achievesarateRandabiterror probabilit ypb;
thenthemutual information I(s;^s)isNR(1 H2(pb)).ButI(s;^s)>NC
isnotachievable,soR>C
1 H2(pb)isnotachievable. 2
Exercise 10.1.[2]Fillinthedetails inthepreceding argumen t.Ifthebiterrors
between^sandsareindependen tthenwehaveI(s;^s)=NR(1 H2(pb)).

<<<PAGE 181>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
10.6: Computing capacit y 169
What ifwehavecomplex correlations among those biterrors? Whydoes
theinequalit yI(s;^s)NR(1 H2(pb))hold?
10.6 Computing capacit y
Wehaveprovedthatthecapacit yofachannel isthemaxim umrateatwhich
reliable comm unication canbeachieved.Howcanwecompute thecapacit yof
agivendiscrete memoryless channel? Weneedtonditsoptimal input distri-
bution. Ingeneral wecanndtheoptimal input distribution byacomputer
search,making useofthederivativeofthemutual information withrespect
totheinput probabilities.
.Exercise 10.2.[2]FindthederivativeofI(X;Y)withrespecttotheinput prob-
abilitypi,@I(X;Y)=@pi,forachannel withconditional probabilities Qjji.
Exercise 10.3.[2]ShowthatI(X;Y)isaconcave_function oftheinput prob-
abilityvectorp.
SinceI(X;Y)isconcave_intheinput distribution p,anyprobabilit ydistri-
bution patwhichI(X;Y)isstationary mustbeaglobal maxim umofI(X;Y).
Soitistempting toputthederivativeofI(X;Y)intoaroutine thatndsa
localmaxim umofI(X;Y),thatis,aninput distribution P(x)suchthat
@I(X;Y)
@pi=foralli; (10.22)
whereisaLagrange multiplier associated withtheconstrain tP
ipi=1.
However,thisapproac hmayfailtondtherightanswer,becauseI(X;Y)
mightbemaximized byadistribution thathaspi=0forsome inputs. A
simple example isgivenbytheternary confusion channel.
Ternary confusion channel .AX=f0;?;1g.AY=f0;1g.
--
  
@@R10
10
?P(y=0jx=0)=1;
P(y=1jx=0)=0;P(y=0jx=?)=1=2;
P(y=1jx=?)=1=2;P(y=0jx=1)=0;
P(y=1jx=1)=1:
Whenev ertheinput?isused, theoutput israndom; theother inputs
arereliable inputs. Themaxim uminformation rateof1bitisachieved
bymaking nouseoftheinput?.
.Exercise 10.4.[2,p.173]Sketchthemutual information forthischannel asa
function oftheinput distribution p.Pickaconvenienttwo-dimensional
represen tation ofp.
Theoptimization routine musttherefore takeaccoun tofthepossibilit ythat,
aswegouphillonI(X;Y),wemayrunintotheinequalit yconstrain tspi0.
.Exercise 10.5.[2,p.174]Describ ethecondition, similar toequation (10.22), that
issatised atapointwhereI(X;Y)ismaximized, anddescrib eacom-
puter program fornding thecapacit yofachannel.

<<<PAGE 182>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
170 10|TheNoisy-Channel CodingTheorem
Results thatmayhelpinnding theoptimal inputdistribution
1.Alloutputs mustbeused.
2.I(X;Y)isaconvex^function ofthechannel parameters. Reminder: The term `convex^'
means `convex', and the term
`conca ve_'means `conca ve';the
little smile andfrownsymbolsare
included simply toremind youwhat
convexandconcavemean.3.There maybeseveraloptimal input distributions, buttheyalllookthe
same attheoutput.
.Exercise 10.6.[2]Provethatnooutputyisunusedbyanoptimal input distri-
bution, unless itisunreac hable, thatis,hasQ(yjx)=0forallx.
Exercise 10.7.[2]ProvethatI(X;Y)isaconvex^function ofQ(yjx).
Exercise 10.8.[2]Provethatalloptimal input distributions ofachannel have
thesame output probabilit ydistribution P(y)=P
xP(x)Q(yjx).
These results, along withthefactthatI(X;Y)isaconcave_function of
theinput probabilit yvectorp,provethevalidityofthesymmetry argumen t
thatwehaveusedwhen nding thecapacit yofsymmetric channels. Ifa
channel isinvariantunder agroup ofsymmetry operations {forexample,
interchanging theinput symbolsandinterchanging theoutput symbols{then,
givenanyoptimal input distribution thatisnotsymmetric, i.e.,isnotinvariant
under these operations, wecancreate another input distribution byaveraging
together thisoptimal input distribution andallitspermutedforms thatwe
canmakebyapplying thesymmetry operations totheoriginal optimal input
distribution. Thepermuteddistributions musthavethesameI(X;Y)asthe
original, bysymmetry ,sothenewinput distribution created byaveraging
musthaveI(X;Y)bigger thanorequal tothatoftheoriginal distribution,
because oftheconcavityofI.
Symmetric channels
Inorder tousesymmetry argumen ts,itwillhelptohaveadenition ofa
symmetric channel. IlikeGallager's (1968) denition.
Adiscrete memoryless channel isasymmetric channel ifthesetof
outputs canbepartitioned intosubsets insuchawaythatforeach
subset thematrix oftransition probabilities hasthepropertythateach
row(ifmorethan1)isapermutation ofeachother rowandeachcolumn
isapermutation ofeachother column.
Example 10.9. Thischannel
P(y=0jx=0)=0:7;
P(y=?jx=0)=0:2;
P(y=1jx=0)=0:1;P(y=0jx=1)=0:1;
P(y=?jx=1)=0:2;
P(y=1jx=1)=0:7:(10.23)
isasymmetric channel because itsoutputs canbepartitioned into(0;1)
and?,sothatthematrix canberewritten:
P(y=0jx=0)=0:7;
P(y=1jx=0)=0:1;P(y=0jx=1)=0:1;
P(y=1jx=1)=0:7;
P(y=?jx=0)=0:2;P(y=?jx=1)=0:2:(10.24)

<<<PAGE 183>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
10.7: Other codingtheorems 171
Symmetry isauseful propertybecause, aswewillseeinalater chapter,
comm unication atcapacit ycanbeachievedoversymmetric channels bylinear
codes.
Exercise 10.10.[2]Provethatforasymmetric channel withanynumberof
inputs, theuniform distribution overtheinputs isanoptimal input
distribution.
.Exercise 10.11.[2,p.174]Arethere channels thatarenotsymmetric whose op-
timal input distributions areuniform? Find one,orprovethere are
none.
10.7 Other codingtheorems
Thenoisy-c hannel codingtheorem thatweprovedinthischapter isquitegen-
eral,applying toanydiscrete memoryless channel; butitisnotveryspecic.
Thetheorem onlysaysthatreliable comm unication witherror probabilit y
andrateRcanbeachievedbyusing codeswithsuciently largeblocklength
N.Thetheorem doesnotsayhowlargeNneeds tobetoachievegivenvalues
ofRand.
Presumably ,thesmallerisandthecloserRistoC,thelargerNhasto
be.Er(R)
RC
Figure 10.8.Atypical
random-co dingexponent
Noisy-channel codingtheorem{version withexplicit N-dependenc e
Foradiscrete memoryless channel, ablocklengthNandarateR,
there existblockcodesoflengthNwhose average probabilit yof
errorsatises:
pBexp[ NEr(R)] (10.25)
whereEr(R)istherandom-co dingexponentofthechannel, a
convex^,decreasing, positivefunction ofRfor0R<C.The
random-co dingexponentisalsoknownasthereliabilit yfunction.
[Byanexpurgation argumen titcanalsobeshownthatthere exist
blockcodesforwhichthemaximal probabilit yoferrorpBMisalso
exponentially small inN.]
Thedenition ofEr(R)isgiveninGallager (1968), p.139.Er(R)approac hes
zeroasR!C;thetypical behaviour ofthisfunction isillustrated ing-
ure10.8. Thecomputation oftherandom-co dingexponentforinteresting
channels isachallenging taskonwhichmucheort hasbeenexpended. Even
forsimple channels likethebinary symmetric channel, there isnosimple ex-
pression forEr(R).
Lowerbounds ontheerrorprobability asafunction ofblocklength
Thetheorem stated aboveasserts thatthere arecodeswithpBsmaller than
exp[ NEr(R)].Buthowsmall cantheerror probabilit ybe?Could itbe
muchsmaller?
ForanycodewithblocklengthNonadiscrete memoryless channel,
theprobabilit yoferrorassuming allsource messages areusedwith
equal probabilit ysatises
pB>exp[ NEsp(R)]; (10.26)

<<<PAGE 184>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
172 10|TheNoisy-Channel CodingTheorem
where thefunctionEsp(R),thesphere-pac kingexponentofthe
channel, isaconvex^,decreasing, positivefunction ofRfor0
R<C.
Foraprecise statemen tofthisresult andfurther references, seeGallager
(1968), p.157.
10.8 Noisy-c hannel codingtheorems andcodingpractice
Imagine acustomer whowantstobuyanerror-correcting codeanddecoder
foranoisy channel. Theresults describ edaboveallowustooerthefollowing
service: ifhetellsustheproperties ofhischannel, thedesired rateRandthe
desired errorprobabilit ypB,wecan,afterworking outtherelevantfunctions
C,Er(R),andEsp(R),advise himthatthere exists asolution tohisproblem
using aparticular blocklengthN;indeed thatalmost anyrandomly chosen
codewiththatblocklength should dothejob.Unfortunately wehavenot
found outhowtoimplemen tthese encodersanddecodersinpractice; thecost
ofimplemen tingtheencoderanddecoderforarandom codewithlargeN
wouldbeexponentially largeinN.
Furthermore, forpractical purposes,thecustomer isunlikelytoknowex-
actly what channel heisdealing with. SoBerlek amp(1980) suggests that
thesensible waytoapproac herror-correction istodesign encoding-deco ding
systems andplottheirperformance onavariety ofidealized channels asa
function ofthechannel's noise level.These charts (oneofwhichisillustrated
onpage568)canthenbeshowntothecustomer, whocanchooseamong the
systems onoerwithout havingtospecifywhat hereally thinks hischannel
islike.With thisattitude tothepractical problem, theimportance ofthe
functionsEr(R)andEsp(R)isdiminished.
10.9 Further exercises
Exercise 10.12.[2]Abinary erasure channel withinputxandoutputyhas
transition probabilit ymatrix:
Q=2
641 q0
qq
01 q3
75
10
--
  @@R
1?0
Findthemutual information I(X;Y)betweentheinput andoutput for
general input distributionfp0;p1g,andshowthatthecapacity ofthis
channel isC=1 qbits.
AZchannel hastransition probabilit ymatrix:
Q="
1q
01 q#
10
--

10
Showthat,using a(2;1)code,twousesofaZchannel canbemade to
emulateoneuseofanerasure channel, andstatetheerasure probabilit y
ofthaterasure channel. Hence showthatthecapacit yoftheZchannel,
CZ,satisesCZ1
2(1 q)bits.
Explain whytheresultCZ1
2(1 q)isaninequalit yrather thanan
equalit y.

<<<PAGE 185>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
10.10: Solutions 173
Exercise 10.13.[3,p.174]Atransatlan ticcable containsN=20indistinguish-
ableelectrical wires. Youhavethejobofguring outwhichwireis
which,thatis,tocreate aconsisten tlabelling ofthewires ateachend.
Youronlytoolsaretheabilitytoconnect wires toeachother ingroups
oftwoormore, andtotestforconnectedness withacontinuitytester.
What isthesmallest numberoftransatlan tictripsyouneedtomake,
andhowdoyoudoit?
Howwouldyousolvetheproblem forlargerNsuchasN=1000?
Asanillustration, ifNwere3thenthetaskcanbesolvedintwosteps
bylabelling onewireatoneenda,connecting theother twotogether,
crossing theAtlantic,measuring whichtwowiresareconnected, labelling
thembandcandtheunconnected onea,thenconnecting btoaand
returning across theAtlantic,whereup onondisconnecting bfromc,the
identities ofbandccanbededuced.
Thisproblem canbesolvedbypersisten tsearch,butthereason itis
posedinthischapter isthatitcanalsobesolvedbyagreedy approac h
based onmaximizing theacquired information .Lettheunkno wnper-
mutation ofwires bex.Havingchosen asetofconnections ofwiresCat
oneend,youcanthenmakemeasuremen tsattheother end,andthese
measuremen tsyconveyinformation aboutx.Howmuch?Andforwhat
setofconnections istheinformation yconveysaboutxmaximized?
10.10 Solutions
Solution toexercise 10.4(p.169).Iftheinput distribution isp=(p0;p?;p1),
--
   
@
@@R1/2
1/2
10
10
?themutual information is
I(X;Y)=H(Y) H(YjX)=H2(p0+p?=2) p?: (10.27)
Wecanbuild agoodsketchofthisfunction intwoways:bycareful inspection
ofthefunction, orbylooking atspecialcases.
Fortheplots, thetwo-dimensional represen tation ofpIwillusehasp0and
p1astheindependen tvariables, sothatp=(p0;p?;p1)=(p0;(1 p0 p1);p1).
Byinspection. Ifweusethequantitiespp0+p?=2andp?asourtwo
degrees offreedom, themutual information becomes verysimple:I(X;Y)=
H2(p) p?.Converting backtop0=p p?=2andp1=1 p p?=2,
weobtain thesketchshownattheleftbelow.Thisfunction islikeatunnel
rising upthedirection ofincreasingp0andp1.Toobtain therequired plotof
I(X;Y)wehavetostripawaytheparts ofthistunnel thatliveoutside the
feasible simplex ofprobabilities; wedothisbyredrawingthesurface, showing
onlytheparts wherep0>0andp1>0.Afullplotofthefunction isshown
attheright.
p0-0.5 0 0.5 1-0.500.51
-1-0.500.51
p1
p00 0.5 100.51
00.51
0 0.5 100.51
00.51
p1

<<<PAGE 186>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
174 10|TheNoisy-Channel CodingTheorem
Specialcases. Inthespecialcasep?=0,thechannel isanoiseless binary
channel, andI(X;Y)=H2(p0).
Inthespecialcasep0=p1,thetermH2(p0+p?=2)isequal to1,so
I(X;Y)=1 p?.
Inthespecialcasep0=0,thechannel isaZchannel witherrorprobabilit y
0.5.Weknowhowtosketchthat,fromtheprevious chapter (gure 9.3).
p00 0.5 100.51
00.51
p1
Figure 10.9.Skeleton ofthe
mutual information forthe
ternary confusion channel.These specialcasesallowustoconstruct theskeleton showningure 10.9.
Solution toexercise 10.5(p.169).Necessary andsucien tconditions forpto
maximizeI(X;Y)are
@I(X;Y)
pi=andpi>0
@I(X;Y)
piandpi=09
=
;foralli; (10.28)
whereisaconstan trelated tothecapacit ybyC=+log2e.
Thisresult canbeusedinacomputer program thatevaluates thederiva-
tivesandincremen tsanddecremen tstheprobabilities piinproportion tothe
dierences betweenthose derivatives.
Thisresult isalsouseful forlazyhuman capacit y-nders whoaregood
guessers. Havingguessed theoptimal input distribution, onecansimply con-
rmthatequation (10.28) holds.
Solution toexercise 10.11 (p.171).Wecertainly expectnonsymmetric chan-
nelswithuniform optimal input distributions toexist, sincewhen inventinga
channel wehaveI(J 1)degrees offreedom whereas theoptimal input dis-
tribution isjust(I 1)-dimensional; sointheI(J 1)-dimensional space of
perturbations around asymmetric channel, weexpectthere tobeasubspace
ofperturbations ofdimensionI(J 1) (I 1)=I(J 2)+1thatleavethe
optimal input distribution unchanged.
Hereisanexplicit example, abitlikeaZchannel.
Q=2
66640:9585 0:0415 0:350:0
0:0415 0:9585 0:00:35
0 0 0:65 0
0 0 00:653
7775(10.29)
Solution toexercise 10.13 (p.173).Thelabelling problem canbesolvedfor
anyN>2withjusttwotrips, oneeachwayacross theAtlantic.
Thekeystepintheinformation-theoretic approac htothisproblem isto
write downtheinformation contentofonepartition ,thecombinatorial object
thatistheconnecting together ofsubsets ofwires. IfNwires aregroup ed
together intog1subsets ofsize1,g2subsets ofsize2,:::;thenthenumberof
suchpartitions is

=N!Y
r(r!)grgr!; (10.30)
andtheinformation contentofonesuchpartition isthelogofthisquantity.
Inagreedy strategy wechoosetherstpartition tomaximize thisinformation
content.
Onegame wecanplayistomaximize thisinformation contentwithre-
specttothequantitiesgr,treated asrealnumbers,subjecttotheconstrain tP
rgrr=N.Introducing aLagrange multiplierfortheconstrain t,the
derivativeis
@
@gr 
log
+X
rgrr!
= logr! loggr+r; (10.31)

<<<PAGE 187>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
10.10: Solutions 175
which,when settozero,leads totherather niceexpression
gr=er
r!; (10.32)
theoptimalgrisproportional toaPoisson distribution! Wecansolveforthe
Lagrange multiplier byplugginggrintotheconstrain tP
rgrr=N,which
givestheimplicit equation
N=e; (10.33)
whereeisaconvenientreparameterization oftheLagrange multiplier.
Figure 10.10a showsagraph of(N);gure 10.10b showsthededuced non-
integer assignmen tsgrwhen=2:2,andnearbyintegersgr=f1;2;2;1;1g
thatmotivatesetting therstpartition to(a)(bc)(de)(fgh)(ijk)(lmno)(p qrst).
Thispartition produces arandom partition attheother end,whichhasan(a)0.511.522.533.544.555.5
1 10 100 1000
(b)00.511.522.5
12345678910
Figure 10.10.Appro ximate
solution ofthecable labelling
problem using Lagrange
multipliers. (a)Theparameter
asafunction ofN;thevalue
(20)=2:2ishighligh ted.(b)
Non-in tegervalues ofthefunction
gr=r/r!areshownbylinesand
integervaluesofgrmotivatedby
those non-in tegervaluesare
shownbycrosses.information contentoflog
=40:4bits,whichisalotmorethanhalfthetotal
information contentweneedtoacquire toinferthetransatlan ticpermutation,
log20!'61bits.[Incontrast, ifallthewires arejoined together inpairs,
theinformation contentgenerated isonlyabout29bits.]Howtochoosethe
second partition islefttothereader. AShannonesque approac hisappropriate,
pickingarandom partition attheother end,using thesamefgrg;youneed
toensure thetwopartitions areasunlikeeachother aspossible.
IfN6=2,5or9,thenthelabelling problem hassolutions thatare
particularly simple toimplemen t,called Knowlton{Graham partitions: par-
titionf1;:::;Ngintodisjoin tsetsintwowaysAandB,subjecttothe
condition thatatmost oneelemen tappearsbothinanAsetofcardinal-
ityjandinaBsetofcardinalit yk,foreachjandk(Graham, 1966;
Graham andKnowlton, 1968).

<<<PAGE 188>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
AboutChapter 11
Before reading Chapter 11,youshould havereadChapters 9and10.
Youwillalsoneedtobefamiliar withtheGaussian distribution .
One-dimensional Gaussian distribution .Ifarandom variableyisGaus-
sianandhasmeanandvariance2,whichwewrite:
yNormal (;2);orP(y)=Normal (y;;2); (11.1)
thenthedistribution ofyis:
P(yj;2)=1p
22exph
 (y )2=22i
: (11.2)
[IusethesymbolPforbothprobabilit ydensities andprobabilities.]
Theinverse-variance1/2issometimes called theprecision ofthe
Gaussian distribution.
Multi-dimensional Gaussian distribution .Ify=(y1;y2;:::;yN)hasa
multivariate Gaussian distribution, then
P(yjx;A)=1
Z(A)exp
 1
2(y x)TA(y x)
; (11.3)
where xisthemean ofthedistribution, Aistheinverseofthe
variance{co variance matrix, andthenormalizing constan tisZ(A)=
(det(A=2)) 1=2.
Thisdistribution hasthepropertythatthevarianceiiofyi,andthe
covarianceijofyiandyjaregivenby
ijE[(yi yi)(yj yj)]=A 1
ij; (11.4)
where A 1istheinverseofthematrix A.
Themarginal distribution P(yi)ofonecomponentyiisGaussian;
thejointmarginal distribution ofanysubset ofthecomponentsis
multivariate-Gaussian; andtheconditional densit yofanysubset, given
thevalues ofanother subset, forexample,P(yijyj),isalsoGaussian.
176

<<<PAGE 189>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
11
Error-Correcting Codes&RealChannels
Thenoisy-c hannel codingtheorem thatwehaveprovedshowsthatthere exist
reliable error-correcting codesforanynoisychannel. Inthischapter weaddress
twoquestions.
First, manypractical channels havereal,rather thandiscrete, inputs and
outputs. What canShannon tellusaboutthese continuouschannels? And
howshould digital signals bemappedintoanalogue waveforms, andviceversa?
Second, howarepractical error-correcting codesmade, andwhat is
achievedinpractice, relativ etothepossibilities provedbyShannon?
11.1 TheGaussian channel
Themostpopular modelofareal-input, real-output channel istheGaussian
channel.
TheGaussian channel hasarealinputxandarealoutputy.Thecondi-
tional distribution ofygivenxisaGaussian distribution:
P(yjx)=1p
22exph
 (y x)2=22i
: (11.5)
Thischannel hasacontinuousinput andoutput butisdiscrete intime.
Wewillshowbelowthatcertain continuous-time channels areequivalent
tothediscrete-time Gaussian channel.
Thischannel issometimes called theadditiv ewhite Gaussian noise
(AWGN)channel.
Aswithdiscrete channels, wewilldiscuss what rateoferror-free information
comm unication canbeachievedoverthischannel.
Motivation interms ofacontinuous-time channel
Consider aphysical (electrical, say)channel withinputs andoutputs thatare
continuousintime. Weputinx(t),andoutcomesy(t)=x(t)+n(t).
Ourtransmission hasapowercost. Theaverage powerofatransmission
oflengthTmaybeconstrained thus:
ZT
0dt[x(t)]2=TP: (11.6)
Thereceivedsignal isassumed todier fromx(t)byadditiv enoisen(t)(for
example Johnson noise), whichwewillmodelaswhite Gaussian noise. The
magnitude ofthisnoise isquantied bythenoise spectral densit yN0.
Howcould suchachannel beusedtocomm unicate information? Consider1(t)
2(t)
3(t)
x(t)
Figure 11.1.Three basisfunctions,
andaweightedcombination of
them,x(t)=PN
n=1xnn(t);with
x1=0:4,x2= 0:2,andx3=0:1.
177

<<<PAGE 190>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
178 11|Error-Correcting CodesandRealChannels
transmitting asetofNrealnumbersfxngN
n=1inasignal ofdurationTmade
upofaweightedcombination oforthonormal basisfunctionsn(t),
x(t)=NX
n=1xnn(t); (11.7)
whereRT
0dtn(t)m(t)=nm.Thereceivercanthencompute thescalars:
ynZT
0dtn(t)y(t)=xn+ZT
0dtn(t)n(t) (11.8)
xn+nn (11.9)
forn=1:::N.Ifthere werenonoise, thenynwouldequalxn.Thewhite
Gaussian noisen(t)addsscalar noisenntotheestimateyn.Thisnoise is
Gaussian:
nnNormal (0;N0=2); (11.10)
whereN0isthespectral densit yintroduced above.Thusacontinuouschannel
usedinthiswayisequivalenttotheGaussian channel dened above.The
powerconstrain tRT
0dt[x(t)]2PTdenes aconstrain tonthesignal ampli-
tudesxn,
X
nx2
nPT)x2nPT
N: (11.11)
Before returning totheGaussian channel, wedene thebandwidth (mea-
sured inHertz) ofthecontinuouschannel tobe:
W=Nmax
2T; (11.12)
whereNmaxisthemaxim umnumberoforthonormal functions thatcanbe
produced inanintervaloflengthT.Thisdenition canbemotivatedby
imagining creating aband-limited signal ofdurationTfromorthonormal co-
sineandsinecurvesofmaxim umfrequencyW.Thenumberoforthonormal
functions isNmax=2WT.Thisdenition relates totheNyquist sampling
theorem: ifthehighest frequency presen tinasignal isW,thenthesignal
canbefullydetermined fromitsvalues ataseries ofdiscrete sample points
separated bytheNyquist intervalt=1/2Wseconds.
Sotheuseofarealcontinuouschannel withbandwidth W,noise spectral
densit yN0andpowerPisequivalenttoN=T=2Wusespersecond ofa
Gaussian channel withnoise level2=N0=2andsubjecttothesignal power
constrain tx2nP/2W.
Denition ofEb=N0
Imagine thattheGaussian channelyn=xn+nnisusedwithanencoding
system totransmit binary source bitsatarateofRbitsperchannel use.How
canwecompare twoencodingsystems thathavedieren tratesofcomm uni-
cationRandthatusedieren tpowersx2n?Transmitting atalargerateRis
good;using small powerisgoodtoo.
Itisconventional tomeasure therate-comp ensated signal tonoiseratioby
theratioofthepowerpersource bitEb=x2n=Rtothenoise spectral densit y
N0: Eb=N0isdimensionless, butitisusu-
allyreported intheunits ofdecibels;
thevaluegivenis10log10Eb=N0. Eb=N0=x2n
22R: (11.13)
Eb=N0isoneofthemeasures usedtocompare codingschemes forGaussian
channels.

<<<PAGE 191>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
11.2: Inferring theinput toarealchannel 179
11.2 Inferring theinput toarealchannel
`Thebestdetectionofpulses'
In1944Shannon wrote amemorandum (Shannon, 1993) ontheproblem of
bestdieren tiating betweentwotypesofpulses ofknownshape,represen ted
byvectors x0andx1,giventhatoneofthem hasbeentransmitted overa
noisy channel. Thisisapattern recognition problem. Itisassumed thatthe
x0
x1
y
Figure 11.2.Twopulses x0and
x1,represen tedas31-dimensional
vectors, andanoisy version ofone
ofthem,y.noise isGaussian withprobabilit ydensit y
P(n)=
detA
21=2
exp
 1
2nTAn
; (11.14)
where Aistheinverseofthevariance{co variance matrix ofthenoise, asym-
metric andpositive-denite matrix. (IfAisamultiple oftheidentitymatrix,
I=2,thenthenoise is`white'. Formore general A,thenoise is`coloured'.)
Theprobabilit yofthereceivedvectorygiventhatthesource signal wass
(either zeroorone)isthen
P(yjs)=
detA
21=2
exp
 1
2(y xs)TA(y xs)
: (11.15)
Theoptimal detector isbased ontheposterior probabilit yratio:
P(s=1jy)
P(s=0jy)=P(yjs=1)
P(yjs=0)P(s=1)
P(s=0)(11.16)
=exp
 1
2(y x1)TA(y x1)+1
2(y x0)TA(y x0)+lnP(s=1)
P(s=0)
=exp(yTA(x1 x0)+); (11.17)
whereisaconstan tindependen tofthereceivedvectory,
= 1
2xT
1Ax1+1
2xT
0Ax0+lnP(s=1)
P(s=0): (11.18)
Ifthedetector isforced tomakeadecision (i.e.,guess eithers=1ors=
0)thenthedecision thatminimizes theprobabilit yoferror istoguess the
most probable hypothesis. Wecanwrite theoptimal decision interms ofa
discriminan tfunction :
a(y)yTA(x1 x0)+ (11.19)
withthedecisionsw
Figure 11.3.Theweightvector
w/x1 x0thatisusedto
discriminate betweenx0andx1.
a(y)>0!guesss=1
a(y)<0!guesss=0
a(y)=0!guess either.(11.20)
Notice thata(y)isalinear function ofthereceivedvector,
a(y)=wTy+; (11.21)
where wA(x1 x0).
11.3 Capacit yofGaussian channel
Untilnowwehaveonlymeasured thejoint,marginal, andconditional entropy
ofdiscrete variables. Inorder todene theinformation conveyedbycontinuous
variables, there aretwoissues wemustaddress {theinnite length ofthereal
line,andtheinnite precision ofrealnumbers.

<<<PAGE 192>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
180 11|Error-Correcting CodesandRealChannels
Innite inputs
Howmuchinformation canweconveyinoneuseofaGaussian channel? If
weareallowedtoputanyrealnumberxintotheGaussian channel, wecould
comm unicate anenormous string ofNdigitsd1d2d3:::dNbysettingx=
d1d2d3:::dN000:::000. Theamoun toferror-free information conveyedin
justasingle transmission could bemade arbitrarily large byincreasingN,
andthecomm unication could bemade arbitrarily reliable byincreasing the
numberofzeroesattheendofx.There isusually somepowercostassociated
withlarge inputs, however,nottomentionpractical limits inthedynamic
range acceptable toareceiver.Itistherefore conventional tointroducea
costfunctionv(x)foreveryinputx,andconstrain codestohaveanaverage
costvlessthanorequal tosome maxim umvalue. Ageneralized channel
codingtheorem, including acostfunction fortheinputs, canbeproved{see
McEliece (1977). Theresult isachannel capacit yC(v)thatisafunction of
thepermitted cost.FortheGaussian channel wewillassume acost
v(x)=x2(11.22)
suchthatthe`average power'x2oftheinput isconstrained. Wemotivatedthis
costfunction aboveinthecaseofrealelectrical channels inwhichthephysical
powerconsumption isindeed quadratic inx.Theconstrain tx2=vmakes
itimpossible tocomm unicate innite information inoneuseoftheGaussian
channel.
Innite precision(a)
(b)-g
...
Figure 11.4.(a)Aprobabilit y
densit yP(x).Question: canwe
dene the`entropy'ofthis
densit y?(b)Wecould evaluate
theentropies ofasequence of
probabilit ydistributions with
decreasing grain-sizeg,butthese
entropies tendto Z
P(x)log1
P(x)gdx,whichisnot
independen tofg:theentropy
goesupbyonebitforevery
halving ofg.Z
P(x)log1
P(x)dxisanillegal
integral.Itistempting todene joint,marginal, andconditional entropies forreal
variables simply byreplacing summations byintegrals, butthisisnotawell
dened operation. Aswediscretize anintervalintosmaller andsmaller divi-
sions, theentropyofthediscrete distribution diverges(asthelogarithm ofthe
granularity)(gure 11.4). Also, itisnotpermissible totakethelogarithm of
adimensional quantitysuchasaprobabilit ydensit yP(x)(whose dimensions
are[x] 1).
There isoneinformation measure, however,thathasawell-behavedlimit,
namely themutual information {andthisistheonethatreally matters, since
itmeasures howmuchinformation onevariable conveysaboutanother. Inthe
discrete case,
I(X;Y)=X
x;yP(x;y)logP(x;y)
P(x)P(y): (11.23)
Nowbecause theargumen tofthelogisaratiooftwoprobabilities overthe
same space, itisOKtohaveP(x;y),P(x)andP(y)beprobabilit ydensities
andreplace thesumbyanintegral:
I(X;Y)=Z
dxdyP(x;y)logP(x;y)
P(x)P(y)(11.24)
=Z
dxdyP(x)P(yjx)logP(yjx)
P(y): (11.25)
Wecannowaskthesequestions fortheGaussian channel: (a)whatprobabilit y
distribution P(x)maximizes themutual information (subjecttotheconstrain t
x2=v)?and(b)doesthemaximal mutual information stillmeasure the
maxim umerrorfreecomm unication rateofthisrealchannel, asitdidforthe
discrete channel?

<<<PAGE 193>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
11.3: Capacit yofGaussian channel 181
Exercise 11.1.[3,p.189]Provethattheprobabilit ydistribution P(x)thatmax-
imizes themutual information (subjecttotheconstrain tx2=v)isa
Gaussian distribution ofmean zeroandvariancev.
.Exercise 11.2.[2,p.189]Showthatthemutual information I(X;Y),inthecase
ofthisoptimized distribution, is
C=1
2log
1+v
2
: (11.26)
Thisisanimportantresult. Weseethatthecapacit yoftheGaussian channel
isafunction ofthesignal tonoise ratiov=2.
InferencesgivenaGaussian inputdistribution
IfP(x)=Normal(x;0;v)andP(yjx)=Normal (y;x;2)thenthemarginal
distribution ofyisP(y)=Normal(y;0;v+2)andtheposterior distribution
oftheinput, giventhattheoutput isy,is:
P(xjy)/P(yjx)P(x) (11.27)
/exp( (y x)2=22)exp( x2=2v) (11.28)
=Normal 
x;v
v+2y;1
v+1
2 1!
: (11.29)
[Thestepfrom(11.28) to(11.29) ismade bycompleting thesquare inthe
exponent.]Thisformuladeserv escareful study.Themean oftheposterior
distribution,v
v+2y,canbeviewedasaweightedcombination ofthevalue
thatbesttstheoutput,x=y,andthevaluethatbesttstheprior,x=0:
v
v+2y=1=2
1=v+1=2y+1=v
1=v+1=20: (11.30)
Theweights1=2and1=varetheprecisions ofthetwoGaussians thatwe
multiplied together inequation (11.28): thepriorandthelikelihood.
Theprecision oftheposterior distribution isthesumofthese twopre-
cisions. Thisisageneral property:whenev ertwoindependen tsources con-
tribute information, viaGaussian distributions, aboutanunkno wnvariable,
theprecisions add.[This isthedualtothebetterknownrelationship `when
independen tvariables areadded, theirvariances add'.]
Noisy-channel codingtheoremfortheGaussian channel
Wehaveevaluated amaximal mutual information. Doesitcorresp ondtoa
maxim umpossible rateoferror-free information transmission? Onewayof
provingthatthisissoistodene asequence ofdiscrete channels, allderived
fromtheGaussian channel, withincreasing numbersofinputs andoutputs,
andprovethatthemaxim ummutual information ofthesechannels tends tothe
assertedC.Thenoisy-c hannel codingtheorem fordiscrete channels applies
toeachofthese derivedchannels, thusweobtain acodingtheorem forthe
continuouschannel. Alternativ ely,wecanmakeanintuitiveargumen tforthe
codingtheorem specicfortheGaussian channel.

<<<PAGE 194>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
182 11|Error-Correcting CodesandRealChannels
Geometric alviewofthenoisy-channel codingtheorem:spherepacking
Consider asequence x=(x1;:::;xN)ofinputs, andthecorresp onding output
y,asdening twopointsinanNdimensional space. ForlargeN,thenoise
powerisverylikelytobeclose(fractionally) toN2.Theoutput yistherefore
verylikelytobeclosetothesurface ofasphere ofradiusp
N2centredonx.
Similarly ,iftheoriginal signalxisgenerated atrandom subjecttoanaverage
powerconstrain tx2=v,thenxislikelytolieclose toasphere, centred
ontheorigin, ofradiusp
Nv;andbecause thetotal average powerofyis
v+2,thereceivedsignalyislikelytolieonthesurface ofasphere ofradiusp
N(v+2),centredontheorigin.
Thevolume ofanN-dimensional sphere ofradiusris
V(r;N)=N=2
 (N=2+1)rN: (11.31)
Nowconsider making acomm unication system based onnon-confusable
inputs x,thatis,inputs whose spheres donotoverlapsignican tly.Themax-
imumnumberSofnon-confusable inputs isgivenbydividing thevolume of
thesphere ofprobable ysbythevolume ofthesphere forygivenx:
S p
N(v+2)p
N2!N
(11.32)
Thusthecapacit yisbounded by:
C=1
NlogM1
2log
1+v
2
: (11.33)
Amore detailed argumen tliketheoneusedintheprevious chapter canes-
tablish equalit y.
Backtothecontinuous channel
Recall thattheuseofarealcontinuouschannel withbandwidth W,noise
spectral densit yN0andpowerPisequivalenttoN=T=2Wusespersecond of
aGaussian channel with2=N0=2andsubjecttotheconstrain tx2nP=2W.
Substituting theresult forthecapacit yoftheGaussian channel, wendthe
capacit yofthecontinuouschannel tobe:
C=Wlog
1+P
N0W
bitspersecond. (11.34)
Thisformulagivesinsigh tintothetradeos ofpractical comm unication. Imag-
inethatwehaveaxedpowerconstrain t.What isthebestbandwidth tomake
useofthatpower?IntroducingW0=P=N0,i.e.,thebandwidth forwhichthe
signal tonoise ratiois1,gure 11.5showsC=W0=W=W0log(1+W0=W)
asafunction ofW=W0.Thecapacit yincreases toanasymptote ofW0loge.
Itisdramatically better(interms ofcapacit yforxedpower)totransmit at
alowsignal tonoise ratiooveralarge bandwidth, thanwithhighsignal to
noise inanarrowbandwidth; thisisonemotivation forwideband comm uni-
cation metho dssuchasthe`direct sequence spread-sp ectrum' approac hused
in3Gmobile phones. Ofcourse, youarenotalone, andyourelectromagnetic
neighboursmaynotbepleased ifyouusealargebandwidth, soforsocialrea-
sons,engineers often havetomakedowithhigher-p ower,narrow-bandwidth
transmitters.00.20.40.60.811.21.4
0123456capacity
bandwidth
Figure 11.5.Capacit yversus
bandwidth forarealchannel:
C=W0=W=W0log(1+W0=W)
asafunction ofW=W0.

<<<PAGE 195>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
11.4: What arethecapabilities ofpractical error-correcting codes? 183
11.4 What arethecapabilities ofpractical error-correcting codes?
Nearly allcodesaregood,butnearly allcodesrequire exponentiallook-up
tables forpractical implemen tation oftheencoderanddecoder{exponential
intheblocklengthN.Andthecodingtheorem requiredNtobelarge.
Byapractical error-correcting code,wemean onethatcanbeencoded
anddecodedinareasonable amoun toftime, forexample, atimethatscales
asapolynomial function oftheblocklengthN{preferably linearly .
TheShannon limitisnotachieve dinpractice
Thenon-constructiv eproofofthenoisy-c hannel codingtheorem showedthat
goodblockcodesexistforanynoisy channel, andindeed thatnearly allblock
codesaregood.Butwriting downanexplicit andpractical encoderandde-
coderthatareasgoodaspromised byShannon isstillanunsolv edproblem.
Verygoodcodes.Givenachannel, afamily ofblockcodesthatachieve
arbitrarily small probabilit yoferror atanycomm unication rateupto
thecapacit yofthechannel arecalled `verygood'codesforthatchannel.
Goodcodesarecodefamilies thatachievearbitrarily small probabilit yof
erroratnon-zero comm unication rates uptosome maxim umratethat
maybelessthanthecapacit yofthegivenchannel.
Badcodesarecodefamilies thatcannot achievearbitrarily small probabilit y
oferror, orthatcanonlyachievearbitrarily small probabilit yoferrorby
decreasing theinformation ratetozero.Repetition codesareanexample
ofabadcodefamily .(Badcodesarenotnecessarily useless forpractical
purposes.)
Practical codesarecodefamilies thatcanbeencodedanddecodedintime
andspace polynomial intheblocklength.
Mostestablishe dcodesarelinearcodes
Letusreview thedenition ofablockcode,andthenaddthedenition ofa
linear blockcode.
An(N;K)blockcodeforachannelQisalistofS=2Kcodewords
fx(1);x(2);:::;x(2K)g,eachoflengthN:x(s)2AN
X.Thesignal tobe
encoded,s,whichcomes fromanalphab etofsize2K,isencodedasx(s).
Alinear (N;K)blockcodeisablockcodeinwhichthecodewordsfx(s)g
makeupaK-dimensional subspace ofAN
X.Theencodingoperation can
berepresen tedbyanNKbinary matrix GTsuchthatifthesignal to
beencoded,inbinary notation, iss(avectoroflengthKbits), thenthe
encodedsignal ist=GTsmodulo2.
Thecodewordsftgcanbedened asthesetofvectors satisfying Ht=
0mod2,where Histheparity-checkmatrix ofthecode.
GT=2
666641
1
1
1
111
111
1113
77775Forexample the(7;4)Hamming codeofsection 1.2takesK=4signal
bits,s,andtransmits them followedbythree parity-checkbits. TheN=7
transmitted symbolsaregivenbyGTsmod2.
Codingtheory wasbornwiththeworkofHamming, whoinventedafam-
ilyofpractical error-correcting codes,eachabletocorrect oneerror ina
blockoflengthN,ofwhichtherepetition codeR3andthe(7;4)codeare

<<<PAGE 196>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
184 11|Error-Correcting CodesandRealChannels
thesimplest. Since thenmostestablished codeshavebeengeneralizations of
Hamming's codes:Bose{Chaudh ury{Ho cquenhem codes,Reed{M ullercodes,
Reed{Solomon codes,andGoppa codes,toname afew.
Convolutional codes
Another family oflinear codesareconvolutional codes,whichdonotdivide
thesource stream intoblocks,butinstead readandtransmit bitscontinuously .
Thetransmitted bitsarealinear function ofthepastsource bits.Usually the
ruleforgenerating thetransmitted bitsinvolvesfeeding thepresen tsource
bitintoalinear feedbac kshiftregister oflengthk,andtransmitting oneor
more linear functions ofthestate oftheshiftregister ateachiteration. The
resulting transmitted bitstream istheconvolution ofthesource stream with
alinear lter. Theimpulse responsefunction ofthisltermayhavenite or
innite duration, depending onthechoice offeedbac kshiftregister.
Wewilldiscuss convolutional codesinChapter 48.
Arelinearcodes`good'?
Onemightask,isthereason thattheShannon limitisnotachievedinpractice
because linear codesareinheren tlynotasgoodasrandom codes?Theanswer
isno,thenoisy-c hannel codingtheorem canstillbeprovedforlinear codes,
atleastforsomechannels (seeChapter 14),though theproofs,likeShannon's
proofforrandom codes,arenon-constructiv e.
Linear codesareeasytoimplemen tattheencodingend. Isdecodinga
linear codealsoeasy? Notnecessarily .Thegeneral decodingproblem (nd
themaxim umlikelihoodsintheequation GTs+n=r)isinfactNP-complete
(Berlek ampetal.,1978). [NP-complete problems arecomputational problems
thatareallequally dicult andwhicharewidely believedtorequire expo-
nentialcomputer timetosolveingeneral.] Soattentionfocuses onfamilies of
codesforwhichthere isafastdecodingalgorithm.
Concatenation
Onetrickforbuilding codeswithpractical decodersistheideaofconcatena-
tion.
Anencoder-channel-deco dersystemC!Q!DcanbeviewedasdeningC0!C!Q!D|{z}!D0
Q0asuper-channelQ0withasmaller probabilit yoferror, andwithcomplex
correlations among itserrors. Wecancreate anencoderC0anddecoderD0for
thissuper-channelQ0.Thecodeconsisting oftheouter codeC0followedby
theinner codeCisknownasaconcatenated code.
Some concatenated codesmakeuseoftheideaofinterleaving.Weread
thedatainblocks,thesizeofeachblockbeinglarger thantheblocklengths
oftheconstituen tcodesCandC0.After encodingthedataofoneblockusing
codeC0,thebitsarereordered within theblockinsuchawaythatnearby
bitsareseparated fromeachother oncetheblockisfedtothesecond code
C.Asimple example ofaninterleaverisarectangular codeorproductcode
inwhichthedataarearranged inaK2K1block,andencodedhorizon tally
using an(N1;K1)linear code,thenvertically using a(N2;K2)linear code.
.Exercise 11.3.[3]Showthateither ofthetwocodescanbeviewedastheinner
codeortheouter code.
Asanexample, gure 11.6showsaproductcodeinwhichweencode
rstwiththerepetition codeR3(alsoknownastheHamming codeH(3;1))

<<<PAGE 197>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
11.4: What arethecapabilities ofpractical error-correcting codes? 185
(a)1
0
1
1
0
0
11
0
1
1
0
0
11
0
1
1
0
0
1(b)?
??
?
?
(c)1
1
1
1
0
1
11
1
1
0
0
0
11
0
1
1
1
0
1(d)1
1
1
1
0
0
11
1
1
1
0
0
11
1
1
1
0
0
1(e)1
0
1
1
0
0
11
0
1
1
0
0
11
0
1
1
0
0
1
(d0)1
1
1
1
1
1
10
1
1
0
0
0
11
0
1
1
0
0
1(e0)1
(1)
1
1
0
0
11
(1)
1
1
0
0
11
(1)
1
1
0
0
1Figure 11.6.Aproductcode.(a)
Astring1011encodedusing a
concatenated codeconsisting of
twoHamming codes,H(3;1)and
H(7;4).(b)anoisepattern that
ips5bits.(c)Thereceiv ed
vector. (d)After decodingusing
thehorizon tal(3;1)decoder,and
(e)aftersubsequen tlyusing the
vertical (7;4)decoder.The
decodedvectormatchesthe
original.
(d0,e0)After decodingintheother
order, three errors stillremain.
horizon tallythenwithH(7;4)vertically .Theblocklength oftheconcatenated
codeis27.Thenumberofsource bitspercodewordisfour,shownbythesmall
rectangle.
Wecandecodeconveniently(though notoptimally) byusing theindividual
decodersforeachofthesubcodesinsome sequence. Itmakesmost sense to
rstdecodethecodewhichhasthelowestrateandhence thegreatest error-
correcting ability.
Figure 11.6(c{e) showswhat happensifwereceivethecodewordofg-
ure11.6a withsomeerrors (vebitsipped,asshown)andapply thedecoder
forH(3;1)rst,andthenthedecoderforH(7;4).Therstdecodercorrects
three oftheerrors, buterroneously modies thethird bitinthesecond row
where there aretwobiterrors. The(7;4)decodercanthencorrect allthree
ofthese errors.
Figure 11.6(d0{e0)showswhat happensifwedecodethetwocodesinthe
other order. Incolumns oneandtwotherearetwoerrors, sothe(7;4)decoder
introduces twoextra errors. Itcorrects theoneerrorincolumn 3.The(3;1)
decoderthencleans upfouroftheerrors, buterroneously infers thesecond
bit.
Interle aving
Themotivation forinterleavingisthatbyspreading outbitsthatarenearby
inonecode,wemakeitpossible toignore thecomplex correlations among the
errors thatareproduced bytheinner code.Maybetheinner codewillmess
upanentirecodeword;butthatcodewordisspread outonebitatatimeover
severalcodewordsoftheouter code.Sowecantreattheerrors introduced by
theinner codeasiftheyareindependen t.
Other channel models
Inaddition tothebinary symmetric channel andtheGaussian channel, coding
theorists keepmore complex channels inmind also.
Burst-error channels areimportantmodelsinpractice. Reed{Solomon
codesuseGalois elds (seeAppendix C.1)withlarge numbersofelemen ts
(e.g.216)astheirinput alphab ets,andthereb yautomatically achieveadegree
ofburst-error tolerance inthatevenif17successiv ebitsarecorrupted, only2
successiv esymbolsintheGalois eldrepresen tation arecorrupted. Concate-
nation andinterleavingcangivefurther protection against burst errors. The
concatenated Reed{Solomon codesusedondigital compact discsareableto
correct bursts oferrors oflength 4000bits.

<<<PAGE 198>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
186 11|Error-Correcting CodesandRealChannels
.Exercise 11.4.[2,p.189]Thetechnique ofinterleaving, whichallowsbursts of
errors tobetreated asindependen t,iswidely used, butistheoretically
apoorwaytoprotect dataagainst burst errors, interms oftheamoun t
ofredundancy required. Explain whyinterleavingisapoormetho d,
using thefollowingburst-error channel asanexample. Time isdivided
intochunksoflengthN=100clockcycles; during eachchunk,there
isaburst withprobabilit yb=0:2;during aburst, thechannel isabi-
narysymmetric channel withf=0:5.Ifthere isnoburst, thechannel
isanerror-free binary channel. Compute thecapacit yofthischannel
andcompare itwiththemaxim umcomm unication ratethatcould con-
ceivablybeachievedifoneusedinterleavingandtreated theerrors as
independen t.
Fading channels arerealchannels likeGaussian channels except thatthe
receivedpowerisassumed tovarywithtime. Amovingmobile phone isan
importantexample. Theincoming radio signal isreected onearbyobjects
sothatthere areinterference patterns andtheintensityofthesignal received
bythephone varieswithitslocation. Thereceivedpowercaneasily varyby
10decibels(afactor often)asthephone's antenna movesthrough adistance
similar tothewavelength oftheradio signal (afewcentimetres).
11.5 Thestateoftheart
What arethebestknowncodesforcomm unicating overGaussian channels?
Allthepractical codesarelinear codes,andareeither based onconvolutional
codesorblockcodes.
Convolutional codes,andcodesbasedonthem
Textbookconvolutional codes.The`defacto standard' error-correcting
codeforsatellite comm unications isaconvolutional codewithconstrain t
length 7.Convolutional codesarediscussed inChapter 48.
Concatenated convolutional codes.Theaboveconvolutional codecanbe
usedastheinnercodeofaconcatenated codewhose outer codeisaReed{
Solomon codewitheight-bitsymbols.Thiscodewasusedindeepspace
comm unication systems suchastheVoyagerspacecraft. Forfurther
reading aboutReed{Solomon codes,seeLinandCostello (1983).
ThecodeforGalileo .Acodeusing thesame format butusing alonger
constrain tlength {15{foritsconvolutional codeandalarger Reed{
Solomon codewasdevelopedbytheJetPropulsion Laboratory (Swan-
son,1988). Thedetails ofthiscodeareunpublished outside JPL,andthe
decodingisonlypossible using aroomfullofspecial-purp osehardw are.
In1992, thiswasthebestknowncodeofrate1/4.
Turbocodes.In1993, Berrou, Glavieux andThitima jshima reported work
onturbocodes.Theencoderofaturbocodeisbased ontheencoders
oftwoconvolutional codes.Thesource bitsarefedintoeachencoder,
theorder ofthesource bitsbeingpermutedinarandom way,andthe
resulting paritybitsfromeachconstituen tcodearetransmitted.
Thedecodingalgorithm involvesiterativ elydecodingeachconstituen t
codeusing itsstandard decodingalgorithm, thenusing theoutput ofC1
C2
--
---
Figure 11.7.Theencoderofa
turbocode.EachboxC1,C2,
containsaconvolutional code.
Thesource bitsarereordered
using apermutationbeforethey
arefedtoC2.Thetransmitted
codewordisobtained by
concatenating orinterleavingthe
outputs ofthetwoconvolutional
codes.Therandom permutation
ischosen when thecodeis
designed, andxedthereafter.
thedecoderastheinput totheother decoder.Thisdecodingalgorithm

<<<PAGE 199>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
11.6: Summary 187
isaninstance ofamessage-passing algorithm called thesum{pro duct
algorithm .
Turbocodesarediscussed inChapter 48,andmessage passing inChap-
ters16,17,25,and26.
Blockcodes
Gallager's low-densit yparity-checkcodes.ThebestblockcodesknownH=
Figure 11.8.Alow-densit y
parity-checkmatrix andthe
corresp onding graph ofarate-1/4
low-densit yparity-checkcode
withblocklengthN=16,and
M=12constrain ts.Eachwhite
circle represen tsatransmitted bit.
Eachbitparticipates inj=3
constrain ts,represen tedby
squares. Eachconstrain tforces
thesumofthek=4bitstowhich
itisconnected tobeeven.This
codeisa(16;4)code.
Outstanding performance is
obtained when theblocklength is
increased toN'10000.forGaussian channels wereinventedbyGallager in1962 butwere
promptly forgotten bymostofthecodingtheory comm unity.They were
redisco veredin1995andshowntohaveoutstanding theoretical andprac-
ticalproperties. Liketurbocodes,theyaredecodedbymessage-passing
algorithms.
Wewilldiscuss these beautifully simple codesinChapter 47.
Theperformances oftheabovecodesarecompared forGaussian channels
ingure 47.17, p.568.
11.6 Summary
Random codesaregood,buttheyrequire exponentialresources toencode
anddecodethem.
Non-random codestendforthemost partnottobeasgoodasrandom
codes. Foranon-random code,encodingmaybeeasy,butevenfor
simply-dened linear codes,thedecodingproblem remains verydicult.
Thebestpractical codes(a)emplo yverylargeblocksizes; (b)arebased
onsemi-random codeconstructions; and(c)makeuseofprobabilit y-
based decodingalgorithms.
11.7 Nonlinear codes
Most practically usedcodesarelinear, butnotall.Digital soundtrac ksare
encodedontocinema lmasabinary pattern. Thelikelyerrors aecting the
lminvolvedirtandscratc hes,whichproducelarge numbersof1sand0s
respectively.Wewantnoneofthecodewordstolooklikeall-1sorall-0s,so
thatitwillbeeasytodetect errors caused bydirtandscratc hes.Oneofthe
codesusedindigital cinema sound systems isanonlinear (8;6)codeconsisting
of64ofthe 8
4binary patterns ofweight4.
11.8 Errors other thannoise
Another source ofuncertain tyforthereceiverisuncertain tyaboutthetim-
ingofthetransmitted signalx(t).Inordinary codingtheory andinfor-
mation theory ,thetransmitter's timetandthereceiver'stimeuareas-
sumed tobeperfectly synchronized. Butifthereceiverreceivesasignal
y(u),where thereceiver'stime,u,isanimperfectly knownfunctionu(t)
ofthetransmitter's timet,thenthecapacit yofthischannel forcomm u-
nication isreduced. Thetheory ofsuchchannels isincomplete, compared
withthesynchronized channels wehavediscussed thusfar.Noteventheca-
pacity ofchannels withsynchronization errors isknown(Levenshtein,1966;
Ferreira etal.,1997); codesforreliable comm unication overchannels with
synchronization errors remain anactiveresearc harea(DaveyandMacKa y,
2001).

<<<PAGE 200>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
188 11|Error-Correcting CodesandRealChannels
Further reading
Forareview ofthehistory ofspread-sp ectrum metho ds,seeScholtz (1982).
11.9 Exercises
TheGaussian channel
.Exercise 11.5.[2,p.190]Consider aGaussian channel witharealinputx,and
signal tonoise ratiov=2.
(a)What isitscapacit yC?
(b)Iftheinput isconstrained tobebinary ,x2fpvg,what isthe
capacit yC0ofthisconstrained channel?
(c)Ifinaddition theoutput ofthechannel isthresholded using the
mapping
y!y0=(
1y>0
0y0;(11.35)
whatisthecapacit yC00oftheresulting channel?
(d)Plotthethree capacities aboveasafunction ofv=2from0.1to2.
[You'llneedtodoanumerical integral toevaluateC0.]
.Exercise 11.6.[3]ForlargeintegersKandN,whatfraction ofallbinary error-
correcting codesoflengthNandrateR=K=Narelinearcodes?[The
answerwilldependonwhether youchoosetodene thecodetobean
orderedlistof2Kcodewords,thatis,amapping froms2f1;2;:::;2Kg
tox(s),ortodene thecodetobeanunordered list,sothattwocodes
consisting ofthesamecodewordsareidentical. Usethelatter denition:
acodeisasetofcodewords;howtheencoderoperates isnotpartofthe
denition ofthecode.]
Erasurechannels
.Exercise 11.7.[5]Design acodeforthebinary erasure channel, andadecoding
algorithm, andevaluate theirprobabilit yoferror. [Thedesign ofgood
codesforerasure channels isanactiveresearc harea(Spielman, 1996;
Byersetal.,1998); seealsoChapter 50.]
.Exercise 11.8.[5]Design acodefortheq-aryerasure channel, whose inputxis
drawnfrom0;1;2;3;:::;(q 1),andwhose outputyisequal toxwith
probabilit y(1 f)andequal to?otherwise. [This erasure channel isa
goodmodelforpacketstransmitted overtheinternet, whichareeither
receivedreliably orarelost.]
Exercise 11.9.[2,p.190]Howdoredundan tarraysofindependen tdisks(RAID)
work? These areinformation storage systems consisting ofaboutten [Some peoplesayRAID stands for`re-
dundan tarrayofinexpensivedisks',
butIthink that's silly{RAID would
stillbeagoodideaevenifthedisks
wereexpensive!]diskdrives,ofwhichanytwoorthree canbedisabled andtheoth-
ersareabletostillabletoreconstruct anyrequested le.What codes
areused, andhowfararethese systems from theShannon limit for
theproblem theyaresolving? Howwouldyoudesign abetterRAID
system? Some information isprovided inthesolution section. See
http://www.acnc.com/raid 2.html;seealsoChapter 50.

<<<PAGE 201>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
11.10: Solutions 189
11.10 Solutions
Solution toexercise 11.1(p.181).IntroduceaLagrange multiplierforthe
powerconstrain tandanother,,fortheconstrain tofnormalization ofP(x).
F=I(X;Y) RdxP(x)x2 RdxP(x) (11.36)
=Z
dxP(x)Z
dyP(yjx)lnP(yjx)
P(y) x2 
:(11.37)
Makethefunctional derivativewithrespecttoP(x).
F
P(x)=Z
dyP(yjx)lnP(yjx)
P(y) x2 
 Z
dxP(x)Z
dyP(yjx)1
P(y)P(y)
P(x): (11.38)
ThenalfactorP(y)=P(x)isfound, usingP(y)=RdxP(x)P(yjx),tobe
P(yjx),andthewhole ofthelasttermcollapses inapuofsmoketo1,which
canbeabsorb edintotheterm.
SubstituteP(yjx)=exp( (y x)2=22)=p
22andsetthederivativeto
zero: Z
dyP(yjx)lnP(yjx)
P(y) x2 0=0 (11.39)
)Z
dyexp( (y x)2=22)p
22ln[P(y)]= x2 0 1
2: (11.40)
Thiscondition mustbesatised byln[P(y)]forallx.
Writing aTaylorexpansion ofln[P(y)]=a+by+cy2+,onlyaquadratic
function ln[P(y)]=a+cy2wouldsatisfy theconstrain t(11.40). (Anyhigher
order termsyp,p>2,wouldproduceterms inxpthatarenotpresen ton
theright-hand side.) ThereforeP(y)isGaussian. Wecanobtain thisoptimal
output distribution byusing aGaussian input distribution P(x).
Solution toexercise 11.2(p.181).GivenaGaussian input distribution ofvari-
ancev,theoutput distribution isNormal (0;v+2),sincexandthenoise
areindependen trandom variables, andvariances addforindependen trandom
variables. Themutual information is:
I(X;Y)=Z
dxdyP(x)P(yjx)logP(yjx) Z
dyP(y)logP(y)(11.41)
=1
2log1
2 1
2log1
v+2(11.42)
=1
2log
1+v
2
: (11.43)
Solution toexercise 11.4(p.186).Thecapacit yofthechannel isoneminus
theinformation contentofthenoisethatitadds. Thatinformation contentis,
perchunk,theentropyoftheselection ofwhether thechunkisbursty,H2(b),
plus, withprobabilit yb,theentropyoftheippedbits,N,whichaddsup
toH2(b)+Nbperchunk(roughly; accurate ifNislarge). So,perbit,the
capacit yis,forN=100,
C=1 1
NH2(b)+b
=1 0:207=0:793: (11.44)
Incontrast, interleaving, whichtreats bursts oferrors asindependen t,causes
thechannel tobetreated asabinary symmetric channel withf=0:20:5=
0:1,whose capacit yisabout0.53.

<<<PAGE 202>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
190 11|Error-Correcting CodesandRealChannels
Interleavingthrowsawaytheuseful information aboutthecorrelated-
nessoftheerrors. Theoretically ,weshould beabletocomm unicate about
(0:79=0:53)'1:6times faster using acodeanddecoderthatexplicitly treat
bursts asbursts.
Solution toexercise 11.5(p.188).
(a)Putting together theresults ofexercises 11.1and11.2,wededuce that
aGaussian channel withrealinputx,andsignal tonoiseratiov=2has
capacit y
C=1
2log
1+v
2
: (11.45)
(b)Iftheinput isconstrained tobebinary ,x2fpvg,thecapacit yis
achievedbyusing these twoinputs withequal probabilit y.Thecapacit y
isreduced toasomewhat messy integral,
C00=Z1
 1dyN(y;0)logN(y;0) Z1
 1dyP(y)logP(y);(11.46)
whereN(y;x)(1=p
2)exp[(y x)2=2],xpv=,andP(y)
[N(y;x)+N(y; x)]=2.Thiscapacit yissmaller thantheunconstrained
capacit y(11.45), butforsmall signal tonoise ratio, thetwocapacities
arecloseinvalue.
(c)Iftheoutput isthresholded, thentheGaussian channel isturned into
abinary symmetric channel whose transition probabilit yisgivenbythe
errorfunction dened onpage156.Thecapacit yis00.20.40.60.811.2
00.511.522.5
0.010.11
0.1 1
Figure 11.9.Capacities (from top
tobottom ineachgraph)C,C0,
andC00,versusthesignal tonoise
ratio(pv=).Thelowergraph is
alog{log plot.
C00=1 H2(f);wheref=(pv=): (11.47)
Solution toexercise 11.9(p.188).There areseveralRAID systems. Oneof
theeasiest tounderstand consists of7diskdriveswhichstore dataatrate
4=7using a(7;4)Hamming code:eachsuccessiv efourbitsareencodedwith
thecodeandthesevencodewordbitsarewritten onetoeachdisk. Twoor
perhaps three diskdrivescangodownandtheothers canrecoverthedata.
Theeectiv echannel modelhereisabinary erasure channel, because itis
assumed thatwecantellwhen adiskisdead.
Itisnotpossible torecoverthedataforsome choices ofthethree dead
diskdrives;canyouseewhy?
Exercise 11.10. Giveanexample ofthreediskdrivesthat,iflost,leadtofailure
oftheaboveRAID system, andthree thatcanbelostwithout failure.
Solution toexercise 11.10 (p.190).The(7;4)Hamming codehascodewords
ofweight3.Ifanysetofthree diskdrivescorresp onding tooneofthose code-
wordsislost,thentheother fourdiskscanonlyrecover3bitsofinformation
aboutthefoursource bits;afourth bitislost.[c.f.exercise 13.13 (p.220)with
q=2:there arenobinary MDS codes.Thisdecit isdiscussed further in
section 13.11]
Anyother setofthree diskdrivescanbelostwithout problems because
thecorresp onding fourbyfoursubmatrix ofthegenerator matrix isinvertible.
Abettercodewouldbethedigital fountain{seeChapter 50.

<<<PAGE 203>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
PartIII
Further Topics inInformation Theory

<<<PAGE 204>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
AboutChapter 12
InChapters 1{11, weconcen trated ontwoaspectsofinformation theory and
codingtheory: source coding{thecompression ofinformation soastomake
ecien tuseofdatatransmission andstorage channels; andchannel coding{
theredundan tencodingofinformation soastobeabletodetect andcorrect
comm unication errors.
Inboththeseareaswestarted byignoring practical considerations, concen-
trating onthequestion ofthetheoretical limitations andpossibilities ofcoding.
Wethendiscussed practical source-co dingandchannel-co dingschemes, shift-
ingtheemphasis towardscomputational feasibilit y.Buttheprime criterion
forcomparing encodingschemes remained theeciency ofthecodeinterms
ofthechannel resources itrequired: thebestsource codeswerethose that
achievedthegreatest compression; thebestchannel codeswerethose that
comm unicated atthehighest ratewithagivenprobabilit yoferror.
Inthischapter wenowshiftourviewp ointalittle, thinking ofeaseof
information retrieval asaprimary goal. Itturns outthattherandom codes
whichweretheoretically useful inourstudy ofchannel codingarealsouseful
forrapid information retriev al.
Ecien tinformation retriev alisoneoftheproblems thatbrains seem to
solveeortlessly ,andcontent-addressable memory isoneofthetopics wewill
study when welookatneural networks.
192

<<<PAGE 205>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
12
Hash Codes:CodesforEcien t
Information Retriev al
12.1 Theinformation retriev alproblem
Asimple example ofaninformation retriev alproblem isthetaskofimple-
mentingaphone directory service, which,inresponsetoaperson's name,
returns (a)aconrmation thatthatperson islisted inthedirectory; and(b)
theperson's phone numberandother details. Wecould formalize thisprob-
lemasfollows,withSbeingthenumberofnames thatmustbestored inthe
directory.string length N'200
numberofstrings S'223
numberofpossible 2N'2200
strings
Figure 12.1.Castofcharacters.
YouaregivenalistofSbinary strings oflengthNbits,fx(1);:::;x(S)g,
whereSisconsiderably smaller thanthetotalnumberofpossible strings, 2N.
Wewillcallthesuperscript `s'inx(s)therecord numberofthestring. The
ideaisthatsrunsovercustomers intheorder inwhichtheyareadded tothe
directory andx(s)isthename ofcustomers.Weassume forsimplicit ythat
allpeople havenames ofthesame length. Thename length mightbe,say,
N=200bits,andwemightwanttostorethedetails oftenmillion customers,
soS'107'223.Wewillignore thepossibilit ythattwocustomers have
identicalnames.
Thetaskistoconstruct theinverseofthemapping fromstox(s),i.e.,to
makeasystem that,givenastringx,returns thevalueofssuchthatx=x(s)
ifoneexists, andotherwise reportsthatnosuchsexists. (Once wehavethe
record number,wecangoandlookinmemory locationsinaseparate memory
fullofphone numberstondtherequired number.)Theaim,when solving
thistask,istouseminimal computational resources interms oftheamoun t
ofmemory usedtostoretheinversemapping fromxtosandtheamoun tof
timetocompute theinversemapping. And, preferably ,theinversemapping
should beimplemen tedinsuchawaythatfurther newstrings canbeadded
tothedirectory inasmall amoun tofcomputer timetoo.
Some standar dsolutions
Thesimplest anddumbestsolutions totheinformation retriev alproblem are
alook-up table andarawlist.
Thelook-up table isapiece ofmemory ofsize2Nlog2S,log2Sbeingthe
amoun tofmemory required tostore aninteger between1andS.In
eachofthe2Nlocations, weputazero,except forthelocations xthat
corresp ondtostrings x(s),intowhichwewrite thevalueofs.
Thelook-up tableisasimple andquicksolution, ifonlythereissucien t
memory forthetable, andifthecostoflooking upentriesinmemory is
193

<<<PAGE 206>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
194 12|Hash Codes:CodesforEcien tInformation Retriev al
independen tofthememory size.Butinourdenition ofthetask, we
assumed thatNisabout200bitsormore, sotheamoun tofmemory
required wouldbeofsize2200;thissolution iscompletely outofthe
question. Bearinmind thatthenumberofparticles inthesolarsystem
isonlyabout2190.
Therawlistisasimple listofordered pairs (s;x(s))ordered bythevalue
ofs.Themapping fromxtosisachievedbysearchingthrough thelist
ofstrings, starting fromthetop,andcomparing theincoming string x
witheachrecord x(s)untilamatchisfound. Thissystem isveryeasy
tomaintain,andusesasmall amoun tofmemory ,aboutSNbits,but
israther slowtouse,sinceonaverage vemillion pairwise comparisons
willbemade.
.Exercise 12.1.[2,p.202]Showthattheaverage timetakentondtherequired
string inarawlist,assuming thattheoriginal names werechosen at
random, isaboutS+Nbinary comparisons. (Note thatyoudon't
havetocompare thewhole string oflengthN,sinceacomparison can
beterminated assoonasamismatc hoccurs; showthatyouneedon
average twobinary comparisons perincorrect string match.)Compare
thiswiththeworst-case searchtime{assuming thatthedevilchooses
thesetofstrings andthesearchkey.
Thestandard wayinwhichphone directories aremade impro vesonthelook-up
table andtherawlistbyusing analphab etically ordered list.
Alphab etical list.Thestringsfx(s)garesorted intoalphab etical order.
Searchingforanentrynowusually takeslesstimethanwasneeded
fortherawlistbecause wecantakeadvantageofthesortedness; for
example, wecanopenthephoneb ookatitsmiddle page, andcompare
thename wendthere withthetarget string; ifthetarget is`greater'
thanthemiddle string thenweknowthattherequired string, ifitexists,
willbefound inthesecond halfofthealphab etical directory .Otherwise,
welookinthersthalf.Byiterating thissplitting-in-the-middle proce-
dure, wecanidentifythetarget string, orestablish thatthestring isnot
listed, indlog2Sestring comparisons. Theexpected numberofbinary
comparisons perstring comparison willtendtoincrease asthesearch
progresses, butthetotalnumberofbinary comparisons required willbe
nogreater thandlog2SeN.
Theamoun tofmemory required isthesameasthatrequired fortheraw
list.
Adding newstrings tothedatabase requires thatweinsert them inthe
correct location inthelist.Tondthatlocation takesaboutdlog2Se
binary comparisons.
Canweimpro veonthewell-established alphab etized list?Letusconsider
ourtaskfromsome newviewp oints.
Thetaskistoconstruct amapping x!sfromNbitstolog2Sbits.This
isapseudo-in vertible mapping, sinceforanyxthatmaps toanon-zeros,the
customer database containsthepair(s;x(s))thattakesusback.Where have
wecome across theideaofmapping fromNbitstoMbitsbefore?
Weencoun tered thisideatwice: rst,insource coding, westudied block
codeswhichweremappings fromstrings ofNsymbolstoaselection ofone
labelinalist.Thetaskofinformation retriev alissimilar tothetask(which

<<<PAGE 207>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
12.2: Hash codes 195
weneveractually solved)ofmaking anencoderforatypical-set compression
code.
Thesecond timethatwemappedbitstrings tobitstrings ofanother
dimensionalit ywaswhen westudied channel codes. There, weconsidered
codesthatmappedfromKbitstoNbits,withNgreater thanK,andwe
made theoretical progress usingrandom codes.
Inhashcodes,weputtogether these twonotions. Wewillstudy random
codesthatmapfromNbitstoMbitswhereMissmallerthanN.
Theideaisthatwewillmaptheoriginal high-dimensional space downinto
alower-dimensional space, oneinwhichitisfeasible toimplemen tthedumb
look-up table metho dwhichwerejected amomen tago.string length N'200
numberofstrings S'223
sizeofhashfunctionM'30bits
sizeofhashtableT=2M
'230
Figure 12.2.Revised castof
characters.
12.2 Hash codes
Firstwewilldescrib ehowahashcodeworks,thenwewillstudy theproperties
ofidealized hashcodes.Ahashcodeimplemen tsasolution totheinformation
retriev alproblem, thatis,amapping fromxtos,withthehelpofapseudo-
random function called ahashfunction ,whichmaps theN-bitstringxtoan
M-bitstringh(x),whereMissmaller thanN.Mistypically chosen suchthat
the`table size'T'2Misalittlebigger thanS{say,tentimes bigger. For
example, ifwewereexpectingStobeaboutamillion, wemightmapxinto
a30-bit hashh(regardless ofthesizeNofeachitemx).Thehashfunction
issome xeddeterministic function whichshould ideally beindistinguishable
fromaxedrandom code.Forpractical purposes,thehashfunction mustbe
quicktocompute.
Twosimple examples ofhashfunctions are:
Division metho d.Thetable sizeTisaprime number,preferably onethat
isnotclosetoapowerof2.Thehashvalueistheremainder when the
integerxisdivided byT.
Variable string addition metho d.Thismetho dassumes thatxisastring
ofbytesandthatthetablesizeTis256.Thecharacters ofxareadded,
modulo256.Thishashfunction hasthedefect thatitmaps strings that
areanagrams ofeachother ontothesame hash.
Itmaybeimpro vedbyputting therunning totalthrough axedpseu-
dorandom permutation aftereachcharacter isadded. Inthevariable
string exclusiv e-ormetho dwithtablesize65536,thestring ishashed
twiceinthisway,withtheinitial running total beingsetto0and1
respectively(algorithm 12.3). Theresult isa16-bit hash.
Havingpickedahashfunction h(x),weimplemen taninformation retriev er
asfollows.
Encoding.Apiece ofmemory called thehashtable iscreated ofsize2Mb
memory units, wherebistheamoun tofmemory needed torepresen tan
integer between0andS.Thistable isinitially settozerothroughout.
Eachmemory x(s)isputthrough thehashfunction, andatthelocation
inthehashtable corresp onding totheresulting vectorh(s)=h(x(s)),
theintegersiswritten {unless thatentryinthehashtable isalready
occupied, inwhichcasewehaveacollision betweenx(s)andsomeearlier
x(s0)whichbothhappentohavethesame hashcode.Collisions canbe
handled invarious ways{wewilldiscuss some inamomen t{butrst
letuscomplete thebasic picture.

<<<PAGE 208>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
196 12|Hash Codes:CodesforEcien tInformation Retriev al
Algorithm 12.3.Ccode
implemen tingthevariable string
exclusiv e-ormetho dtocreate a
hashhintherange 0:::65535
fromastringx.Author: Thomas
Niemann.unsigned charRand8[256]; //Thisarraycontains arandom
permutation from0..255to0..255
intHash(char *x){ //xisapointer tothefirstchar;
inth; //*xisthefirstcharacter
unsigned charh1,h2;
if(*x==0)return0;//Special handling ofemptystring
h1=*x;h2=*x+1;//Initialize twohashes
x++; //Proceed tothenextcharacter
while(*x){
h1=Rand8[h1 ^*x];//Exclusive-or withthetwohashes
h2=Rand8[h2 ^*x];//andputthrough therandomizer
x++;
} //Endofstringisreached when*x=0
h=((int)(h1)<<8) |//Shifth1left8bitsandaddh2
(int)h2;
returnh; //Hashisconcatenation ofh1andh2
}
x(1)
h(x(1))! 1@
@
@Rx(2)h(x(2))! 2

x(3)
h(x(3))! 3@
@
@R
x(s)
h(x(s))! sA
A
A
A
AAUHash
function- StringshashesHash table
...
... -N-M
?6
2M6
?SFigure 12.4.Useofhashfunctions
forinformation retriev al.Foreach
stringx(s),thehashh=h(x(s))
iscomputed, andthevalueofsis
written intothehthrowofthe
hashtable. Blank rowsinthe
hashtablecontainthevaluezero.
Thetable sizeisT=2M.

<<<PAGE 209>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
12.3: Collision resolution 197
Decoding.Toretriev eapieceofinformation corresp onding toatarget vector
x,wecompute thehashhofxandlookatthecorresp onding location
inthehashtable. Ifthere isazero,thenweknowimmediately thatthe
stringxisnotinthedatabase. Thecostofthisansweristhecostofone
hashfunction evaluation andonelook-up inthetable ofsize2M.If,on
theother hand, there isanon-zero entrysinthetable, there aretwo
possibilities: either thevectorxisindeed equal tox(s);orthevectorx(s)
isanother vectorthathappenstohavethesamehashcodeasthetarget
x.(Athirdpossibilit yisthatthisnon-zero entrymighthavesomething
todowithouryet-to-b e-discussed collision-resolution system.)
Tocheckwhether xisindeed equal tox(s),wetakethetentativeanswer
s,lookupx(s)intheoriginal forwarddatabase, andcompare itbitby
bitwithx;ifitmatchesthenwereportsasthedesired answer.This
successful retriev alhasanoverallcostofonehash-function evaluation,
onelook-up inthetable ofsize2M,another look-up inatable ofsize
S,andNbinary comparisons {whichmaybemuchcheaperthanthe
simple solutions presen tedinsection 12.1.
.Exercise 12.2.[2,p.202]Ifwehavecheckedtherstfewbitsofx(s)withxand
found them tobeequal, what istheprobabilit ythatthecorrect entry
hasbeenretriev ed,ifthealternativ ehypothesis isthatxisactually not
inthedatabase? Assume thattheoriginal source strings arerandom,
andthehashfunction isarandom hashfunction. Howmanybinary
evaluations areneeded tobesurewithoddsofabillion toonethatthe
correct entryhasbeenretriev ed?
Thehashing metho dofinformation retriev alcanbeusedforstrings xof
arbitrary length, ifthehashfunction h(x)canbeapplied tostrings ofany
length.
12.3 Collision resolution
Wewillstudy twowaysofresolving collisions: appending inthetable, and
storing elsewhere.
Appending intable
When encoding, ifacollision occurs, wecontinuedownthehashtable and
write thevalueofsintothenextavailable location inmemory thatcurren tly
contains azero. Ifwereachthebottom ofthetable before encoun tering a
zero,wecontinuefromthetop.
When decoding, ifwecompute thehashcodeforxandndthatthes
contained inthetable doesn'tpointtoanx(s)thatmatchesthecuex,we
continuedownthehashtableuntilweeither ndanswhose x(s)doesmatch
thecuex,inwhichcasewearedone, orelseencoun terazero,inwhichcase
weknowthatthecuexisnotinthedatabase.
Forthismetho d,itisessentialthatthetablebesubstan tially bigger insize
thanS.If2M<Sthentheencodingrulewillbecome stuckwithnowhere to
putthelaststrings.
Storing elsewher e
Amore robust andexible metho distousepointerstoadditional pieces of
memory inwhichcollided strings arestored. There aremanywaysofdoing

<<<PAGE 210>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
198 12|Hash Codes:CodesforEcien tInformation Retriev al
this.Asanexample, wecould storeinlocation hinthehashtable apointer
(whichmustbedistinguishable fromavalidrecord numbers)toa`bucket'
where allthestrings thathavehashcodeharestored inasorted list.The
encodersortsthestrings ineachbucketalphab etically asthehashtable and
bucketsarecreated.
Thedecodersimply hastogoandlookintherelevantbucketandthen
checktheshort listofstrings thatarethere byabriefalphab etical search.
Thismetho dofstoring thestrings inbucketsallowstheoption ofmaking
thehashtablequitesmall, whichmayhavepractical benets. Wemaymakeit
sosmall thatalmost allstrings areinvolvedincollisions, soallbucketscontain
asmall numberofstrings. Itonlytakesasmall numberofbinary comparisons
toidentifywhichofthestrings inthebucketmatchesthecuex.
12.4 Planning forcollisions: abirthda yproblem
Exercise 12.3.[2,p.202]IfwewishtostoreSentriesusing ahashfunction whose
output hasMbits,howmanycollisions should weexpecttohappen,
assuming thatourhashfunction isanideal random function? What
sizeMofhashtable isneeded ifwewouldliketheexpected numberof
collisions tobesmaller than1?
What sizeMofhashtableisneeded ifwewouldliketheexpected number
ofcollisions tobeasmall fraction, say1%,ofS?
[Notice thesimilarit yofthisproblem toexercise 9.20(p.156).]
12.5 Other rolesforhashcodes
Checkingarithmetic
Ifyouwishtocheckanaddition thatwasdonebyhand, youmaynduseful
themetho dofcasting outnines.Incasting outnines, onends thesum,
modulonine, ofallthedigits ofthenumberstobesummed andcompares
itwiththesum,modulonine, ofthedigits oftheputativ eanswer.[With a
littlepractice, these sums canbecomputed muchmore rapidly thanthefull
original addition.]
Example 12.4. Inthecalculation showninthemargin thesum,modulonine,of 189
+1254
+238
1681thedigits in189+1254+238 is7,andthesum,modulonine,of1+6+8+1
is7.Thecalculation thuspasses thecasting-out-nines test.
Casting outnines givesasimple example ofahashfunction. Forany
addition expression oftheforma+b+c+,wherea;b;c;:::aredecimal
numberswedeneh2f0;1;2;3;4;5;6;7;8gby
h(a+b+c+)=summodulonineofalldigits ina;b;c; (12.1)
thenitisnicepropertyofdecimal arithmetic thatif
a+b+c+=m+n+o+ (12.2)
thenthehashesh(a+b+c+)andh(m+n+o+)areequal.
.Exercise 12.5.[1,p.203]What evidence doesacorrect casting-out-nines match
giveinfavourofthehypothesis thattheaddition hasbeendonecor-
rectly?

<<<PAGE 211>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
12.5: Other rolesforhashcodes 199
Errordetectionamong friends
Aretwolesthesame? Ifthelesareonthesame computer, wecould just
compare them bitbybit.Butifthetwolesareonseparate machines, it
wouldbenicetohaveawayofconrming thattwolesareidenticalwithout
havingtotransfer oneofthelesfromAtoB.[Andevenifwedidtransfer one
oftheles,wewouldstilllikeawaytoconrm whether ithasbeenreceived
without modications!]
Thisproblem canbesolvedusing hashcodes.LetAlice andBobbethe
holders ofthetwoles;Alice senttheletoBob,andtheywishtoconrm
ithasbeenreceivedwithout error. IfAlice computes thehashofherleand
sends ittoBob,andBobcomputes thehashofhisle,using thesameM-bit
hashfunction, andthetwohashes match,thenBobcandeduce thatthetwo
lesarealmost surely thesame.
Example 12.6. What istheprobabilit yofafalsenegativ e,i.e.,theprobabilit y,
giventhatthetwolesdodier, thatthetwohashes arenevertheless
identical?
Ifweassume thatthehashfunction israndom andthattheprocessthatcauses
thelestodier knowsnothing aboutthehashfunction, thentheprobabilit y
ofafalsenegativ eis2 M. 2
A32-bit hashgivesaprobabilit yoffalsenegativ eofabout10 10.Itis
common practice tousealinear hashfunction called a32-bit cyclic redundancy
checktodetect errors inles.(Acyclic redundancy checkisasetof32parity-
checkbitssimilar tothe3parity-checkbitsofthe(7;4)Hamming code.)
Tohaveafalse-negativ eratesmaller thanoneinabillion,M=32
bitsisplenty,iftheerrors areproduced bynoise.
.Exercise 12.7.[2,p.203]Suchasimple parity-checkcodeonlydetects errors; it
doesn'thelpcorrect them. Since error-correcting codesexist, whynot
useoneofthem togetsome error-correcting capabilit ytoo?
Tamperdetection
What ifthedierences betweenthetwolesarenotsimply `noise', butare
introduced byanadversary ,acleverforger called Fiona, whomodies the
original letomakeaforgery thatpurportstobeAlice's le?HowcanAlice
makeadigital signature forthelesothatBobcanconrm thatno-one has
tamperedwiththele?AndhowcanwepreventFiona fromlistening inon
Alice's signature andattachingittoother les?
Let'sassume thatAlice computes ahashfunction fortheleandsends it
securely toBob.IfAlice computes asimple hashfunction forthelelikethe
linear cyclic redundancy check,andFiona knowsthatthisisthemetho dof
verifying thele'sintegrity,Fiona canmakeherchosen modications tothe
leandtheneasily identify(bylinear algebra) afurther 32-or-so single bits
that, when ipped,restore thehashfunction oftheletoitsoriginal value.
Linearhashfunctions givenosecurity against forgers.
Wemusttherefore require thatthehashfunction behardtoinvert sothat
no-one canconstruct atampering thatleavesthehashfunction unaected.
Wewouldstilllikethehashfunction tobeeasytocompute, however,sothat
Bobdoesn'thavetodohours ofworktoverifyeverylehereceived.Such
ahashfunction {easytocompute, buthardtoinvert{iscalled aone-w ay

<<<PAGE 212>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
200 12|Hash Codes:CodesforEcien tInformation Retriev al
hashfunction .Finding suchfunctions isoneoftheactiveresearc hareas of
cryptograph y.
Ahash-function thatiswidely usedinthefreesoftwarecomm unityto
conrm thattwolesdonotdier isMD5,whichproduces a128bithash. The
details ofhowitworksarequite complicated, involving convoluted exclusiv e-
or-ing andif-ing andand-ing.1
Evenwithagoodone-w ayhashfunction, thedigital signatures describ ed
abovearestillvulnerable toattack,ifFiona hasaccess tothehashfunction.
Fiona could takethetamperedleandhuntforafurther tinymodication to
itsuchthatitshashmatchestheoriginal hashofAlice's le.Thiswouldtake
sometime{onaverage, about232attempts, ifthehashfunction has32bits{
buteventually Fiona wouldndatamperedlethatmatchesthegivenhash.
Tobesecure against forgery ,digital signatures musteither haveenough bits
forsucharandom searchtotaketoolong,orthehashfunction itselfmustbe
keptsecret.
Fiona hastohash2Mlestocheat. 232lemodications isnot
verymany,soa32-bit hashfunction isnotlargeenough forforgery
prevention.
Another person whomighthaveamotivation forforgery isAlice herself.
Forexample, shemightbemaking abetontheoutcome ofarace,without
wishing tobroadcast herprediction publicly; ametho dforplacing betswould
beforhertosendtoBobthebookiethehashofherbet.Later on,shecould
sendBobthedetails ofherbet.Everyonecanconrm thatherbetisconsis-
tentwiththepreviously publicized hash. [This metho dofsecret publication
wasusedbyIsaac Newton andRobertHookewhen theywished toestablish
priorit yforscienticideas without revealing them. Hooke'shashfunction
wasalphab etization asillustrated bytheconversion ofUTTENSIO, SICVIS
intotheanagram CEIIINOSSSTTUV .]Suchaprotocolrelies ontheassumption
thatAlice cannot change herbetaftertheeventwithout thehashcoming
outwrong. Howbigahashfunction doweneedtousetoensure thatAlice
cannot cheat? Theanswerisdieren tfromthesizeofthehashweneeded in
order todefeat Fiona above,because Alice istheauthor ofbothles. Alice
could cheatbysearchingfortwolesthathaveidenticalhashes toeachother.
Forexample, ifshe'd liketocheatbyplacing twobetsfortheprice ofone,
shecould makealargenumberN1ofversions ofbetone(diering fromeach
other inminor details only), andalargenumberN2ofversions ofbettwo,and
hashthem all.Ifthere's acollision betweenthehashes oftwobetsofdieren t
types,thenshecansubmit thecommon hashandthusbuyherself theoption
ofplacing either bet.
Example 12.8. IfthehashhasMbits,howbigdoN1andN2needtobefor
Alice tohaveagoodchance ofnding twodieren tbetswiththesame
hash?
Thisisabirthda yproblem likeexercise 9.20(p.156).IfthereareN1Montagues
andN2Capulets ataparty,andeachisassigned a`birthda y'ofMbits,the
expected numberofcollisions betweenaMontague andaCapulet is
N1N22 M; (12.3)
1http://www.freesoft.org/CIE/RF C/1321/3.htm

<<<PAGE 213>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
12.6: Further exercises 201
sotominimize thenumberofleshashed,N1+N2,Alice should makeN1
andN2equal, andwillneedtohashabout2M=2lesuntilshendstwothat
match. 2
Alice hastohash2M=2lestocheat. [This isthesquare rootofthe
numberofhashes Fiona hadtomake.]
IfAlice hastheuseofC=106computers forT=10years,eachcomputer
takingt=1nstoevaluate ahash, thebet-comm unication system issecure
against Alice's dishonest yonlyifM2log2CT=t'160bits.
Further reading
Ihighly recommend thestory ofDoug McIlro y'sspell program, astoldin
section 13.8ofProgramming Pearls(Bentley,2000). Thisastonishing pieceof
softwaremakesuseofa64-kilob ytedatastructure tostorethespellings ofall
thewordsof75000-w orddictionary .
12.6 Further exercises
Exercise 12.9.[1]What istheshortest theaddress onatypical international
letter could be,ifitistogettoaunique human recipien t?(Assume
thepermitted characters are[A-Z,0-9] .)Howlongaretypical email
addresses?
Exercise 12.10.[2,p.203]Howlongdoesapieceoftextneedtobeforyoutobe
prettysurethatnohuman haswritten thatstring ofcharacters before?
Howmanynotes arethere inanewmelodythathasnotbeencomposed
before?
.Exercise 12.11.[2,p.204]Pattern recognition bymolecules .
Some proteins produced inacellhavearegulatory role. Aregulatory
protein controlsthetranscription ofspecic genes inthegenome. This
controlofteninvolvestheprotein's binding toaparticular DNA sequence
inthevicinit yoftheregulated gene. Thepresence oftheboundprotein
either promotes orinhibits transcription ofthegene.
(a)Useinformation-theoretic argumen tstoobtain alowerbound on
thesizeofatypical protein thatactsasaregulator specictoone
geneinthewhole human genome. Assume thatthegenome isa
sequence of3109nucleotides drawnfromafourletter alphab et
fA;C;G;Tg;aprotein isasequence ofamino acids drawnfroma
twentyletter alphab et.[Hint:establish howlongtherecognized
DNA sequence hastobeinorder forthatsequence tobeunique
tothevicinit yofonegene, treating therestofthegenome asa
random sequence. Then discuss howbigtheprotein mustbeto
recognize asequence ofthatlength uniquely .]
(b)Some ofthesequences recognized byDNA-binding regulatory pro-
teinsconsist ofasubsequence thatisrepeated twiceormore, for
example thesequence
GCCCCCCACCCCTGCCCCC (12.4)

<<<PAGE 214>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
202 12|Hash Codes:CodesforEcien tInformation Retriev al
isabinding sitefound upstream ofthealpha-actin geneinhumans.
Doesthefactthatsome binding sitesconsist ofarepeated subse-
quence inuence youranswertopart(a)?
12.7 Solutions
Solution toexercise 12.1(p.194).Firstimagine comparing thestringxwith
another random string x(s).Theprobabilit ythattherstbitsofthetwo
strings matchis1=2.Theprobabilit ythatthesecond bitsmatchis1=2.As-
suming westopcomparing oncewehittherstmismatc h,theexpected number
ofmatchesis1,sotheexpected numberofcomparisons is2(exercise 2.34,
p.38).
Assuming thecorrect string islocated atrandom intherawlist,wewill
havetocompare withanaverage ofS=2strings beforewendit,whichcosts
2S=2binary comparisons; andcomparing thecorrect strings takesNbinary
comparisons, giving atotalexpectation ofS+Nbinary comparisons, ifthe
strings arechosen atrandom.
Intheworstcase(whichmayindeed happeninpractice), theother strings
areverysimilar tothesearchkey,sothatalength ysequence ofcomparisons
isneeded tondeachmismatc h.Theworstcaseiswhen thecorrect string
islastinthelist,andalltheother strings dier inthelastbitonly,giving a
requiremen tofSNbinary comparisons.
Solution toexercise 12.2(p.197).Thelikelihoodratioforthetwohypotheses,
H0:x(s)=x,andH1:x(s)6=x,contributed bythedatum `therstbitsof
x(s)andxareequal' is
P(DatumjH0)
P(DatumjH1)=1
1=2=2: (12.5)
Iftherstrbitsallmatch,thelikelihoodratiois2rtoone.Onnding that
30bitsmatch,theoddsareabillion tooneinfavourofH0,assuming westart
fromevenodds. [Foracomplete answer,weshould compute theevidence
givenbytheprior information thatthehashentryshasbeenfound inthe
table ath(x).Thisfactgivesfurther evidence infavourofH0.]
Solution toexercise 12.3(p.198).Letthehashfunction haveanoutput al-
phabetofsizeT=2M.IfMwereequal tolog2Sthenwewouldhaveexactly
enough bitsforeachentrytohaveitsownunique hash. Theprobabilit ythat
oneparticular pairofentriescollide under arandom hashfunction is1=T.The
numberofpairsisS(S 1)=2.Sotheexpected numberofcollisions between
pairsisexactly
S(S 1)=(2T): (12.6)
Ifwewouldlikethistobesmaller than1,thenweneedT>S(S 1)=2so
M>2log2S: (12.7)
Weneedtwiceasmany bitsasthenumberofbits,log2S,thatwouldbe
sucien ttogiveeachentryaunique name.
Ifwearehappytohaveoccasional collisions, involving afractionfofthe
namesS,thenweneedT>S=f(since theprobabilit ythatoneparticular
name iscollided-with isf'S=T)so
M>log2S+log2[1=f]; (12.8)

<<<PAGE 215>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
12.7: Solutions 203
whichmeans forf'0:01thatweneedanextra 7bitsabovelog2S.
Theimportantpointtonoteisthescaling ofTwithSinthetwocases
(12.7, 12.8). Ifwewantthehashfunction tobecollision-free, thenwemust
haveTgreater thanS2.Ifwearehappytohaveasmall frequency of
collisions, thenTneeds tobeoforderSonly.
Solution toexercise 12.5(p.198).Theposterior probabilit yratioforthetwo
hypotheses,H+=`calculation correct' andH =`calculation incorrect' isthe
productofthepriorprobabilit yratioP(H+)=P(H )andthelikelihoodratio,
P(matc hjH+)=P(matc hjH ).Thissecond factor istheanswertothequestion.
ThenumeratorP(matc hjH+)isequal to1.Thedenominator's valuedepends
onourmodeloferrors. Ifweknowthatthehuman calculator isprone toerrors
involving multiplication oftheanswerby10,ortotransp osition ofadjacen t
digits, neither ofwhichaects thehashvalue, thenP(matc hjH )could be
equal to1also,sothatthecorrect matchgivesnoevidence infavourofH+.
Butifweassume thaterrors are`random fromthepointofviewofthehash
function' thentheprobabilit yofafalsepositiveisP(matc hjH )=1=9,and
thecorrect matchgivesevidence 9:1infavourofH+.
Solution toexercise 12.7(p.199).IfyouaddatinyM=32extra bitsofhash
toahugeN-bitleyougetprettygooderrordetection {theprobabilit ythat
anerrorisundetected is2 M,lessthanoneinabillion. Todoerrorcorrection
requires farmore checkbits,thenumberdepending ontheexpected typesof
corruption, andonthelesize.Forexample, ifjusteightrandom bitsina
megab ytelearecorrupted, itwouldtakeaboutlog2 223
8'238'180
bitstospecifywhicharethecorrupted bits,andthenumberofparitycheck
bitsusedbyasuccessful error-correcting codewouldhavetobeatleastthis
number,bythecountingargumen tofexercise 1.10(solution, p.20).
Solution toexercise 12.10 (p.201).WewanttoknowthelengthLofastring
suchthatitisveryimprobable thatthatstring matchesanypartoftheentire
writings ofhumanit y.Let'sestimate thatthese writings totalaboutonebook
foreachperson living, andthateachbookcontainstwomillion characters (200
pages with10000characters perpage) {that's 1016characters, drawnfrom
analphab etof,say,37characters.
Theprobabilit ythatarandomly chosen string oflengthLmatchesatone
pointinthecollected worksofhumanit yis1=37L.Sotheexpected number
ofmatchesis1016=37L,whichisvanishingly small ifL16=log1037'10.
Because oftheredundancy andrepetition ofhumanit y'swritings, itispossible
thatL'10isanoverestimate.
So,ifyouwanttowrite something unique, sitdownandcomposeastring
oftencharacters. Butdon't writegidnebinzz ,because Ialready though tof
thatstring.
Asforanewmelody,ifwefocusonthesequence ofnotes, ignoring duration
andstress, andallowleaps ofuptoanoctaveateachnote, thenthenumber
ofchoices pernoteis23.Thepitchoftherstnoteisarbitrary .Thenumber
ofmelodiesoflengthrnotes inthisrather uglyensem bleofSchonbergian
tunes is23r 1;forexample, there are250000oflengthr=5.Restricting
thepermitted intervalswillreduce thisgure; including duration andstress
willincrease itagain. [Ifwerestrict thepermitted intervalstorepetitions and
tones orsemitones, thereduction isparticularly severe;isthiswhythemelody
of`OdetoJoy'sounds soboring?] Thenumberofrecorded compositions is
probably lessthanamillion. Ifyoulearn100newmelodiesperweekforevery
weekofyourlifethenyouwillhavelearned 250000melodiesatage50.Based

<<<PAGE 216>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
204 12|Hash Codes:CodesforEcien tInformation Retriev al
onempirical experience ofplayingthegame `guessthattune',itseems to
methatwhereas manyfour-note sequences areshared incommon between
melodies,thenumberofcollisions betweenve-note sequences israther smaller
{mostfamous ve-note sequences areunique.
Solution toexercise 12.11 (p.201).(a)LettheDNA-binding protein recognize
asequence oflengthLnucleotides. That is,itbinds preferen tially tothat
DNA sequence, andnottoanyother pieces ofDNA inthewhole genome. (In
reality,therecognized sequence maycontainsome wildcard characters, e.g.,
the*inTATAA*A ,whichdenotes `anyofA,C,GandT';so,tobeprecise, weare
assuming thattherecognized sequence containsLnon-wildcard characters.)
Assuming therestofthegenome is`random', i.e.,thatthesequence con-
sistsofrandom nucleotides A,C,GandTwithequal probabilit y{whichis
obviously untrue,butitshouldn't maketoomuchdierence toourcalculation
{thechance ofthere beingnoother occurence ofthetarget sequence inthe
whole genome, oflengthNnucleotides, isroughly
(1 (1=4)L)N'exp( N(1=4)L); (12.9)
whichisclosetooneonlyif
N4 L1; (12.10)
thatis,
L>logN=log4: (12.11)
UsingN=3109,werequire therecognized sequence tobelonger than
Lmin=16nucleotides.
What sizeofprotein doesthisimply?
Aweaklowerboundcanbeobtained byassuming thattheinformation
contentoftheprotein sequence itself isgreater thantheinformation
contentofthenucleotide sequence theprotein prefers tobindto(which
wehaveargued abovemustbeatleast32bits). Thisgivesaminim um
protein length of32=log2(20)'7amino acids.
Thinking realistically ,therecognition oftheDNA sequence bythepro-
teinpresumably involvestheprotein coming intocontactwithallsixteen
nucleotides inthetarget sequence. Iftheprotein isamonomer, itmust
bebigenough thatitcansimultaneously makecontactwithsixteen nu-
cleotides ofDNA. Onehelical turnofDNA containing tennucleotides
hasalength of3.4nm,soacontiguous sequence ofsixteen nucleotides
hasalength of5.4nm.Thediameter oftheprotein musttherefore be
about5.4nmorgreater. Egg-white lysozyme isasmall globular pro-
teinwithalength of129amino acids andadiameter ofabout4nm.
Assuming thatvolume isproportional tosequence length andthatvol-
umescales asdiameter cubed,aprotein ofdiameter 5.4nmmusthavea
sequence oflength 2:5129'324amino acids.
(b)If,however,atarget sequence consists ofatwice-rep eated sub-sequence, we
cangetbywithamuchsmaller protein thatrecognizes onlythesub-sequence,
andthatbinds totheDNA strongly onlyifitcanformadimer ,bothhalves
ofwhicharebound totherecognized sequence. Halving thediameter ofthe
protein, wenowonlyneedaprotein whose length isgreater than324/8 =40
amino acids. Aprotein oflength smaller thanthiscannot byitselfserveas
aregulatory protein specic toonegene, because it'ssimply toosmall tobe
abletomakeasucien tlyspecicmatch{itsavailable surface doesnothave
enough information content.

<<<PAGE 217>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
AboutChapter 13
InChapters 8{11, weestablished Shannon's noisy-c hannel codingtheorem
forageneral channel withanyinput andoutput alphab ets.Agreat dealof
attentionincodingtheory focuses onthespecialcaseofchannels withbinary
inputs. Constraining ourselv estothese channels simplies matters, andleads
usintoanexceptionally richworld,whichwewillonlytasteinthisbook.
Oneoftheaimsofthischapter istopointoutacontrastbetweenShannon's
aimofachieving reliable comm unication overanoisychannel andtheapparen t
aimofmanyintheworldofcodingtheory .Manycodingtheorists takeas
theirfundamen talproblem thetaskofpackingasmanyspheres aspossible,
withradius aslargeaspossible, intoanN-dimensional space, withnospheres
overlapping .Prizes areawarded topeople whondpackings thatsqueeze in
anextra fewspheres. While thisisafascinating mathematical topic, weshall
seethattheaimofmaximizing thedistance betweencodewordsinacodehas
onlyatenuousrelationship toShannon's aimofreliable comm unication.
205

<<<PAGE 218>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
13
Binary Codes
We'veestablished Shannon's noisy-c hannel codingtheorem forageneral chan-
nelwithanyinput andoutput alphab ets.Agreat dealofattentionincoding
theory focuses onthespecialcaseofchannels withbinary inputs, therst
implicit choice beingthebinary symmetric channel.
Theoptimal decoderforacode,givenabinary symmetric channel, nds
thecodewordthatisclosest tothereceivedvector, closest inHamming dis-
tance.TheHamming distance betweentwobinary vectors isthenumberof
coordinates inwhichthetwovectors dier. Decodingerrors willoccurifthe Example:
TheHamming distance
between00001111
and11001101
is3.noise takesusfromthetransmitted codewordttoareceivedvectorrthat
iscloser tosome other codeword.Thedistances betweencodewordsarethus
relevanttotheprobabilit yofadecodingerror.
13.1 Distance properties ofacode
Thedistance ofacodeisthesmallest separation betweentwoofitscodewords.
Example 13.1. The(7;4)Hamming code(p.9)hasdistanced=3.Allpairsof
itscodewordsdier inatleast3bits.Theminim umnumberoferrors
itcancorrect ist=1;ingeneral acodewithdistancedisb(d 1)=2c-
error-correcting.
Amoreprecise termfordistance istheminim umdistance ofthecode.The
distance ofacodeisoften denoted bydordmin.
We'llnowconstrain ourattentiontolinear codes.Inalinear code,all
codewordshaveidenticaldistance properties, sowecansummarize allthe
distances betweenthecode'scodewordsbycountingthedistances fromthe
all-zero codeword.
Theweightenumerator function ofacode,A(w),isdened tobethe
numberofcodewordsinthecodethathaveweightw.TheweightenumeratorwA(w)
0 1
3 7
4 7
7 1
Total 16
012345678
01234567
Figure 13.1.Thegraph ofthe
(7;4)Hamming code,andits
weightenumerator function.function isalsoknownasthedistance distribution ofthecode.
Example 13.2. Theweightenumerator functions ofthe(7;4)Hamming code
andthedodecahedron codeareshowningures 13.1and13.2.
13.2 Obsession withdistance
Since theminim umnumberoferrors thatacodecanguaranteetocorrect,
t,isrelated toitsdistancedbyt=b(d 1)=2c,manycodingtheorists focus d=2t+1ifdisodd,and
d=2t+2ifdiseven. onthedistance ofacode,searchingforcodesofagivensizethathavethe
biggest possible distance. Muchofpractical codingtheory hasfocused on
decodersthatgivetheoptimal decodingforallerrorpatterns ofweightupto
thehalf-distance toftheircodes.
206

<<<PAGE 219>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
13.2: Obsession withdistance 207
wA(w)
0 1
5 12
8 30
9 20
10 72
11 120
12 100
13 180
14 240
15 272
16 345
17 300
18 200
19 120
20 36
Total 2048050100150200250300350
0581015202530
110100
0581015202530Figure 13.2.Thegraph dening
the(30;11)dodecahedron code
(thecircles arethe30transmitted
bitsandthetriangles arethe20
paritychecks,oneofwhichis
redundan t)andtheweight
enumerator function (solid lines).
Thedotted linesshowtheaverage
weightenumerator function ofall
random linear codeswiththe
samesizeofgenerator matrix,
whichwillbecomputed shortly .
Thelowergure showsthesame
functions onalogscale.
Abounded-distance decoderisadecoderthatreturns theclosest code-
wordtoareceivedbinary vectorrifthedistance fromrtothatcodeword
islessthanorequal tot;otherwise itreturns afailure message.
Therationale fornottrying todecodewhen morethanterrors haveoccurred
mightbe`wecan'tguaranteethatwecancorrect more thanterrors, sowe
won'tbother trying {whowouldbeinterested inadecoderthatcorrects some
errorpatterns ofweightgreater thant,butnotothers?' Thisdefeatist attitude
isanexample ofworst-case-ism ,awidespread mentalailmen twhichthisbook
isintended tocure.
Thefactisthatbounded-distance decoderscannot reachtheShannon limit
ofthebinary symmetric channel; onlyadecoderthatoftencorrects morethan
terrors candothis.Thestateoftheartinerror-correcting codeshavedecoders
thatworkwaybeyondtheminim umdistance ofthecode.
Denitions ofgoodandbaddistanc eproperties
Givenafamily ofcodesofincreasing blocklengthN,andwithratesapproac h-
ingalimitR>0,wemaybeabletoputthatfamily inoneofthefollowing
categories, whichhavesome similarities tothecategories of`good'and`bad'
codesdened earlier (p.183):
Asequence ofcodeshas`good'distance ifd=Ntends toaconstan t
greater thanzero.
Asequence ofcodeshas`bad'distance ifd=Ntends tozero.
Asequence ofcodeshas`verybad'distance ifdtends toaconstan t.Figure 13.3.Thegraph ofa
rate-1/2low-densit ygenerator
matrix code.TherightmostMof
thetransmitted bitsareeach
connected toasingle distinct
parityconstrain t.
Example 13.3. Alow-densit ygenerator-matrix codeisalinear codewhoseK
Ngenerator matrix Ghasasmall numberd0of1sperrow,regardless
ofhowbigNis.Theminim umdistance ofsuchacodeisatmostd0,so
low-densit ygenerator-matrix codeshave`verybad'distance.
While havinglarge distance isnobadthing, we'llsee,lateron,whyan
emphasis ondistance canbeunhealth y.

<<<PAGE 220>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
208 13|Binary Codes
. . .t
12ttFigure 13.4.Schematic picture of
partofHamming space perfectly
lledbyt-spheres centredonthe
codewordsofaperfect code.
13.3 Perfect codes
At-sphere (orasphere ofradiust)inHamming space, centredonapointx,
isthesetofpointswhose Hamming distance fromxislessthanorequal tot.
The(7;4)Hamming codehasthebeautiful propertythatifweplace 1-
spheres abouteachofits16codewords,those spheres perfectly llHamming
space without overlapping. AswesawinChapter 1,everybinary vector of
length 7iswithin adistance oft=1ofexactly onecodewordoftheHamming
code.
Acodeisaperfectt-error-correcting codeifthesetoft-spheres cen-
tredonthecodewordsofthecodelltheHamming space without over-
lapping. (Seegure 13.4.)
Let's recap ourcastofcharacters. ThenumberofcodewordsisS=2K.
ThenumberofpointsintheentireHamming space is2N.Thenumberof
pointsinaHamming sphere ofradiustis
tX
w=0 
N
w!
: (13.1)
Foracodetobeperfect withthese parameters, werequireStimes thenumber
ofpointsinthet-sphere toequal 2N:
foraperfect code,2KtX
w=0 
N
w!
=2N(13.2)
or,equivalently,tX
w=0 
N
w!
=2N K: (13.3)
Foraperfect code,thenumberofnoise vectors inonesphere mustequal
thenumberofpossible syndromes. The(7;4)Hamming codesatises this
numerological condition because
1+ 
7
1!
=23: (13.4)

<<<PAGE 221>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
13.3: Perfect codes 209
12. . .t
12. . .t12. . .t
12. . .tFigure 13.5.Schematic picture of
Hamming space notperfectly
lledbyt-spheres centredonthe
codewordsofacode.Thegrey
regions showpointsthatareata
Hamming distance ofmorethant
fromanycodeword.Thisisa
misleading picture, as,forany
codewithlargetinhigh
dimensions, thegreyspace
betweenthespheres takesup
almost allofHamming space.
Howhappy wewould betouseperfectcodes
Ifthere werelarge numbersofperfect codestochoosefrom, withawide
range ofblocklengths andrates, thenthese wouldbetheperfect solution to
Shannon's problem. Wecould comm unicate overabinary symmetric channel
withnoise levelf,forexample, bypickingaperfectt-error-correcting code
withblocklengthNandt=fN,wheref=f+andNandarechosen
suchthattheprobabilit ythatthenoise ipsmore thantbitsissatisfactorily
small.
However,therearealmost noperfectcodes.Theonlynontrivial perfect
binary codesare
1.theHamming codes,whichareperfect codeswitht=1andblocklength
N=2M 1,dened below;therateofaHamming codeapproac hes1
asitsblocklengthNincreases;
2.therepetition codesofoddblocklengthN,whichareperfect codeswith
t=(N 1)=2;therateofrepetition codesgoestozeroas1=N;and
3.oneremark able3-error-correcting codewith212codewordsofblock-
lengthN=23knownasthebinary Golaycode.[Asecond 2-error-
correcting GolaycodeoflengthN=11overaternary alphab etwasdis-
coveredbyaFinnish football-p oolenthusiast called Juhani Virtak allio
in1947.]
There arenoother binary perfect codes.Whythisshortage ofperfect codes?
Isitbecause precise numerological coincidences likethose satised bythe
parameters oftheHamming code(13.4) andtheGolaycode,
1+ 
23
1!
+ 
23
2!
+ 
23
3!
=211; (13.5)
arerare? Arethere plentyof`almost-p erfect' codesforwhichthet-spheres ll
almost thewhole space?
No.Infact,thepicture ofHamming spheres centredonthecodewords
almost lling Hamming space (gure 13.5)isamisleading one:formostcodes,
whether theyaregoodcodesorbadcodes,almost alltheHamming space is
takenupbythespacebetweent-spheres (whichisshowningreyingure 13.5).
Havingestablished thisgloomypicture, wespendamomen tlling inthe
properties oftheperfect codesmentioned above.

<<<PAGE 222>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
210 13|Binary Codes
0000 11111111100000
0000
0000001 1 111111111 0100000000000
11111111000000
111111111000000 000000
NuN vN wNxNFigure 13.6.Three codewords.
TheHamming codes
The(7;4)Hamming codecanbedened asthelinear codewhose 37parity-
checkmatrix contains, asitscolumns, allthe7(=23 1)non-zero vectors of
length 3.Since these 7vectors arealldieren t,anysingle bit-ip produces a
distinct syndrome, soallsingle-bit errors canbedetected andcorrected.
Wecangeneralize thiscode,withM=3parityconstrain ts,asfollows.The
Hamming codesaresingle-error-correcting codesdened bypickinganumber
ofparity-checkconstrain ts,M;theblocklengthNisN=2M 1;theparity-
checkmatrix contains, asitscolumns, alltheNnon-zero vectors oflengthM
bits.
TherstfewHamming codeshavethefollowingrates:
Checks,M(N;K)R=K=N
2 (3,1)1/3 repetition codeR3
3 (7,4)4/7 (7;4)Hamming code
4 (15,11) 11/15
5 (31,26) 26/31
6 (63,57) 57/63
Exercise 13.4.[2,p.223]What istheprobabilit yofblockerror ofthe(N;K)
Hamming codetoleading order, when thecodeisusedforabinary
symmetric channel withnoise densit yf?
13.4 Perfectness isunattainable {rstproof
Wewillshowinseveralwaysthatuseful perfect codesdonotexist (here,
`useful' means `havinglargeblocklengthN,andratecloseneither to0nor1').
Shannon provedthat, givenabinary symmetric channel withanynoise
levelf,there existcodeswithlarge blocklengthNandrateascloseasyou
liketoC(f)=1 H2(f)thatenable comm unication witharbitrarily small
errorprobabilit y.ForlargeN,thenumberoferrors perblockwilltypically by
aboutfN,sothesecodesofShannon are`almost-certainly- fN-error-correcting'
codes.
Let's pickthespecialcaseofanoisy channel withf2(1=3;1=2).Can
wendalargeperfectcodethatisfN-error-correcting? Well,let'ssuppose
thatsuchacodehasbeenfound, andexamine justthree ofitscodewords.
(Remem berthatthecodeoughttohaverateR'1 H2(f),soitshould have
anenormous number(2NR)ofcodewords.) Without lossofgeneralit y,we
chooseoneofthecodewordstobetheall-zero codewordanddene theother
twotohaveoverlaps withitasshowningure 13.6. Thesecond codeword
diers fromtherstinafractionu+vofitscoordinates. Thethirdcodeword
diers fromtherstinafractionv+w,andfromthesecond inafraction
u+w.Afractionxofthecoordinates havevaluezeroinallthree codewords.
Now,ifthecodeisfN-error-correcting, itsminim umdistance mustbegreater

<<<PAGE 223>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
13.5: Weightenumerator function ofrandom linear codes 211
than2fN,so
u+v>2f;v+w>2f;andu+w>2f: (13.6)
Summing these three inequalities anddividing bytwo,wehave
u+v+w>3f: (13.7)
Soiff>1=3,wecandeduceu+v+w>1,sothatx<0,whichisimpossible.
Suchacodecannot exist. Sothecodecannot havethreecodewords,letalone
2NR.
Weconclude that, whereas Shannon provedthere areplentyofcodesfor
comm unicating overabinary symmetric channel withf>1=3,thereareno
perfectcodesthatcandothis.
Wenowstudy amore general argumen tthatindicates thatthere areno
largeperfect linear codesforgeneral rates(other than0and1).Wedothis
bynding thetypical distance ofarandom linear code.
13.5 Weightenumerator function ofrandom linear codes
Imagine making acodebypickingthebinary entriesintheMNparity-checkNz}| {2
666666664101010100100110100010110
001110111100011001101000
101110111001011000110100
000010111100101101001000
000000110011110100000100
110010001111100000101110
101111100010100001001110
110010110001101011101010
100011100101000010111101
010001000010101001101010
010111110111111110111010
1011101010010011010000113
7777777759
>>>>>>>>=
>>>>>>>>;M
Figure 13.7.Arandom binary
parity-checkmatrix.
matrix Hatrandom. What weightenumerator function should weexpect?
Theweightenumerator ofoneparticular codewithparity-checkmatrix H,
A(w)H,isthenumberofcodewordsofweightw,whichcanbewritten
A(w)H=X
x:jxj=w
 [Hx=0]; (13.8)
where thesumisoverallvectors xwhose weightiswandthetruth function [Hx=0]equals oneifHx=0andzerootherwise.
Wecanndtheexpected valueofA(w),
hA(w)i=X
HP(H)A(w)H (13.9)
=X
x:jxj=wX
HP(H)
 [Hx=0]; (13.10)
byevaluating theprobabilit ythataparticular wordofweightw>0isa
codewordofthecode(averaging overallbinary linear codesinourensem ble).
Bysymmetry ,thisprobabilit ydependsonlyontheweightwoftheword,not
onthedetails oftheword.Theprobabilit ythattheentiresyndrome Hxis
zerocanbefound bymultiplying together theprobabilities thateachofthe
Mbitsinthesyndrome iszero. Eachbitzmofthesyndrome isasum(mod
2)ofwrandom bits,sotheprobabilit ythatzm=0is1/2.Theprobabilit ythat
Hx=0isthusX
HP(H)
 [Hx=0]=(1/2)M=2 M; (13.11)
independen tofw.
Theexpected numberofwordsofweightw(13.10) isgivenbysumming,
overallwordsofweightw,theprobabilit ythateachwordisacodeword.The
numberofwordsofweightwis N
w,so
hA(w)i= 
N
w!
2 Mforanyw>0: (13.12)

<<<PAGE 224>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
212 13|Binary Codes
ForlargeN,wecanuselog N
w'NH2(w=N)andR'1 M=Ntowrite
log2hA(w)i'NH2(w=N) M (13.13)
'N[H2(w=N) (1 R)]foranyw>0:(13.14)
Asaconcrete example, gure 13.8showstheexpected weightenumerator
function ofarate-1=3random linear codewithN=540andM=360.01e+522e+523e+524e+525e+526e+52
0100200300400500
1e-1201e-1001e-801e-601e-401e-2011e+201e+401e+60
0100200300400500
Figure 13.8.Theexpected weight
enumerator functionhA(w)iofa
random linear codewithN=540
andM=360.Lowergure shows
hA(w)ionalogarithmic scale.Gilbert{Varshamov distanc e
ForweightswsuchthatH2(w=N)<(1 R),theexpectation ofA(w)is
smaller than1;forweightssuchthatH2(w=N)>(1 R),theexpectation is
greater than1.Wethusexpect,forlargeN,thattheminim umdistance ofa
random linear codewillbeclosetothedistancedGVdened by
H2(dGV=N)=(1 R): (13.15)
Denition. Thisdistance,dGVNH 1
2(1 R),istheGilbert{Varshamo v
distance forrateRandblocklengthN.
TheGilbert{Varshamo vconjecture ,widely believed,asserts that(forlarge
N)itisnotpossible tocreate binary codeswithminim umdistance signican tly
greater thandGV.
Denition. TheGilbert{Varshamo vrateRGVistheminim umrateatwhich
youcanreliably comm unicate withabounded-distance decoder(asdened on
p.207),assuming thattheGilbert{Varshamo vconjecture istrue.
Whysphere-packing isabadperspective, andanobsession withdistanc e
isinappr opriate
Ifoneusesabounded-distance decoder,themaxim umtolerable noise level
willipafractionfbd=1
2dmin=Nofthebits.So,assumingdminisequal to
theGilbertdistancedGV(13.15), wehave:00.51
0 0.25 0.5Capacity
R_GV
f
Figure 13.9.Contrastbetween
Shannon's channel capacit yCand
theGilbertrateRGV{the
minim umcomm unication rate
achievableusing a
bounded-distance decoder,asa
function ofnoiselevelf.Forany
givenrate,R,theminim um
tolerable noiselevelforShannon
istwiceasbigastheminim um
tolerable noiselevelfora
`worst-case-ist' whousesa
bounded-distance decoder.H2(2fbd)=(1 RGV): (13.16)
RGV=1 H2(2fbd): (13.17)
Now,here's thecrunch:what didShannon sayisachievable? Hesaidthe
maxim umpossible rateofcomm unication isthecapacit y,
C=1 H2(f): (13.18)
SoforagivenrateR,themaxim umtolerable noiselevel,according toShannon,
isgivenby
H2(f)=(1 R): (13.19)
Ourconclusion: imagine agoodcodeofrateRhasbeenchosen; equations
(13.16) and(13.19) respectivelydene themaxim umnoise levelstolerable by
abounded-distance decoder,fbd,andbyShannon's decoder,f.
fbd=f=2 (13.20)
Those whousebounded-distance decoderscanonlyevercopewithhalfthe
noise-lev elthatShannon provedistolerable!
Howdoesthisrelate toperfect codes? Acodeisperfect ifthere aret-
spheres around itscodewordsthatllHamming space without overlapping.

<<<PAGE 225>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
13.6: Berlek amp's bats 213
Butwhen atypical random linear codeisusedtocomm unicate overabi-
narysymmetric channel neartotheShannon limit, thetypical numberofbits
ippedisfN,andtheminim umdistance betweencodewordsisalsofN,or
alittlebigger, ifwearealittlebelowtheShannon limit. SothefN-spheres
around thecodewordsoverlap witheachother sucien tlythateachsphere
almost contains thecentreofitsnearest neighbour! Thereason whythis
Figure 13.10.Twooverlapping
spheres whose radius isalmost as
bigasthedistance betweentheir
centres.overlapisnotdisastrous isbecause, inhighdimensions, thevolume associated
withtheoverlap, shownshaded ingure 13.10, isatinyfraction ofeither
sphere, sotheprobabilit yoflanding initisextremely small.
Themoral ofthestory isthatworst-case-ism canbebadforyou,halving
yourabilitytotolerate noise. Youhavetobeabletodecodewaybeyondthe
minim umdistance ofacodetogettotheShannon limit!
Nevertheless, theminim umdistance ofacodeisofinterest inpractice,
because, under some conditions, theminim umdistance dominates theerrors
made byacode.
13.6 Berlek amp's bats
Ablind batlivesinacave.Itiesaboutthecentreofthecave,whichcorre-
spondstoonecodeword,withitstypical distance fromthecentrecontrolled
byafriskiness parameter f.(The displacemen tofthebatfromthecentre
corresp ondstothenoise vector.) Theboundaries ofthecavearemade up
ofstalactites thatpointintowardsthecentreofthecave.Eachstalactite is
analogous totheboundary betweenthehome codewordandanother code-
word.Thestalactite isliketheshaded region ingure 13.10, butreshap edto
conveytheideathatitisaregion ofverysmall volume.
Decodingerrors corresp ondtothebat'sintended trajectory passing inside
astalactite. Collisions withstalactites atvarious distances fromthecentre
arepossible.
Ifthefriskiness isverysmall, thebatisusually veryclosetothecentre
ofthecave;collisions willberare,andwhen theydooccur,theywillusually
involvethestalactites whose tipsareclosest tothecentrepoint.Similarly ,
under low-noise conditions, decodingerrors willberare,andtheywilltypi-
callyinvolvelow-weightcodewords. Under low-noise conditions, theminim um
distance ofacodeisrelevanttothe(verysmall) probabilit yoferror.
. . .t
12Figure 13.11.Berlek amp's
schematic picture ofHamming
space inthevicinit yofa
codeword.Thejagged solidline
encloses allpointstowhichthis
codewordistheclosest. The
t-sphere around thecodeword
takesupasmall fraction ofthis
space.

<<<PAGE 226>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
214 13|Binary Codes
Ifthefriskiness ishigher, thebatmayoften makeexcursions beyondthe
safedistancetwhere thelongest stalactites start, butitwillcollide mostfre-
quentlywithmoredistan tstalactites, owingtotheirgreater number.There's
onlyatinynumberofstalactites attheminim umdistance, sotheyarerela-
tivelyunlikelytocause theerrors. Similarly ,errors inarealerror-correcting
codedependontheproperties oftheweightenumerator function.
Atveryhighfriskiness, thebatisalwaysalongwayfromthecentreof
thecave,andalmost allitscollisions involvecontactwithdistan tstalactites.
Under these conditions, thebat's collision frequency hasnothing todowith
thedistance fromthecentretotheclosest stalactite.
13.7 Concatenation ofHamming codes
Itisinstructiv etoplaysomemorewiththeconcatenation ofHamming codes,
aconcept werstvisited ingure 11.6,because wewillgetinsigh tsintothe
notion ofgoodcodesandtherelevanceorotherwise oftheminim umdistance
ofacode.
Wecancreate aconcatenated codeforabinary symmetric channel with
noise densit yfbyencodingwithseveralHamming codesinsuccession.
Thetable recaps thekeyproperties oftheHamming codes,indexed by
numberofconstrain ts,M.AlltheHamming codeshaveminim umdistance
d=3andcancorrect oneerrorinN.
N=2M 1blocklength
K=N M numberofsource bits
pB=3
N N
2f2probabilit yofblockerrortoleading orderR
00.20.40.60.81
024681012C
Figure 13.12.TherateRofthe
concatenated Hamming codeasa
function ofthenumberof
concatenations, C.
Ifwemakeaproductcodebyconcatenating asequence ofCHamming
codeswithincreasingM,wecanchoosethose parametersfMcgC
c=1insucha
waythattherateoftheproductcode
RC=CY
c=1Nc Mc
Nc(13.21)
tends toanon-zero limit asCincreases. Forexample, ifwesetM1=2,
M2=3,M3=4,etc.,thentheasymptotic rateis0.093 (gure 13.12).
TheblocklengthNisarapidly growing function ofC,sothese codes
aresomewhat impractical. Afurther weakness ofthese codesisthattheir11000001e+101e+151e+201e+25
024681012C
Figure 13.13.TheblocklengthNC
andminim umdistancedCofthe
concatenated Hamming codeasa
function ofthenumberof
concatenations C.minim umdistance isnotverygood(gure 13.13). Everyoneoftheconstituen t
Hamming codeshasminim umdistance 3,sotheminim umdistance oftheCth
productis3C.TheblocklengthNgrowsfaster than3C,sotheratiod=Ntends
tozeroasCincreases. Incontrast, fortypical random codes,theratiod=N
tends toaconstan tsuchthatH2(d=N)=1 R.Concatenated Hamming
codesthushave`bad'distance.
Nevertheless, itturns outthatthissimple sequence ofcodesyields good
codesforsome channels {butnotverygoodcodes(seesection 11.4torecall
thedenitions oftheterms `good'and`verygood').Rather thanprovethis
result, wewillsimply explore itnumerically .
Figure 13.14 showsthebiterrorprobabilit ypboftheconcatenated codes
assuming thattheconstituen tcodesaredecodedinsequence, asdescrib ed
insection 11.4. [This one-co de-at-a-time decodingissuboptimal, aswesaw
there.] Thehorizon talaxisshowstherates ofthecodes. Asthenumber
ofconcatenations increases, theratedrops to0.093 andtheerrorprobabilit y
drops towardszero.Thechannel assumed inthegure isthebinary symmetric

<<<PAGE 227>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
13.8: Distance isn'teverything 215
channel withf=0:0588. Thisisthehighest noise levelthatcanbetolerated
using thisconcatenated code.pb
1e-141e-121e-101e-081e-060.00010.011
00.20.40.60.81N=321
315
61525
10^13
R
Figure 13.14.Thebiterror
probabilities versustheratesRof
theconcatenated Hamming codes,
forthebinary symmetric channel
withf=0:0588. Thesolidline
showstheShannon limitforthis
channel.
Thebiterrorprobabilit ydrops to
zerowhile theratetends to0.093,
sotheconcatenated Hamming
codesarea`good'codefamily .Thetake-home message fromthisstory isdistanc eisn'teverything .The
minim umdistance ofacode,although widely worshipp edbycodingtheorists,
isnotoffundamen talimportance toShannon's mission ofachieving reliable
comm unication overnoisy channels.
.Exercise 13.5.[3]Provethatthere existfamilies ofcodeswith`bad' distance
thatare`verygood'codes.
13.8 Distance isn'teverything
Let's getaquantitativ efeeling fortheeect oftheminim umdistance ofa
code,forthespecialcaseofabinary symmetric channel.
Theerrorprobability associatedwithonelow-weight codeword
Letabinary codehaveblocklengthNandjusttwocodewords,whichdier in
dplaces. Forsimplicit y,let'sassumediseven.What istheerrorprobabilit y
ifthiscodeisusedonabinary symmetric channel withnoise levelf?
Bitipsmatter onlyinplaces where thetwocodewordsdier. Theerror
probabilit yisdominated bytheprobabilit ythatd=2ofthese bitsareipped.
What happenstotheother bitsisirrelev ant,sincetheoptimal decoderignores
them.
P(blockerror)' 
d
d=2!
fd=2(1 f)d=2: (13.22)
Thiserrorprobabilit yassociated withasingle codewordofweightdisplotted
ingure 13.15. Using theapproximation forthebinomial coecien t(1.16),1e-201e-151e-101e-051
0.0001 0.001 0.01 0.1d=10
d=20
d=30
d=40
d=50
d=60
Figure 13.15.Theerror
probabilit yassociated witha
single codewordofweightd, d
d=2
fd=2(1 f)d=2,asafunction
off.wecanfurther approximate
P(blockerror)'h
2f1=2(1 f)1=2id(13.23)
[(f)]d; (13.24)
where(f)=2f1=2(1 f)1=2iscalled theBhattac haryyaparameter ofthe
channel.
Now,consider ageneral linear codewithdistanced.Itsblockerrorprob-
abilitymustatleastthan d
d=2fd=2(1 f)d=2,independen toftheblocklength
Nofthecode.Forthisreason, asequence ofcodesofincreasing blocklength
Nandconstan tdistanced(i.e.,`verybad'distance) cannot haveablocker-
rorprobabilit ythattends tozero,onanybinary symmetric channel. Ifwe
areinterested inmaking superberror-correcting codeswithtiny,tinyerror
probabilit y,wemighttherefore shuncodeswithbaddistance. However,being
pragmatic, weshould lookmore carefully atgure 13.15. InChapter 1we
argued thatcodesfordiskdrivesneedanerrorprobabilit ysmaller thanabout
10 18.Iftherawerrorprobabilit yinthediskdriveisabout0:001,theerror
probabilit yassociated withonecodewordatdistanced=20issmaller than
10 24.Iftherawerror probabilit yinthediskdriveisabout0:01,theerror
probabilit yassociated withonecodewordatdistanced=30issmaller than
10 20.Forpractical purposes,therefore, itisnotessentialforacodetohave
gooddistance. Forexample, codesofblocklength 10000,knowntohavemany
codewordsofweight32,cannevertheless correct errors ofweight320withtiny
errorprobabilit y.

<<<PAGE 228>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
216 13|Binary Codes
Iwouldn't wantyoutothink Iamrecommending theuseofcodeswith
baddistance; inChapter 47wewilldiscuss low-densit yparity-checkcodes,my
favourite codes,whichhavebothexcellen tperformance andgooddistance.
13.9 Theunion bound
Theerror probabilit yofacodeonthebinary symmetric channel canbe
bounded interms ofitsweightenumerator function byadding upappropriate
multiples oftheerrorprobabilit yassociated withasingle codeword(13.24):
P(blockerror)X
w>0A(w)[(f)]w: (13.25)
Thisinequalit y,whichisanexample ofaunion bound,isaccurate forlow
noise levelsf,butinaccurate forhighnoise levels,because itovercoun tsthe
contribution oferrors thatcause confusion withmorethanonecodewordata
time.
.Exercise 13.6.[3]Poorman's noisy-channel codingtheorem.
Pretending thattheunion bound(13.25) isaccurate, andusing theaver-
ageweightenumerator function ofarandom linear code(13.14) (section
13.5) asA(w),estimate themaxim umrateRUB(f)atwhichonecan
comm unicate overabinary symmetric channel.
Or,tolookatitmore positively,using theunion bound (13.25) asan
inequalit y,showthatcomm unication atratesuptoRUB(f)ispossible
overthebinary symmetric channel.
Inthefollowingchapter, byanalysing theprobabilit yoferrorofsyndrome
decodingforabinary linear code,andusing aunion bound, wewillprove
Shannon's noisy-c hannel codingtheorem (forsymmetric binary channels), and
thusshowthatverygoodlinearcodesexist.
13.10 Dualcodes
Aconcept thathassome importance incodingtheory ,though wewillhave
noimmediate useforitinthisbook,istheideaofthedualofalinear error-
correcting code.
An(N;K)linear error-correcting codecanbethough tofasasetof2K
codewordsgenerated byadding together allcombinations ofKindependen t
basiscodewords. Thegenerator matrix ofthecodeconsists ofthoseKbasis
codewords, conventionally written asrowvectors. Forexample, the(7;4)
Hamming code'sgenerator matrix (from p.10)is
G=2
66641000101
0100110
0010111
00010113
7775(13.26)
anditssixteen codewordsweredispla yedintable 1.14(p.9).Thecode-
wordsofthiscodearelinear combinations ofthefourvectors [1000101],
[0100110],[0010111],and[0001011].
An(N;K)codemayalsobedescrib edinterms ofanMNparity-check
matrix (whereM=N K)asthesetofvectorsftgthatsatisfy
Ht=0: (13.27)

<<<PAGE 229>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
13.10: Dualcodes 217
Onewayofthinking ofthisequation isthateachrowofHspecies avector
towhichtmustbeorthogonal ifitisacodeword.
Thegenerator matrix speciesKvectors fromwhich allcodewords
canbebuilt, andtheparity-checkmatrix species asetofMvectors
towhich allcodewordsareorthogonal.
Thedualofacodeisobtained byexchanging thegenerator matrix
andtheparity-checkmatrix.
Denition. Thesetofallvectors oflengthNthatareorthogonal toallcode-
wordsinacode,C,iscalled thedualofthecode,C?.
Iftisorthogonal toh1andh2,thenitisalsoorthogonal toh3h1+h2;
soallcodewordsareorthogonal toanylinear combination oftheMrowsof
H.Sothesetofalllinear combinations oftherowsoftheparity-checkmatrix
isthedualcode.
ForourHamming (7;4)code,theparity-checkmatrix is(from p.12):
H=h
PI3i
=2
641110100
0111010
10110013
75: (13.28)
Thedualofthe(7;4)Hamming codeH(7;4)isthecodeshownintable13.16.
0000000
00101110101101
01110101001110
10110011100011
1110100Table13.16.Theeightcodewords
ofthedualofthe(7;4)Hamming
code.[Compare withtable1.14,
p.9.]
Apossibly unexp ected propertyofthispairofcodesisthatthedual,
H?
(7;4),iscontained within thecodeH(7;4)itself: everywordinthedualcode
isacodewordoftheoriginal (7;4)Hamming code.Thisrelationship canbe
written using setnotation:
H?
(7;4)H(7;4): (13.29)
Thepossibilit ythatthesetofdualvectors canoverlapthesetofcodeword
vectors iscounterintuitiveifwethink ofthevectors asrealvectors {howcan
avectorbeorthogonal toitself?Butwhen weworkinmodulo-t woarithmetic,
manynon-zero vectors areindeed orthogonal tothemselv es!
.Exercise 13.7.[1,p.223]Giveasimple rulethatdistinguishes whether abinary
vectorisorthogonal toitself, asiseachofthethreevectors [1110100],
[0111010],and[1011001].
Some moreduals
Ingeneral, ifacodehasasystematic generator matrix,
G=[IKjPT]; (13.30)
where PisaKMmatrix, thenitsparity-checkmatrix is
H=[PjIM]: (13.31)

<<<PAGE 230>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
218 13|Binary Codes
Example 13.8. Therepetition codeR3hasgenerator matrix
G=h
111i
; (13.32)
itsparity-checkmatrix is
H="
110
101#
: (13.33)
Thetwocodewordsare[111]and[000].
Thedualcodehasgenerator matrix
G?=H="
110
101#
(13.34)
orequivalently,modifying G?intosystematic formbyrowadditions,
G?="
101
011#
: (13.35)
Wecallthisdualcodethesimple paritycodeP3;itisthecodewithone
parity-checkbit,whichisequal tothesumofthetwosource bits.The
dualcode'sfourcodewordsare[110],[101],[000],and[011].
Inthiscase, theonlyvector common tothecodeandthedualisthe
all-zero codeword.
Goodness ofduals
Ifasequence ofcodesis`good',aretheirduals goodtoo?Examples canbe
constructed ofallcases: goodcodeswithgoodduals (random linear codes);
badcodeswithbadduals; andgoodcodeswithbadduals. Thelastcategory
isespecially important:manystate-of-the-art codeshavethepropertythat
theirduals arebad.Theclassic example isthelow-densit yparity-checkcode,
whose dualisalow-densit ygenerator-matrix code.
.Exercise 13.9.[3]Showthatlow-densit ygenerator-matrix codesarebad. A
family oflow-densit ygenerator-matrix codesisdened bytwoparam-
etersj;k,whicharethecolumn weightandrowweightofallrowsand
columns respectivelyofG.These weightsarexed, independen tofN;
forexample, (j;k)=(3;6).[Hint:showthatthecodehaslow-weight
codewords,thenusetheargumen tfromp.215.]
Exercise 13.10.[5]Showthatlow-densit yparity-checkcodesaregood,andhave
gooddistance. (Forsolutions, seeGallager (1963) andMacKa y(1999b).)
Self-dual codes
The(7;4)Hamming codehadthepropertythatthedualwascontained inthe
codeitself. Acodeisself-orthogonal ifitiscontained initsdual. Forexample,
thedualofthe(7;4)Hamming codeisaself-orthogonal code.Onewayof
seeing thisisthatoverlap betweenanypairofrowsofHiseven.Codesthat
containtheir duals areimportantinquantumerror-correction (Calderbank
andShor, 1996).
Itisintriguing, though notnecessarily useful, tolookatcodesthatare
self-dual .AcodeCisself-dual ifthedualofthecodeisidenticaltothecode.
C?=C: (13.36)
Some properties ofself-dual codescanbededuced:

<<<PAGE 231>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
13.11: Generalizing perfectness toother channels 219
1.Ifacodeisself-dual, thenitsgenerator matrix isalsoaparity-check
matrix forthecode.
2.Self-dual codeshaverate1=2,i.e.,M=K=N=2.
3.Allcodewordshaveevenweight.
.Exercise 13.11.[2,p.223]What propertymustthematrix Psatisfy ,ifthecode
withgenerator matrix G=[IKjPT]isself-dual?
Examples ofself-dual codes
1.Therepetition codeR2isasimple example ofaself-dual code.
G=H=h
11i
: (13.37)
2.Thesmallest non-trivial self-dual codeisthefollowing(8;4)code.
G=h
I4PTi
=2
666410000111
01001011
00101101
000111103
7775: (13.38)
.Exercise 13.12.[2,p.223]Findtherelationship oftheabove(8;4)codetothe
(7;4)Hamming code.
Duals andgraphs
Letacodeberepresen tedbyagraph inwhichthere arenodesoftwotypes,
parity-checkconstrain ts,andequalit yconstrain ts,joined byedges whichrep-
resentthebitsofthecode(notallofwhichneedbetransmitted).
Thedualcode'sgraph isobtained byreplacing allparity-checknodesby
equalit ynodesandviceversa.Thistypeofgraph iscalled anormal graph by
Forney (2001).
Further reading
Duals areimportantincodingtheory because functions involving acode(such
astheposterior distribution overcodewords) canbetransformed byaFourier
transform intofunctions overthedualcode.Foranaccessible introduction
toFourier analysis onnite groups, seeTerras(1999). SeealsoMacWilliams
andSloane (1977).
13.11 Generalizing perfectness toother channels
Havinggivenuponthesearchforperfect codesforthebinary symmetric
channel, wecould console ourselv esbychanging channel. Wecould calla
code`aperfectu-error-correcting codeforthebinary erasure channel' ifit
canrestore anyuerased bits,andnevermore thanu.Rather thanusing the
wordperfect, however,theconventional termforsuchacodeisa`maxim um
distance separable code',orMDS code.
Aswealready noted inexercise 11.10 (p.190),the(7;4)Hamming codeis
notanMDS code.Itcanrecoversome setsof3erased bits,butnotall.If
any3bitscorresp onding toacodewordofweight3areerased, thenonebitof

<<<PAGE 232>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
220 13|Binary Codes
information isunreco verable. Thisiswhythe(7;4)codeisapoorchoice for
aRAID system.
Atinyexample ofamaxim umdistance separable codeisthesimple parity-
checkcodeP3whose parity-checkmatrix isH=[111].Thiscodehas4
codewords, allofwhichhaveevenparity.Allcodewordsareseparated by
adistance of2.Anysingle erased bitcanberestored bysetting ittothe
parityoftheother twobits.Therepetition codesarealsomaxim umdistance
separable codes.
.Exercise 13.13.[5,p.224]Canyoumakean(N;K)code,withM=N K
paritysymbols,foraq-aryerasure channel, suchthatthedecodercan
recoverthecodewordwhenanyMsymbolsareerased inablockofN?
[Example: forthechannel withq=4symbolsthereisan(N;K)=(5;2)
codewhichcancorrect anyM=3erasures.]
Fortheq-aryerasure channel withq>2,there arelargenumbersofMDS
codes,ofwhichtheReed{Solomon codesarethemostfamous andmostwidely
used. Aslongastheeldsizeqisbigger thantheblocklengthN,MDS block
codesofanyratecanbefound. (Forfurther reading, seeLinandCostello
(1983).)
13.12 Summary
Shannon's codesforthebinary symmetric channel canalmost alwayscorrect
fNerrors, buttheyarenotfN-error-correcting codes.
Reasons whythedistanc eofacodehaslittlerelevanc e
1.TheShannon limitshowsyouthatyoumustbeabletocopewithanoise
leveltwiceasbigasthemaxim umnoise levelforabounded-distance
decoder.
2.When thebinary symmetric channel hasf>1=4,nocodewitha
bounded-distance decodercancomm unicate atall;butShannon says
goodcodesexistforsuchchannels.
3.Concatenation showsthatwecangetgoodperformance evenifthedis-
tance isbad.
Thewhole weightenumerator function isrelevanttothequestion of
whether acodeisagoodcode.
Therelationship betweengoodcodesanddistance properties isdiscussed
further inexercise 13.14 (p.220).
13.13 Further exercises
Exercise 13.14.[3,p.224]Acodewordtisselected fromalinear (N;K)code
C,anditistransmitted overanoisy channel; thereceivedsignal isy.
Weassume thatthechannel isamemoryless channel suchasaGaus-
sianchannel. Givenanassumed channel modelP(yjt),there aretwo
decodingproblems.
Thecodeworddecodingproblem isthetaskofinferring which
codewordtwastransmitted giventhereceivedsignal.

<<<PAGE 233>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
13.13: Further exercises 221
Thebitwisedecodingproblem isthetaskofinferring foreach
transmitted bittnhowlikelyitisthatthatbitwasaonerather
thanazero.
Consider optimal decodersforthese twodecodingproblems. Provethat
theprobabilit yoferroroftheoptimal bitwise-deco derisclosely related
totheprobabilit yoferroroftheoptimal codeword-deco der,byproving
thefollowingtheorem.
Theorem 13.1Ifabinary linearcodehasminimum distanc edmin,
then, foranygiven channel, thecodewordbiterrorprobability ofthe
optimal bitwise decoder,pb,andtheblockerrorprobability ofthemaxi-
mumlikeliho oddecoder,pB,arerelatedby:
pBpb1
2dmin
NpB: (13.39)
Exercise 13.15.[1]What aretheminim umdistances ofthe(15;11)Hamming
codeandthe(31;26)Hamming code?
.Exercise 13.16.[2]LetA(w)betheaverage weightenumerator function ofa
rate-1=3random linear codewithN=540andM=360. Estimate,
fromrstprinciples, thevalueofA(w)atw=1.
Exercise 13.17.[3C]Acodewithminimum distance greater thandGV.Arather
nice(15;5)codeisgenerated bythisgenerator matrix, whichisbased
onmeasuring theparities ofallthe 5
3=10triplets ofsource bits:
G=2
6666641111111
1111111
1111111
1111111
11111113
777775:(13.40)
Findtheminim umdistance andweightenumerator function ofthiscode.
Exercise 13.18.[3C]Find theminim umdistance ofthe`pentagonful' low-Figure 13.17.Thegraph ofthe
pentagonful low-densit y
parity-checkcodewith15bit
nodes(circles) and10
parity-checknodes(triangles).
densit yparity-checkcodewhose parity-checkmatrix is
H=2
66666666666666664111
111
111
111
111
111
111
111
111
1113
77777777777777775:(13.41)
Showthatnineofthetenrowsareindependen t,sothecodehasparam-
etersN=15,K=6.Using acomputer, nditsweightenumerator
function.

<<<PAGE 234>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
222 13|Binary Codes
.Exercise 13.19.[3]Replicate thecalculations used toproduce gure 13.12.
Checktheassertion thatthehighest noise levelthat's correctable is
0.0588. Explore alternativ econcatenated sequences ofcodes.Canyou
ndabettersequence ofconcatenated codes{betterinthesense thatit
haseither higher asymptotic rateRorcantolerate ahigher noise level
f?
Exercise 13.20.[3,p.226]Investigate thepossibilit yofachieving theShannon
limit withlinear blockcodes,using thefollowing countingargumen t.
Assume alinear codeoflargeblocklengthNandrateR=K=N.The
code'sparity-checkmatrix HhasM=N Krows.Assume thatthe
code'soptimal decoder,whichsolvesthesyndrome decodingproblem
Hn=z,allowsreliable comm unication overabinary symmetric channel
withipprobabilit yf.
Howmany`typical' noise vectors narethere?
Roughly howmanydistinct syndromes zarethere?
Sincenisreliably deduced fromzbytheoptimal decoder,thenumber
ofsyndromes mustbegreater thanorequal tothenumberoftypical
noise vectors. What doesthistellyouaboutthelargest possible value
ofrateRforagivenf?
.Exercise 13.21.[2]Linear binary codesusetheinput symbols0and1with
equal probabilit y,implicitly treating thechannel asasymmetric chan-
nel.Investigate howmuchlossincomm unication rateiscaused bythis
assumption, ifinfactthechannel isahighly asymmetric channel. Take
asanexample aZ-channel. Howmuchsmaller isthemaxim umpossible
rateofcomm unication using symmetric inputs thanthecapacit yofthe
channel? [Answ er:about6%.]
Exercise 13.22.[2]Showthatcodeswith`verybad'distance are`bad'codes,as
dened insection 11.4(p.183).
Exercise 13.23.[3]Onelinear codecanbeobtained fromanother bypunctur-
ing.Puncturing means taking eachcodewordanddeleting adened set
ofbits. Puncturing turns an(N;K)codeintoan(N0;K)code,where
N0<N.
Another waytomakenewlinear codesfromoldisshortening .Shortening
means constraining adened setofbitstobezero,andthendeleting
them fromthecodewords. Typically ifweshorten byonebit,halfofthe
code'scodewordsarelost. Shortening typically turns an(N;K)code
intoan(N0;K0)code,whereN N0=K K0.
Another waytomakeanewlinear codefromtwooldonesistomake
theintersection ofthetwocodes:acodewordisonlyretained inthenew
codeifitispresen tinbothofthetwooldcodes.
Discuss theeect onacode'sdistance properties ofpuncturing, short-
ening, andintersection. Isitpossible toturnacodefamily withbad
distance intoacodefamily withgooddistance, orviceversa,byeachof
these three manipulations?
Exercise 13.24.[3,p.226]ToddEbert's`hatpuzzle' .
Three playersenteraroomandaredorbluehatisplaced oneach
person's head. Thecolour ofeachhatisdetermined byacointoss,with

<<<PAGE 235>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
13.14: Solutions 223
theoutcome ofonecointosshavingnoeect ontheothers. Eachperson
canseetheother players'hatsbutnothisown. Ifyoualready knowthehatpuzzle,
youcould trythe`Scottish version' of
therules inwhichtheprize isonly
awarded tothegroup iftheyallguess
correctly .
Inthe`Reformed Scottish version',
alltheplayersmustguess correctly ,
andthere aretworounds ofguessing.
Those playerswhoguess during round
oneleavetheroom. Theremain-
ingplayersmustguess inround two.
What strategy should theteamadopt
tomaximize theirchance ofwinning?Nocomm unication ofanysortisallowed,except foraninitial strategy
session beforethegroup enterstheroom.Once theyhavehadachance
tolookattheother hats, theplayersmustsimultaneously guess their
ownhat'scolour orpass. Thegroup shares a$3million prizeifatleast
oneplayer guesses correctlyandnoplayers guessincorrectly.
Thesame game canbeplayedwithanynumberofplayers.Thegeneral
problem istondastrategy forthegroup thatmaximizes itschances of
winning theprize. Findthebeststrategies forgroups ofsizethree and
seven.
[Hint:when you'vedonethree andseven,youmightbeabletosolve
fteen .]
Exercise 13.25.[5]Estimate howmanybinary low-densit yparity-checkcodes
haveself-orthogonal duals. [Note thatwedon't expectahugenumber,
since almost alllow-densit yparity-checkcodesare`good',butalow-
densit yparity-checkcodethatcontainsitsdualmustbe`bad'.]
Exercise 13.26.[2]Ingure 13.15 weplotted theerror probabilit yassociated
withasingle codewordofweightdasafunction ofthenoise levelfof
abinary symmetric channel. Makeanequivalentplotforthecaseof
theGaussian channel, showingtheerror probabilit yassociated witha
single codewordofweightdasafunction oftherate-comp ensated signal
tonoise ratioEb=N0.BecauseEb=N0dependsontherate,youhaveto
chooseacoderate.ChooseR=1=2,2=3,3=4,or5=6.
13.14 Solutions
Solution toexercise 13.4(p.210).Theprobabilit yofblockerror toleading
order ispB=3
N N
2f2.
Solution toexercise 13.7(p.217).Abinary vector isperpendicular toitselfif
ithasevenweight,i.e.,anevennumberof1s.
Solution toexercise 13.11 (p.219).Theself-dual codehastwoequivalent
parity-checkmatrices, H1=G=[IKjPT]andH2=[PjIK];these mustbe
equivalenttoeachother through rowadditions, thatis,there isamatrix U
suchthatUH2=H1,so
[UPjUIK]=[IKjPT]: (13.42)
Fromtheright-hand sidesofthisequation, wehaveU=PT,sotheleft-hand
sidesbecome:
PTP=IK: (13.43)
Thusifacodewithgenerator matrix G=[IKjPT]isself-dual thenPisan
orthogonal matrix, modulo2,andviceversa.
Solution toexercise 13.12 (p.219).The(8;4)and(7;4)codesareintimately
related. The(8;4)code,whose parity-checkmatrix is
H=h
PI4i
=2
666401111000
10110100
11010010
111000013
7775; (13.44)

<<<PAGE 236>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
224 13|Binary Codes
isobtained by(a)appending anextra parity-checkbitwhichistheparityof
allsevenbitsofthe(7;4)Hamming code;and(b)reordering therstfour
bits.
Solution toexercise 13.13 (p.220).Ifan(N;K)code,withM=N Kparity
symbols,hasthepropertythatthedecodercanrecoverthecodewordwhenany
Msymbolsareerased inablockofN,thenthecodeissaidtobemaxim um
distance separable (MDS).
NoMDS binary codesexist, apart fromtherepetition codesandsimple
paritycodes.Forq>2,some MDS codescanbefound.
Asasimple example, hereisa(9;2)codeforthe8-ary erasure channel.
Thecodeisdened interms ofthemultiplication andaddition rulesofGF(8),
whicharegiveninAppendix C.1. Theelemen tsoftheinput alphab etare
f0;1;A;B;C;D;E;Fgandthegenerator matrix ofthecodeis
G="
101ABCDEF
011111111#
: (13.45)
Theresulting 64codewordsare:
000000000 011111111 0AAAAAAAA 0BBBBBBBB 0CCCCCCCC 0DDDDDDDD 0EEEEEEEE 0FFFFFFFF
101ABCDEF 110BADCFE 1AB01EFCD 1BA10FEDC 1CDEF01AB 1DCFE10BA 1EFCDAB01 1FEDCBA10
A0ACEB1FD A1BDFA0EC AA0EC1BDF AB1FD0ACE ACE0AFDB1 ADF1BECA0 AECA0DF1B AFDB1CE0A
B0BEDFC1A B1AFCED0B BA1CFDEB0 BB0DECFA1 BCFA1B0DE BDEB0A1CF BED0B1AFC BFC1A0BED
C0CBFEAD1 C1DAEFBC0 CAE1DC0FB CBF0CD1EA CC0FBAE1D CD1EABF0C CEAD10CBF CFBC01DAE
D0D1CAFBE D1C0DBEAF DAFBE0D1C DBEAF1C0D DC1D0EBFA DD0C1FAEB DEBFAC1D0 DFAEBD0C1
E0EF1DBAC E1FE0CABD EACDBF10E EBDCAE01F ECABD1FE0 EDBAC0EF1 EE01FBDCA EF10EACDB
F0FDA1ECB F1ECB0FDA FADF0BCE1 FBCE1ADF0 FCB1EDA0F FDA0FCB1E FE1BCF0AD FF0ADE1BC
Solution toexercise 13.14 (p.220).Quick, rough proofofthetheorem. Letx
denote thedierence betweenthereconstructed codewordandthetransmitted
codeword.Foranygivenchannel output r,there isaposterior distribution
overx.Thisposterior distribution ispositiveonlyonvectors xbelonging
tothecode;thesums thatfollowareovercodewordsx.Theblockerror
probabilit yis:
pB=X
x6=0P(xjr): (13.46)
Theaverage biterrorprobabilit y,averaging overallbitsinthecodeword,is:
pb=X
x6=0P(xjr)w(x)
N; (13.47)
wherew(x)istheweightofcodewordx.Nowtheweightsofthenon-zero
codewordssatisfy
1w(x)
Ndmin
N: (13.48)
Substituting theinequalities (13.48) intothedenitions (13.46, 13.47), weob-
tain:
pBpbdmin
NpB; (13.49)
whichisafactor oftwostronger, ontheright,thanthestated result (13.39).
Inmaking theproofwatertigh t,Ihaveweakenedtheresult alittle.
Carefulproof.Thetheorem relates theperformance oftheoptimal blockde-
codingalgorithm andtheoptimal bitwisedecodingalgorithm.
Weintroduce another pairofdecoding algorithms, called theblock-
guessing decoderandthebit-guessing decoder.Theideaisthatthese two

<<<PAGE 237>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
13.14: Solutions 225
algorithms aresimilar totheoptimal blockdecoderandtheoptimal bitwise
decoder,butlendthemselv esmore easily toanalysis.
Wenowdene these decoders. Letxdenote theinferred codeword.For
anygivencode:
Theoptimal blockdecoderreturns thecodewordxthatmaximizes the
posterior probabilit yP(xjr),whichisproportional tothelikelihood
P(rjx).
Theprobabilit yoferrorofthisdecoderiscalledpB.
Theoptimal bitdecoderreturns foreachoftheNbits,xn,the
valueofathatmaximizes theposterior probabilit yP(xn=ajr)=P
xP(xjr)
 [xn=a].
Theprobabilit yoferrorofthisdecoderiscalledpb.
Theblock-guessing decoderreturns arandom codewordxwithprobabil-
itydistribution givenbytheposterior probabilit yP(xjr).
Theprobabilit yoferrorofthisdecoderiscalledpG
B.
Thebit-guessing decoderreturns foreachoftheNbits,xn,arandom bit
fromtheprobabilit ydistribution P(xn=ajr).
Theprobabilit yoferrorofthisdecoderiscalledpG
b.
Thetheorem states thattheoptimal biterrorprobabilit ypbisbounded above
bypBandbelowbyagivenmultiple ofpB(13.39).
Theleft-hand inequalit yin(13.39) istrivially true{ifablockiscorrect, all
itsconstituen tbitsarecorrect; soiftheoptimal blockdecoderoutperformed
theoptimal bitdecoder,wecould makeabetterbitdecoderfromtheblock
decoder.
Weprovetheright-hand inequalit ybyestablishing that:
(a)thebit-guessing decoderisnearly asgoodastheoptimal bitdecoder:
pG
b2pb: (13.50)
(b)thebit-guessing decoder's error probabilit yisrelated totheblock-
guessing decoder'sby
pG
bdmin
NpG
B: (13.51)
Then sincepG
BpB,wehave
pb>1
2pG
b1
2dmin
NpG
B1
2dmin
NpB: (13.52)
Wenowprovethetwolemmas.
Near-optimalit yofguessing: Consider rstthecaseofasingle bit,withposterior
probabilit yfp0;p1g.Theoptimal bitdecoderhasprobabilit yoferror
Poptimal=min(p0;p1): (13.53)
Theguessing decoderpicksfrom0and1.Thetruth isalsodistributed with
thesameprobabilit y.Theprobabilit ythattheguesser andthetruth matchis
p2
0+p2
1;theprobabilit ythattheymismatc histheguessing errorprobabilit y,
Pguess=2p0p12min(p0;p1)=2Poptimal: (13.54)

<<<PAGE 238>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
226 13|Binary Codes
SincepG
bistheaverage ofmanysucherrorprobabilities, Pguess,andpbisthe
average ofthecoresp onding optimal error probabilities, Poptimal,weobtain
thedesired relationship (13.50) betweenpG
bandpb. 2
Relationship betweenbiterrorprobabilit yandblockerrorprobabilit y:Thebit-
guessing andblock-guessing decoderscanbecombined inasingle system: we
candrawasamplexnfromthemarginal distribution P(xnjr)bydrawing
asample (xn;x)fromthejointdistribution P(xn;xjr),thendiscarding the
valueofx.
Wecandistinguish betweentwocases: thediscarded valueofxisthe
correct codeword,ornot. Theprobabilit yofbiterror forthebit-guessing
decodercanthenbewritten asasumoftwoterms:
pG
b=P(xcorrect )P(biterrorjxcorrect )
+P(xincorrect )P(biterrorjxincorrect )
=0+pG
BP(biterrorjxincorrect ):
Now,whenev ertheguessed xisincorrect, thetruexmustdier fromitinat
leastdbits,sotheprobabilit yofbiterrorinthese cases isatleastd=N.So
pG
bd
NpG
B:
QED. 2
Solution toexercise 13.20 (p.222).Thenumberof`typical' noise vectors n
isroughly 2NH2(f).Thenumberofdistinct syndromes zis2M.Soreliable
comm unication implies
MNH2(f); (13.55)
or,interms oftherateR=1 M=N,
R1 H2(f); (13.56)
aboundwhichagrees precisely withthecapacit yofthechannel.
Thisargumen tisturned intoaproofinthefollowingchapter.
Solution toexercise 13.24 (p.222).Inthethree-pla yercase,itispossible for
thegroup towinthree-quarters ofthetime.
Three-quarters ofthetime, twooftheplayerswillhavehatsofthesame
colour andthethird player'shatwillbetheoppositecolour. Thegroup can
wineverytimethishappensbyusing thefollowingstrategy .Eachplayerlooks
attheother twoplayers'hats. Ifthetwohatsaredierentcolours, hepasses.
Iftheyarethesame colour, theplayerguesses hisownhatistheopposite
colour.
Thisway,everytimethehatcolours aredistributed twoandone,one
playerwillguess correctly andtheothers willpass,andthegroup willwinthe
game. When allthehatsarethesame colour, however,allthreeplayerswill
guess incorrectly andthegroup willlose.
When anyparticular playerguesses acolour, itistruethatthere isonlya
50:50 chance thattheirguess isright.Thereason thatthegroup wins75%of
thetimeisthattheirstrategy ensures thatwhen playersareguessing wrong,
agreat manyareguessing wrong.
Forlarger numbersofplayers,theaimistoensure thatmostofthetime
nooneiswrong andoccasionally everyoneiswrong atonce. Inthegame with
7players,there isastrategy forwhichthegroup wins7outofevery8times

<<<PAGE 239>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
13.14: Solutions 227
theyplay.Inthegame with15players,thegroup canwin15outof16times.
Ifyouhavenotgured outthese winning strategies forteams of7and15,
Irecommend thinking aboutthesolution tothethree-pla yergame interms
ofthelocations ofthewinning andlosing states onthethree-dimensional
hypercube,andthinking laterally .
Ifthenumberofplayers,N,is2r 1,theoptimal strategy canbedened
using aHamming codeoflengthN,andtheprobabilit yofwinning theprize
isN=(N+1).Eachplayerisidentied withanumbern21:::N.Thetwo
colours aremappedonto0and1.Anystateoftheirhatscanbeviewedasa
receivedvector outofabinary channel. Arandom binary vector oflengthN
iseither acodewordoftheHamming code,withprobabilit y1=(N+1),orit
diers inexactly onebitfromacodeword.Eachplayerlooksatalltheother
bitsandconsiders whether hisbitcanbesettoacolour suchthatthestateis
acodeword(whichcanbededuced using thedecoderoftheHamming code).
Ifitcan,thentheplayerguesses thathishatistheother colour. Ifthestateis
actually acodeword,allplayerswillguess andwillguess wrong. Ifthestateis
anon-co deword,onlyoneplayerwillguess, andhisguess willbecorrect. It's
quite easytotrain sevenplayerstofollowtheoptimal strategy ifthecyclic
represen tation ofthe(7;4)Hamming codeisused(p.19).

<<<PAGE 240>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
AboutChapter 14
Inthischapter wewilldrawtogether severalideas thatwe'veencoun tered
sofarinoneniceshort proof.Wewillsimultaneously provebothShannon's
noisy-c hannel codingtheorem (forsymmetric binary channels) andhissource
codingtheorem (forbinary sources). While thisproofhasconnections tomany
preceding chapters inthebook,it'snotessentialtohavereadthem all.
Onthenoisy-c hannel codingside,ourproofwillbemoreconstructiv ethan
theproofgiveninChapter 10;there, weprovedthatalmost anyrandom code
is`verygood'.Herewewillshowthatalmost anylinearcodeisverygood.We
willmakeuseoftheideaoftypical sets(Chapters 4and10),andwe'llborrow
fromtheprevious chapter's calculation oftheweightenumerator function of
random linear codes(section 13.5).
Onthesource codingside,ourproofwillshowthatrandom linearhash
functions canbeusedforcompression ofcompressible binary sources, thus
giving alinktoChapter 12.
228

<<<PAGE 241>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
14
VeryGoodLinear CodesExist
Inthischapter we'lluseasingle calculation toprovesimultaneously thesource
codingtheorem andthenoisy-c hannel codingtheorem forthebinary symmet-
ricchannel.
Inciden tally,thisproofworksformuchmore general channel models,not
onlythebinary symmetric channel. Forexample, theproofcanbereworked
forchannels withnon-binary outputs, fortime-v arying channels andforchan-
nelswithmemory ,aslongastheyhavebinary inputs satisfying asymmetry
property,c.f.section 10.6.
14.1 Asimultaneous proofofthesource codingandnoisy-c hannel
codingtheorems
Weconsider alinear error-correcting codewithbinary parity-checkmatrix H.
Thematrix hasMrowsandNcolumns. Later intheproofwewillincrease
NandM,keepingM/N.Therateofthecodesatises
R1 M
N: (14.1)
IfalltherowsofHareindependen tthenthisisanequalit y,R=1 M=N.
Inwhatfollows,we'llassume theequalit yholds. Eager readers mayworkout
theexpected rankofarandom binary matrix H(it'sveryclosetoM)and
pursue theeect thatthedierence (M rank)hasontherestofthisproof
(it'snegligible).
Acodewordtisselected, satisfying
Ht=0mod2; (14.2)
andabinary symmetric channel addsnoisex,giving thereceivedsignal Inthischapter xdenotes thenoise
added bythechannel, nottheinput
tothechannel. r=t+xmod2: (14.3)
Thereceiveraimstoinferbothtandxfromrusing asyndrome decoding
approac h.Syndrome decodingwasrstintroduced insection 1.2(p.10and
11).Thereceivercomputes thesyndrome
z=Hrmod2=Ht+Hxmod2=Hxmod2: (14.4)
Thesyndrome onlydependsonthenoisex,andthedecodingproblem isto
ndthemostprobable xthatsatises
Hx=zmod2: (14.5)
229

<<<PAGE 242>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
230 14|VeryGoodLinear CodesExist
Thisbestestimate forthenoisevector, ^x,isthensubtracted fromrtogivethe
bestguess fort.Ouraimistoshowthat,aslongasR<1 H(X)=1 H2(f),
wherefistheipprobabilit yofthebinary symmetric channel, theoptimal
decoderforthissyndrome decodingproblem hasvanishing probabilit yoferror,
asNincreases, forrandom H.
Weprovethisresult bystudying asub-optimal strategy forsolving the
decodingproblem. Neither theoptimal decodernorthistypicalsetdecoder
wouldbeeasytoimplemen t,butthetypical setdecoderiseasier toanalyze.
Thetypical setdecoderexamines thetypical setTofnoise vectors, theset
ofnoise vectors x0thatsatisfy log1/P(x0)'NH(X),checkingtoseeifanyof We'llleaveoutthesandsthat
makeatypical setdenition rigorous.
Enthusiasts areencouraged torevisit
section 4.4andputthese details into
thisproof.those typical vectors x0satises theobserv edsyndrome,
Hx0=z: (14.6)
Ifexactly onetypical vectorx0doesso,thetypical setdecoderreportsthat
vector asthehypothesized noise vector. Ifnotypical vector matchesthe
observ edsyndrome, ormorethanonedoes,thenthetypical setdecoderreports
anerror.
Theprobabilit yoferrorofthetypical setdecoder,foragivenmatrix H,
canbewritten asasumoftwoterms,
PTSjH=P(I)+P(II)
TSjH; (14.7)
whereP(I)istheprobabilit ythatthetruenoise vectorxisitselfnottypical,
andP(II)
TSjHistheprobabilit ythatthetruexistypical andatleastoneother
typical vectorclashes withit.Therstprobabilit yvanishes asNincreases, as
weprovedwhen werststudied typical sets(Chapter 4).Weconcen trateon
thesecond probabilit y.Torecap, we'reimagining atruenoise vector,x;and
ifanyofthetypical noisevectors x0,dieren tfromx,satises H(x0 x)=0,
thenwehaveanerror. Weusethetruth function H(x0 x)=0; (14.8)
whose valueisoneifthestatemen tH(x0 x)=0istrueandzerootherwise.
WecanboundthenumberoftypeIIerrors made when thenoise isxthus:
[Numberoferrors givenxandH]X
x0:x02T
x06=x
 H(x0 x)=0: (14.9)
Thenumberoferrors iseither zeroorone;thesumontheright-hand side
mayexceed one,incases where severaltypical noise vectors havethesame
syndrome.
Wecannowwritedowntheprobabilit yofatype-IIerrorbyaveraging over
x:
P(II)
TSjHX
x2TP(x)X
x0:x02T
x06=x
 H(x0 x)=0: (14.10)
Now,wewillndtheaverage ofthisprobabilit yoftype-IIerroroveralllinear
codesbyaveraging overH.Byshowingthattheaverageprobabilit yoftype-II
errorvanishes, wewillthusshowthatthere existlinear codeswithvanishing
errorprobabilit y,indeed, thatalmost alllinear codesareverygood.
Wedenote averaging overallbinary matrices Hbyh:::iH.Theaverage
probabilit yoftype-IIerroris
P(II)
TS=X
HP(H)P(II)
TSjH=D
P(II)
TSjHE
H(14.11)

<<<PAGE 243>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
14.2: Data compression bylinear hashcodes 231
=*X
x2TP(x)X
x0:x02T
x06=x
 H(x0 x)=0+
H(14.12)
=X
x2TP(x)X
x0:x02T
x06=x
 H(x0 x)=0
H: (14.13)
Now,thequantityh
 [H(x0 x)=0]iHalready croppedupwhen wewere
calculating theexpected weightenumerator function ofrandom linear codes
(section 13.5): foranynon-zero binary vectorv,theprobabilit ythatHv=0,
averaging overallmatrices H,is2 M.So
P(II)
TS= X
x2TP(x)!
(jTj 1)2 M(14.14)
jTj2 M; (14.15)
wherejTjdenotes thesizeofthetypical set.Asyouwillrecall fromChapter
4,there areroughly 2NH(X)noise vectors inthetypical set.So
P(II)
TS2NH(X)2 M: (14.16)
Thisboundontheprobabilit yoferroreither vanishes orgrowsexponentially
asNincreases (remem bering thatwearekeepingMproportional toNasN
increases). Itvanishes if
H(X)<M=N: (14.17)
Substituting R=1 M=N,wehavethusestablished thenoisy-c hannel coding
theorem forthebinary symmetric channel: verygoodlinear codesexistfor
anyrateRsatisfying
R<1 H(X); (14.18)
whereH(X)istheentropyofthechannel noise, perbit. 2
Exercise 14.1.[3]Redo theproofforamore general channel.
14.2 Datacompression bylinear hashcodes
Thedecodinggame wehavejustplayedcanalsobeviewedasanuncompres-
siongame. Theworldproduces anoise vectorxfromasourceP(x).The
noise hasredundancy (iftheipprobabilit yisnot0.5).Wecompress itwith
alinear compressor thatmaps theN-bitinputx(thenoise) totheM-bit
output z(thesyndrome). Ouruncompression taskistorecovertheinputx
fromtheoutput z.Therateofthecompressor is
RcompressorM=N: (14.19)
[Wedon't careaboutthepossibilit yoflinear redundancies inourdenition
oftherate,here.] Theresult thatwejustfound, thatthedecodingproblem
canbesolved,foralmost anyH,withvanishing errorprobabilit y,aslongas
H(X)<M=N,thusinstan tlyprovesasource codingtheorem:
Givenabinary sourceXofentropyH(X),andarequired com-
pressed rateR>H(X),there exists alinear compressor x!z=
Hxmod2havingrateM=Nequal tothatrequired rateR,andan
associated uncompressor, thatisvirtually lossless.
Thistheorem istruenotonlyforasource ofindependen tidentically dis-
tributed symbolsbutalsoforanysource forwhichatypical setcanbede-
ned: sources withmemory ,andtime-v arying sources, forexample; allthat's
required isthatthesource beergodic.

<<<PAGE 244>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
232 14|VeryGoodLinear CodesExist
Notes
Thismetho dforprovingthatcodesaregoodcanbeapplied toother linear
codes,suchaslow-densit yparity-checkcodes(MacKa y,1999b; Ajietal.,2000).
Foreachcodeweneedanapproximation ofitsexpected weightenumerator
function.

<<<PAGE 245>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
15
Further Exercises onInformation Theory
Themost exciting exercises, whichwillintroduceyoutofurther ideas inin-
formation theory ,aretowardstheendofthischapter.
Refresherexercisesonsourcecodingandnoisychannels
.Exercise 15.1.[2]LetXbeanensem blewithAX=f0;1gandPX=
f0:995;0:005g.Consider source codingusing theblockcodingofX100
where everyx2X100containing 3orfewer1sisassigned adistinct
codeword,while theotherxsareignored.
(a)Iftheassigned codewordsareallofthesame length, ndthemin-
imumlength required toprovidetheabovesetwithdistinct code-
words.
(b)Calculate theprobabilit yofgetting anxthatwillbeignored.
.Exercise 15.2.[2]LetXbeanensem blewithPX=f0:1;0:2;0:3;0:4g.Theen-
sembleisencodedusing thesymbolcodeC=f0001;001;01;1g.Consider
thecodewordcorresp onding tox2XN,whereNislarge.
(a)Compute theentropyofthefourth bitoftransmission.
(b)Compute theconditional entropyofthefourth bitgiventhethird
bit.
(c)Estimate theentropyofthehundredth bit.
(d)Estimate theconditional entropyofthehundredth bitgiventhe
ninety-ninthbit.
Exercise 15.3.[2]Twofairdicearerolled byAlice andthesumisrecorded.
Bob's taskistoaskasequence ofquestions withyes/no answerstond
outthisnumber.Devise indetail astrategy thatachievestheminim um
possible average numberofquestions.
.Exercise 15.4.[2]Howcanyouuseacointodrawstrawsamong 3people?
.Exercise 15.5.[2]Inamagic trick,there arethree participan ts:themagician,
anassistan t,andavolunteer.Theassistan t,whoclaims tohaveparanor-
malabilities, isinasoundpro ofroom.Themagician givesthevolunteer
sixblank cards, vewhite andoneblue. Thevolunteerwrites adif-
ferentinteger from1to100oneachcard, asthemagician iswatching.
Thevolunteerkeepsthebluecard. Themagician arranges thevewhite
cards insomeorder andpasses themtotheassistan t.Theassistan tthen
announces thenumberonthebluecard.
Howdoesthetrickwork?
233

<<<PAGE 246>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
234 15|Further Exercises onInformation Theory
.Exercise 15.6.[3]Howdoesthistrickwork?
`Here's anordinary packofcards, shued intorandom order.
Please choosevecards fromthepack,anythatyouwish.
Don't letmeseetheirfaces. No,don't givethem tome:pass
them tomyassistan tEsmerelda. Shecanlookatthem.
`Now,Esmerelda, showmefourofthecards. Hmm:::nine
ofspades, sixofclubs, fourofhearts, tenofdiamonds. The
hidden card,then, mustbethequeen ofspades!'
Thetrickcanbeperformed asdescrib edaboveforapackof52cards.
Useinformation theory togiveanupperboundonthenumberofcards
forwhichthetrickcanbeperformed.
.Exercise 15.7.[2]Find aprobabilit ysequence p=(p1;p2;:::)suchthat
H(p)=1.
.Exercise 15.8.[2]Consider adiscrete memoryless source withAX=fa;b;c;dg
andPX=f1=2;1=4;1=8;1=8g.There are48=65536eight-letter words
thatcanbeformed fromthefourletters. Findthetotalnumberofsuch
wordsthatareinthetypical setTN(equation 4.29) whereN=8and
=0:1.
.Exercise 15.9.[2]Consider thesourceAS=fa;b;c;d;eg,PS=
f1/3;1/3;1/9;1/9;1/9gandthechannel whose transition probabilit ymatrix
is
Q=2
66641000
002/30
0101
001/303
7775: (15.1)
Notethatthesource alphab ethasvesymbols,butthechannel alphab et
AX=AY=f0;1;2;3ghasonlyfour.Assume thatthesource produces
symbolsatexactly 3/4theratethatthechannel accepts channel sym-
bols.Foragiven(tiny)>0,explain howyouwoulddesign asystem
forcomm unicating thesource's output overthechannel withanaver-
ageerror probabilit ypersource symbollessthan.Beasexplicit as
possible. Inparticular, donotinvokeShannon's noisy-c hannel coding
theorem.
.Exercise 15.10.[2]Consider abinary symmetric channel andacodeC=
f0000;0011;1100;1111g;assume thatthefourcodewordsareusedwith
probabilitiesf1=2;1=8;1=8;1=4g.
What isthedecodingrulethatminimizes theprobabilit yofdecoding
error? [Theoptimal decodingruledependsonthenoise levelfofthe
binary symmetric channel. Givethedecodingruleforeachrange of
values off,forfbetween0and1=2.]
Exercise 15.11.[2]Find thecapacit yandoptimal input distribution forthe
three-input, three-output channel whose transition probabilities are:
Q=2
64100
02/31/3
01/32/33
75: (15.2)

<<<PAGE 247>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
15|Further Exercises onInformation Theory 235
Exercise 15.12.[3,p.239]Theinput toachannelQisawordof8bits. The
output isalsoawordof8bits.Eachtimeitisused, thechannel ips
exactly oneofthetransmitted bits,butthereceiverdoesnotknowwhich
one. Theother sevenbitsarereceivedwithout error. All8bitsare
equally likelytobetheonethatisipped.Derivethecapacit yofthis
channel.
Show,bydescribing anexplicit encoderanddecoderthatitispossible
reliably (that is,withzeroerrorprobabilit y)tocomm unicate 5bitsper
cycleoverthischannel.
.Exercise 15.13.[2]Achannel withinputx2fa;b;cgandoutputy2fr;s;t;ug
hasconditional probabilit ymatrix:
Q=2
66641/200
1/21/20
01/21/2
001/23
7775:
cba
***
HHjHHjHHj
utsr
What isitscapacit y?
.Exercise 15.14.[3]Theten-digit numberonthecoverofabookknownasthe
ISBN incorp orates anerror-detecting code.Thenumberconsists ofnine
0-521-64298-1
1-010-00000-4
Table15.1.Some validISBNs.
[Thehyphens areincluded for
legibilit y.]source digitsx1;x2;:::;x9,satisfyingxn2f0;1;:::;9g,andatenth
checkdigitwhose valueisgivenby
x10= 9X
n=1nxn!
mod11:
Herex102f0;1;:::;9;10g:Ifx10=10thenthetenthdigitisshown
using theroman numeral X.
ShowthatavalidISBN satises:
 10X
n=1nxn!
mod11=0:
Imagine thatanISBN iscomm unicated overanunreliable human chan-
nelwhichsometimes modies digits andsometimes reordersdigits.
Showthatthiscodecanbeusedtodetect (butnotcorrect) allerrors in
whichanyoneofthetendigits ismodied (forexample, 1-010-00000-4
!1-010-00080-4).
Showthatthiscodecanbeusedtodetect allerrors inwhichanytwoad-
jacentdigits aretransp osed(forexample, 1-010-00000-4!1-100-00000-
4).
What other transp ositions ofpairs ofnon-adjac entdigits canbede-
tected?
Ifthetenthdigitweredened tobe
x10= 9X
n=1nxn!
mod10;
whywouldthecodenotworksowell?(Discuss thedetection ofboth
modications ofsingle digits andtransp ositions ofdigits.)

<<<PAGE 248>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
236 15|Further Exercises onInformation Theory
Exercise 15.15.[3]Achannel withinputxandoutputyhastransition proba-
dcba
--
--
  
  @@R@@R
dcbabilitymatrix:
Q=2
66641 ff 0 0
f1 f0 0
0 01 gg
0 0g1 g3
7775:
Assuming aninput distribution oftheform
PX=p
2;p
2;1 p
2;1 p
2
;
writedowntheentropyoftheoutput,H(Y),andtheconditional entropy
oftheoutput giventheinput,H(YjX).
Showthattheoptimal input distribution isgivenby
p=1
1+2 H2(g)+H2(f);
whereH2(f)=flog21
f+(1 f)log21
(1 f). Remem berd
dpH2(p)=log21 p
p.
Writedowntheoptimal input distribution andthecapacit yofthechan-
nelinthecasef=1=2,g=0,andcommen tonyouranswer.
.Exercise 15.16.[2]What arethedierences intheredundancies needed inan
error-detecting code(whichcanreliably detect thatablockofdatahas
beencorrupted) andanerror-correcting code(whichcandetect andcor-
recterrors)?
Further talesfrominformation theory
Thefollowingexercises giveyouthechance todiscoverforyourself theanswers
tosome more surprising results ofinformation theory .
Exercise 15.17.[3]Communication ofcorrelated information. Imagine thatwe
wanttocomm unicate datafromtwodatasourcesX(A)andX(B)toacentral
location Cvianoise-free one-w aycomm unication channels (gure 15.2a). The
signalsx(A)andx(B)arestrongly correlated, sotheirjointinformation content
isonlyalittlegreater thanthemarginal information contentofeither ofthem.
Forexample, Cisaweather collator whowishes toreceiveastring ofreports
sayingwhether itisraining inAllerton (x(A))andwhether itisraining in
Bognor (x(B)).Thejointprobabilit yofx(A)andx(B)mightbe
P(x(A);x(B)):x(A)
01
x(B)00.490.01
10.010.49 (15.3)
Theweather collator wouldliketoknowNsuccessiv evaluesofx(A)andx(B)
exactly ,but,since hehastopayforeverybitofinformation hereceives,he
isinterested inthepossibilit yofavoiding buyingNbitsfromsourceAand
NbitsfromsourceB.Assuming thatvariablesx(A)andx(B)aregenerated
repeatedly fromthisdistribution, cantheybeencodedatratesRAandRBin
suchawaythatCcanreconstruct allthevariables, withthesumofinformation
transmission ratesonthetwolinesbeinglessthantwobitspercycle?

<<<PAGE 249>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
15|Further Exercises onInformation Theory 237
(a)x(A)
x(B)-encode
RA
-encode
RBt(A)
t(B)HHHj
*C
(b)
                                                                                                                                                                                                                                                                                                                                                    
Achievable
H(X(B)jX(A))H(X(B))H(X(A);X(B))RB
RA H(X(A)jX(B))H(X(A))Figure 15.2.Comm unication of
correlated information. (a)x(A)
andx(B)arecorrelated sources
(thecorrelation isrepresen tedby
thedotted arrow).Strings of
valuesofeachvariable are
encodedusing codesofrateRA
andRBintotransmissions t(A)
andt(B),whicharecomm unicated
overnoise-free channels toa
receiv erC.(b)Theachievable
rateregion. Bothstrings canbe
conveyedwithout erroreven
thoughRA<H(X(A))and
RB<H(X(B)).Theanswer,whichyoushould demonstrate, isindicated ingure 15.2.
Inthegeneral caseoftwocorrelated sourcesX(A)andX(B),there exist
codesforthetwotransmitters thatcanachievereliable comm unication of
bothX(A)andX(B)toC,aslongas:theinformation ratefromX(A),
RA,exceedsH(X(A)jX(B));theinformation ratefromX(B),RB,exceeds
H(X(B)jX(A));andthetotal information rateRA+RBexceeds thejoint
information H(X(A);X(B)).
Sointhecaseofx(A)andx(B)above,eachtransmitter musttransmit at
arategreater thanH2(0:02)=0:14bits,andthetotalrateRA+RBmust
begreater than1.14bits,forexampleRA=0:6,RB=0:6.There existcodes
thatcanachievethese rates. Yourtaskistogure outwhythisisso.
Trytondanexplicit solution inwhichoneofthesources issentasplain
text,t(B)=x(B),andtheother isencoded.
Exercise 15.18.[3]Multiple access channels .Consider achannel withtwosets
ofinputs andoneoutput {forexample, ashared telephone line(gure 15.3a).
Asimple modelsystem hastwobinary inputsx(A)andx(B)andaternary
outputyequal tothearithmetic sumofthetwoinputs, that's 0,1or2.There
isnonoise. UsersAandBcannot comm unicate witheachother, andthey
cannot heartheoutput ofthechannel. Iftheoutput isa0,thereceivercan
becertain thatbothinputs weresetto0;andiftheoutput isa2,thereceiver
canbecertain thatbothinputs weresetto1.Butiftheoutput is1,then
itcould bethattheinput statewas(0;1)or(1;0).Howshould usersAand
Busethischannel sothattheirmessages canbededuced fromthereceived
signals? HowfastcanAandBcomm unicate?
Clearly thetotal information ratefromAandBtothereceivercannot
betwobits.Ontheother hand itiseasytoachieveatotalinformation rate
RA+RBofonebit.Canreliable comm unication beachievedatrates(RA;RB)
suchthatRA+RB>1?
Theanswerisindicated ingure 15.3.
Some practical codesformulti-user channels arepresen tedinRatzer and
MacKa y(2003).
Exercise 15.19.[3]Broadcast channels .Abroadcast channel consists ofasingle
transmitter andtwoormore receivers.Theproperties ofthechannel arede-
nedbyaconditional distribution Q(y(A);y(B)jx).(We'llassume thechannel
ismemoryless.) Thetaskistoaddanencoderandtwodecoderstoenable
xy(A)
y(B)HHj*
Figure 15.4.Thebroadcast
channel.xisthechannel input;
y(A)andy(B)aretheoutputs.reliable comm unication ofacommon message atrateR0tobothreceivers,an
individual message atrateRAtoreceiverA,andanindividual message atrate
RBtoreceiverB.Thecapacit yregion ofthebroadcast channel istheconvex
hullofthesetofachievableratetriplets (R0;RA;RB).
Asimple benchmark forsuchachannel isgivenbytime-sharing (time-
division signaling). Ifthecapacities ofthetwochannels, considered separately ,

<<<PAGE 250>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
238 15|Further Exercises onInformation Theory
(a)x(A)
x(B)-
-P(yjx(A);x(B)) y-
(b)y:x(A)
01
x(B)001
112 (c)
1=21=2
11
RARB
AchievableFigure 15.3.Multiple access
channels. (a)Ageneral multiple
access channel withtwo
transmitters andonereceiv er.(b)
Abinary multiple access channel
withoutput equal tothesumof
twoinputs. (c)Theachievable
region.
areC(A)andC(B),thenbydevoting afractionAofthetransmission time
tochannelAandB=1 Atochannel B,wecanachieve(R0;RA;RB)=
(0;AC(A);BC(B)).
-6
@
@
@
@@C(B)RB
RA C(A)
Figure 15.5.Rates achievableby
simple timesharing.Wecandobetterthanthis,however.Asananalogy ,imagine speaking
simultaneously toanAmerican andaBelarusian; youareuentinAmerican
andinBelarusian, butneither ofyourtworeceiversunderstands theother's
language. Ifeachreceivercandistinguish whether awordisintheir own
language ornot,thenanextra binary lecanbeconveyedtobothrecipien tsby
using itsbitstodecide whether thenexttransmitted wordshould befromthe
American source textorfromtheBelarusian source text. Eachrecipien tcan
concatenate thewordsthattheyunderstand inorder toreceivetheirpersonal
message, andcanalsorecoverthebinary string.
Anexample ofabroadcast channel consists oftwobinary symmetric chan-
nelswithacommon input. Thetwohalvesofthechannel haveipprob-
abilitiesfAandfB.We'llassume thatAhasthebetter half-channel, i.e.,
fA<fB<1/2.[Aclosely related channel isa`degraded' broadcast channel,
inwhichtheconditional probabilities aresuchthattherandom variables have
thestructure ofaMarkovchain,
x!y(A)!y(B); (15.4)
i.e.,y(B)isafurther degraded version ofy(A).]Inthisspecialcase,itturns
outthatwhatev erinformation isgetting through toreceiverBcanalsobe
recoveredbyreceiverA.Sothere isnopointdistinguishing betweenR0and
RB:thetaskistondthecapacit yregion fortheratepair(R0;RA),where
R0istherateofinformation reachingbothAandB,andRAistherateof
theextra information reachingA.
Thefollowing exercise isequivalenttothisone,andasolution toitis
illustrated ingure 15.8.
Exercise 15.20.[3]Variable-rate error-correcting codesforchannels withunkno wn
noise level.Inreallife,channels maysometimes notbewellcharacterizedfAfBA
BCCR
f
Figure 15.6.Rateofreliable
comm unicationR,asafunction of
noiselevelf,forShannonesque
codesdesigned tooperateatnoise
levelsfA(solid line)andfB
(dashed line).
beforetheencoderisinstalled. Asamodelofthissituation, imagine thata
channel isknowntobeabinary symmetric channel withnoise leveleitherfA
orfB.LetfB>fA,andletthetwocapacities beCAandCB.
Those wholiketolivedangerously mightinstall asystem designed fornoise
levelfAwithrateRA'CA;intheeventthatthenoise levelturns outtobe
fB,ourexperience ofShannon's theories wouldleadustoexpectthatthere

<<<PAGE 251>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
15|Further Exercises onInformation Theory 239
wouldbeacatastrophic failure tocomm unicate information reliably (solid line
ingure 15.6).
fAfBA
BCCR
f
Figure 15.7.Rateofreliable
comm unicationR,asafunction of
noiselevelf,foradesired
variable-rate code.Aconserv ativeapproac hwoulddesign theencodingsystem fortheworst-
casescenario, installing acodewithrateRB'CB(dashed lineingure 15.6).
Intheeventthatthelowernoise level,fA,holds true,themanagers would
haveafeeling ofregret because ofthewasted capacit ydierenceCA RB.
Isitpossible tocreate asystem thatnotonlytransmits reliably atsome
rateR0whatev erthenoise level,butalsocomm unicates some extra, `lower-
priorit y'bitsifthenoise levelislow,asshowningure 15.7? Thiscode
comm unicates thehigh-priorit ybitsreliably atallnoiselevelsbetweenfAand
fB,andcomm unicates thelow-priorit ybitsalsoifthenoise levelisfAor
below.
Thisproblem ismathematically equivalenttotheprevious problem, the
degraded broadcast channel. Thelowerrateofcomm unication wastherecalled
R0,andtherateatwhichthelow-priorit ybitsarecomm unicated ifthenoise
levelislowwascalledRA.
00.20.40.6
00.20.40.60.81
Figure 15.8.Anachievableregion
forthechannel withunkno wn
noiselevel.Assuming thetwo
possible noise levelsarefA=0:01
andfB=0:1,thedashed lines
showtheratesRA;RBthatare
achievableusing asimple
time-sharing approac h,andthe
solidlineshowsratesachievable
using amorecunning approac h.Anillustrativ eanswerisshowningure 15.8,forthecasefA=0:01and
fB=0:1.(This gure alsoshowstheachievableregion forabroadcast channel
whose twohalf-channels havenoise levelsfA=0:01andfB=0:1.)Iadmit I
ndthegapbetweenthesimple time-sharing solution andthecunning solution
disapp ointingly small.
InChapter 50wewilldiscuss codesforaspecialclassofbroadcast channels,
namely erasure channels, where everysymboliseither receivedwithout error
orerased. These codeshavethenicepropertythattheyarerateless {the
numberofsymbolstransmitted isdetermined ontheysuchthatreliable
comunication isachieved,whatev ertheerasure statistics ofthechannel.
Exercise 15.21.[3]Multiterminal information networksarebothimportantpracti-
callyandintriguing theoretically .Consider thefollowingexample ofatwo-way
binary channel (gure 15.9a,b): twopeoplebothwishtotalkoverthechannel,
andtheybothwanttohearwhattheother person issaying;butyoucanonly
hearthesignal transmitted bytheother person ifyouaretransmitting azero.
What simultaneous information rates fromAtoBandfromBtoAcanbe
achieved,andhow?Everydayexamples ofsuchnetworksinclude theVHF
channels usedbyships, andcomputer ethernet networks(inwhichallthe
devices areunable tohearanything iftwoormore devices arebroadcasting
simultaneously).
Obviously ,wecanachieverates of1/2inbothdirections bysimple time-
sharing. Butcanthetwoinformation rates bemade larger? Finding the
capacit yofageneral two-waychannel isstillanopenproblem. However,
wecanobtain interesting results concerning achievablepointsforthesimple
binary channel discussed above,asindicated ingure 15.9c. There existcodes
thatcanachieverates uptotheboundary shown.There mayexistbetter
codestoo.
Solutions
Solution toexercise 15.12 (p.235).C(Q)=5bits.
Hintforthelastpart: asolution exists thatinvolvesasimple (8;5)code.

<<<PAGE 252>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
240 15|Further Exercises onInformation Theory
(a)x(A)
y(A)-
P(y(A);y(B)jx(A);x(B))y(B)
x(B)-

(b)y(A):x(A)
01
x(B)000
110y(B):x(A)
01
x(B)001
100
(c)00.20.40.60.81
00.20.40.60.81R(B)
R(A)AchievableFigure 15.9.(a)Ageneral
two-waychannel. (b)Therules
forabinary two-waychannel.
Thetwotables showtheoutputs
y(A)andy(B)thatresult foreach
stateoftheinputs. (c)Achievable
region forthetwo-waybinary
channel. Rates belowthesolid
lineareachievable.Thedotted
lineshowsthe`obviously
achievable'region whichcanbe
attained bysimple time-sharing.

<<<PAGE 253>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
16
Message Passing
Oneofthethemes ofthisbookistheideaofdoing complicated calculations
using simple distributed hardw are.Itturns outthatquite afewinteresting
problems canbesolvedbymessage-passing algorithms, inwhichsimple mes-
sages arepassed locallyamong simple processors whose operations lead,after
some time, tothesolution ofaglobal problem.
16.1 Counting
Asanexample, consider alineofsoldiers walking inthemist. Thecommander
wishes toperform thecomplex calculation ofcountingthenumberofsoldiers
intheline.Thisproblem could besolvedintwoways.
Firstthere isasolution thatusesexpensivehardw are:theloudbooming
voicesofthecommander andhismen. Thecommander could shout `allsoldiers
reportbacktomewithin oneminute!', thenhecould listen carefully asthe
menrespond`Molesw orthheresir!',`Fotherington{Thomas heresir!',andso
on.Thissolution relies onseveralexpensivepieces ofhardw are:theremustbe
areliable comm unication channel toandfromeverysoldier; thecommander
mustbeabletolisten toalltheincoming messages {evenwhen there are
hundreds ofsoldiers {andmustbeabletocount;andallthesoldiers mustbe
well-fed iftheyaretobeabletoshout backacross thepossibly-large distance
separating them fromthecommander.
Thesecond wayofnding thisglobal function, thenumberofsoldiers,
doesnotrequire global comm unication hardw are,highIQ,orgoodfood;we
simply require thateachsoldier cancomm unicate single integers withthetwo
adjacen tsoldiers intheline,andthatthesoldiers arecapable ofadding one
toanumber.Eachsoldier followsthese rules:
1.Ifyouarethefrontsoldier intheline,saythenumber`one'tothe
soldier behind you.
2.Ifyouaretherearmost soldier intheline,saythenumber`one'to
thesoldier infrontofyou.
3.Ifasoldier ahead oforbehind yousaysanumbertoyou,addone
toit,andsaythenewnumbertothesoldier ontheother side.Algorithm 16.1.Message-passing
rule-set A.
Iftheclevercommander cannotonlyaddonetoanumber,butalsoadd
twonumberstogether, thenhecanndtheglobal numberofsoldiers bysimply
adding together:
241

<<<PAGE 254>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
242 16|Message Passing
thenumbersaidtohimbythe
soldier infrontofhim,(whichequals thetotalnumberof
soldiers infront)
+thenumbersaidtothecom-
mander bythesoldier behind
him,(whichisthenumberbehind)
+one (tocountthecommander himself ).
Thissolution requires onlylocalcomm unication hardw areandsimple compu-
tations (storage andaddition ofintegers).
Commander41
32
23
14 Figure 16.2.Alineofsoldiers
countingthemselv esusing
message-passing rule-set A.The
commander canadd`3'fromthe
soldier infront,`1'fromthe
soldier behind, and`1'forhimself,
anddeduce thatthere are5
soldiers intotal.
Separation
Thisclevertrickmakesuseofaprofound propertyofthetotal numberof
soldiers: thatitcanbewritten asthesumofthenumberofsoldiers infront
ofapointandthenumberbehind thatpoint,twoquantities whichcanbe
computed separately,because thetwogroups areseparated bythecommander.
Ifthesoldiers werenotarranged inalinebutweretravelling inaswarm,
thenitwouldnotbeeasytoseparate them intotwogroups inthisway.The
Commander
JimFigure 16.3.Aswarmofguerillas.
guerillas ingure 16.3could notbecountedusing theabovemessage-passing
rule-set A,because, while theguerillas dohaveneighbours(shownbylines),
itisnotclearwhois`infront'andwhois`behind'; furthermore, since the
graph ofconnections betweentheguerillas contains cycles, itisnotpossible
foraguerilla inacycle(suchas`Jim') toseparate thegroup intotwogroups,
`those infront',and`those behind'.
Aswarmofguerillas canbecountedbyamodied message-passing algo-
rithm iftheyarearrangedinagraphthatcontains nocycles .
Rule-set Bisamessage-passing algorithm forcountingaswarmofguerillas
whose connections formacycle-free graph ,alsoknownasatree,asillustrated
ingure 16.4. Anyguerilla candeduce thetotalinthetreefromthemessages
thattheyreceive.

<<<PAGE 255>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
16.1: Counting 243
Commander
JimFigure 16.4.Aswarmofguerillas
whose connections formatree.
1.Countyournumberofneighbours,N.
2.Keep countofthenumberofmessages youhavereceivedfromyour
neighbours,m,andofthevaluesv1,v2,:::;vNofeachofthose
messages. LetVbetherunning total ofthemessages youhave
received.
3.Ifthenumberofmessages youhavereceived,m,isequal toN 1,
thenidentifytheneighbourwhohasnotsentyouamessage andtell
them thenumberV+1.
4.Ifthenumberofmessages youhavereceivedisequal toN,then:
(a)thenumberV+1istherequired total.
(b)foreachneighbournf
saytoneighbournthenumberV+1 vn.
gAlgorithm 16.5.Message-passing
rule-set B.
BA Figure 16.6.Atriangular 4141
grid.Howmanypaths arethere
fromAtoB?Onepathisshown.

<<<PAGE 256>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
244 16|Message Passing
16.2 Path-coun ting
Amore profound taskthancountingsquaddies isthetaskofcountingthe
numberofpaths through agrid,andnding howmanypaths passthrough
anygivenpointinthegrid.
Figure 16.6showsarectangular grid,andapaththrough thegrid,con-
necting pointsAandB.Avalidpathisonethatstarts fromAandproceeds
toBbyrightwardanddownwardmoves.Ourquestions are:
1.Howmanysuchpaths arethere fromAtoB?
2.Ifarandom pathfromAtoBisselected, whatistheprobabilit ythatit
passes through aparticular nodeinthegrid? [When wesay`random',
wemean thatallpathshaveexactly thethesame probabilit yofbeing
selected.]
3.Howcanarandom pathfromAtoBbeselected?
Countingallthepaths fromAtoBdoesn'tseemstraigh tforward.Thenumber
ofpaths isexpected tobeprettybig{evenifthepermitted gridwereadiagonal
striponlythree nodeswide, there wouldstillbeabout2N=2possible paths.PA
BMN
Figure 16.7.EverypathfromAto
PentersPthrough anupstream
neighbourofP,either MorN;so
wecanndthenumberofpaths
fromAtoPbyadding the
numberofpaths fromAtoMand
fromAtoN.
Thecomputational breakthrough istorealize thattondthenumberof
paths, wedonothavetoenumerate allthepaths explicitly .PickapointPin
thegridandconsider thenumberofpaths fromAtoP.EverypathfromA
toPmustcome intoPthrough oneofitsupstream neighbours(`upstream'
meaning aboveortotheleft). Sothenumberofpaths fromAtoPcanbe
found byadding upthenumberofpaths fromAtoeachofthose neighbours.
Thismessage-passing algorithm isillustrated ingure 16.8forasimple
gridwithtenvertices connected bytwelvedirected edges. Westartbysend-1A1
1
B11
553
22
Figure 16.8.Messages sentinthe
forwardpass.ingthe`1'message fromA.When anynodehasreceivedmessages fromallits
upstream neighbours,itsends thesumofthem ontoitsdownstream neigh-
bours. AtB,thenumber5emerges: wehavecountedthenumberofpaths
fromAtoBwithout enumerating them all. Asasanity-check,gure 16.9
BA
Figure 16.9.Thevepaths.showsthevedistinct paths fromAtoB.
Havingcountedallpaths, wecannowmoveontomore challenging prob-
lems: computing theprobabilit ythatarandom pathgoesthrough agiven
vertex, andcreating arandom path.
Probability ofpassing throughanode
Bymaking abackwardpassaswellastheforwardpass,wecandeduce how
manyofthepaths gothrough eachnode;andifwedivide thatbythetotal
numberofpaths, weobtain theprobabilit ythatarandomly selected path
passes through thatnode. Figure 16.10 showsthebackward-passing mes-A
B1
5111
1
3
5
52
2
2
111
253
1
1
Figure 16.10.Messages sentinthe
forwardandbackwardpasses.sages inthelower-righ tcorners ofthetables, andtheoriginal forward-passing
messages intheupper-left corners. Bymultiplying these twonumbersata
givenvertex, wendthetotalnumberofpaths passing through thatvertex.
Forexample, fourpaths passthrough thecentralvertex.
Figure 16.11 showstheresult ofthiscomputation forthetriangular 41
41grid. Theareaofeachblobisproportional totheprobabilit yofpassing
through eachnode.
Random pathsampling
Exercise 16.1.[1,p.247]Ifonecreates a`random' pathfromAtoBbyipping
afaircoinateveryjunction where there isachoice oftwodirections, is

<<<PAGE 257>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
16.3: Finding thelowest-cost path 245
theresulting pathauniform random sample fromthesetofallpaths?
[Hint:imagine trying itforthegridofgure 16.8.]
There isaneatinsigh ttobehadhere,andI'dlikeyoutohavethesatisfaction
ofguring itout.
Exercise 16.2.[2,p.247]Havingruntheforwardandbackwardalgorithms be-
tweenpointsAandBonagrid,howcanonedrawonepathfromAto
Buniformly atrandom? (Figure 16.11.)
(a) (b) BA Figure 16.11.(a)Theprobabilit y
ofpassing through eachnode,and
(b)arandomly chosen path.
Themessage-passing algorithm weusedtocountthepaths toBisan
example ofthesum{pro ductalgorithm .The`sum' takesplace ateachnode
when itaddstogether themessages coming fromitspredecessors; the`product'
wasnotmentioned, butyoucanthink ofthesumasaweightedsuminwhich
allthesummed terms happenedtohaveweight1.
16.3 Finding thelowest-cost path
Imagine youwishtotravelasquicklyaspossible fromAmbridge (A)toBognor
(B).Thevarious possible routes areshowningure 16.12, along withthecost
inhours oftraversing eachedgeinthegraph. Forexample, theroute A{I{L{
AH
IJ
K
LM
NB4
12
1
2
12
12
31
3*
HHHj*
HHHj
*
HHHj*
HHHj
*HHHj
*HHHj
Figure 16.12.Route diagram from
Ambridge toBognor, showingthe
costsassociated withtheedges.N{Bhasacostof8hours. Wewouldliketondthelowest-cost pathwithout
explicitly evaluating thecostofallpaths. Wecandothisecien tlybynding
foreachnodewhat thecostofthelowest-cost pathtothatnodefromAis.
These quantities canbecomputed bymessage-passing, starting fromnode
A.Themessage-passing algorithm iscalled themin{sum algorithm .orthe
Viterbi algorithm .
Forbrevit y,we'llcallthecostofthelowest-cost pathfrom nodeAto
nodex`thecostofx'.Eachnodecanbroadcast itscosttoitsdescendan ts
onceitknowsthecostsofallitspossible predecessors. Let'sstepthrough the
algorithm byhand. ThecostofAiszero. WepassthisnewsontoHandI.
Asthemessage passes along eachedgeinthegraph, thecostofthatedgeis
added.WendthecostsofHandIare4and1respectively(gure 16.13a).
Similarly then, thecostsofJandLarefound tobe6and2respectively,but
whataboutK?OutoftheedgeH{Kcomes themessage thatapathofcost5
exists fromAtoKviaH;andfromedgeI{Kwelearnofanalternativ epathof
cost3(gure 16.13b). Themin{sum algorithm setsthecostofKequal tothe
minim umofthese (the`min'), andrecords whichwasthesmallest-cost route
intoKbyretaining onlytheedgeH{Kandpruning awaytheother edges
leading toK(gure 16.13c). Figure 16.13d andeshowtheremaining two
iterations ofthealgorithm whichrevealthatthere isapathfromAtoBwith
cost6.[Ifthemin{sum algorithm encoun tersatie,where theminim umcost

<<<PAGE 258>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
246 16|Message Passing
pathtoanodeisachievedbymore thanoneroute toit,thenthealgorithm
canpickanyofthose routes atrandom.]
Wecanrecoverthislowest-cost pathbybacktrackingfromB,following
thetrailofsurviving edges backtoA.Wededuce thatthelowest-cost pathis
A{I{K{M{B.
(a)
0
A4
H
1
IJ
K
LM
NB4
12
1
2
12
12
31
3*
HHHj*
HHHj
*
HHHj*
HHHj
*HHHj
*HHHj
(b)
0
A4
H
1
I6
J
K5
3
2
LM
NB4
12
1
2
12
12
31
3*
HHHj*
HHj
*
HHHj*
HHHj
*HHHj
*HHHj
(c)
0
A4
H
1
I6
J
3
K
2
LM
NB4
12
1
2
12
12
31
3*
HHHj*
*
HHHj*
HHHj
*HHHj
*HHHj
(d)
0
A4
H
1
I6
J
3
K
2
L5
M
4
NB4
12
1
2
12
12
31
3*
HHHj*
*
HHHj*
HHHj*HHHj
(e)
0
A4
H
1
I6
J
3
K
2
L5
M
4
N6
B4
12
1
2
12
12
31
3*
HHHj*
*
HHHj*
HHHjHHHj
Figure 16.13.Min{sum
message-passing algorithm tond
thecostofgetting toeachnode,
andthence thelowestcostroute
fromAtoB.Other applications ofthemin{sum algorithm
Imagine thatyoumanage theproduction ofaproductfromrawmaterials
viaalarge setofoperations. Youwishtoidentifythecritical pathinyour
process,thatis,thesubset ofoperations thatareholding upproduction. If
anyoperations onthecritical pathwerecarried outalittlefaster thenthe
timetogetfromrawmaterials toproductwouldbereduced.
Thecritical pathofasetofoperations canbefound using themin{sum
algorithm.
InChapter 25themin{sum algorithm willbeusedinthedecodingof
error-correcting codes.
16.4 Summary andrelated ideas
Some global functions haveaseparabilit yproperty.Forexample, thenumber
ofpaths fromAtoPseparates intothesumofthenumberofpaths fromAtoM
(thepointtoP'sleft)andthenumberofpaths fromAtoN(thepointabove
P).Suchfunctions canbecomputed ecien tlybymessage-passing. Other
functions donothavesuchseparabilit yproperties, forexample
1.thenumberofpairsofsoldiers inatroopwhoshare thesame birthda y;
2.thesizeofthelargest group ofsoldiers whoshare acommon height
(rounded tothenearest centimetre);
3.thelength oftheshortest tourthatatravelling salesman could takethat
visits everysoldier inatroop.
Oneofthechallenges ofmachinelearning istondlow-cost solutions toprob-
lemslikethese. Theproblem ofnding alargesubset variables thatareap-
proximately equal canbesolvedwithaneural networkapproac h(Hopeld and
Brody,2000; Hopeld andBrody,2001). Aneural approac htothetravelling
salesman problem willbediscussed insection 42.9.
16.5 Further exercises
.Exercise 16.3.[2]Describ etheasymptotic properties oftheprobabilities de-
picted ingure 16.11a, foragridinatriangle ofwidth andheightN.
.Exercise 16.4.[2]Inimage processing, theintegral imageI(x;y)obtained from
animagef(x;y)(wherexandyarepixelcoordinates) isdened by
I(x;y)xX
u=0yX
v=0f(u;v): (16.1)
Showthattheintegral imageI(x;y)canbeecien tlycomputed bymes-
sagepassing.
Showthat,fromtheintegral image, some simple functions oftheimage
canbeobtained. Forexample, giveanexpression forthesumofthe(0;0)y2
y1
x1x2
image intensitiesf(x;y)forall(x;y)inarectangular region extending
from(x1;y1)to(x2;y2).

<<<PAGE 259>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
16.6: Solutions 247
16.6 Solutions
Solution toexercise 16.1(p.244).Since there arevepaths through thegrid
ofgure 16.8,theymustallhaveprobabilit y1=5.Butastrategy based onfair
coin-ips willproducepaths whose probabilities arepowersof1=2.
Solution toexercise 16.2(p.245).Tomakeauniform random walk,eachstep
ofthewalkshould bechosen using adieren tbiased coinateachjunction,
withthebiases chosen inproportion tothebackwar dmessages emanating from
thetwooptions. Forexample, attherstchoice afterleavingA,there isa`3'
message coming fromtheEast, anda`2'coming fromSouth, sooneshould
goEastwithprobabilit y3=5andSouth withprobabilit y2=5.Thisishowthe
pathingure 16.11 wasgenerated.

<<<PAGE 260>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
17
Comm unication overConstrained
Noiseless Channels
Inthischapter westudy thetaskofcomm unicating ecien tlyoveracon-
strained noiseless channel {aconstrained channel overwhichnotallstrings
fromtheinput alphab etmaybetransmitted.
Wemakeuseoftheideaintroduced inChapter 16,thatglobalproperties
ofgraphscanbecompute dbyalocalmessage-p assing algorithm .
17.1 Three examples ofconstrained binary channels
Aconstrained channel canbedened byrulesthatdene whichstrings are
permitted.
Example 17.1. InChannel Aevery1mustbefollowedbyatleastone0.Channel A:
thesubstring 11isforbidden. Avalidstring forthischannel is
00100101001010100010 : (17.1)
Asamotivation forthismodel,consider achannel inwhich1sarerepre-
sentedbypulses ofelectromagnetic energy ,andthedevice thatproduces
those pulses requires arecoverytimeofoneclockcycleaftergenerating
apulse beforeitcangenerate another.
Example 17.2. Channel Bhastherulethatall1smustcome ingroups oftwo
ormore, andall0smustcome ingroups oftwoormore.Channel B:
101and010areforbidden. Avalidstring forthischannel is
00111001110011000011 : (17.2)
Asamotivation forthismodel,consider adiskdriveinwhichsucces-
sivebitsarewritten ontoneighbouring pointsinatrackalong thedisk
surface; thevalues0and1arerepresen tedbytwooppositemagnetic
orientiations. Thestrings101and010areforbidden because asingle
isolated magnetic domain surrounded bydomains havingtheopposite
orientation isunstable, sothat101mightturninto111,forexample.
Example 17.3. Channel Chastherulethatthelargest permitted runlength is
two,thatis,eachsymbolcanberepeated atmostonce.Channel C:
111and000areforbidden. Avalidstring forthischannel is
10010011011001101001 : (17.3)
248

<<<PAGE 261>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
17.1: Three examples ofconstrained binary channels 249
Aphysical motivation forthismodelisadiskdriveinwhichtherateof
rotation ofthediskisnotknownaccurately ,soitisdicult todistinguish
betweenastring oftwo1sandastring ofthree1s,whicharerepresen ted
byorientedmagnetizations ofduration 2and3respectively,where
isthe(poorlyknown)timetakenforonebittopassby;toavoid
thepossibilit yofconfusion, andtheresulting lossofsynchronization of
sender andreceiver,weforbid thestring ofthree1sandthestring of
three0s.
Allthree ofthese channels areexamples ofrunlength-limited channels .
Therulesconstrain theminim umandmaxim umnumbersofsuccessiv e1sand
0s.
Channel Runlength of1s Runlength of0s
minim ummaxim umminim ummaxim um
unconstrained 11 11
A 1 1 11
B 21 21
C 1 2 1 2
Inchannel A,runsof0smaybeofanylength butrunsof1sarerestricted to
length one.Inchannel Ballrunsmustbeoflength twoormore. Inchannel
C,allrunsmustbeoflength oneortwo.
Thecapacit yoftheunconstrained binary channel isonebitperchannel
use.What arethecapacities ofthethree constrained channels? [Tobefair,
wehaven'tdened the`capacit y'ofsuchchannels yet;please understand `ca-
pacity'asmeaning howmanybitscanbeconveyedreliably perchannel-use.]
Some codesforaconstrainedchannel
Letusconcen trate foramomen tonchannel A,inwhichrunsof0smaybe
ofanylength butrunsof1sarerestricted tolength one.Wewouldliketo
comm unicate arandom binary leoverthischannel asecien tlyaspossible.CodeC1
st
000
110Asimple starting pointisa(2;1)codethatmaps eachsource bitintotwo
transmitted bits,C1.Thisisarate-1/2code,anditrespectstheconstrain tsof
channel A,sothecapacit yofchannel Aisatleast0.5.Canwedobetter?
C1isredundan tbecause iftherstoftworeceivedbitsisazero,weknow
thatthesecond bitwillalsobeazero. Wecanachieveasmaller average
transmitted length using acodethatomits theredundan tzeroesinC1.
CodeC2
st
00
110C2issuchavariable-length code.Ifthesource symbolsareusedwith
equal frequency thentheaverage transmitted length persource bitis
L=1
21+1
22=3
2; (17.4)
sotheaverage comm unication rateis
R=2/3; (17.5)
andthecapacit yofchannel Amustbeatleast2/3.
CanwedobetterthanC2?There aretwowaystoargue thattheinfor-
mation ratecould beincreased aboveR=2/3.
Therstargumen tassumes wearecomfortable withtheentropyasa
measure ofinformation content.Theideaisthat,starting fromcodeC2,we
canreduce theaverage message length, without greatly reducing theentropy

<<<PAGE 262>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
250 17|Comm unication overConstrained Noiseless Channels
ofthemessage wesend, bydecreasing thefraction of1sthatwetransmit.
Imagine feeding intoC2astream ofbitsinwhichthefrequency of1sisf.[Such
astream could beobtained fromanarbitrary binary lebypassing thesource
leintothedecoderofanarithmetic codethatisoptimal forcompressing
binary strings ofdensit yf.]Theinformation rateRachievedistheentropy
ofthesource,H2(f),divided bythemean transmitted length,
L(f)=(1 f)+2f=1+f: (17.6)
Thus
R(f)=H2(f)
L(f)=H2(f)
1+f: (17.7)
Theoriginal codeC2,without prepro cessor, corresp ondstof=1/2.What
happensifweperturbfalittletowardssmallerf,setting
f=1
2+; (17.8)
forsmall negativ e?Inthevicinit yoff=1/2,thedenominator L(f)varies
linearly with.Incontrast, thenumeratorH2(f)onlyhasasecond-order
dependence on.
.Exercise 17.4.[1]Find, toorder2,theTaylorexpansion ofH2(f)asafunction
of.
Torstorder,R(f)increases linearly withdecreasing.Itmustbepossible
toincreaseRbydecreasingf.Figure 17.1showsthese functions;R(f)does012
00.25 0.5 0.75 11+f
H_2(f)
00.10.20.30.40.50.60.7
00.25 0.5 0.75 1R(f) = H_2(f)/(1+f)
Figure 17.1.Top:Theinformation
contentpersource symboland
mean transmitted length per
source symbolasafunction ofthe
source densit y.Bottom: The
information contentper
transmitted symbol,inbits,asa
function off.
indeed increase asfdecreases andhasamaxim umofabout0.69bitsper
channel useatf'0:38.
Bythisargumen twehaveshownthatthecapacit yofchannel Aisatleast
max fR(f)=0:69.
.Exercise 17.5.[2,p.257]Ifalecontaining afractionf=0:51sistransmitted
byC2,whatfraction ofthetransmitted stream is1s?
What fraction ofthetransmitted bitsis1sifwedrivecodeC2witha
sparse source ofdensit yf=0:38?
Asecond, more fundamen talapproac hcounts howmanyvalidsequences
oflengthNthere are,SN.Wecancomm unicate logSNbitsinNchannel
cycles bygiving onename toeachofthese validsequences.
17.2 Thecapacit yofaconstrained noiseless channel
Wedened thecapacit yofanoisy channel interms ofthemutual information
betweenitsinput anditsoutput, thenweprovedthatthisnumber,thecapac-
ity,wasrelated tothenumberofdistinguishable messagesS(N)thatcould be
reliably conveyedoverthechannel inNusesofthechannel by
C=lim
N!11
NlogS(N): (17.9)
Inthecaseoftheconstrained noiseless channel, wecanadopt thisidentityas
ourdenition ofthechannel's capacit y.However,thenames,which,when
weweremaking codesfornoisy channels (section 9.6),ranovermessages
s=1:::S,isabouttotakeonanewrole:labelling thestates ofourchannel;

<<<PAGE 263>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
17.3: Countingthenumberofpossible messages 251
(a)01
01
0(c)f0-0   1
ff
01s1
-0@
@@R0
   1
ff
01s2
-0@
@@R0
   1
ff
01s3
-0@
@@R0
   1
ff
01s4
-0@
@@R0
   1
ff
01s5
-0@
@@R0
   1
ff
01s6
-0@
@@R0
   1
ff
01s7
-0@
@@R0
   1
ff
01s8
(b)
jj
01sn
-0@
@
@@R0
    1
jj
01sn+1
(d)A=(from)
10
(to)1
0"
0
11
1#
Figure 17.2.(a)State diagram for
channel A.(b)Trellissection. (c)
Trellis. (d)Connection matrix.
00
1
000011
1
11
mmmm
000111sn
-0@
@
@@R0A
A
A
A
A
A
AAU0

1    1-1
mmmm
000111sn+1
00
0011
11
10
10
nnnn
000111sn
@
@
@@R0@
@
@@R0A
A
A
A
A
A
AAU0

1    1    1
nnnn
000111sn+1
B A=2
6641100
0001
1000
00113
775C A=2
6640100
0011
1100
00103
775Figure 17.3.State diagrams, trellis
sections andconnection matrices
forchannels BandC.
sointhischapter wewilldenote thenumberofdistinguishable messages of
lengthNbyMN,anddene thecapacit ytobe:
C=lim
N!11
NlogMN: (17.10)
Once wehavegured outthecapacit yofachannel wewillreturn tothe
taskofmaking apractical codeforthatchannel.
17.3 Countingthenumberofpossible messages
Firstletusintroducesomerepresen tations ofconstrained channels. Inastate
diagram ,states ofthetransmitter arerepresen tedbycircles labelledwiththe
name ofthestate. Directed edges fromonestate toanother indicate that
thetransmitter ispermitted tomovefromtherststatetothesecond, anda
labelonthatedgeindicates thesymbolemitted when thattransition ismade.
Figure 17.2a showsthestatediagram forchannel A.Ithastwostates, 0and
1.When transitions tostate0aremade, a0istransmitted; when transitions
tostate1aremade, a1istransmitted; transitions fromstate1tostate1are
notpossible.
Wecanalsorepresen tthestatediagram byatrellis section ,whichshows
twosuccessiv estates intime attwosuccessiv ehorizon tallocations (g-
ure17.2b). Thestate ofthetransmitter attimeniscalledsn.Thesetof
possible statesequences canberepresen tedbyatrellis asshowningure 17.2c.
Avalidsequence corresp ondstoapaththrough thetrellis, andthenumberof

<<<PAGE 264>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
252 17|Comm unication overConstrained Noiseless Channels
h0-    
11M1=2
hh
01
h0-    
11M1=2
hh
01
-@
@
@@R    
21M2=3
hh
01
h0-    
11M1=2
hh
01
-@
@
@@R    
21M2=3
hh
01
-@
@
@@R    
32M3=5
hh
01Figure 17.4.Countingthenumber
ofpaths inthetrellis ofchannel
A.Thecountsnexttothenodes
areaccum ulated bypassing from
lefttorightacross thetrellises.
Figure 17.5.Countingthenumberofpaths inthetrellises ofchannels A,B,andC.Weassume thatat
thestarttherstbitispreceded by00,sothatforchannels AandB,anyinitial character
ispermitted, butforchannel C,therstcharacter mustbea1.(a)Channel A
h0-    
11M1=2
hh
01
-@
@
@@R    
21M2=3
hh
01
-@
@
@@R    
32M3=5
hh
01
-@
@
@@R    
53M4=8
hh
01
-@
@
@@R    
85M5=13
hh
01
-@
@
@@R    
138M6=21
hh
01
-@
@
@@R    
2113M7=34
hh
01
-@
@
@@R    
3421M8=55
hh
01
(b)Channel B
h00-
11M1=2
hhhh
000111
-@
@
@@RA
A
A
A
A
AAU
    -
111M2=3
hhhh
000111
-@
@
@@RA
A
A
A
A
AAU
    -
1112M3=5
hhhh
000111
-@
@
@@RA
A
A
A
A
AAU
    -
2213M4=8
hhhh
000111
-@
@
@@RA
A
A
A
A
AAU
    -
4324M5=13
hhhh
000111
-@
@
@@RA
A
A
A
A
AAU
    -
7446M6=21
hhhh
000111
-@
@
@@RA
A
A
A
A
AAU
    -
116710M7=34
hhhh
000111
-@
@
@@RA
A
A
A
A
AAU
    -
17101117M8=55
hhhh
000111
(c)Channel C
h001M1=1
hhhh
000111
@
@
@@R@
@
@@RA
A
A
A
A
AAU

        
11M2=2
hhhh
000111
@
@
@@R@
@
@@RA
A
A
A
A
AAU

        
111M3=3
hhhh
000111
@
@
@@R@
@
@@RA
A
A
A
A
AAU

        
1121M4=5
hhhh
000111
@
@
@@R@
@
@@RA
A
A
A
A
AAU

        
1322M5=8
hhhh
000111
@
@
@@R@
@
@@RA
A
A
A
A
AAU

        
3442M6=13
hhhh
000111
@
@
@@R@
@
@@RA
A
A
A
A
AAU

        
4674M7=21
hhhh
000111
@
@
@@R@
@
@@RA
A
A
A
A
AAU

        
611107M8=34
hhhh
000111

<<<PAGE 265>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
17.3: Countingthenumberofpossible messages 253
nMnMn=Mn 1log2Mn1
nlog2Mn
1 2 1.0 1.00
2 3 1.500 1.6 0.79
3 5 1.667 2.3 0.77
4 8 1.600 3.0 0.75
5 13 1.625 3.7 0.74
6 21 1.615 4.4 0.73
7 34 1.619 5.1 0.73
8 55 1.618 5.8 0.72
9 89 1.618 6.5 0.72
10 144 1.618 7.2 0.72
11 233 1.618 7.9 0.71
12 377 1.618 8.6 0.71
100 910201.618 69.7 0.70
200 710411.618 139.1 0.70
300 610621.618 208.5 0.70
400 510831.618 277.9 0.69Figure 17.6.Countingthenumber
ofpaths inthetrellis ofchannel A.
validsequences isthenumberofpaths. Forthepurposeofcountinghowmany
paths there arethrough thetrellis, wecanignore thelabelsontheedges and
summarize thetrellis section bytheconnection matrix A,inwhichAss0=1
ifthere isanedgefromstatestos0,andAss0=0otherwise (gure 17.2d).
Figure 17.3showsthestatediagrams, trellis sections andconnection matrices
forchannels BandC.
Let's countthenumberofpaths forchannel Abymessage-passing inits
trellis. Figure 17.4showstherstfewsteps ofthiscountingprocess,and
gure 17.5a showsthenumberofpaths ending ineachstateafternsteps for
n=1:::8.Thetotalnumberofpaths oflengthn,Mn,isshownalong the
top.WerecognizeMnastheFibonacci series.
.Exercise 17.6.[1]Showthattheratioofsuccessiv eterms intheFibonacci series
tends tothegolden ratio,
1+p
5
2=1:618: (17.11)
Thus,towithin aconstan tfactor,MNscales asMNNasN!1,sothe
capacit yofchannel Ais
C=lim1
Nlog2h
constan tNi
=log2=log21:618=0:694:(17.12)
Howcanwedescrib ewhatwejustdid?Thecountofthenumberofpaths
isavectorc(n);wecanobtain c(n+1)fromc(n)using:
c(n+1)=Ac(n): (17.13)
So
c(N)=ANc(0); (17.14)
wherec(0)isthestatecountbeforeanysymbolsaretransmitted. Ingure 17.5
weassumed c(0)=[0;1]T,i.e.,thateither ofthetwosymbolsispermitted at
theoutset. Thetotalnumberofpaths isMn=P
sc(n)
s=c(n)n.Inthelimit,
c(N)becomes dominated bytheprincipal right-eigen vector ofA.
c(N)!constan tN
1e(0)
R: (17.15)

<<<PAGE 266>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
254 17|Comm unication overConstrained Noiseless Channels
Here,1istheprincipal eigenvalueofA.
Sotondthecapacit yofanyconstrained channel, allweneedtodoisnd
theprincipal eigenvalue,1ofitsconnection matrix. Then
C=log21: (17.16)
17.4 Backtoourmodelchannels
Comparing gure 17.5a andgure 17.5b andcitlooksasifchannels BandC
havethesame capacit yaschannel A.Theprincipal eigenvalues ofthethree
trellises arethesame (theeigenvectors forchannels AandBaregivenatthe
bottom oftable C.4,p.608).Andindeed thechannels areintimately related.z0-t
hd
sz1
6
-
z0hdt z1
?-s-
Figure 17.7.Anaccum ulator and
adieren tiator.
Equivalenc eofchannels AandB
Ifwetakeanyvalidstringsforchannel Aandpassitthrough anaccum ulator ,
obtaining tdened by:
t1=s1
tn=tn 1+snmod2forn2,(17.17)
thentheresulting string isavalidstring forchannel B,because there areno
11sins,sothere arenoisolated digits int.Theaccum ulator isaninvertible
operator, so,similarly ,anyvalidstringtforchannel Bcanbemappedontoa
validstringsforchannel Athrough thebinary dieren tiator,
s1=t1
sn=tn tn 1mod2forn2.(17.18)
Because +and areequivalentinmodulo2arithmetic, thedieren tiator is
alsoablurrer, convolving thesource stream withthelter(1;1).
Channel Cisalsointimately related tochannels AandB.
.Exercise 17.7.[1,p.257]What istherelationship ofchannel Ctochannels A
andB?
17.5 Practical comm unication overconstrained channels
OK,howtodoitinpractice? Since allthree channels areequivalent,wecan
concen trateonchannel A.
Fixed-length solutions
Westartwithexplicitly-en umerated codes.Thecodeinthetable17.8achievessc(s)
100000
210000
301000
400100
500010
610100
701010
810010
Table17.8.Arunlength-limited
codeforchannel A.
arateof3/5=0:6.
.Exercise 17.8.[1,p.257]Similarly ,enumerate allstrings oflength 8thatendin
thezerostate. (There are34ofthem.) Hence showthatwecanmap5
bits(32source strings) to8transmitted bitsandachieverate5/8=0:625.
What ratecanbeachievedbymapping anintegernumberofsource bits
toN=16transmitted bits?

<<<PAGE 267>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
17.5: Practical comm unication overconstrained channels 255
Optimal variable-length solution
Theoptimal waytoconveyinformation overtheconstrained channel istond
theoptimal transition probabilities forallpointsinthetrellis,Qs0js,andmake
transitions withthese probabilities.
When discussing channel A,weshowedthatasparse source withdensit y
f=0:38,driving codeC2,wouldachievecapacit y.Andweknowhowto
makesparsiers (Chapter 6):wedesign anarithmetic codethatisoptimal
forcompressing asparse source; thenitsassociated decodergivesanoptimal
mapping fromdense (i.e.,random binary) strings tosparse strings.
Thetaskofnding theoptimal probabilities isgivenasanexercise.
Exercise 17.9.[3]Showthattheoptimal transition probabilities Qcanbefound
asfollows.
Findtheprincipal right-andleft-eigen vectors ofA,thatisthesolutions
ofAe(R)=e(R)ande(L)TA=e(L)Twithlargest eigenvalue.Then
construct amatrix Qwhose invariantdistribution isproportional to
e(R)
ie(L)
i,namely
Qs0js=e(L)
s0As0s
e(L)
s: (17.19)
[Hint:exercise 16.2(p.245)mightgivehelpful cross-fertilization here.]
.Exercise 17.10.[3,p.258]Showthatwhen sequences aregenerated using theop-
timal transition probabilit ymatrix (17.19), theentropyoftheresulting
sequence isasymptotically log2persymbol.[Hint:consider thecondi-
tional entropyofjustonesymbolgiventheprevious one,assuming the
previous one's distribution istheinvariantdistribution.]
Inpractice, wewouldprobably usenite-precision approximations tothe
optimal variable-length solution. Onemightdislikevariable-length solutions
because oftheresulting unpredictabilit yoftheactual encodedlength inany
particular case. Perhaps insome applications wewouldlikeaguaran teethat
theencodedlength ofasource leofsizeNbitswillbelessthanagiven
length suchasN=(C+).Forexample, adiskdriveiseasier tocontrolif
allblocksof512bytesareknowntotakeexactly thesame amoun tofdisk
real-estate. Forsomeconstrained channels wecanmakeasimple modication
toourvariable-length encodingandoersuchaguaran tee,asfollows.We
ndtwocodes,twomappings ofbinary strings tovariable-length encodings,
havingthepropertythatforanysource string x,iftheencodingofxunder
therstcodeisshorter thanaverage, thentheencodingofxunder thesecond
codeislonger thanaverage, andviceversa.Then totransmit astring xwe
encodethewhole string withbothcodesandsendwhicheverencodinghasthe
shortest length, prepended byasuitably encodedsingle bittoconveywhich
ofthetwocodesisbeingused.2
4010
001
1113
5112
0
01
00
2
6640100
0010
0001
11113
775
1121
0
01
0003
Figure 17.9.State diagrams and
connection matrices forchannels
withmaxim umrunlengths for1s
equal to2and3..Exercise 17.11.[3C,p.258]Howmanyvalidsequences oflength 8starting with
a0arethere fortherun-length-limited channels showningure 17.9?
What arethecapacities ofthese channels?
Using acomputer, ndthematrices Qforgenerating arandom path
through thetrellises ofthechannel A,andthetworun-length-limited
channels showningure 17.9.

<<<PAGE 268>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
256 17|Comm unication overConstrained Noiseless Channels
.Exercise 17.12.[3,p.258]Consider therun-length-limited channel inwhichany
length ofrunof0sispermitted, andthemaxim umrunlength of1sisa
largenumberLsuchasnineorninety.
Estimate thecapacit yofthischannel. (Givethersttwoterms ina
series expansion involvingL.)
What, roughly ,istheformoftheoptimal matrix Qforgenerating a
random paththrough thetrellis ofthischannel? Focusonthevalues of
theelemen tsQ1j0,theprobabilit yofgenerating a1givenapreceding 0,
andQLjL 1,theprobabilit yofgenerating a1givenapreceding runof
L 11s.Checkyouranswerbyexplicit computation forthechannel in
whichthemaxim umrunlength of1sisnine.
17.6 Variable symboldurations
Wecanaddafurther frilltothetaskofcomm unicating overconstrained
channels byassuming thatthesymbolswesendhavedieren tdurations ,and
thatouraimistocomm unicate atthemaxim umpossible rateperunittime.
Suchchannels cancome intwoavours: unconstrained, andconstrained.
Unconstrainedchannels withvariable symboldurations
Weencoun tered anunconstrained noiseless channel withvariable symboldu-
rations inexercise 6.18(p.125).Solvethatproblem, andyou'vedone this
topic. Thetaskistodetermine theoptimal frequencies withwhichthesym-
bolsshould beused, giventheirdurations.
There isaniceanalogy betweenthistaskandthetaskofdesigning an
optimal symbolcode(Chapter 4).When wemakeanbinary symbolcode
forasource withunequal probabilities pi,theoptimal message lengths are
l
i=log21/pi,so
pi=2 l
i: (17.20)
Similarly ,when wehaveachannel whose symbolshavedurationsli(insome
units oftime), theoptimal probabilit ywithwhichthose symbolsshould be
usedis
p
i=2 li; (17.21)
whereisthecapacit yofthechannel inbitsperunittime.
Constr ainedchannels withvariable symboldurations
Once youhavegraspedthepreceding topics inthischapter, youshould be
abletogure outhowtodene andndthecapacit yofthese, thetrickiest
constrained channels.
Exercise 17.13.[3]Aclassic example ofaconstrained channel withvariable
symboldurations isthe`Morse' channel, whose symbolsare
thedot d,
thedash D,
theshort space (used betweenletters inmorse code)s,and
thelongspace (used betweenwords) S;
theconstrain tsarethatspaces mayonlybefollowedbydotsanddashes.
Findthecapacit yofthischannel inbitsperunittimeassuming (a)that
allfoursymbolshaveequal durations; or(b)thatthesymboldurations
are2,4,3and6timeunits respectively.

<<<PAGE 269>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
17.7: Solutions 257
Exercise 17.14.[4]Howwell-designed isMorse codeforEnglish (with, say,the
probabilit ydistribution ofgure 2.1)?
Exercise 17.15.[3C]Howdicult isittogetDNAintoanarrowtube?
Toaninformation theorist, theentropyassociated withaconstrained
channel revealshowmuchinformation canbeconveyedoverit.Insta-
tistical physics, thesamecalculations aredoneforadieren treason: to
predict thethermo dynamics ofpolymers, forexample.
Asatoyexample, consider apolymer oflengthNthatcaneither sit
inaconstraining tube,ofwidthL,orintheopenwhere there areno
constrain ts.Intheopen,thepolymer adopts astate drawnatrandom
from thesetofonedimensional random walks, with, say,3possible
directions perstep. Theentropyofthiswalkislog3perstep,i.e.,a
Figure 17.10.ModelofDNA
squashed inanarrowtube.The
DNA willhaveatendency topop
outofthetube,because, outside
thetube,itsrandom walkhas
greater entropy.total ofNlog3.[Thefreeenergy ofthepolymer isdened tobekT
times this,whereTisthetemperature.] Inthetube,thepolymer's one-
dimensional walkcangoin3directions unless thewallisintheway,so
thetrellis section is,forexample (ifL=10),
2
666666666666641100000000
1110000000
0111000000
0011100000
0001110000
.........
0000000111
00000000113
77777777777775:
Now,whatistheentropyofthepolymer? What isthechange inentropy
associated withthepolymer entering thetube?Ifpossible, obtain an
expression asafunction ofL.Useacomputer tondtheentropyofthe
walkforaparticular valueofL,e.g.20,andplottheprobabilit ydensit y
ofthepolymer's transv erselocation inthetube.
Notice thedierence incapacit ybetweentwochannels, oneconstrained
andoneunconstrained, isdirectly proportional totheforcerequired to
pulltheDNA intothetube.
17.7 Solutions
Solution toexercise 17.5(p.250).Aletransmitted byC2contains, onaver-
age,one-third 1sandtwo-thirds0s.
Iff=0:38,thefraction of1sisf=(1+f)=( 1:0)=(2 1:0)=0:2764.
Solution toexercise 17.7(p.254).Avalidstring forchannel Ccanbeobtained
fromavalidstring forchannel Abyrstinverting it[1!0;0!1],then
passing itthrough anaccum ulator. These operations areinvertible, soany
validstring forCcanalsobemappedontoavalidstring forA.Theonly
provisoherecomes fromtheedgeeects. Ifweassume thattherstcharacter
transmitted overchannel Cispreceded byastring ofzeroes,sothattherst
character isforced tobea1(gure 17.5c) thenthetwochannels areexactly
equivalentonlyifweassume thatchannel A'srstcharacter mustbeazero.
Solution toexercise 17.8(p.254).WithN=16transmitted bits,thelargest
integernumberofsource bitsthatcanencodedis10,sothemaxim umrateof
axedlength codewithN=16is0.625.

<<<PAGE 270>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
258 17|Comm unication overConstrained Noiseless Channels
Solution toexercise 17.10 (p.255).Lettheinvariantdistribution be
P(s)=e(L)
se(R)
s; (17.22)
whereisanormalization constan t.TheentropyofStgivenSt 1,assuming Here, asinChapter 4,Stdenotes the
ensem blewhose random variable is
thestate st.St 1comes fromtheinvariantdistribution, is
H(StjSt 1)= X
s;s0P(s)P(s0js)logP(s0js) (17.23)
= X
s;s0e(L)
se(R)
se(L)
s0As0s
e(L)
sloge(L)
s0As0s
e(L)
s(17.24)
= X
s;s0e(R)
se(L)
s0As0s
h
loge(L)
s0+logAs0s log loge(L)
si
: (17.25)
Now,As0siseither 0or1,sothecontributions fromtheterms proportional to
As0slogAs0sareallzero. So
H(StjSt 1)=log+ 
X
s0 X
sAs0se(R)
s!
e(L)
s0loge(L)
s0+

X
s X
s0e(L)
s0As0s!
e(R)
sloge(L)
s (17.26)
=log 
X
s0e(R)
s0e(L)
s0loge(L)
s0+
X
se(L)
se(R)
sloge(L)
s(17.27)
=log: (17.28)
Solution toexercise 17.11 (p.255).Theprincipal eigenvaluesoftheconnection
matrices ofthetwochannels are1.839 and1.928. Thecapacities (log)are
0.879 and0.947 bits.
Solution toexercise 17.12 (p.256).Thechannel issimilar totheunconstrained
binary channel; runsoflength greater thanLarerareifLislarge, soweonly
expectweakdierences fromthischannel; these dierences willshowupin
contexts where therunlength isclosetoL.Thecapacit yofthechannel is
veryclosetoonebit.
Alowerbound onthecapacit yisobtained byconsidering thesimple
variable-length codeforthischannel whichreplaces occurrences ofthemaxi-
mumrunlength string111:::1by111:::10,andotherwise leavesthesource le
unchanged. Theaverage rateofthiscodeis1=(1+2 L)because theinvariant
distribution willhitthe`addanextra zero'stateafraction 2 Lofthetime.
Wecanreuse thesolution forthevariable-length channel inexercise 6.18
(p.125).Thecapacit yisthevalueofsuchthattheequation
Z()=L+1X
l=12 l=1 (17.29)
issatised. TheL+1terms inthesumcorresp ondtotheL+1possible strings
thatcanbeemitted, 0,10,110,:::,11:::10.Thesumisexactly givenby:"NX
n=0arn=a(rN+1 1)
r 1#
Z()=2 
2 L+1 1
2  1: (17.30)

<<<PAGE 271>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
17.7: Solutions 259
Weanticipate thatshould bealittlelessthan1inorder forZ()toequal
1.Rearranging andsolving approximately for,using ln(1+x)'x,
Z()=1 (17.31)
)'1 2 (L+2)=ln2: (17.32)
Weevaluated thetruecapacities forL=2andL=3inanearlier exercise.
Thetable compares theapproximate capacit ywiththetruecapacit yforaL  Truecapacit y
20.910 0.879
30.955 0.947
40.977 0.975
50.9887 0.9881
60.9944 0.9942
90.9993 0.9993
selection ofvalues ofL.
Theelemen tQ1j0willbecloseto1=2(justatinybitlarger), sinceinthe
unconstrained binary channelQ1j0=1=2.When arunoflengthL 1has
occurred, weeectiv elyhaveachoiceofprinting10or0.Lettheprobabilit yof
selecting 10bef.Letusestimate theentropyoftheremainingNcharacters
inthestream asafunction off,assuming therestofthematrix Qtohave
beensettoitsoptimal value. TheentropyofthenextNcharacters inthe
stream istheentropyoftherstbit,H2(f),plustheentropyoftheremaining
characters, whichisroughly (N 1)bitsifweselect0astherstbitand
(N 2)bitsif1isselected. More precisely ,ifCisthecapacit yofthechannel
(whichisroughly 1),
H(thenextNchars)'H2(f)+[(N 1)(1 f)+(N 2)f]C
=H2(f)+NC fC'H2(f)+N f:(17.33)
Dieren tiating andsetting tozerotondtheoptimalf,weobtain:
log21 f
f'1)1 f
f'2)f'1=3: (17.34)
Theprobabilit yofemitting a1thusdecreases fromabout0.5toabout1=3as
thenumberofemitted1sincreases.
Hereistheoptimal matrix:
2
666666666666666640:3334 0 0 0 0 0 0 0 0
00:4287 0 0 0 0 0 0 0
00 0:4669 0 0 0 0 0 0
00 0 0:4841 0 0 0 0 0
00 0 0 0:4923 0 0 0 0
00 0 0 0 0:4963 0 0 0
00 0 0 0 0 0:4983 0 0
00 0 0 0 0 0 0:4993 0
00 0 0 0 0 0 0 0:4998
1:6666:5713:5331:5159:5077:5037:5017:5007:50023
77777777777777775: (17.35)
Ourrough theory works.

<<<PAGE 272>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
18
Crossw ordsandCodebreaking
Inthischapter wemakearandom walkthrough afewtopics related tolan-
guage modelling.
18.1 Crossw ords
Therulesofcrossw ord-making maybethough tofasdening aconstrained
channel. Thefactthatmany validcrossw ordscanbemade demonstrates that
thisconstrained channel hasacapacit ygreater thanzero.
There aretwoarchetypalcrossw ordformats. Ina`typeA'(orAmerican)
SDLIGDUTSFFUD
UEIDAOTITRAFA
REDIRVALOOTOT
ESOOGREHTOMHSA
TLUCSLIVE
SABELOSSSERTS
TORRETTUSETIC
EROCREENSRIEH
ETTAMSALTAMUM
PAHSIMLUAPEPO
ETRACCIPE
HARYNNEKRETBIS
EERYNORIAHOLA
LRAEETALSERIS
MOTARESYERBAS
BPDJVPB
REHSUEHCNALAVA
IIENALRN
SELTTENNOELLAG
TWIONIE
LEBONFEEBTSAOR
EAUEIM
SETATORRENMERB
TCHENA
AILARTSUASETIK
UEATPLE
SESUCXESTEKCOR
TTKPOTAI
ETAREPSEDNOTLE
NRRYASS
Figure 18.1.Crossw ordsoftypes
A(American) andB(British).crossw ord,everyrowandcolumn consists ofasuccession ofwordsoflength 2
ormoreseparated byoneormorespaces. Ina`typeB'(orBritish) crossw ord,
eachrowandcolumn consists ofamixture ofwordsandsingle characters,
separated byoneormorespaces, andeverycharacter liesinatleastoneword
(horizon talorvertical). Whereas inatypeAcrossw ordeveryletter liesina
horizon talwordandavertical word,inatypical typeBcrossw ordonlyabout
halfoftheletters doso;theother halflieinonewordonly.
TypeAcrossw ordsareharder tocreatethantypeBbecause ofthecon-
straintthatnosingle characters arepermitted. TypeBcrossw ordsaregener-
allyharder tosolve because there arefewerconstrain tspercharacter.
Whyarecrosswor dspossible?
Ifalanguage hasnoredundancy ,thenanyletters written onagridforma
validcrossw ord.Inalanguage withhighredundancy ,ontheother hand, it
ishardtomakecrossw ords(except perhaps asmall numberoftrivial ones).
Thepossibilit yofmaking crossw ordsinalanguage thusdemonstrates abound
ontheredundancy ofthatlanguage. Crossw ordsarenotnormally written
ingenuineEnglish. They arewritten inthe`word-English', thelanguage
consisting ofstrings ofwordsfromadictionary ,separated byspaces.
.Exercise 18.1.[2]Estimate thecapacit yofword-English, inbitspercharacter.
[Hint:think ofword-English asdening aconstrained channel (Chapter
17)andseeexercise 6.18(p.125).]
Thefactthatmanycrossw ordscanbemade leads toalowerbound onthe
entropyofword-English.
Forsimplicit y,wenowmodelword-English byWenglish, thelanguage in-
troduced insection 4.1whichconsists ofWwordsalloflengthL.Theentropy
ofsuchalanguage, percharacter, including inter-wordspaces, is:
HWlog2W
L+1: (18.1)
260

<<<PAGE 273>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
18.1: Crossw ords 261
We'llndthattheconclusions wecome todependonthevalueofHWand
arenotterribly sensitiv etothevalueofL.Consider alargecrossw ordofsize
Ssquares inarea. LetthenumberofwordsbefwSandletthenumberof
letter-o ccupied squares bef1S.Fortypical crossw ordsoftypesAandBmade
ofwordsoflengthL,thetwofractionsfwandf1haveroughly thevalues in
table 18.2.A B
fw2
L+11
L+1
f1L
L+13
4L
L+1
Table18.2.Factorsfwandf1by
whichthenumberofwordsand
numberofletter-squares
respectivelyaresmaller thanthe
totalnumberofsquares.Wenowestimate howmanycrossw ordsthereareofsizeSusing oursimple
modelofWenglish. Weassume thatWenglish iscreated atrandom bygener-
atingWstrings fromamonogram (i.e.,memoryless) source withentropyH0.
If,forexample, thesource usedallA=26characters withequal probabilit y
thenH0=log2A=4:7bits.Ifinstead weuseChapter 2'sdistribution then
theentropyis4.2.Theredundancy ofWenglish stems fromthese twosources:
ittends tousesome letters more thanothers; andthere areonlyWwordsin
thedictionary .
Let's nowcounthowmanycrossw ordsthere arebyimagining lling in
thesquares ofacrossw ordatrandom using thesame distribution thatpro-
duced theWenglish dictionary andevaluating theprobabilit ythatthisrandom
scribbling produces validwordsinallrowsandcolumns. Thetotalnumberof
typicalllings-in ofthef1Ssquares inthecrossw ordthatcanbemade is
jTj=2f1SH0: (18.2)
Theprobabilit ythatonewordoflengthLisvalidly lledinis
=W
2LH0; (18.3)
andtheprobabilit ythatthewhole crossw ord,made offwSwords,isvalidly
lledinbyasingle typical in-lling is
fwS: (18.4)
Sothelogofthenumberofvalidcrossw ordsofsizeSisestimated tobe
logfwSjTj=S[(fl fwL)H0+fwlogW]logfwSjTj(18.5)
=S[(fl fwL)H0+fw(L+1)Hw] (18.6)
whichisanincreasing function ofSonlyif
(f1 fwL)H0+fw(L+1)Hw>0: (18.7)
Soarbitrarily manycrossw ordscanbemade onlyifthere's enough wordsin
theWenglish dictionary that
HW>(fwL f1)
fw(L+1)H0: (18.8)
Plugging inthevaluesoff1andfwfromprevious page, wendthefollowing.
Crossw ordtype A B
Condition forcrossw ordsHW>1
2L
L+1H0HW>1
4L
L+1H0
IfwesetH0=4:2bitsandassume there areW=4000wordsinanormal
English-sp eaker'sdictionary ,allwithlengthL=5,thenwendthatthe
condition forcrossw ordsoftypeBissatised, butthecondition forcrossw ords
oftypeAisonlyjustsatised. Thistswithmyexperience thatcrossw ords
oftypeAusually containmore obscure words.

<<<PAGE 274>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
262 18|Crossw ordsandCodebreaking
Further reading
These observ ations aboutcrossw ordswererstmade byShannon; Ilearned
aboutthem from WolfandSiegel (1998). Thetopic isclosely related to
thecapacit yoftwo-dimensional constrained channels. Anexample ofatwo-
dimensional constrained channel isatwo-dimensional bar-co de,asseenon
parcels.
Exercise 18.2.[3]Atwo-dimensional channel isdened bytheconstrain tthat,
oftheeightneighboursofeveryinterior pixelinanNNrectangular
grid,fourmustbeblackandfourwhite. (Thecountsofblackandwhite
pixels around boundary pixels arenotconstrained.) Abinary pattern
satisfying thisconstrain tisshowningure 18.3. What isthecapacit yFigure 18.3.Abinary pattern in
whicheverypixelisadjacen tto
fourblackandfourwhite pixels.ofthischannel, inbitsperpixel, forlargeN?
18.2 Simple language models
TheZipf{Mandelbr otdistribution
Thecrudest modelforalanguage isthemonogram model,whichasserts that
eachsuccessiv ewordisdrawnindependen tlyfromadistribution overwords.
What isthenature ofthisdistribution overwords?
Zipf'slaw(Zipf, 1949) asserts thattheprobabilit yoftherthmostprobable
wordinalanguage isapproximately
P(r)=
r; (18.9)
where theexponenthasavaluecloseto1,andisaconstan t.According
toZipf,alog{log plotoffrequency versus word-rank should showastraigh t
linewithslope .
Mandelbrot's (1982) modication ofZipf'slawintroduces athird param-
eterv,asserting thattheprobabilities aregivenby
P(r)=
(r+v): (18.10)
Forsomedocumen ts,suchasJaneAusten's Emma ,theZipf{Mandelbrot dis-
tribution tswell{gure 18.4.
Other documen tsgivedistributions thatarenotsowelltted byaZipf{
Mandelbrot distribution. Figure 18.5showsaplotoffrequency versusrankfor
theLATEXsource ofthisbook.Qualitativ ely,thegraph issimilar toastraigh t
line,butacurveisnoticeable. Tobefair,thissource leisnotwritten in
pureEnglish {itisamixofEnglish, maths symbolssuchas`x',andLATEX
commands.
1e-050.00010.0010.010.1
1 10 100 1000 10000totheandof
I
is
Harriet
information
probabilityFigure 18.4.Fitofthe
Zipf{Mandelbrot distribution
(18.10) (curve)totheempirical
frequencies ofwordsinJane
Austen's Emma (dots). Thetted
parameters are=0:56;v=8:0;
=1:26.

<<<PAGE 275>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
18.2: Simple language models 263
0.1
0.01
0.001
0.0001
0.00001
1 10 100 1000the
ofais
x
probability
information
Shannon
BayesFigure 18.5.Log{log plotof
frequency versusrankforthe
wordsintheLATEXleofthis
book.
0.1
0.01
0.001
0.0001
0.00001
1 10 100 1000 10000alpha=1000alpha=100alpha=10alpha=1
bookFigure 18.6.Zipfplotsforfour
`languages' randomly generated
fromDirichletprocesses with
parameterranging from1to
1000. AlsoshownistheZipfplot
forthisbook.
TheDirichlet process
Assuming weareinterested inmonogram modelsforlanguages, what model
should weuse?Onedicult yinmodelling alanguage istheunboundedness
ofvocabulary .Thegreater thesample oflanguage, thegreater thenumber
ofwordsencoun tered. Agenerativ emodelforalanguage should emulate
thisproperty.Ifasked`what isthenextwordinanewly-disco veredwork
ofShakespeare?' ourprobabilit ydistribution overwordsmustsurely include
some non-zero probabilit yforwordsthatShakesp earenever usedbefore.Our
generativ emonogram modelforlanguage should alsosatisfy aconsistency
rulecalled exchangeabilit y.Ifweimagine generating anewlanguage from
ourgenerativ emodel,producing anever-gro wingcorpus oftext,allstatistical
properties ofthetextshould behomogeneous: theprobabilit yofnding a
particular wordatagivenlocation inthestream oftextshould bethesame
everywhere inthestream.
TheDirichletprocessmodelisamodelforastream ofsymbols(whichwe
think ofas`words') thatsatises theexchangeabilit yruleandthatallowsthe
vocabulary ofsymbolstogrowwithout limit. Themodelhasoneparameter
.Asthestream ofsymbolsisproduced, weidentifyeachnewsymbolbya
unique integerw.When wehaveseenastream oflengthFsymbols,wedene
theprobabilit yofthenextsymbolinterms ofthecountsfFwgofthesymbols
seensofarthus:theprobabilit ythatthenextsymbolisanewsymbol,never
seenbefore, is
F+: (18.11)
Theprobabilit ythatthenextsymbolissymbolwis
Fw
F+: (18.12)
Figure 18.6showsZipfplots(i.e.,plotsofsymbolfrequency versusrank) for
million-sym bol`documen ts'generated byDirichletprocesspriors withvalues
ofranging from1to1000.
Itiseviden tthataDirichletprocessisnotanadequate modelforobserv ed
distributions thatroughly obeyZipf'slaw.

<<<PAGE 276>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
264 18|Crossw ordsandCodebreaking
0.1
0.01
0.001
0.0001
0.00001
1 10 100 1000 10000Figure 18.7.Zipfplotsforthe
wordsoftwo`languages'
generated bycreating successiv e
characters fromaDirichlet
processwith=2,anddeclaring
onecharacter tobethespace
character. Thetwocurvesresult
fromtwodieren tchoices ofthe
space character.
With asmall tweak,however,Dirichletprocesses canproducerather nice
Zipfplots. Imagine generating alanguage composedofelemen tarysymbols
using aDirichletprocesswitharather small valueoftheparameter,sothat
thenumberofreasonably frequen tsymbolsisabout27.Ifwethendeclare
oneofthose symbols(nowcalled `characters' rather thanwords) tobeaspace
character, thenwecanidentifythestrings betweenthespace characters as
`words'. Ifwegenerate alanguage inthiswaythenthefrequencies ofwords
oftencomeoutasveryniceZipfplots, asshowningure 18.7.Whichcharacter
isselected asthespace character determines theslopeoftheZipfplot{aless
probable space character givesrisetoaricherlanguage withashallowerslope.
18.3 Units ofinformation content
Theinformation contentofanoutcome,x,whose probabilit yisP(x),isdened
tobe
h(x)=log1
P(x): (18.13)
Theentropyofanensem bleisanaverage information content,
H(X)=X
xP(x)log1
P(x): (18.14)
When wecompare hypotheses witheachother inthelightofdata, itisof-
tenconvenienttocompare thelogoftheprobabilit yofthedataunder the
alternativ ehypotheses,
`logevidence forHi'=logP(DjHi); (18.15)
or,inthecasewhere justtwohypotheses arebeingcompared, weevaluate the
`logodds',
logP(DjH1)
P(DjH2); (18.16)
whichhasalsobeencalled the`weightofevidence infavourofH1'.Thelogev-
idence forahypothesis, logP(DjHi)isthenegativ eoftheinformation content
ofthedataD:ifthedatahavelargeinformation content,givenahypothesis,
thentheyaresurprising tothathypothesis; ifsome other hypothesis isnot
sosurprised bythedata, thenthathypothesis becomes more probable. `In-
formation content',`surprise value', andloglikelihoodorlogevidence arethe
same thing.
Allthese quantities arelogarithms ofprobabilities, orweightedsums of
logarithms ofprobabilities, sotheycanallbemeasured inthesame units.
Theunitsdependonthechoice ofthebaseofthelogarithm.
Thenames thathavebeengiventothese units areshownintable 18.8.

<<<PAGE 277>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
18.4: AtasteofBanburism us 265
Unit Expression thathasthose units
bit log2p
nat logep
ban log10p
deciban (db) 10log10pTable18.8.Units ofmeasuremen t
ofinformation content.
Thebitistheunitthatweusemostinthisbook.Because theword`bit'
hasother meanings, abackupname forthisunitistheshannon .Abyteis
8bits. Amegab yteis220'106bytes. Ifoneworksinnatural logarithms,
information contentsandweightsofevidence aremeasured innats.Themost
interesting units arethebanandthedeciban.
Thehistory oftheban
Letmetellyouwhyafactor ofteninprobabilit yiscalled aban.When Alan
Turing andtheother codebreak ersatBletchleyParkwerebreaking eachnew
day'sEnigma code,theirtaskwasahugeinference problem: toinfer, given
theday'scyphertext, whichthree wheels wereintheEnigma machines that
day;whattheirstarting positions were;whatfurther letter substitutions were
inuseonthesteckerboard; and,notleast, whattheoriginal German messages
were.These inferences wereconducted using Bayesian metho ds(ofcourse!),
andthechosen unitsweredecibans orhalf-decibans, thedeciban beingjudged
thesmallest weightofevidence discernible toahuman. Theevidence infavour
ofparticular hypotheses wastallied using sheets ofpaperthatwerespecially
printedinBanbury, atownabout30miles fromBletchley. Theinference task
wasknownasBanburism us,andtheunits inwhichBanburism uswasplayed
werecalled bans, afterthattown.
18.4 AtasteofBanburism us
Thedetails ofthecode-breaking metho dsofBletchleyParkwerekeptsecret
foralongtime, butsome aspectsofBanburism uscanbepieced together.
Ihopethefollowing description ofasmall partofBanburism usisnottoo
inaccurate.1
Howmuchinformation wasneeded? Thenumberofpossible settings of
theEnigma machinewasabout81012.Todeduce thestateofthemachine,
`itwastherefore necessary tondabout129decibans fromsomewhere', as
Goodputsit.Banburism uswasaimed notatdeducing theentirestateofthe
machine, butonlyatguring outwhichwheels wereinuse;thelogic-based
bombes,fedwithguesses oftheplaintext(cribs), werethenusedtocrackwhat
thesettings ofthewheels were.
TheEnigma machine, onceitswheels andplugs wereputinplace, im-
plemen tedacontinually-c hanging permutation cypher thatwandered deter-
ministically through astatespace of263permutations. Because anenormous
numberofmessages weresenteachday,there wasagoodchance thatwhat-
everstateonemachinewasinwhen sending onecharacter ofamessage, there
wouldbeanother machineinthesamestatewhile sending aparticular char-
acter inanother message. Because theevolution ofthemachine's state was
deterministic, thetwomachines wouldremain inthesamestateaseachother
1I'vebeenmost helpedbydescriptions givenbyTonySale (http://www.
codesandciphers.org.uk/lectures /)andbyJackGood(1979), whoworkedwithTuring
atBletchley.

<<<PAGE 278>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
266 18|Crossw ordsandCodebreaking
fortherestofthetransmission. Theresulting correlations betweentheout-
putsofsuchpairsofmachines provided adribble ofinformation-con tentfrom
whichTuring andhisco-workersextracted theirdaily129decibans.
Howtodetectthattwomessages camefrommachines withacommon
statesequence
Thehypotheses arethenullhypothesis,H0,whichstates thatthemachines
areindierentstates, andthatthetwoplainmessages areunrelated; andthe
`match'hypothesis,H1,whichsaysthatthemachines areinthesame state,
andthatthetwoplain messages areunrelated. Noattempt isbeingmade
heretoinferwhat thestate ofeither machineis.Thedataprovided arethe
twocyphertexts xandy;let'sassume theybothhavelengthTandthatthe
alphab etsizeisA(26inEnigma). What istheprobabilit yofthedata, given
thetwohypotheses?
First, thenullhypothesis. Thishypothesis asserts thatthetwocyphertexts
aregivenby
x=x1x2x3:::=c1(u1)c2(u2)c3(u3)::: (18.17)
and
y=y1y2y3:::=c0
1(v1)c0
2(v2)c0
3(v3):::; (18.18)
where thecodesctandc0
taretwounrelated time-v arying permutations ofthe
alphab et,andu1u2u3:::andv1v2v3:::aretheplaintextmessages. Anexact
computation oftheprobabilit yofthedata(x;y)woulddependonalanguage
modeloftheplaintext,andamodeloftheEnigma machine's guts,butifwe
assume thateachEnigma machineisanidealrandom time-v arying permuta-
tion,thentheprobabilit ydistribution ofthetwocyphertexts isuniform. All
cyphertexts areequally likely.
P(x;yjH0)=1
A2T
forallx;yoflengthT: (18.19)
What aboutH1?Thishypothesis asserts thatasingle time-v arying permuta-
tionctunderlies both
x=x1x2x3:::=c1(u1)c2(u2)c3(u3)::: (18.20)
and
y=y1y2y3:::=c1(v1)c2(v2)c3(v3):::: (18.21)
What istheprobabilit yofthedata(x;y)?Wehavetomakesomeassumptions
abouttheplaintextlanguage. Ifitwerethecasethattheplaintextlanguage
wascompletely random, thentheprobabilit yofu1u2u3:::andv1v2v3:::would
beuniform, andsowouldthatofxandy,sotheprobabilit yP(x;yjH1)
wouldbeequal toP(x;yjH0),andthetwohypothesesH0andH1wouldbe
indistinguishable.
Wemakeprogress byassuming thattheplaintextisnotcompletely ran-
dom. Bothplaintextsarewritten inalanguage, andthatlanguage hasredun-
dancies. Assume forexample thatparticular plaintextletters areusedmore
often thanothers. So,eventhough thetwoplaintextmessages areunrelated,
theyareslightlymorelikelytousethesameletters aseachother; ifH1istrue,
twosynchronized letters fromthetwocyphertexts areslightlymore likelyto
beidentical. Similarly ,ifalanguage usesparticular bigrams andtrigrams
frequen tly,thenthetwoplaintextmessages willoccasionally containthesame
bigrams andtrigrams atthesametimeaseachother, giving rise,ifH1istrue,

<<<PAGE 279>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
18.4: AtasteofBanburism us 267
uLITTLE-JACK-HORN ER-SAT-IN-THE-CORNER-EATING-A-CHRISTMAS-PIE--HE-PUT-IN-H
vRIDE-A-COCK-HORS E-TO-BANBURY-CROSS-TO-SEE-A-FINE-LADY-UPON-A-WHITE-HORSE
matches: .*....*..******. *..............*...........*................*...........
Table18.9.Twoaligned pieces of
English plaintext,uandv,with
matchesmarkedby*.Notice that
there aretwelvematches,
including arunofsix,whereas the
expected numberofmatchesin
twocompletely random strings of
lengthT=74wouldbeabout3.
Thetwocorresp onding
cyphertexts fromtwomachines in
identicalstates wouldalsohave
twelvematches.toalittle burst of2or3identicalletters. Table18.9showssuchacoinci-
dence intwoplaintextmessages thatareunrelated, except thattheyareboth
written inEnglish.
Thecodebreak ershuntedamong pairsofmessages forpairsthatweresus-
piciously similar toeachother, countingupthenumbersofmatchingmono-
grams, bigrams, trigrams, etc. Thismetho dwasrstusedbythePolish
codebreak erRejewski.
Let'slookatthesimple caseofamonogram language modelandestimate
howlongamessage isneeded tobeabletodecide whether twomachines
areinthesame state. I'llassume thesource language ismonogram-English,
thelanguage inwhichsuccessiv eletters aredrawni.i.d.fromtheprobabilit y
distributionfpigofgure 2.1.Theprobabilit yofxandyisnonuniform:
consider twosingle characters,xt=ct(ut)andyt=ct(vt);theprobabilit y
thattheyareidenticalis
X
ut;vtP(ut)P(vt)
 [ut=vt]=X
ip2
im: (18.22)
Wegivethisquantitythenamem,for`matchprobabilit y';forbothEnglish
andGerman,misabout2=26rather than1=26(thevaluethatwouldhold
foracompletely random language). Assuming thatctisanideal random
permutation, theprobabilit yofxtandytis,bysymmetry ,
P(xt;ytjH1)=(m
Aifxt=yt
(1 m)
A(A 1)forxt6=yt.(18.23)
Givenapairofcyphertexts xandyoflengthTthatmatchinMplaces and
donotmatchinNplaces, thelogevidence infavourofH1isthen
logP(x;yjH1)
P(x;yjH0)=Mlogm=A
1=A2+Nlog(1 m)
A(A 1)
1=A2(18.24)
=MlogmA+Nlog(1 m)A
A 1: (18.25)
Everymatchcontributes logmAinfavourofH1;everynon-matc hcontributes
logA 1
(1 m)AinfavourofH0.
Matchprobabilit yformonogram-English m 0.076
Coinciden talmatchprobabilit y 1=A 0.037
log-evidence forH1permatch 10log10mA 3.1db
log-evidence forH1pernon-matc h 10log10(1 m)A
(A 1) 0:18db
Ifthere wereM=4matchesandN=47non-matc hesinapairoflength
T=51,forexample, theweightofevidence infavourofH1wouldbe+4
decibans, oralikelihoodratioof2.5to1infavour.
Theexpectedweightofevidence from alineoftextoflengthT=20
characters istheexpectation of(18.25), whichdependsonwhetherH1orH0
istrue.IfH1istruethenmatchesareexpected toturnupatratem,andthe
expected weightofevidence is1.4decibans per20characters. IfH0istrue
thenspurious matchesareexpected toturnupatrate1=A,andtheexpected

<<<PAGE 280>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
268 18|Crossw ordsandCodebreaking
weightofevidence is 1:1decibans per20characters. Typically ,roughly 400
characters needtobeinspected inorder tohaveaweightofevidence greater
thanahundred toone(20decibans) infavourofonehypothesis ortheother.
So,twoEnglish plaintexts havemore matchesthantworandom strings.
Furthermore, because consecutiv echaracters inEnglish arenotindependen t,
thebigram andtrigram statistics ofEnglish arenonuniform andthematches
tendtooccurinbursts ofconsecutiv ematches.[Thesame observ ations also
apply toGerman.] Using betterlanguage models,theevidence contributed
byrunsofmatcheswasmore accurately computed. Suchascoring system
wasworkedoutbyTuring andrened byGood.Positiveresults werepassed
ontoautomated andhuman-p oweredcodebreak ers.According toGood,the
longest false-p ositivethatarose inthisworkwasastring of8consecutiv e
matchesbetweentwomachines thatwereactually inunrelated states.
Further reading
Forfurther reading aboutTuring andBletchleyPark,seeHodges(1983) and
Good(1979). Foranin-depth readaboutcryptograph y,Schneier's (1996)
bookishighly recommended. Itisreadable, clear, andentertaining.
18.5 Exercises
.Exercise 18.3.[2]Another weakness inthedesign oftheEnigma machine,
whichwasintended toemulateaperfectly random time-v arying permu-
tation, isthatitnevermappedaletter toitself. When youpressQ,what
comes outisalwaysadieren tletter fromQ.Howmuchinformation per
character isleakedbythisdesign aw?Howlongacribwouldbeneeded
tobeconden tthatthecribiscorrectly aligned withthecyphertext?
Andhowlongacribwouldbeneeded tobeableconden tlytoidentify
thecorrect key?
[Acribisaguess forwhat theplaintextwas.Imagine thattheBrits
knowthataveryimportantGerman istravelling fromBerlin toAachen,
andtheyintercept Enigma-enco dedmessages senttoAachen. Itisa
goodbetthatoneormore oftheoriginal plaintextmessages contains
thestringOBERSTURMBANNFUEHRERXGRA FXHEINRICHXVONXWEIZSAECKER,
thename oftheimportantchap.Acribcould beusedinabrute-force
approac htondthecorrect Enigma key(feed thereceivedmessages
through allpossible Engima machines andseeifanyoftheputativ e
decodedtextsmatchtheaboveplaintext). Thisquestion centresonthe
ideathatthecribcanalsobeusedinamuchlessexpensivemanner:
slidetheplaintextcribalong alltheencodedmessages untilaperfect
mismatch ofthecribandtheencodedmessage isfound; ifcorrect, this
alignmen tthentellsyoualotaboutthekey.]

<<<PAGE 281>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
19
WhyhaveSex?Information Acquisition
andEvolution
Evolution hasbeenhappening onearth foraboutthelast109years. Un-
deniably ,information hasbeenacquiredduring thisprocess. Thanks tothe
tireless workoftheBlind Watchmaker,some cellsnowcarry within them all
theinformation required tobeoutstanding spiders; other cellscarry allthe
information required tomakeexcellen toctupuses. Where didthisinformation
come from?
Theentireblueprin tofallorganisms ontheplanet hasemerged inateach-
ingprocessinwhichtheteacherisnatural selection: tter individuals have
more progen y,thetness beingdened bythelocalenvironmen t(including
theother organisms). Theteachingsignal isonlyafewbitsperindividual: an
individual simply hasasmaller orlarger numberofgrandc hildren, depending
ontheindividual's tness. `Fitness' isabroad termthatcould cover
theabilityofanantelopetorunfaster thanother antelopesandhence
avoidbeingeaten byalion;
theabilityofaliontobewell-enough camouaged andrunfastenough
tocatchoneantelopeperday;
theabilityofapeacocktoattract apeahen tomate withit;
theabilityofapeahen torearmanyyoungsimultaneously .
Thetness ofanorganism islargely determined byitsDNA {boththecoding
regions, orgenes, andthenon-co dingregions (whichplayanimportantrole
inregulating thetranscription ofgenes). We'llthink oftness asafunction
oftheDNA sequence andtheenvironmen t.
HowdoestheDNA determine tness, andhowdoesinformation getfrom
natural selection intothegenome? Well,ifthegenethatcodesforoneofan
antelope'sproteins isdefectiv e,thatantelopemightgeteaten byalionearly
inlifeandhaveonlytwograndc hildren rather thanforty.Theinformation
contentofnatural selection isfullycontained inaspecication ofwhicho-
spring surviv edtohavechildren {aninformation contentofatmostonebit
perospring .Theteachingsignal doesnotcomm unicate totheecosystem
anydescription oftheimperfections intheorganism thatcaused ittohave
fewerchildren. Thebitsoftheteachingsignal arehighly redundan t,because,
throughout aspecies, untindividuals whoaresimilar toeachother willbe
failing tohaveospring forsimilar reasons.
So,howmanybitspergeneration areacquired bythespeciesasawhole
bynatural selection? Howmanybitshasnatural selection succeeded incon-
veying tothehuman branchofthetreeoflife,since thedivergence between
269

<<<PAGE 282>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
270 19|WhyhaveSex?Information Acquisition andEvolution
Australopithecines andapes4000000yearsago?Assuming ageneration time
of10yearsforreproduction, there havebeenabout400000generations of
human precursors sincethedivergence fromapes.Assuming apopulation of
109individuals, eachreceiving acouple ofbitsofinformation fromnatural
selection, thetotal numberofbitsofinformation responsible formodifying
thegenomes of4million B.C.intotoday'shuman genome isabout81014
bits. However,aswenoted, natural selection isnotsmart atcollating the
information thatitdishes outtothepopulation, andthere isagreat dealof
redundancy inthatinformation. Ifthepopulation sizeweretwiceasgreat,
woulditevolvetwiceasfast? No,because natural selection willsimply be
correcting thesame defects twiceasoften.
JohnMaynard Smith hassuggested thattherateofinformation acquisition
byaspeciesisindependen tofthepopulation size,andisoforder 1bitper
generation. Thisgure wouldonlyallowfor400000bitsofdierence between
apesandhumans, anumberthatismuchsmaller thanthetotalsizeofthe
human genome {6109bits. [Onehuman genome contains about3109
nucleotides.] Itiscertainly thecasethatthegenomic overlap betweenapes
andhumans ishuge,butisthedierence thatsmall?
Inthischapter, we'lldevelopacrude modeloftheprocessofinformation
acquisition through evolution, based ontheassumption thatagenewithtwo
defects istypically likelytobemoredefectiv ethanagenewithonedefect, and
anorganism withtwodefectiv egenes islikelytobelesstthananorganism
withonedefectiv egene. Undeniably ,thisisacrude model,sincerealbiological
systems arebaroqueconstructions withcomplex interactions. Nevertheless,
wepersist withasimple modelbecause itreadily yields striking results.
What wendfromthissimple modelisthat
1.John Maynard Smith's gure of1bitpergeneration iscorrect foran
asexual ly-reproducing population;
2.incontrast, ifthespeciesreproducessexual ly,therateofinformation
acquisition canbeaslarge asp
Gbitspergeneration, whereGisthe
sizeofthegenome.
We'llalsondinteresting results concerning themaxim ummutation rate
thataspeciescanwithstand.
19.1 Themodel
Westudy asimple modelofareproducing population ofNindividuals with
agenome ofsizeGbits:variation isproduced bymutation orbyrecom bina-
tion(i.e.,sex)andtruncation selection selects theNttest children ateach
generation tobetheparentsofthenext. Wendstriking dierences between
populations thathaverecom bination andpopulations thatdonot.
Thegenotypeofeachindividual isavectorxofGbits,eachhavingagood
statexg=1andabadstatexg=0.ThetnessF(x)ofanindividual issimply
thesumofherbits:
F(x)=GX
g=1xg: (19.1)
Thebitsinthegenome could beconsidered tocorresp ondeither togenes
thathavegoodalleles (xg=1)andbadalleles (xg=0),ortothenucleotides
ofagenome. Wewillconcen trateonthelatter interpretation. Theessential
propertyoftness thatweareassuming isthatitislocallyaroughly linear
function ofthegenome, thatis,thatthere aremanypossible changes one

<<<PAGE 283>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
19.2: Rateofincrease oftness 271
could maketothegenome, eachofwhichhasasmall eect ontness, and
thatthese eects combineapproximately linearly .
Wedene thenormalized tnessf(x)F(x)=G.
Weconsider evolution bynatural selection under twomodelsofvariation.
Variation bymutation .Themodelassumes discrete generations. Ateach
generation, t,everyindividual produces twochildren. Thechildren's
genotypesdier fromtheparent'sbyrandom mutations. Natural selec-
tionselects thettestNprogen yinthechildpopulation toreproduce,
andanewgeneration starts.
[Theselection ofthettestNindividuals ateachgeneration isknown
astruncation selection.]
Thesimplest modelofmutations isthatthechild's bitsfxggarein-
dependen t.Eachbithasasmall probabilit yofbeingipped,which,
thinking ofthebitsascorresp onding roughly tonucleotides, istakento
beaconstan tm,independen tofxg.[Ifalternativ elywethough tofthe
bitsascorresp onding togenes, thenwewouldmodeltheprobabilit yof
thediscoveryofagoodgene,P(xg=0!xg=1),asbeingasmaller
numberthantheprobabilit yofadeleterious mutation inagoodgene,
P(xg=1!xg=0).]
Variation byrecom bination (orcrosso ver,orsex).Ourorganisms are
haploid, notdiploid. They enjoysexbyrecom bination. TheNindivid-
ualsinthepopulation aremarried intoM=N=2couples, atrandom,
andeachcouple hasCchildren {withC=4children beingourstan-
dardassumption, soastohavethepopulation double andhalveevery
generation, asbefore. TheCchildren's genotypesareindependen tgiven
theparents'.Eachchildobtains itsgenotypezbyrandom crosso verof
itsparents'genotypes,xandy.Thesimplest modelofrecom bination
hasnolinkage,sothat:
zg=(
xgwithprobabilit y1=2
ygwithprobabilit y1=2.(19.2)
OncetheMCprogen yhavebeenborn,theparentspassaway,thettest
Nprogen yareselected bynatural selection, andanewgeneration starts.
Wenowstudy these twomodelsofvariation indetail.
19.2 Rateofincrease oftness
Theoryofmutations
Weassume thatthegenotypeofanindividual withnormalized tnessf=F=G
issubjected tomutations thatipbitswithprobabilit ym.Werstshowthat
iftheaverage normalized tnessfofthepopulation isgreater than1=2,then
theoptimal mutation rateissmall, andtherateofacquisition ofinformation
isatmostoforder onebitpergeneration.
Since itiseasytoachieveanormalized tness off=1=2bysimple muta-
tion,we'llassumef>1=2andworkinterms oftheexcess normalized tness
ff 1=2.Ifanindividual withexcess normalized tnessfhasachild
andthemutation ratemissmall, theprobabilit ydistribution oftheexcess
normalized tness ofthechildhasmean
fchild=(1 2m)f (19.3)

<<<PAGE 284>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
272 19|WhyhaveSex?Information Acquisition andEvolution
andvariance
m(1 m)
G'm
G: (19.4)
Ifthepopulation ofparentshasmeanf(t)andvariance2(t)m=G,then
thechildpopulation, beforeselection, willhavemean (1 2m)f(t)andvari-
ance(1+)m=G.Natural selection choosestheupperhalfofthisdistribution,
sothemean tness andvariance oftness atthenextgeneration aregivenby
f(t+1)=(1 2m)f(t)+q
(1+)rm
G; (19.5)
2(t+1)=(1+)m
G; (19.6)
whereisthemean deviation fromthemean, measured instandard devia-
tions, andisthefactor bywhichthechilddistribution's variance isreduced
byselection. Thenumbersandareoforder 1.ForthecaseofaGaussian
distribution, =p
2='0:8and=(1 2=)'0:36.Ifweassume that
thevariance isindynamic equilibrium, i.e.,2(t+1)'2(t),then
(1+)=;so(1+)=1
1 ; (19.7)
andthefactorp
(1+)inequation (19.5) isequal to1,ifwetaketheresults
fortheGaussian distribution, anapproximation thatbecomes poorestwhen
thediscreteness oftness becomes important,i.e.,forsmallm.Therateof
increase ofnormalized tness isthus:
df
dt' 2mf+rm
G; (19.8)
which,assumingG(f)21,ismaximized for
mopt=1
16G(f)2; (19.9)
atwhichpoint,df
dt
opt=1
8G(f): (19.10)
Sotherateofincrease oftnessF=fGisatmost
dF
dt=1
8(f)pergeneration: (19.11)
Forapopulation withlowtness (f<0:125), therateofincrease oftness
mayexceed 1unitpergeneration. Indeed, iff<1=p
G,therateofincrease, if
m=1/2,isoforderp
G;thisinitial spurt canonlylastoforderp
Ggenerations.
Forf>0:125,therateofincrease oftness issmaller thanonepergeneration.
Asthetness approac hesG,theoptimal mutation ratetends tom=1=(4G),so
thatanaverage of1=4bitsareippedpergenotype,andtherateofincrease of
tness isalsoequal to1=4;information isgained atarateofabout0:5bitsper
generation. Ittakesabout2Ggenerations forthegenotypesofallindividuals
inthepopulation toattain perfection.
Forxedm,thetness isgivenby
f(t)=1
2p
mG(1 ce 2mt); (19.12)
subjecttotheconstrain tf(t)1=2,wherecisaconstan tofintegration,
equal to1iff(0)=1=2.Ifthemean numberofbitsippedpergenotype,

<<<PAGE 285>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
19.2: Rateofincrease oftness 273
Nosex Sex
Histogram ofparents'tness
Histogram ofchildren's tness
Selected children's tness
Figure 19.1.Whysexisbetter
thansex-free reproduction. If
mutations areusedtocreate
variation among children, thenit
isunavoidable thattheaverage
tness ofthechildren islower
thantheparents'tness; the
greater thevariation, thegreater
theaverage decit. Selection
bumps upthemean tness again.
Incontrast, recom bination
produces variation without a
decrease inaverage tness. The
typical amoun tofvariation scales
asp
G,whereGisthegenome
size,soafterselection, theaverage
tness risesbyO(p
G).mG,exceeds 1,thenthetnessFapproac hesanequilibrium valueFeqm=
(1=2+1=(2p
mG))G.
Thistheory issomewhat inaccurate inthatthetrueprobabilit ydistribu-
tionoftness isnon-Gaussian, asymmetrical, andquantizedtointegervalues.
Allthesame, thepredictions ofthetheory arenotgrossly atvariance with
theresults ofsimulations describ edbelow.
Theoryofsex
Theanalysis ofthesexual population becomes tractable withtwoapproxi-
mations: rst,weassume thatthegene-p oolmixes sucien tlyrapidly that
correlations betweengenes canbeneglected; second, weassume homogeneity ,
i.e.,thatthefractionfgofbitsgthatareinthegoodstateisthesame,f(t),
forallg.
Giventhese assumptions, iftwoparentsoftnessF=fGmate, theprob-
abilitydistribution oftheirchildren's tness hasmean equal totheparents'
tness,F;thevariation produced bysexdoesnotreduce theaverage tness.
Thestandard deviation ofthetness ofthechildren scales asp
Gf(1 f).
Since, afterselection, theincrease intness isproportional tothisstandard
deviation, thetness increasepergenerationscalesasthesquarerootofthe
sizeofthegenome,p
G.Asshowninbox19.2, themean tness F=fG
evolvesinaccordance withthedieren tialequation:
dF
dt'q
f(t)(1 f(t))G; (19.13)
wherep2=(+2).Thesolution ofthisequation is
f(t)=1
2
1+sinp
G(t+c)
;fort+c2
 
2p
G=;
2p
G=
,(19.14)
wherecisaconstan tofintegration,c=sin 1(2f(0) 1).Sothisidealized
system reachesastate ofeugenic perfection (f=1)within anite time:
(=)p
Ggenerations.
Simulations
Figure 19.3a showsthetness ofasexual population ofN=1000individ-
ualswithagenome sizeofG=1000starting fromarandom initial state
withnormalized tness 0:5.Italsoshowsthetheoretical curvef(t)Gfrom
equation (19.14), whichtsremark ablywell.
Incontrast, gures 19.3(b) and(c)showtheevolving tness when variation
isproduced bymutation atratesm=0:25=Gandm=6=Grespectively.Note
thedierence inthehorizon talscales frompanel (a).

<<<PAGE 286>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
274 19|WhyhaveSex?Information Acquisition andEvolution
Box19.2.Details ofthetheory of
sex. Howdoesf(t+1)dependonf(t)?Let's rstassume thetwoparentsofachildboth
haveexactly f(t)Ggoodbits,and,byourhomogeneit yassumption, thatthose bitsare
independen trandom subsets oftheGbits.Thenumberofbitsthataregoodinboth
parentsisroughly f(t)2G,andthenumberthataregoodinoneparentonlyisroughly
2f(t)(1 f(t))G,sothetness ofthechildwillbef(t)2Gplusthesumof2f(t)(1 f(t))G
faircoinips,whichhasabinomial distribution ofmean f(t)(1 f(t))Gandvariance
1
2f(t)(1 f(t))G.Thetness ofachildisthusroughly distributed as
FchildNormal
mean =f(t)G;variance =1
2f(t)(1 f(t))G
:
Theimportantpropertyofthisdistribution, contrasted withthedistribution under
mutation, isthatthemean tness isequal totheparents'tness; thevariation produced
bysexdoesnotreduce theaverage tness.
Ifweinclude theparentalpopulation's variance, whichwewillwrite as2(t)=
(t)1
2f(t)(1 f(t))G,thechildren's tnesses aredistributed as
FchildNormal
mean =f(t)G;variance =
1+
21
2f(t)(1 f(t))G
:
Natural selection selects thechildren ontheuppersideofthisdistribution. Themean
increase intness willbe
F(t+1) F(t)=[(1+=2)1=2=p
2]p
f(t)(1 f(t))G;
andthevariance ofthesurviving children willbe
2(t+1)=(1+=2)1
2f(t)(1 f(t))G;
where =p
2=and=(1 2=).Ifthere isdynamic equilibrium [2(t+1)=2(t)]
thenthefactor in(19.2) is
(1+=2)1=2=p
2=r
2
(+2)'0:62:
Dening thisconstan ttobep
2=(+2),weconclude that,under sexandnatural
selection, themean tness ofthepopulation increases atarateproportional tothe
squarerootofthesizeofthegenome ,
dF
dt'p
f(t)(1 f(t))Gbitspergeneration :

<<<PAGE 287>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
19.3: Themaximal tolerable mutation rate 275
(a)5006007008009001000
01020304050607080
(b)5006007008009001000
02004006008001000120014001600sex
no sex
(c)5006007008009001000
050100150200250300350sex
no sexFigure 19.3.Fitness asafunction
oftime. Thegenome sizeis
G=1000. Thedotsshowthe
tness ofsixrandomly selected
individuals fromthebirth
population ateachgeneration.
Theinitial population of
N=1000hadrandomly
generated genomes with
f(0)=0:5(exactly). (a)Variation
produced bysexalone. Line
showstheoretical curve(19.14) for
innite homogeneous population.
(b,c)Variation produced by
mutation, withandwithout sex,
when themutation rateis
mG=0:25(b)or6(c)bitsper
genome. Thedashed lineshows
thecurve(19.12).
G=1000 G=100000
mG
05101520
0.650.70.750.80.850.90.95 1with sex
without sex
05101520253035404550
0.650.70.750.80.850.90.95 1with sex
without sex
f fFigure 19.4.Maximal tolerable
mutation rate,shownasnumber
oferrors pergenome (mG),versus
normalized tnessf=F=G.Left
panel: genome sizeG=1000;
right:G=100000.
Independen tofgenome size,a
parthenogenetic species(nosex)
canonlytolerate oforder 1error
pergenome pergeneration; a
speciesthatusesrecom bination
(sex)cantolerate fargreater
mutation rates.
Exercise 19.1.[3]Dependence onpopulation size.Howdotheresults forasexual
population dependonthepopulation size? Weanticipate thatthere is
aminim umpopulation sizeabovewhichthetheory ofsexisaccurate.
Inwhatwayisthatminim umpopulation sizerelated toG?
Exercise 19.2.[3]Dependence oncrossover mechanism .Inthesimple modelof
sex,eachbitistakenatrandom fromoneofthetwoparents,thatis,we
allowcrosso verstooccurwithprobabilit y50%betweenanytwoadjacen t
nucleotides. Howisthemodelaected (a)ifthecrosso verprobabilit yis
smaller? (b)ifcrosso versexclusiv elyoccurathot-sp otslocated everyd
bitsalong thegenome?
19.3 Themaximal tolerable mutation rate
What ifwecombinethetwomodelsofvariation? What isthemaxim um
mutation ratethatcanbetolerated byaspeciesthathassex?
Therateofincrease oftness isgivenby
df
dt' 2mf+p
2s
m+f(1 f)=2
G; (19.15)

<<<PAGE 288>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
276 19|WhyhaveSex?Information Acquisition andEvolution
whichispositiveifthemutation ratesatises
m<s
f(1 f)
G: (19.16)
Letuscompare thisratewiththeresult intheabsence ofsex,which,from
equation (19.8), isthatthemaxim umtolerable mutation rateis
m<1
G1
(2f)2: (19.17)
Thetolerable mutation ratewithsexisoforderp
Gtimes greater thanthat
without sex!
Aparthenogenetic (non-sexual) speciescould trytowriggle outofthis
boundonitsmutuation ratebyincreasing itslittersizes. Butifmutation ips
onaveragemGbits,theprobabilit ythatnobitsareippedinonegenome
isroughlye mG,soamother needs tohaveroughlyemGospring inorder
tohaveagoodchance ofhavingonechildwiththesame tness asher.The
litter sizeofanon-sexual speciesthushastobeexponentialinmG(ifmGis
bigger than1),ifthespeciesistopersist.
Sothemaxim umtolerable mutation rateispinned closeto1=G,foranon-
sexual species, whereas itisalarger numberoforder 1=p
G,foraspecieswith
recom bination.
Turning these results around, wecanpredict thelargest possible genome
sizeforagivenxed mutation rate,m.Foraparthenogenetic species, the
largest genome sizeisoforder 1=m,andforasexual species, 1=m2.Taking
thegurem=10 8asthemutation ratepernucleotide pergeneration (Eyre-
WalkerandKeigh tley,1999), andallowingforamaxim umbroodsizeof20000
(that is,mG'10),wepredict thatallspecieswithmorethanG=109coding
nucleotides makeatleastoccasional useofrecom bination. Ifthebroodsizeis
12,thenthisnumberfallstoG=2:5108.
19.4 Fitness increase andinformation acquisition
Forthissimple modelitispossible torelate increasing tness toinformation
acquisition.
Ifthebitsaresetatrandom, thetness isroughlyF=G=2.Ifevolution
leadstoapopulation inwhichallindividuals havethemaxim umtnessF=G,
thenGbitsofinformation havebeenacquired bythespecies, namely foreach
bitxg,thespecieshasgured outwhichofthetwostates isthebetter.
Wedene theinformation acquired atanintermediate tness tobethe
amoun tofselection (measured inbits)required toselect theperfect state
fromthegenepool.Letafractionfgofthepopulation havexg=1.Because
log2(1=f)istheinformation required tondablackballinanurncontaining
blackandwhite ballsintheratiof:1 f,wedene theinformation acquired
tobe
I=X
glog2fg
1=2bits: (19.18)
Ifallthefractionsfgareequal toF=G,then
I=Glog22F
G; (19.19)
whichiswellapproximated by
~I2(F G=2): (19.20)

<<<PAGE 289>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
19.5: Discussion 277
Therateofinformation acquisition isthusroughly twotimes therateofin-
crease oftness inthepopulation.
19.5 Discussion
These results quantifythewellknownargumen tforwhyspeciesreproduce
bysexwithrecom bination, namely thatrecom bination allowsuseful muta-
tionstospread morerapidly through thespeciesandallowsdeleterious muta-
tionstobemore rapidly cleared fromthepopulation (Maynard Smith, 1978;
Felsenstein, 1985; Maynard Smith, 1988; Maynard Smith andSzathmary ,
1995). Apopulation thatreproduces byrecom bination canacquire informa-
tionfromnatural selection atarateoforderp
Gtimes faster thanapartheno-
genetic population, anditcantolerate amutation ratethatisoforderp
G
times greater. Forgenomes ofsizeG'108codingnucleotides, thisfactor ofp
Gissubstan tial.
Thisenormous advantageconferred bysexhasbeennoted beforebyKon-
drasho v(1988), butthismeme, whichKondrasho vcalls`thedeterministic
mutation hypothesis', doesnotseem tohavediused throughout theevolu-
tionary researc hcomm unity,asthere arestillnumerous papersinwhichthe
prevalence ofsexisviewedasamystery tobeexplained byelaborate mecha-
nisms.
`Thecostofmales' {stability ofageneforsexorpartheno genesis
Whydopeople declare sextobeamystery? Themain motivation forbeing
mystied isanideacalled the`costofmales'. Sexual reproduction isdisad-
vantageous compared withasexual reproduction, it'sargued, because ofevery
twoospring produced bysex,one(onaverage) isauseless male, incapable
ofchild-b earing, andonlyoneisaproductiv efemale. Inthesame time, a
parthenogenetic mother could givebirth totwofemale clones. Toputitan-
other way,thebigadvantageofparthenogenesis, fromthepointofviewof
theindividual, isthatoneisabletopasson100% ofone's genome toone's
children, instead ofonly50%. Thusifthere weretwoversions ofaspecies, one
reproducing withandonewithout sex,thesingle mothers wouldbeexpected
tooutstrip theirsexual cousins. Thesimple modelpresen tedthusfardidnot
include either genders ortheabilitytoconvertfromsexual reproduction to
asexual, butwecaneasily modifythemodel.
WemodifythemodelsothatoneoftheGbitsinthegenome determines
whether anindividual prefers toreproduceparthenogenetically (x=1)orsex-
ually (x=0).Theresults dependonthenumberofchildren hadbyasingle
parthenogenetic mother,Kpandthenumberofchildren bornbyasexual
couple,Ks.Both (Kp=2,Ks=4)and(Kp=4,Ks=4)arereasonable mod-
els.Theformer (Kp=2,Ks=4)wouldseem most appropriate inthecase
ofunicellular organisms, where thecytoplasm ofbothparentsgoesintothe
children. Thelatter (Kp=4,Ks=4)isappropriate ifthechildren aresolely
nurtured byoneoftheparents,sosingle mothers havejustasmanyospring
asasexual pair.Iconcen trateonthelatter model,sinceitgivesthegreatest
advantagetotheparthenogens, whoaresupposedly expected tooutbreed the
sexual comm unity.Because parthenogens havefourchildren pergeneration,
themaxim umtolerable mutation rateforthem istwicetheexpression (19.17)
derivedbeforeforKp=2.Ifthetness islarge, themaxim umtolerable rate
ismG'2.
Initially thegenomes aresetrandomly withF=G=2,withhalfofthepop-

<<<PAGE 290>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
278 19|WhyhaveSex?Information Acquisition andEvolution
(a)mG=4 (b)mG=1Fitnesses
5006007008009001000
0 50 100 150 200 250sexual fitness
parthen fitness
5006007008009001000
0 50 100 150 200 250sexual fitness
parthen fitnessPercentage
020406080100
0 50 100 150 200 250020406080100
0 50 100 150 200 250Figure 19.5.Results when there is
ageneforparthenogenesis, andno
interbreeding, andsingle mothers
produceasmany childrenas
sexual couples .G=1000,
N=1000. (a)mG=4;(b)
mG=1.Vertical axisshowsboth
tness andpercentageofthe
population thatis
parthenogenetic.
ulation havingthegeneforparthenogenesis. Figure 19.5showstheoutcome.
During the`learning' phase ofevolution, inwhichthetness isincreasing
rapidly ,pocketsofparthenogens appearbriey ,butthendisapp earwithin
acouple ofgenerations astheirsexual cousins overtakethem intness and
leavethem behind. Once thepopulation reachesitstoptness, however,the
parthenogens cantakeover,ifthemutation rateissucien tlylow(mG=1).
Inthepresence ofahigher mutation rate(mG=4),however,thepartho-
negens nevertakeover.Thebreadth ofthesexual population's tness isof
orderp
G,soamutantparthenogenetic colonyarising withslightlyabove-
average tness willlastforaboutp
G=(mG)=1=(mp
G)generations before
itstness fallsbelowthatofitssexual cousins. Aslongasthepopulation size
issucien tlylargeforsomesexual individuals tosurviv eforthistime, sexwill
notdieout.
Inasucien tlyunstable environmen t,where thetness function iscontin-
uallychanging, theparthenogens willalwayslagbehind thesexual comm unity.
These results areconsisten twiththeargumen tofHaldane andHamilton that
sexishelpful inanarmsracewithparasites. Theparasites dene aneectiv e
tness function whichchanges withtime, andasexual population willalways
ascend thecurren ttness function more rapidly .
Additive tness function
Ofcourse, ourresults dependonthetness function thatweassume, andon
ourmodelofselection. Isitreasonable tomodeltness, torstorder, asasum
ofindependen tterms? Maynard Smith (1968) argues thatitis:themoregood
genes youhave,thehigher youcome inthepeckingorder, forexample. The
directional selection modelhasbeenusedextensiv elyintheoretical popula-
tiongenetic studies (Bulmer, 1985). Wemightexpectrealtness functions to
involveinteractions, inwhichcasecrosso vermightreduce theaverage tness.
However,sincerecom bination givesthebiggest advantagetospecieswhose t-
nessfunctions areadditiv e,wemightpredict thatevolution willhavefavour ed
speciesthatusedarepresentation ofthegenome thatcorrespondstoatness

<<<PAGE 291>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
19.5: Discussion 279
function thathasonlyweakinteractions .Andevenifthere areinteractions,
itseems plausible thatthetness wouldstillinvolveasumofsuchinteracting
terms, withthenumberofterms beingsome fraction ofthegenome sizeG.
Exercise 19.3.[3C]Investigate howfastsexual andasexual species evolveif
theyhaveatness function withinteractions. Forexample, letthetness
beasumofexclusiv e-orsofpairsofbits;compare theevolving tnesses
withthose ofthesexual andasexual specieswithasimple additiv etness
function.
Furthermore, ifthetness function wereahighly nonlinear function ofthe
genotype,itcould bemade more smoothandlocallylinear bytheBaldwin
eect. TheBaldwin eect (Baldwin, 1896; HintonandNowlan, 1987) hasbeen
widely studied asamechanism whereb ylearning guides evolution, anditcould
alsoactattheleveloftranscription andtranslation. Consider theevolution
ofapeptide sequence foranewpurpose,assuming theeectiv eness ofthe
peptide ishighly nonlinear function ofthesequence, perhaps havingasmall
island ofgoodsequences surrounded byanoceanofequally badsequences.
Inanorganism whose transcription andtranslation machinery isawless, the
tness willbeanequally nonlinear function oftheDNA sequence andevolution
willwander around theoceanmaking progress towardstheisland onlybya
random walk.Incontrast, anorganism havingthesame DNA sequence, but
whose DNA-to-RNA transcription orRNA-to-protein translation is`faulty',
willoccasionally ,bymistranslation ormistranscription, acciden tallyproducea
working enzyme; anditwilldosowithgreater probabilit yifitsDNA sequence
isclosetoagoodsequence. Onecellmightproduce1000proteins fromthe
onemRNA sequence, ofwhich999havenoenzymatic eect, andonedoes.
Theoneworking catalyst willbeenough forthatcelltohaveanincreased
tness relativ etorivalswhose DNA sequence isfurther fromtheisland of
goodsequences. Forthisreason Iconjecture that,atleastearlyinevolution,
andperhaps stillnow,thegenetic codewasnotimplemen tedperfectly butwas
implemen tednoisily ,withsome codonscodingforadistribution ofpossible
amino acids. Thisnoisy codecould evenbeswitchedonandofromcell
tocellinanorganism byhavingmultiple aminoacyl-tRNA synthetases, some
more reliable thanothers.
Whilst ourmodelassumed thatthebitsofthegenome donotinteract,
ignored thefactthattheinformation isrepresen tedredundan tly,assumed
thatthereisadirect relationship betweenphenot ypictness andthegenotype,
andassumed thatthecrosso verprobabilit yinrecom bination ishigh,Ibelieve
these qualitativ eresults wouldstillholdifmorecomplex modelsoftness and
crosso verwereused: therelativ ebenet ofsexwillstillscaleasp
G.Onlyin
small, in-bred populations arethebenets ofsexexpected tobediminished.
Insummary: Whyhavesex?Because sexisgoodforyourbits!
Further reading
Howdidahigh-information-con tentself-replicating system everemerge inthe
rstplace? Inthegeneral areaoftheorigins oflifeandother trickyquestions
aboutevolution, Ihighly recommend Maynard Smith andSzathmary (1995),
Maynard Smith andSzathmary (1999), Kondrasho v(1988), Maynard Smith
(1988), Dyson (1985), Cairns-Smith (1985), andHopeld (1978).

<<<PAGE 292>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
280 19|WhyhaveSex?Information Acquisition andEvolution
19.6 Further exercises
Exercise 19.4.[3]Howgoodmusttheerror-correcting machinery inDNA repli-
cation be,giventhatmammals havenotalldiedoutlongago?Estimate the
probabilit yofnucleotide substitution, percelldivision. [SeeAppendix C.4.]
Exercise 19.5.[4]GiventhatDNA replication isachievedbybumbling Brow-
nianmotion andordinary thermo dynamics inabiochemical porridge ata
temperature of35C,it'sastonishing thattheerror-rate ofDNA replication
isabout10 9perreplicated nucleotide. Howcanthisreliabilit ybeachieved,
giventhattheenergetic dierence betweenacorrect base-pairing andanincor-
rectoneisonlyoneortwohydrogen bondsandthethermal energykTisonly
aboutafactor offoursmaller thanthefreeenergy associated withahydrogen
bond.Ifordinary thermo dynamics iswhatfavourscorrect base-pairing, surely
thefrequency ofincorrect base-pairing should beabout
f=exp( E/kT); (19.21)
where Eisthefreeenergy dierence, i.e.,anerrorfrequency off'10 4?
HowhasDNA replication cheated thermo dynamics?
Thesituation isequally perplexing inthecaseofprotein synthesis, which
translates anmRNA sequence intoapolypeptide inaccordance withthege-
neticcode.Twospecic chemical reactions areprotected against errors: the
binding oftRNA molecules toamino acids, andtheproduction ofthepolypep-
tideintheribosome, which,likeDNA replication, involvesbase-pairing.
Again, thedelityishigh(anerror rateofabout10 4),andthisdelity
can't becaused bytheenergy ofthe`correct' nalstatebeingespecially low
{thecorrect polypeptide sequence isnotexpected tobesignican tlylowerin
energy thananyother sequence. Howdocellsperform errorcorrection? (See
Hopeld (1974), Hopeld (1980)).
Exercise 19.6.[2]While thegenome acquires information through natural se-
lection atarateofafewbitspergeneration, yourbrain acquires information
atagreater rate.
Estimate atwhatratenewinformation canbestored inlongtermmemory
byyourbrain. Think oflearning thewordsofanewlanguage, forexample.
19.7 Solutions
Solution toexercise 19.1(p.275).Forsmall enoughN,whilst theaverage t-
nessofthepopulation increases, some unluckybitsbecome frozen intothe
badstate. (These badgenes aresometimes knownashitchhikers.) Theho-
mogeneit yassumption breaks down.Eventually,allindividuals haveidentical
genotypesthataremainly 1-bits, buthavesome 0-bits too.Thesmaller the
population, thegreater thenumberoffrozen 0-bits isexpected tobe.How
small canthepopulation sizeNbeifthetheory ofsexisaccurate?
Wendexperimen tallythatthetheory based onassuming homogeneit y
onlytspoorlyifthepopulation sizeNissmaller thanp
G.IfNissigni-
cantlysmaller thanp
G,information cannot possibly beacquired atarateas
bigasp
G,sincetheinformation contentoftheBlind Watchmaker'sdecisions
cannot beanygreater than2Nbitspergeneration, thisbeingthenumberof
bitsrequired tospecifywhichofthe2Nchildren gettoreproduce. Baum etal.
(1995), analyzing asimilar model,showthatthepopulation sizeNshould be
aboutp
G(logG)2tomakehitchhikersunlikelytoarise.

<<<PAGE 293>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
PartIV
Probabilities andInference

<<<PAGE 294>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
AboutPartIV
Thenumberofinference problems thatcan(andperhaps should) betackled
byBayesian inference metho dsisenormous. Inthisbook,forexample, we
discuss thedecodingproblem forerror-correcting codes,thetaskofinferring
clusters fromdata, thetaskofinterpolation through noisy data, andthetask
ofclassifying patterns givenlabelledexamples. Most techniques forsolving
these problems canbecategorized asfollows.
Exact metho dscompute therequired quantities directly .Onlyafewinter-
esting problems haveadirect solution, butexact metho dsareimportant
astoolsforsolving subtasks within larger problems. Metho dsforthe
exact solution ofinference problems arethesubjectofChapters 21,24,
25,and26.
Appro ximate metho dscanbesubdivided into
1.deterministic approximations ,whichinclude maxim umlikeli-
hood(Chapter 22),Laplace's metho d(Chapters 27and28)and
variational metho ds(Chapter 33);and
2.MonteCarlo metho ds{techniques inwhichrandom numbers
playanintegral part{whichwillbediscussed inChapters 29,30,
and32.
Thispartofthebookdoesnotformaone-dimensional story.Rather, the
ideasmakeupawebofinterrelated threads whichwillrecom bineinsubsequen t
chapters.
Chapter 3,whichisanhonorary memberofthispart,discussed arange of
simple examples ofinference problems andtheirBayesian solutions.
Togivefurther motivationforthetoolboxofinference metho dsdiscussed in
thispart,Chapter 20discusses theproblem ofclustering; subsequen tchapters
discuss theprobabilistic interpretation ofclustering asmixture modelling.
Chapter 21discusses theoption ofdealing withprobabilit ydistributions
bycompletely enumerating allhypotheses. Chapter 22introduces theidea
ofmaximization metho dsasawayofavoiding thelargecostassociated with
complete enumeration, andpointsoutreasons whymaxim umlikelihoodis
notgoodenough. Chapter 23reviews theprobabilit ydistributions thatarise
most often inBayesian inference. Chapters 24,25,and26discuss another
wayofavoiding thecostofcomplete enumeration: marginalization. Chapter
25discusses message-passing metho dsappropriate forgraphical models,using
thedecodingoferror-correcting codesasanexample. Chapter 26combines
these ideas withmessage-passing concepts fromChapters 16and17.These
chapters areaprerequisite fortheunderstanding ofadvanced error-correcting
codes.
Chapter 27discusses deterministic approximations including Laplace's
metho d.Thischapter isaprerequisite forunderstanding thetopicofcomplex-
itycontrolinlearning algorithms, anideathatisdiscussed ingeneral terms
inChapter 28.
282

<<<PAGE 295>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
AboutPartIV 283
Chapter 29discusses MonteCarlo metho ds.Chapter 30givesdetails of
state-of-the-art MonteCarlo techniques.
Chapter 31introduces theIsingmodelasatest-b edforprobabilistic meth-
ods.Anexact message-passing metho dandaMonteCarlo metho daredemon-
strated. Amotivation forstudying theIsing modelisthatitisintimately
related toseveralneural networkmodels.Chapter 32describ es`exact' Monte
Carlo metho dsanddemonstrates theirapplication totheIsingmodel.
Chapter 33discusses variational metho dsandtheir application toIsing
modelsandtosimple statistical inference problems including clustering. This
chapter willhelpthereader understand theHopeld network(Chapter 42)
andtheEMalgorithm, whichisanimportantmetho dinlatent-variable mod-
elling. Chapter 34discusses aparticularly simple latentvariable modelcalled
independen tcomponentanalysis.
Chapter 35discusses aragbag ofassorted inference topics. Chapter 36
discusses asimple example ofdecision theory .Chapter 37discusses dierences
betweensampling theory andBayesian metho ds.
Atheme: whatinferenceisabout
Awidespread misconception isthattheaimofinference istondthemost
probableexplanation forsomedata. While thismostprobable hypothesis may
beofinterest, andsomeinference metho dsdolocateit,thishypothesis isjust
thepeakofaprobabilit ydistribution, anditisthewhole distribution thatis
ofinterest. AswesawinChapter 4,themostprobableoutcome fromasource
isoften notatypicaloutcome fromthatsource. Similarly ,themostprobable
hypothesis givensome datamaybeatypical ofthewhole setofreasonably-
plausible hypotheses.
AboutChapter 20
Before reading thenextchapter, exercise 2.17(p.36)andsection 11.2(inferring
theinput toaGaussian channel) arerecommended reading.

<<<PAGE 296>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
20
AnExample Inference Task:Clustering
Human brains aregoodatnding regularities indata. Onewayofexpressing
regularit yistoputasetofobjectsintogroups thataresimilar toeachother.
Forexample, biologists havefound thatmost objectsinthenatural world
fallintooneoftwocategories: things thatarebrownandrunaway,and
things thataregreen anddon't runaway.Therstgroup theycallanimals,
andthesecond, plants.We'llcallthisoperation ofgrouping things together
clustering .Ifthebiologist further sub-divides thecluster ofplantsintosub-
clusters, wewouldcallthis`hierarc hicalclustering'; butwewon'tbetalking
abouthierarc hicalclustering yet.Inthischapter we'lljustdiscuss waysto
takeasetofNobjectsandgroup them intoKclusters.
There areseveralmotivations forclustering. First, agoodclustering has
predictiv epower.When anearlybiologist encoun tersanewgreen thing hehas
notseenbefore, hisinternal modelofplantsandanimals llsinpredictions for
attributes ofthegreen thing: it'sunlikelytojump onhimandeathim;ifhe
touchesit,hemightgetgrazed orstung; ifheeatsit,hemightfeelsick.Allof
these predictions, while uncertain, areuseful, because theyhelpthebiologist
investhisresources (forexample, thetimespentwatchingforpredators) well.
Thus,weperform clustering because webelievetheunderlying cluster labels
aremeaningful, willleadtoamore ecien tdescription ofourdata, andwill
helpuschoosebetter actions. Thistypeofclustering issometimes called
`mixture densit ymodelling', andtheobjectivefunction thatmeasures how
wellthepredictiv emodelisworking istheinformation contentofthedata,
log1=P(fxg).
Second, clusters canbeauseful aidtocomm unication because theyallow
lossycompression. Thebiologist cangivedirections toafriend suchas`goto
thethirdtreeontherightthentakearightturn' (rather than`gopastthe
largegreen thing withredberries, thenpastthelargegreen thing withthorns,
then:::').Thebriefcategory name `tree' ishelpful because itissucien tto
identifyanobject.Similarly ,inlossyimage compression, theaimistoconvey
inasfewbitsaspossible areasonable reproduction ofapicture; onewaytodo
thisistodivide theimage intoNsmall patches,andndaclosematchtoeach
patchinanalphab etofKimage-templates; thenwesendaclosettothe
image bysending thelistoflabelsk1;k2;:::;kNofthematchingtemplates.
Thetaskofcreating agoodlibrary ofimage-templates isequivalenttonding
asetofcluster centres. Thistypeofclustering issometimes called `vector
quantization'.
Wecanformalize avector quantizerinterms ofanassignmen trulex!
k(x)forassigning datap ointsxtooneofKcodenames, andareconstruction
rulek!m(k),theaimbeingtochoosethefunctionsk(x)andm(k)soasto
284

<<<PAGE 297>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
20.1: K-means clustering 285
minimize theexpected distortion ,whichmightbedened tobe
D=X
xP(x)1
2h
m(k(x)) xi2: (20.1)
[Theidealobjectivefunction wouldbetominimize thepsychologically per-
ceiveddistortion oftheimage. Since itishardtoquantifythedistortion
perceivedbyahuman, vector quantization andlossycompression arenotso
crisply dened problems asdatamodelling andlossless compression.] Invec-
torquantization, wedon't necessarily believethatthetemplatesfm(k)ghave
anynatural meaning; theyaresimply toolstodoajob.Wenoteinpassing
thesimilarit yoftheassignmen trule(i.e.,theencoder)ofvector quantization
tothedecodingproblem when decodinganerror-correcting code.
Athird reason formaking acluster modelisthatfailures ofthecluster
modelmayhighligh tinteresting objectsthatdeserv especialattention. If
wehavetrained avector quantizertodoagoodjobofcompressing satellite
pictures ofoceansurfaces, thenmaybepatchesofimage thatarenotwell
compressed bythevector quantizerarethepatchesthatcontainships! Ifthe
biologist encoun tersagreen thing andseesitrun(orslither) away,thismist
withhiscluster model(whichsaysgreen things don't runaway)cueshim
topayspecialattention. Onecan't spendallone's timebeingfascinated by
things; thecluster modelcanhelpsiftoutfromthemultitude ofobjectsin
one's worldtheonesthatreally deserv eattention.
Figure 20.1.N=40datapoints.Afourth reason forliking clustering algorithms isthattheymayserve
asmodelsoflearning processes inneural systems. Theclustering algorithm
thatwenowdiscuss, theK-means algorithm, isanexample ofacompetitive
learning algorithm. Thealgorithm worksbyhavingtheKclusters compete
witheachother fortherighttoownthedatapoints.
20.1 K-means clustering
TheK-means algorithm isanalgorithm forputtingNdatapointsinanI-Aboutthename... AsfarasIknow,
the`K'inK-means clustering simply
refers tothechosen numberofclus-
ters.IfNewton hadfollowedthesame
naming policy,maybewewouldlearn
atschoolabout`calculus forthevari-
ablex'.It'sasillyname, butweare
stuckwithit.dimensional space intoKclusters. Eachcluster isparameterized byavector
m(k)called itsmean.
Thedatapointswillbedenoted byx(n)where thesuperscriptnrunsfrom
1tothenumberofdatapointsN.EachvectorxisavectorwithIcomponents
xi.Wewillassume thatthespace thatxlivesinisarealspace andthatwe
haveametric thatdenes distances betweenpoints,forexample,
d(x;y)=1
2X
i(xi yi)2: (20.2)
Tostart theK-means algorithm (algorithm 20.2), theKmeansfm(k)g
areinitialized insome way,forexample torandom values. Kmeans isthen
aniterativ etwo-step algorithm. Intheassignmen tstep,eachdatapointnis
assigned tothenearest mean. Intheupdatestep,themeans areadjusted to
matchthesample means ofthedatapointsthattheyareresponsible for.
TheK-means algorithm isdemonstrated foratoytwo-dimensional dataset
ingure 20.3,where 2means areused. Theassignmen tsofthepointstothe
twoclusters areindicated bytwopointstyles,andthetwomeans areshown
bythecircles. Thealgorithm converges afterthree iterations, atwhichpoint
theassignmen tsareunchanged sothemeans remain unmovedwhen updated.
TheK-means algorithm alwaysconverges toaxedpoint.

<<<PAGE 298>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
286 20|AnExample Inference Task:Clustering
Initialization .SetKmeansfm(k)gtorandom values.
Assignmen tstep.Eachdatapointnisassigned tothenearest mean.
Wedenote ourguess fortheclusterk(n)thatthepointx(n)belongs
toby^k(n).
^k(n)=argmin
kfd(m(k);x(n))g: (20.3)
Analternativ e,equivalentrepresen tation ofthisassignmen tof
pointstoclusters isgivenby`responsibilities', whichareindicator
variablesrnk.Intheassignmen tstep,wesetr(n)
ktooneifmeank
istheclosest mean todatap ointx(n);otherwiser(n)
kiszero.
r(n)
k=(
1if^k(n)=k
0if^k(n)6=k:(20.4)
What aboutties? {Wedon't expecttwomeans tobeexactly the
same distance fromadatapoint,butifatiedoeshappen,^k(n)is
settothesmallest ofthewinningfkg.
Updatestep.Themodelparameters, themeans, areadjusted tomatch
thesample means ofthedatapointsthattheyareresponsible for.
m(k)=X
nr(n)
kx(n)
R(k)(20.5)
whereR(k)isthetotalresponsibilit yofmeank,
R(k)=X
nr(n)
k: (20.6)
What aboutmeanswithnoresponsibilities? {IfR(k)=0,thenwe
leavethemeanm(k)where itis.
Repeattheassignmen tstepandupdatestep untilthe assign-
mentsdonotchange.Algorithm 20.2.TheK-means
clustering algorithm.

<<<PAGE 299>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
20.1: K-means clustering 287
Data:
Assignmen t Update Assignmen t Update Assignmen t UpdateFigure 20.3.Kmeans algorithm
applied toadatasetof40points.
K=2means evolvetostable
locations afterthree iterations.
Run1
Run2Figure 20.4.Kmeans algorithm
applied toadatasetof40points.
Twoseparate runs,bothwith
K=4means, reachdieren t
solutions. Eachframe showsa
successiv eassignmen tstep.
Exercise 20.1.[4,p.291]SeeifyoucanprovethatK-means alwaysconverges.
[Hint:ndaphysical analogy andanassociated Lyapuno vfunction.]
[ALyapuno vfunction isafunction ofthestate ofthealgorithm that
decreases whenev erthestate changes andthatisbounded below.Ifa
system hasaLyapuno vfunction thenitsdynamics converge.]
TheK-means algorithm withalarger numberofmeans, 4,isdemonstrated in
gure 20.4. Theoutcome ofthealgorithm dependsontheinitial condition.
Intherstcase,afterveiterations, asteady stateisfound inwhichthedata
pointsarefairly evenlysplitbetweenthefourclusters. Inthesecond case,
aftersixiterations, halfthedatapointsareinonecluster, andtheothers are
shared among theother three clusters.
Questions aboutthisalgorithm
TheK-means algorithm hasseveraladhocfeatures. Whydoestheupdatestep
setthe`mean' tothemean oftheassigned points?Where didthedistanced
comefrom? What ifweusedadieren tmeasure ofdistance betweenxandm?
Howcanwechoosethe`best'distance? [Invector quantization, thedistance

<<<PAGE 300>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
288 20|AnExample Inference Task:Clustering
(a)
0246810
0246810(b)
0246810
0246810Figure 20.5.Kmeans algorithm
foracasewithtwodissimilar
clusters. (a)The\little 'n'large"
data. (b)Astable setof
assignmen tsandmeans. Notethat
fourpointsbelonging tothebroad
cluster havebeenincorrectly
assigned tothenarrowercluster.
(a) (b)Figure 20.6.Twoelongated
clusters, andthestable solution
found bytheK-means algorithm.
function isprovided aspartoftheproblem denition; butI'massuming we
areinterested indata-mo delling rather thanvector quantization.] Howdowe
chooseK?Havingfound multiple alternativ eclusterings foragivenK,how
canwechooseamong them?
Cases whereK-meansmight beviewedasfailing.
Further questions arisewhen welookforcases where thealgorithm behaves
badly (compared withwhat themaninthestreet wouldcall`clustering').
Figure 20.5a showsasetof75datapointsgenerated fromamixture oftwo
Gaussians. Theright-hand Gaussian haslessweight(only onefthofthedata
points),anditisalessbroad cluster. Figure 20.5b showstheoutcome ofusing
K-means clustering withK=2means. Fourofthebigcluster's datapoints
havebeenassigned tothesmall cluster, andbothmeans endupdisplaced
totheleftofthetruecentresoftheclusters. TheK-means algorithm takes
accoun tonlyofthedistance betweenthemeans andthedatapoints;ithas
norepresen tation oftheweightorbreadth ofeachcluster. Consequen tly,data
pointswhichactually belong tothebroad cluster areincorrectly assigned to
thenarrowcluster.
Figure 20.6showsanother caseofK-means behavingbadly.Thedata
eviden tlyfallsintotwoelongated clusters. Buttheonlystable state ofthe
K-means algorithm isthatshowningure 20.6b: thetwoclusters havebeen
sliced inhalf!These twoexamples showthatthere issomething wrong with
thedistancedintheK-means algorithm. TheK-means algorithm hasnoway
ofrepresen tingthesizeorshapeofacluster.
Analcriticism ofK-means isthatitisa`hard' rather than a`soft'
algorithm: pointsareassigned toexactly onecluster andallpointsassigned
toacluster areequals inthatcluster. Pointslocated neartheborder between
twoormore clusters should, arguably ,playapartial roleindetermining the
locations ofalltheclusters thattheycould plausibly beassigned to.Butin
theK-means algorithm, eachborderline pointisdump edinonecluster, and

<<<PAGE 301>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
20.2: SoftK-means clustering 289
hasanequal votewithalltheother pointsinthatcluster, andnovoteinany
other clusters.
20.2 SoftK-means clustering
These criticisms ofK-means motivatethe`softK-means algorithm', algo-
rithm 20.7. Thealgorithm hasoneparameter, ,whichwecould termthe
stiness .
Assignmen tstep.Eachdatapointx(n)isgivenasoft`degree ofas-
signmen t'toeachofthemeans. Wecallthedegree towhichx(n)
isassigned toclusterktheresponsibilit yr(n)
k(theresponsibilit yof
clusterkforpointn).
r(n)
k=exp
 d(m(k);x(n))
P
k0exp  d(m(k0);x(n)): (20.7)
ThesumoftheKresponsibilities forthenthpointis1.
Updatestep.Themodelparameters, themeans, areadjusted tomatch
thesample means ofthedatapointsthattheyareresponsible for.
m(k)=X
nr(n)
kx(n)
R(k)(20.8)
whereR(k)isthetotalresponsibilit yofmeank,
R(k)=X
nr(n)
k: (20.9)Algorithm 20.7.SoftK-means
algorithm, version 1.
Notice thesimilarit yofthissoftK-means algorithm tothehardK-means
algorithm 20.2. Theupdatestepisidentical; theonlydierence isthatthe
responsibilities r(n)
kcantakeonvalues between0and1.Whereas theassign-
ment^k(n)intheK-means algorithm involveda`min' overthedistances, the
ruleforassigning theresponsibilities isa`soft-min' (20.7).
.Exercise 20.2.[2]Showthatasthestinessgoesto1,thesoftKmeans algo-
rithm becomes identicaltotheoriginal hardK-means algorithm, except
forthewayinwhichmeans withnoassigned pointsbehave.Describ e
whatthose means doinstead ofsitting still.
Dimensionally ,thestinessisaninverse-length-squared, sowecanas-
sociate alengthscale, 1=p,withit.ThesoftK-means algorithm is
demonstrated ingure 20.8. Thelengthscale isshownbytheradius ofthe
circles surrounding thefourmeans. Eachpanel showsthenalxed point
reachedforadieren tvalueofthelengthscale .
20.3 Conclusion
Atthispoint,wemayhavexedsome oftheproblems withtheoriginal K-
means algorithm byintroducing anextra complexit y-controlparameter.But
howshould weset?Andwhatabouttheproblem oftheelongated clusters,

<<<PAGE 302>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
290 20|AnExample Inference Task:Clustering
Large:::
:::
:::smallFigure 20.8.SoftK-means
algorithm, version 1,applied toa
datasetof40points.K=4.
Implicit lengthscale parameter
=1=1=2variedfromalargeto
asmall value. Eachpicture shows
thestateofallfourmeans, with
theimplicit lengthscale shownby
theradius ofthefourcircles, after
running thealgorithm forseveral
tensofiterations. Atthelargest
lengthscale, allfourmeans
convergeexactly tothedata
mean. Then thefourmeans
separate intotwogroups oftwo.
Atshorter lengthscales, eachof
these pairsitselfbifurcates into
subgroups.
andtheclusters ofunequal weightandwidth? Adding onestiness parameter
isnotgoing tomakeallthese problems goaway.
We'llcome backtothese questions inalaterchapter, aswedevelopthe
mixture-densit y-modelling viewofclustering.
Further reading
Foravector-quan tization approac htoclustering see(Luttrell, 1989; Luttrell,
1990).
20.4 Exercises
.Exercise 20.3.[3,p.291]Explore theproperties ofthesoftK-means algorithm,
version 1,assuming thatthedatap ointsfxgcomefromasingle separable
two-dimensional Gaussian distribution withmean zeroandvariances
(var(x1);var(x2))=(2
1;2
2),with2
1>2
2.SetK=2,assumeNis
large, andinvestigate thexedpointsofthealgorithm asisvaried.
[Hint:assume thatm(1)=(m;0)andm(2)=( m;0).]
.Exercise 20.4.[3]Consider thesoftK-means algorithm applied toalarge
-11amoun tofone-dimensional datathatcomes fromamixture oftwoequal-
weightGaussians withtruemeans=1andstandard deviationP,
forexampleP=1.ShowthatthehardK-means algorithm withK=2
leads toasolution inwhichthetwomeans arefurther apart thanthe
twotruemeans. Discuss what happensforother values of,andnd
thevalueofsuchthatthesoftalgorithm putsthetwomeans inthe
correct places.

<<<PAGE 303>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
20.5: Solutions 291
20.5 Solutions
Solution toexercise 20.1(p.287).Wecanassociatean`energy' withthestate
oftheK-means algorithm byconnecting aspring betweeneachpointx(n)and
themean thatisresponsible forit.Theenergy ofonespring isproportional to
itssquared-length, namelyd(x(n);m(k))whereisthestiness ofthespring.
Thetotalenergy ofallthesprings isaLyapuno vfunction forthealgorithm,
because (a)theassignmen tstepcanonlydecrease theenergy {apointonly
changes itsallegiance ifthelength ofitsspring wouldbereduced; (b)the
updatestepcanonlydecrease theenergy {movingm(k)tothemean isthe
waytominimize theenergy ofitssprings; and(c)theenergy isbounded below
{whichisthesecond condition foraLyapuno vfunction. Since thealgorithm
hasaLyapuno vfunction, itconverges.
Solution toexercise 20.3(p.290).Ifthemeans areinitialized tom(1)=(m;0)
andm(1)=( m;0),theassignmen tstepforapointatlocationx1;x2givesm1m2m2m1
Figure 20.9.Schematic diagram of
thebifurcation asthelargest data
variance1increases frombelow
1=1=2toabove1=1=2.Thedata
variance isindicated bythe
ellipse.r1(x)=exp( (x1 m)2=2)
exp( (x1 m)2=2)+exp( (x1+m)2=2)(20.10)
=1
1+exp( 2mx1); (20.11)
andtheupdatedmis
m0=Rdx1P(x1)x1r1(x)Rdx1P(x1)r1(x)(20.12)
=2Z
dx1P(x1)x11
1+exp( 2mx1): (20.13)
Now,m=0isaxedpoint,butthequestion is,isitstable orunstable? For
-4-3-2-101234
00.511.522.533.54-2-1012Data density
Mean locations
-2-1012
Figure 20.10.Thestable mean
locations asafunction of1,for
constan t,found numerically
(thicklines), andthe
approximation (20.22) (thinlines).tinym(that is,1m1),wecanTaylorexpand
1
1+exp( 2mx1)'1
2(1+mx1)+ (20.14)
so
m0'Z
dx1P(x1)x1(1+mx1) (20.15)
=2
1m: (20.16)
Forsmallm,meither growsordecaysexponentially under thismapping,
depending onwhether2
1isgreater thanorlessthan1.Thexed point
m=0isstable if
2
11= (20.17)
andunstable otherwise. [Inciden tally,thisderivation showsthatthisresult is
general, holding foranytrueprobabilit ydistribution P(x1)havingvariance
2
1,notjusttheGaussian.]
If2
1>1=thenthereisabifurcation andtherearetwostable xedpoints
surrounding theunstable xedpointatm=0.Toillustrate thisbifurcation,
gure 20.10 showstheoutcome ofrunning thesoftK-means algorithm with
=1onone-dimensional datawithstandard deviation1forvarious valuesof
1.Figure 20.11 showsthispitchfork bifurcation fromtheother pointofview,
where thedata's standard deviation1isxedandthealgorithm's lengthscale
=1=1=2isvariedonthehorizon talaxis.-0.8-0.6-0.4-0.200.20.40.60.8
00.5 11.5 2-2-1012Data density
Mean locns.
-2-1012
Figure 20.11.Thestable mean
locations asafunction of1=1=2,
forconstan t1.

<<<PAGE 304>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
292 20|AnExample Inference Task:Clustering
Hereisacheaptheory tomodelhowthetted parameters mbehavebeyondthe
bifurcation, based oncontinuingtheseries expansion. Thiscontinuation oftheseries
israther suspect,since theseries isn'tnecessarily expected toconvergebeyondthe
bifurcation point,butthetheory tswellanyway.
Wetakeouranalytic approac honetermfurther intheexpansion
1
1+exp( 2mx1)'1
2(1+mx1 1
3(mx1)3)+ (20.18)
thenwecansolvefortheshapeofthebifurcation toleading order, whichdependson
thefourth momen tofthedistribution:
m0'Z
dx1P(x1)x1(1+mx1 1
3(mx1)3) (20.19)
=2
1m 1
3(m)334
1: (20.20)
[At(20.20) weusethefactthatP(x1)isGaussian tondthefourth momen t.]This
maphasaxedpointatmsuchthat
2
1(1 (m)22
1)=1; (20.21)
i.e.,
m= 1=2(2
1 1)1=2
2
1: (20.22)
Thethinlineingure 20.10 showsthistheoretical approximation. Figure 20.10 shows
thebifurcation asafunction of1forxed;gure 20.11 showsthebifurcation asa
function of1=1=2forxed1.
.Exercise 20.5.[2,p.292]Whydoesthepitchfork ingure 20.11 tendtotheval-
ues0:8as1=1=2!0?Giveananalytic expression forthisasymp-
tote.
Solution toexercise 20.5(p.292).Theasymptote isthemean oftherectied
Gaussian,R1
0Normal (x;1)xdx
1=2=q
2='0:798: (20.23)

<<<PAGE 305>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
21
Exact Inference byComplete
Enumeration
Weopenourtoolboxofmetho dsforhandling probabilities bydiscussing a
brute-force inference metho d:complete enumeration ofallhypotheses, and
evaluation oftheirprobabilities. Thisapproac hisanexact metho d,andthe
dicult yofcarrying itoutwillmotivatethesmarter exact andapproximate
metho dsintroduced inthefollowingchapters.
21.1 Theburglar alarm
Bayesian probabilit ytheory issometimes called `common sense, amplied'.
When thinking aboutthefollowingquestions, please askyourcommon sense
whatitthinks theanswersare;wewillthenseehowBayesianmetho dsconrm
youreverydayintuition.jEarthquak e
@@RjBurglar
  	jAlarm  	j
Radio@@Rj
Phonecall
Figure 21.1.Belief networkforthe
burglar alarm problem.Example 21.1. FredlivesinLosAngeles andcomm utes60miles towork.
Whilst atwork,hereceivesaphone-call fromhisneighboursayingthat
Fred'sburglar alarm isringing. What istheprobabilit ythatthere was
aburglar inhishouse today?While driving home toinvestigate, Fred
hears ontheradio thatthere wasasmall earthquak ethatdaynearhis
home. `Oh', hesays,feeling relieved,`itwasprobably theearthquak e
thatsetothealarm'. What istheprobabilit ythatthere wasaburglar
inhishouse? (After Pearl,1988).
Let'sintroducevariablesb(aburglar waspresen tinFred'shouse today),
a(thealarm isringing),p(Fredreceivesaphonecall fromtheneighbourre-
porting thealarm),e(asmall earthquak etookplace todaynearFred'shouse),
andr(theradio reportofearthquak eisheard byFred). Theprobabilit yof
allthese variables mightfactorize asfollows:
P(b;e;a;p;r)=P(b)P(e)P(ajb;e)P(pja)P(rje); (21.1)
andplausible values fortheprobabilities are:
1.Burglar probabilit y:
P(b=1)=;P(b=0)=1 ; (21.2)
e.g.,=0:001givesamean burglary rateofonceeverythree years.
2.Earthquak eprobabilit y:
P(e=1)=;P(e=0)=1 ; (21.3)
293

<<<PAGE 306>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
294 21|Exact Inference byComplete Enumeration
with, e.g.,=0:001;ourassertion thattheearthquak esareindependen t
ofburglars, i.e.,thepriorprobabilit yofbandeisP(b;e)=P(b)P(e),
seems reasonable unless wetakeintoaccoun topportunistic burglars who
strikeimmediately afterearthquak es.
3.Alarm ringing probabilit y:weassume thealarm willringifanyofthe
followingthree eventshappens:(a)aburglar entersthehouse, andtrig-
gersthealarm (let'sassume thealarm hasareliabilit yofb=0:99,i.e.,
99%ofburglars trigger thealarm); (b)anearthquak etakesplace, and
triggers thealarm (perhapse=1%ofalarms aretriggered byearth-
quakes?);or(c)some other eventcauses afalsealarm; let'sassume the
falsealarm ratefis0.001, soFredhasfalsealarms fromnon-earthquak e
causes onceeverythree years.[This typeofdependence ofaonbande
isknownasa`noisy-or'.] Theprobabilities ofagivenbandearethen:
P(a=0jb=0;e=0)=(1 f); P(a=1jb=0;e=0)=f
P(a=0jb=1;e=0)=(1 f)(1 b);P(a=1jb=1;e=0)=1 (1 f)(1 b)
P(a=0jb=0;e=1)=(1 f)(1 e);P(a=1jb=0;e=1)=1 (1 f)(1 e)
P(a=0jb=1;e=1)=(1 f)(1 b)(1 e);P(a=1jb=1;e=1)=1 (1 f)(1 b)(1 e)
or,innumbers,
P(a=0jb=0;e=0)=0:999;P(a=1jb=0;e=0)=0:001
P(a=0jb=1;e=0)=0:00999;P(a=1jb=1;e=0)=0:99001
P(a=0jb=0;e=1)=0:98901;P(a=1jb=0;e=1)=0:01099
P(a=0jb=1;e=1)=0:0098901;P(a=1jb=1;e=1)=0:9901099:
Weassume theneighbourwouldneverphone ifthealarm isnotringing
[P(p=1ja=0)=0];andthattheradio isatrustworthyreporter too
[P(r=1je=0)=0];wewon'tneedtospecifytheprobabilities P(p=1ja=1)
orP(r=1je=1)inorder toanswerthequestions above,sincetheoutcomes
p=1andr=1giveuscertain tyrespectivelythata=1ande=1.
Wecananswerthetwoquestions abouttheburglar bycomputing the
posterior probabilities ofallhypotheses giventheavailable information. Let's
startbyreminding ourselv esthattheprobabilit ythatthereisaburglar, before
eitherporrisobserv ed,isP(b=1)==0:001,andtheprobabilit ythatan
earthquak etookplace isP(e=1)==0:001,andthese twopropositions are
independent .
First, whenp=1,weknowthatthealarm isringing:a=1.Theposterior
probabilit yofbandebecomes:
P(b;eja=1)=P(a=1jb;e)P(b)P(e)
P(a=1): (21.4)
Thenumerator's fourpossible values are
P(a=1jb=0;e=0)P(b=0)P(e=0)=0:0010:9990:999=0:000998
P(a=1jb=1;e=0)P(b=1)P(e=0)=0:990010:0010:999=0:000989
P(a=1jb=0;e=1)P(b=0)P(e=1)=0:010990:9990:001=0:000010979
P(a=1jb=1;e=1)P(b=1)P(e=1)=0:99010990:0010:001=9:910 7:
Thenormalizing constan tisthesumofthese fournumbers,P(a=1)=0:002,
andtheposterior probabilities are
P(b=0;e=0ja=1)=0:4993
P(b=1;e=0ja=1)=0:4947
P(b=0;e=1ja=1)=0:0055
P(b=1;e=1ja=1)=0:0005:(21.5)

<<<PAGE 307>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
21.2: Exact inference forcontinuoushypothesis spaces 295
Toanswerthequestion, `what's theprobabilit yaburglar wasthere?' we
marginalize overtheearthquak evariablee:
P(b=0ja=1)=P(b=0;e=0ja=1)+P(b=0;e=1ja=1)=0:505
P(b=1ja=1)=P(b=1;e=0ja=1)+P(b=1;e=1ja=1)=0:495:
(21.6)
Sothere isnearly a50%chance thatthere wasaburglar presen t.Itisimpor-
tanttonotethatthevariablesbande,whichwereindependen tapriori,are
nowdependent .Theposterior distribution (21.5) isnotaseparable function of
bande.Thisfactisillustrated mostsimply bystudying theeect oflearning
thate=1.
When welearne=1,theposterior probabilit yofbisgivenby
P(bje=1;a=1)=P(b;e=1ja=1)=P(e=1ja=1),i.e.,bydividing thebot-
tomtworowsof(21.5), bytheirsumP(e=1ja=1)=0:0060. Theposterior
probabilit yofbis:
P(b=0je=1;a=1)=0:92
P(b=1je=1;a=1)=0:08:(21.7)
There isthusnowan8%chance thataburglar wasinFred's house. Itis
inaccordance witheverydayintuition thattheprobabilit ythatb=1(apos-
siblecause ofthealarm) reduces when Fredlearns thatanearthquak e,an
alternativ eexplanation ofthealarm, hashappened.
Explaining away
Thisphenomenon, thatoneofthepossible causes (b=1)ofsome data(the
datainthiscasebeinga=1)becomes lessprobable when another ofthecauses
(e=1)becomes more probable, eventhough those twocauses wereindepen-
dentvariables apriori,isknownasexplaining away.Explaining awayisan
importantfeature ofcorrect inferences, andonethatanyarticial intelligence
should replicate.
Ifwebelievethattheneighbourandtheradio service areunreliable or
capricious, sothatwearenotcertain thatthealarm really isringing orthat
anearthquak ereally hashappened, thecalculations become more complex,
buttheexplaining-a wayeect persists; thearrivaloftheearthquak ereportr
simultaneously makesitmoreprobable thatthealarm truly isringing, and
lessprobable thattheburglar waspresen t.
Insummary ,wesolvedtheinference questions abouttheburglar byenu-
merating allfourhypotheses aboutthevariables (b;e),nding theirposterior
probabilities, andmarginalizing toobtain therequired inferences aboutb.
.Exercise 21.2.[2]After Fredreceivesthephone-call abouttheburglar alarm,
butbeforehehears theradio report,what, fromhispointofview, isthe
probabilit ythatthere wasasmall earthquak etoday?
21.2 Exact inference forcontinuoushypothesis spaces
Manyofthehypothesis spaces wewillconsider arenaturally though tofas
continuous. Forexample, theunkno wndecaylengthofsection 3.1(p.48)
livesinacontinuousone-dimensional space; andtheunkno wnmean andstan-
darddeviation ofaGaussian;liveinacontinuoustwo-dimensional space.
Inanypractical computer implemen tation, suchcontinuousspaces willneces-
sarily bediscretized, however,andsocan,inprinciple, beenumerated {ata
gridofparameter values, forexample. Ingure 3.2weplotted thelikelihood

<<<PAGE 308>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
296 21|Exact Inference byComplete Enumeration
Figure 21.2.Enumeration ofan
entire(discretized) hypothesis
space foroneGaussian with
parameters (horizon talaxis)
and(vertical).
function forthedecaylength asafunction ofbyevaluating thelikelihood
atanely-spaced series ofpoints.
Atwo-parameter model
Let'slookattheGaussian distribution asanexample ofamodelwithatwo-
dimensional hypothesis space. Theone-dimensional Gaussian distribution is
parameterized byameanandastandard deviation:
P(xj;)=1p
2exp 
 (x )2
22!
Normal (x;;2): (21.8)
Figure 21.2showsanenumeration ofonehundred hypotheses aboutthemean
andstandard deviation ofaone-dimensional Gaussian distribution. These
hypotheses areevenlyspaced inatenbytensquare gridcovering tenvalues
ofandtenvaluesof.Eachhypothesis isrepresen tedbyapicture showing
theprobabilit ydensit ythatitputsonx.Wenowexamine theinference of-0.5 00.5 11.5 22.5
Figure 21.3.Fivedatap oints
fxng5
n=1.Thehorizon tal
coordinate isthevalueofthe
datum,xn;thevertical coordinate
hasnomeaning.
andgivendatapointsxn,n=1;:::;N,assumed tobedrawnindependen tly
fromthisdensit y.
Imagine thatweacquire data, forexample thevepointsshowning-
ure21.3. Wecannowevaluate theposterior probabilit yofeachoftheone
hundred subhypotheses byevaluating thelikelihoodofeach,thatis,thevalue
ofP(fxng5
n=1j;).Thelikelihoodvalues areshowndiagrammatically in
gure 21.4using thelinethickness toencodethevalueofthelikelihood.Sub-
hypotheses withlikelihoodsmaller thane 8times themaxim umlikelihood
havebeendeleted.
Using anergrid,wecanrepresen tthesame information byplotting the
likelihoodasasurface plotorcontourplotasafunction ofand(gure 21.5).
Ave-parameter mixtur emodel
Eyeballing thedata(gure 21.3), youmightagree thatitseems more plau-
siblethattheycome notfromasingle Gaussian butfromamixture oftwo

<<<PAGE 309>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
21.2: Exact inference forcontinuoushypothesis spaces 297
Figure 21.4.Likelihoodfunction,
giventhedataofgure 21.3,
represen tedbylinethickness.
Subhypotheses havinglikelihood
smaller thane 8times the
maxim umlikelihoodarenot
shown.
00.511.52
0.20.40.60.8100.010.020.030.040.050.06
meansigma0 0.5 1 1.5 20.10.20.30.40.50.60.70.80.91
meansigmaFigure 21.5.Thelikelihood
function fortheparameters ofa
Gaussian distribution.
Surface plotandcontourplotof
theloglikelihoodasafunction of
and.ThedatasetofN=5
pointshadmean x=1:0and
S2=P(x x)2=1:0.

<<<PAGE 310>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
298 21|Exact Inference byComplete Enumeration
Figure 21.6.Enumeration ofthe
entire(discretized) hypothesis
space foramixture oftwo
Gaussians. Weightofthemixture
componentsis1;2=0:6;0:4in
thetophalfand0:8;0:2inthe
bottom half.Means1and2
varyhorizon tally,andstandard
deviations1and2vary
vertically .
Gaussians, dened bytwomeans, twostandard deviations, andtwomixing
coecien ts1and2,satisfying1+2=1,i0.
P(xj1;1;1;2;2;2)=1p
21exp
 (x 1)2
22
1
+2p
22exp
 (x 2)2
22
2
Let'senumerate thesubhypotheses forthisalternativ emodel.Theparameter
space isve-dimensional, soitbecomes challenging torepresen titonasingle
page. Figure 21.6enumerates 800subhypotheses withdieren tvalues ofthe
veparameters 1;2;1;2;1.Themeans arevaried betweenvevalues
eachinthehorizon taldirections. Thestandard deviations takeonfourvalues
eachvertically .And1takesontwovalues vertically .Wecanrepresen tthe
inference aboutthese veparameters inthelightofthevedatap ointsas
showningure 21.7.
Ifwewishtocompare theone-Gaussian modelwiththemixture-of-t wo
model,wecanndthemodels' posterior probabilities byevaluating the
marginal likelihoodorevidence foreachmodelH,P(fxgjH).Theevidence
isgivenbyintegrating overtheparameters, ;theintegration canbeimple-
mentednumerically bysumming overthealternativ eenumerated values of
,
P(fxgjH)=X P()P(fxgj;H); (21.9)
whereP()isthepriordistribution overthegridofparameter values, which
Itaketobeuniform.
Forthemixture oftwoGaussians thisintegral isave-dimensional integral;
ifitistobeperformed atallaccurately ,thegridofpointswillneedtobe
muchnerthanthegridsshowninthegures. Iftheuncertain tyabouteach
ofKparameters hasbeenreduced by,say,afactor oftenbyobserving the

<<<PAGE 311>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
21.2: Exact inference forcontinuoushypothesis spaces 299
Figure 21.7.Inferring amixture of
twoGaussians. Likelihood
function, giventhedataof
gure 21.3,represen tedbyline
thickness. Thehypothesis space is
identicaltothatshownin
gure 21.6.Subhypotheses having
likelihoodsmaller thane 8times
themaxim umlikelihoodarenot
shown,hence theblank regions,
whichcorresp ondtohypotheses
thatthedatahaveruled out.
-0.5 00.5 11.5 22.5
data, thenbrute forceintegration requires agridofatleast10Kpoints.This
exponentialgrowthofcomputation withmodelsizeisthereason whycomplete
enumeration israrely afeasible computational strategy .
Exercise 21.3.[1]Imagine tting amixture oftenGaussians todata ina
twenty-dimensional space. Estimate thecomputational costofimple-
mentinginferences forthismodelbyenumeration ofagridofparameter
values.

<<<PAGE 312>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
22
Maxim umLikelihoodandClustering
Rather thanenumerate allhypotheses {whichmaybeexponentialinnumber
{wecansavealotoftimebyhoming inononegoodhypothesis thatts
thedatawell.Thisisthephilosoph ybehind themaxim umlikelihoodmetho d,
whichidenties thesetting oftheparameter vector thatmaximizes the
likelihood,P(Dataj;H).
Forsome modelsthemaxim umlikelihoodparameters canbeidentied
instan tlyfromthedata; formorecomplex models,nding themaxim umlike-
lihoodparameters mayrequire aniterativ ealgorithm.
Foranymodel,itisusually easiest toworkwiththelogarithm ofthe
likelihoodrather thanthelikelihood,sincelikelihoods,beingproducts ofthe
probabilities ofmanydatapoints,tendtobeverysmall. Likelihoodsmultiply;
loglikelihoodsadd.
22.1 Maxim umlikelihoodforoneGaussian
Wereturn totheGaussian forourrstexamples. Assume wehavedata
fxngN
n=1.Theloglikelihoodis:
lnP(fxngN
n=1j;)= Nln(p
2) X
n(xn )2=(22):(22.1)
Thelikelihoodcanbeexpressed interms oftwofunctions ofthedata, the
sample mean
xNX
n=1xn=N; (22.2)
andthesumofsquare deviations
SX
n(xn x)2: (22.3)
lnP(fxngN
n=1j;)= Nln(p
2) [N( x)2+S]=(22):(22.4)
Because thelikelihooddependsonthedataonlythrough xandS,these two
quantities areknownassucien tstatistics .
Example 22.1. Dieren tiatetheloglikelihoodwithrespecttoandshowthat,
ifthestandard deviation isknowntobe,themaxim umlikelihoodmean
ofaGaussian isequal tothesample mean x,foranyvalueof.
Solution.
@
@lnP= N( x)
2(22.5)
=0when=x. 2(22.6)
300

<<<PAGE 313>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
22.1: Maxim umlikelihoodforoneGaussian 301
(a1)
00.511.52
0.20.40.60.8100.010.020.030.040.050.06
meansigma
(a2)0 0.5 1 1.5 20.10.20.30.40.50.60.70.80.91
meansigma
(b)
00.511.522.533.544.5
00.20.40.60.811.21.41.61.82Posterior
meansigma=0.2
sigma=0.4
sigma=0.6
(c)
00.010.020.030.040.050.060.070.080.09
0.2 0.4 0.60.811.21.41.61.82mu=1   
mu=1.25
mu=1.5 Figure 22.1.Thelikelihood
function fortheparameters ofa
Gaussian distribution.
(a1,a2)Surface plotandcontour
plotoftheloglikelihoodasa
function ofand.Thedataset
ofN=5pointshadmean x=1:0
andS2=P(x x)2=1:0.
(b)Theposterior probabilit yof
forvarious valuesof.
(c)Theposterior probabilit yof
forvarious xedvaluesof.
IfweTaylor-expand theloglikelihoodaboutthemaxim um,wecande-
neapproximate error barsonthemaxim umlikelihoodparameter: weuse
aquadratic approximation toestimate howfarfromthemaxim um-lik elihood
parameter setting wecangobeforethelikelihoodfallsbysomestandard factor,
forexamplee1=2,ore4=2,InthespecialcaseofalikelihoodthatisaGaussian
function oftheparameters, thequadratic approximation isexact.
Example 22.2. Findthesecond derivativeoftheloglikelihoodwithrespectto
,andndtheerrorbarson,giventhedataand.
Solution.
@2
@2lnP= N
2: 2(22.7)
Comparing thiscurvature withthecurvature ofthelogofaGaussian distri-
bution overofstandard deviation,exp( 2=(22
)),whichis1=2
,we
candeduce thattheerrorbarson(deriv edfromthelikelihoodfunction) are
=p
N: (22.8)
Theerrorbarshavethisproperty:atthetwopoints=x,thelikelihood
issmaller thanitsmaxim umvaluebyafactor ofe1=2.
Example 22.3. Findthemaxim umlikelihoodstandard deviationofaGaus-
sian,whose mean isknowntobe,inthelightofdatafxngN
n=1.Find
thesecond derivativeoftheloglikelihoodwithrespecttoln,anderror
barsonln.
Solution. Thelikelihood'sdependence onis
lnP(fxngN
n=1j;)= Nln(p
2) Stot
(22); (22.9)
whereStot=P
n(xn )2.Tondthemaxim umofthelikelihood,wecan
dieren tiatewithrespecttoln.[It'softenmosthygienic todieren tiatewith

<<<PAGE 314>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
302 22|Maxim umLikelihoodandClustering
respecttolnurather thanu,whenuisascalevariable; weusedun=d(lnu)=
nun.]
@lnP(fxngN
n=1j;)
@ln= N+Stot
2(22.10)
Thisderivativeiszerowhen
2=Stot
N; (22.11)
i.e.,
=sPN
n=1(xn )2
N: (22.12)
Thesecond derivativeis
@2lnP(fxngN
n=1j;)
@(ln)2= 2Stot
2; (22.13)
andatthemaxim um-lik elihoodvalueof2,thisequals 2N.Soerrorbars
onlnare
ln=1p
2N: 2 (22.14)
.Exercise 22.4.[1]Showthatthevaluesofandlnthatjointlymaximize the
likelihoodare:f;gML=n
x;N=p
S=No
;where
NsPN
n=1(xn x)2
N: (22.15)
22.2 Maxim umlikelihoodforamixture ofGaussians
Wenowderiveanalgorithm fortting amixture ofGaussians toone-
dimensional data. Infact,thisalgorithm issoimportanttounderstand that,
you,gentlereader, gettoderivethealgorithm. Please workthrough thefol-
lowingexercise.
Exercise 22.5.[2,p.310]Arandom variablexisassumed tohaveaprobabilit y
distribution thatisamixtur eoftwoGaussians ,
P(xj1;2;)="2X
k=1pk1p
22exp 
 (x k)2
22!#
; (22.16)
where thetwoGaussians aregiventhelabelsk=1andk=2;theprior
probabilit yoftheclasslabelkisfp1=1=2;p2=1=2g;fkgarethemeans
ofthetwoGaussians; andbothhavestandard deviation.Forbrevit y,we
denote these parameters byffkg;g.
Adatasetconsists ofNpointsfxngN
n=1whichareassumed tobeindepen-
dentsamples fromthisdistribution. Letkndenote theunkno wnclasslabelof
thenthpoint.
Assuming thatfkgandareknown,showthattheposterior probabilit y
oftheclasslabelknofthenthpointcanbewritten as
P(kn=1jxn;)=1
1+exp[ (w1xn+w0)]
P(kn=2jxn;)=1
1+exp[+(w1xn+w0)];(22.17)
andgiveexpressions forw1andw0.

<<<PAGE 315>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
22.3: Enhancemen tstosoftK-means 303
Assume nowthatthemeansfkgarenotknown,andthatwewishto
inferthem fromthedatafxngN
n=1.(Thestandard deviationisknown.)In
theremainder ofthisquestion wewillderiveaniterativ ealgorithm fornding
values forfkgthatmaximize thelikelihood,
P(fxngN
n=1jfkg;)=Y
nP(xnjfkg;): (22.18)
LetLdenote thelogofthelikelihood.Showthatthederivativeofthelog
likelihoodwithrespecttokisgivenby
@
@kL=X
npkjn(xn k)
2; (22.19)
wherepkjnP(kn=kjxn;)appeared aboveatequation (22.17).
Show,neglecting terms in@
@kP(kn=kjxn;),thatthesecond derivative
isapproximately givenby
@2
@2
kL= X
npkjn1
2: (22.20)
Hence showthatfromaninitial state1;2,anapproximate Newton{Raphson
stepupdates these parameters to0
1;0
2,where
0
k=P
npkjnxnP
npkjn: (22.21)
[TheNewton{Raphson metho dformaximizing L()updatesto0= h
@L
@.
@2L
@2i
.]
0123456
Assuming that=1,sketchacontourplotofthelikelihoodfunctionLasa
function of1and2forthedatasetshownabove.Thedatasetconsists of
32points.Describ ethepeaksinyoursketchandindicate theirwidths.
Notice thatthealgorithm youhavederivedformaximizing thelikelihood
isidenticaltothesoftK-means algorithm ofsection 20.4. Nowthatitisclear
thatclustering canbeviewedasmixture-densit y-modelling, weareableto
deriveenhancemen tstotheK-means algorithm, whichrectify theproblems
wenoted earlier.
22.3 Enhancemen tstosoftK-means
Algorithm 22.2showsaversion ofthesoft-K-means algorithm corresp onding
toamodelling assumption thateachcluster isaspherical Gaussian havingits
ownwidth (eachcluster hasitsown(k)=1/2
k).Thealgorithm updates the
lengthscales kforitself. Thealgorithm alsoincludes cluster weightparame-
ters1;2;:::;Kwhichalsoupdatethemselv es,allowingaccurate modelling
ofdatafromclusters ofunequal weights.Thisalgorithm isdemonstrated in
gure 20.8andgure 22.3fortwodatasetsthatwe'veseenbefore. Thesec-
ondexample showsthatconvergence cantakealongtime, buteventually the
algorithm identiesthesmall cluster andthelargecluster.

<<<PAGE 316>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
304 22|Maxim umLikelihoodandClustering
Assignmen tstep.Theresponsibilities are
r(n)
k=k1
(p
2k)Iexp 
 1
2
kd(m(k);x(n))!
P
k0k1
(p
2k0)Iexp 
 1
2
k0d(m(k0);x(n))! (22.22)
whereIisthedimensionalit y.
Updatestep.Eachcluster's parameters, m(k),k,and2
k,areadjusted
tomatchthedatapointsthatitisresponsible for.
m(k)=X
nr(n)
kx(n)
R(k)(22.23)
2
k=X
nr(n)
k(x(n) m(k))2
IR(k)(22.24)
k=R(k)
P
kR(k)(22.25)
whereR(k)isthetotalresponsibilit yofmeank,
R(k)=X
nr(n)
k; (22.26)
andIisthedimensionalit yofx.Algorithm 22.2.ThesoftK-means
algorithm, version 2.
t=0 t=1 t=2 t=3 t=9
t=0 t=1 t=10 t=20 t=30 t=35Figure 22.3.SoftK-means
algorithm, withK=2,applied
(a)tothe40-pointdatasetof
gure 20.3;(b)tothelittle'n'
largedatasetofgure 20.5.
r(n)
k=k1
QI
i=1p
2(k)
iexp 
 IX
i=1(m(k)
i x(n)
i)2.
2((k)
i)2!
P
k0(numerator, withk0inplace ofk)(22.27)
2
i(k)=X
nr(n)
k(x(n)
i m(k)
i)2
R(k)(22.28)Algorithm 22.4.ThesoftK-means
algorithm, version 3,which
corresp ondstoamodelof
axis-aligned Gaussians.

<<<PAGE 317>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
22.4: Afatalawofmaxim umlikelihood 305
t=0 t=10 t=20 t=30 Figure 22.5.SoftK-means
algorithm, version 3,applied to
thedataconsisting oftwo
cigar-shap edclusters.K=2(c.f.
gure 20.6).
t=0 t=10 t=20 t=26 t=32 Figure 22.6.SoftK-means
algorithm, version 3,applied to
thelittle'n'largedataset.K=2.
SoftK-means, version 2,isamaxim um-lik elihoodalgorithm fortting a
mixture ofspheric alGaussians todata{`spherical' meaning thatthevariance Aproofthatthealgorithm doesin-
deed maximize thelikelihoodisde-
ferred tosection 33.7.oftheGaussian isthesame inalldirections. Thisalgorithm isstillnogood
atmodelling thecigar-shap edclusters ofgure 20.6. Ifwewishtomodelthe
clusters byaxis-aligned Gaussians withpossibly-unequal variances, wereplace
theassignmen trule(22.22) andthevariance updaterule(22.24) bytherules
(22.27) and(22.28) displa yedinalgorithm 22.4.
Thisthird version ofsoftK-means isdemonstrated ingure 22.5onthe
`twocigars' datasetofgure 20.6. After 30iterations, thealgorithm has
correctly located thetwoclusters. Figure 22.6showsthesame algorithm
applied tothelittle'n'largedataset,where, again, thecorrect cluster locations
arefound.
22.4 Afatalawofmaxim umlikelihood
Finally ,gure 22.7sounds acautionary note: when wetK=4means toour
rsttoydataset,wesometimes ndthatverysmall clusters form, covering
justoneortwodatapoints.Thisisapathological propertyofsoftK-means
clustering, versions 2and3.
.Exercise 22.6.[2]Investigate what happensifonemeanm(k)sitsexactly on
topofonedatapoint;showthatifthevariance2
kissucien tlysmall,
thennoreturn ispossible:2
kbecomes eversmaller.
t=0 t=5 t=10 t=20 Figure 22.7.SoftKmeans
algorithm applied toadatasetof
40points.K=4.Notice thatat
convergence, oneverysmall
cluster hasformed betweentwo
datapoints.

<<<PAGE 318>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
306 22|Maxim umLikelihoodandClustering
KABOOM!
SoftK-means canblowup.Putonecluster exactly ononedatapointandletits
variance gotozero{youcanobtain anarbitrarily largelikelihood!Maxim um
likelihoodmetho dscanbreak downbynding highly tuned modelsthattpart
ofthedataperfectly .Thisphenomenon isknownasovertting. Thereason
wearenotinterested inthese solutions withenormous likelihoodisthis:sure,
these parameter-settings mayhaveenormous posterior probabilit ydensity ,
butthedensit yislargeoveronlyaverysmallvolume ofparameter space. So
theprobabilit ymass associated withthese likelihoodspikesisusually tiny.
Weconclude thatmaxim umlikelihoodmetho dsarenotasatisfactory gen-
eralsolution todatamodelling problems: thelikelihoodmaybeinnitely large
atcertain parameter settings. Evenifthelikelihooddoesnothaveinnitely-
largespikes,themaxim umofthelikelihoodisoften unrepresen tative,inhigh-
dimensional problems.
Eveninlow-dimensional problems, maxim umlikelihoodsolutions canbe
unrepresen tative.Asyoumayknowfrombasic statistics, themaxim umlike-
lihoodestimator (22.15) foraGaussian's standard deviation,N,isabiased
estimator, atopic thatwe'lltakeupinChapter 24.
Themaximum aposteriori (MAP) method
Apopular replacemen tformaximizing thelikelihoodismaximizing the
Bayesian posterior probabilit ydensit yoftheparameters instead. However,
multiplying thelikelihoodbyaprior andmaximizing theposterior does
notmaketheaboveproblems goaway;theposterior densit yoften alsohas
innitely-large spikes,andthemaxim umoftheposterior probabilit ydensit y
isoften unrepresen tativeofthewhole posterior distribution. Think backto
theconcept oftypicalit y,whichweencoun tered inChapter 4:inhighdimen-
sions, most oftheprobabilit ymass isinatypical setwhose properties are
quite dieren tfromthepointsthathavethemaxim umprobabilit ydensit y.
Maxima areatypical.
Afurther reason fordisliking themaxim umaposteriori isthatitisbasis-
dependent .Ifwemakeanonlinear change ofbasis fromtheparameterto
theparameteru=f()thentheprobabilit ydensit yofistransformed to
P(u)=P()@
@u: (22.29)
Themaxim umofthedensit yP(u)willusually notcoincide withthemaxim um
ofthedensit yP().(Forgures illustrating suchnonlinear changes ofbasis,
seethenextchapter.) Itseems undesirable touseametho dwhose answers
change when wechange represen tation.
Further reading
ThesoftK-means algorithm isattheheart oftheautomatic classication
package,AutoClass (Hanson etal.,1991b; Hanson etal.,1991a).
22.5 Further exercises
Exerciseswheremaximum likeliho odmaybeuseful
Exercise 22.7.[3]Makeaversion oftheK-means algorithm thatmodelsthe
dataasamixture ofKarbitrary Gaussians, i.e.,Gaussians thatarenot
constrained tobeaxis-aligned.

<<<PAGE 319>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
22.5: Further exercises 307
.Exercise 22.8.[2](a)Aphoton counterispointedataremote starforone
minute,inorder toinferthebrightness, i.e.,therateofphotons
arriving atthecounterperminute,.Assuming thenumberof
photons collectedrhasaPoisson distribution withmean,
P(rj)=exp( )r
r!; (22.30)
whatisthemaxim umlikelihoodestimate for,givenr=9?Find
errorbarsonln.
(b)Same situation, butnowweassume thatthecounterdetects not
onlyphotons fromthestarbutalso`background' photons. The
background rateofphotons isknowntobeb=13photons per
minute.Weassume thenumberofphotons collected,r,hasaPois-
sondistribution withmean+b.Now,givenr=9detected photons,
whatisthemaxim umlikelihoodestimate for?Commen tonthis
answer,discussing alsotheBayesianposterior distribution, andthe
`unbiased estimator' ofsampling theory ,^r b.
Exercise 22.9.[2]AbentcoinistossedNtimes, givingNaheads andNbtails.
Assume abetadistribution prior fortheprobabilit yofheads,p,for
example theuniform distribution. Findthemaxim umlikelihoodand
maxim umaposteriori values ofp,thenndthemaxim umlikelihood
andmaxim umaposteriori valuesofthelogitaln[p=(1 p)].Compare
withthepredictiv edistribution, i.e.,theprobabilit ythatthenexttoss
willcome upheads.
.Exercise 22.10.[2]Twomenlookedthroughprison bars;onesawstars, the
othertriedtoinferwherethewindow framewas.
(xmin;ymin)(xmax;ymax)
?
??
??
?Fromtheother sideofaroom,youlookthrough awindo wandsee
starsatlocationsf(xn;yn)g.Youcan't seethewindo wedges because
itistotally darkapart fromthestars. Assuming thewindo wisrectan-
gular andthatthevisible stars's locations areindependen tlyrandomly
distributed, whataretheinferred values of(xmin;ymin,xmax,ymax),ac-
cording tomaxim umlikelihood?Sketchthelikelihoodasafunction of
xmax,forxedxmin,ymin,andymax.
.Exercise 22.11.[3]Asailor infers hislocation (x;y)bymeasuring thebearings
b
bb
        
(x1;y1)
AAAAAAAAA
(x2;y2)QQQQQQ(x3;y3)
Figure 22.8.Thestandard wayof
drawingthree slightlyinconsisten t
bearings onachartproduces a
triangle called acockedhat.
Where isthesailor?ofthree buoyswhose locations (xn;yn)aregivenonhischart. Letthe
truebearings ofthebuoysben.Assuming thathismeasuremen t~nof
eachbearing issubjecttoGaussian noiseofsmall standard deviation,
whatishisinferred location, bymaxim umlikelihood?
Thesailor's ruleofthumbsaysthattheboat'sposition canbetakento
bethecentreofthecockedhat,thetriangle produced bytheintersection
ofthethreemeasured bearings (gure 22.8). Canyoupersuade himthat
themaxim umlikelihoodanswerisbetter?
.Exercise 22.12.[3,p.310]Maximum likelihoodtting ofanexponential-family
model.
Assume thatavariable xcomes fromaprobabilit ydistribution ofthe
form
P(xjw)=1
Z(w)exp X
kwkfk(x)!
; (22.31)

<<<PAGE 320>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
308 22|Maxim umLikelihoodandClustering
where thefunctionsfk(x)aregiven,andtheparameters w=fwkgare
notknown.Adatasetfx(n)gofNpointsissupplied.
Showbydieren tiating theloglikelihoodthatthemaxim um-lik elihood
parameters wMLsatisfy
X
xP(xjwML)fk(x)=1
NX
nfk(x(n)); (22.32)
where theleft-hand sumisoverallx,andtheright-hand sumisoverthe
datapoints.Ashorthand forthisresult isthateachfunction-a verage
under thetted modelmustequal thefunction-a verage found inthe
data:
hfkiP(xjwML)=hfkiData: (22.33)
.Exercise 22.13.[3]`Maximum entrop y'tting ofmodelstoconstraints.
When confron tedbyaprobabilit ydistribution P(x)aboutwhichonlya
fewfactsareknown,themaxim umentropyprinciple (maxen t)oers a
ruleforchoosing adistribution thatsatises those constrain ts.Accord-
ingtomaxen t,youshould select theP(x)thatmaximizes theentropy
H=X
xP(x)log1=P(x); (22.34)
subjecttotheconstrain ts.Assuming theconstrain tsassert thatthe
averagesofcertain functionsfk(x)areknown,i.e.,
hfkiP(x)=Fk; (22.35)
show,byintroducing Lagrange multipliers (oneforeachconstrain t,in-
cluding normalization), thatthemaxim um-en tropydistribution hasthe
form
P(x)Maxen t=1
Zexp X
kwkfk(x)!
; (22.36)
where theparameters Zandfwkgaresetsuchthattheconstrain ts
(22.35) aresatised.
Andhence themaxim umentropymetho dgivesidenticalresults tomax-
imumlikelihoodtting ofanexponential-family model(previous exer-
cise).
Themaxim umentropymetho dhassometimes beenrecommended asametho d
forassigning prior distributions inBayesian modelling. While theoutcomes
ofthemaxim umentropymetho daresometimes interesting andthough t-
provoking, Idonotadvocatemaxen tastheapproac htoassigning priors.
Maxim umentropyisalsosometimes proposedasametho dforsolving inference
problems {forexample, `giventhatthemean score ofthisunfair six-sided die
is2.5,what isitsprobabilit ydistribution (p1;p2;p3;p4;p5;p6)?'Ithink itisa
badideatousemaxim umentropyinthisway;itcangiveverysillyanswers.
Thecorrect waytosolveinference problems istouseBayes'theorem.
Exerciseswheremaximum likeliho odandMAP havediculties
.Exercise 22.14.[2]Thisexercise explores theideathatmaximizing aproba-
bilitydensit yisapoorwaytondapointthatisrepresen tativeof
thedensit y.Consider aGaussian distribution inak-dimensional space,
P(w)=(1=p
2W)kexp( Pk
1w2
i=22
W).Showthatnearly allofthe
probabilit ymass ofaGaussian isinathinshellofradiusr=p
kW

<<<PAGE 321>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
22.5: Further exercises 309
andofthickness proportional tor=p
k.Forexample, in1000dimen-
sions, 90%ofthemassofaGaussian withW=1isinashellofradius
31.6andthickness 2.8.However,theprobabilit ydensity attheorigin is
ek=2'10217times bigger thanthedensit yatthisshellwhere most of
theprobabilit ymassis.
Nowconsider twoGaussian densities in1000dimensions thatdier in
radiusWbyjust1%,andthatcontainequal total probabilit ymass.
Showthatthemaxim umprobabilit ydensit yisgreater atthecentreof
theGaussian withsmallerWbyafactor ofexp(0:01k)'20000.
Inill-posedproblems, atypical posterior distribution isoftenaweighted
superposition ofGaussians withvarying means andstandard deviations,
sothetrueposterior hasaskewpeak,withthemaxim umoftheprob-
abilitydensit ylocated nearthemean oftheGaussian distribution that
hasthesmallest standard deviation, nottheGaussian withthegreatest
weight.
.Exercise 22.15.[3]Theseven scientists .Ndatap ointsfxngaredrawnfrom
Ndistributions, allofwhichareGaussian withacommon meanbut
withdieren tunkno wnstandard deviationsn.What arethemaxim um
likelihoodparameters ;fnggiventhedata? Forexample, seven-30-20-1001020A BCD-G
Scientistxn
A 27.020
B 3.570
C 8.191
D 9.898
E 9.603
F 9.945
G 10.056
Figure 22.9.Sevenmeasuremen ts
fxngofaparameterbyseven
scientistseachhavinghisown
noise-lev eln.
scientists(A,B,C,D,E,F,G)withwildly-diering experimen talskills
measure.Youexpectsome ofthem todoaccurate work(i.e.,tohave
smalln),andsome ofthem toturninwildly inaccurate answers(i.e.,
tohaveenormousn).Figure 22.9showstheirsevenresults. What is
,andhowreliable iseachscientist?
Ihopeyouagree that,intuitively,itlooksprettycertain thatAandB
arebothinept measurers, thatD{Garebetter, andthatthetruevalue
ofissomewhere closeto10.Butwhatdoesmaximizing thelikelihood
tellyou?
Exercise 22.16.[3]Problems withMAP metho d.Acollection ofwidgetsi=
1:::khaveapropertycalled `wodge',wi,whichwemeasure, widget by
widget, innoisy experimen tswithaknownnoise level=1:0.Our
modelforthese quantities isthattheycome from aGaussian prior
P(wij)=Normal (0;1/),where=1=2
Wisnotknown.Ourprior
forthisvariance isatoverlogWfromW=0:1toW=10.
Scenario1.Supposefourwidgets havebeenmeasured andgivethefol-
lowingdata:fd1;d2;d3;d4g=f2.2, 2:2,2.8, 2:8g.Weareinterested
ininferring thewodgesofthese fourwidgets.
(a)Findthevaluesofwandthatmaximize theposterior probabilit y
P(w;logjd).
(b)Marginalize overandndtheposterior probabilit ydensit yofw
giventhedata. [Integration skillsrequired. SeeMacKa y(1999a) for
solution.] Findmaxima ofP(wjd).[Answ er:twomaxima {oneat
wMP=f1:8; 1:8;2:2; 2:2g;witherrorbarsonallfourparameters
(obtained fromGaussian approximation totheposterior)0:9;and
oneatw0
MP=f0:03; 0:03;0:04; 0:04gwitherrorbars0:1.]
Scenario2.Supposeinaddition tothefourmeasuremen tsaboveweare
nowinformed thatthere arefourmorewidgets thathavebeenmeasured
withamuchlessaccurate instrumen t,having0
=100:0.Thuswenow
havebothwell-determined andill-determined parameters, asinatypical

<<<PAGE 322>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
310 22|Maxim umLikelihoodandClustering
ill-posedproblem. Thedatafromthese measuremen tswereastring of
uninformativ evalues,fd5;d6;d7;d8g=f100, 100;100, 100g.
Weareagain askedtoinferthewodgesofthewidgets. Intuitively,our
inferences aboutthewell-measured widgets should benegligibly aected
bythisvacuous information aboutthepoorly-measured widgets. But
whathappenstotheMAP metho d?
(a)Findthevaluesofwandthatmaximize theposterior probabilit y
P(w;logjd).
(b)Findmaxima ofP(wjd).[Answ er:onlyonemaxim um,wMP=
f0:03, 0:03,0:03, 0:03,0:0001, 0:0001, 0:0001, 0:0001g,with
errorbarsonalleightparameters0:11.]
22.6 Solutions
Solution toexercise 22.5(p.302).Figure 22.10 showsacontourplotofthe0123 5401235
4
Figure 22.10.Thelikelihoodasa
function of1and2.likelihoodfunction forthe32datapoints.Thepeaksarepretty-near centred
onthepoints(1;5)and(5;1),andarepretty-near circular intheircontours.
Thewidth ofeachofthepeaksisastandard deviation of=p
16=1/4.The
peaksareroughly Gaussian inshape.
Solution toexercise 22.12 (p.307).Theloglikelihoodis:
lnP(fx(n)gjw)= NlnZ(w)+X
nX
kwkfk(x(n)): (22.37)
@
@wklnP(fx(n)gjw)= N@
@wklnZ(w)+X
nfk(x): (22.38)
Now,thefunpartiswhat happenswhen wedieren tiatethelogofthenor-
malizing constan t:
@
@wklnZ(w)=1
Z(w)X
x@
@wkexp X
k0wk0fk0(x)!
=1
Z(w)X
xexp X
k0wk0fk0(x)!
fk(x)=X
xP(xjw)fk(x);(22.39)
so@
@wklnP(fx(n)gjw)= NX
xP(xjw)fk(x)+X
nfk(x); (22.40)
andatthemaxim umofthelikelihood,
X
xP(xjwML)fk(x)=1
NX
nfk(x(n)): (22.41)

<<<PAGE 323>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
23
Useful Probabilit yDistributions
00.050.10.150.20.250.3
012345678910
1e-061e-050.00010.0010.010.11
012345678910
r
Figure 23.1.Thebinomial
distribution P(rjf=0:3;N=10),
onalinear scale(top) anda
logarithmic scale(bottom).InBayesian datamodelling, there's asmall collection ofprobabilit ydistribu-
tionsthatcome upagain andagain. Thepurposeofthischapter istointro-
ducethese distributions sothattheywon'tbeintimidating when encoun tered
incombatsituations.
There isnoneedtomemorize anyofthem, except perhaps theGaussian;
ifadistribution isimportantenough, itwillmemorize itself, andotherwise, it
caneasily belookedup.
23.1 Distributions overintegers
Binomial, Poisson, exponential
Wealready encoun teredthebinomial distribution andthePoisson distribution
onpage2.
Thebinomial distribution foranintegerrwithparameters f(thebias,
f2[0;1])andN(thenumberoftrials) is:
P(rjf;N)= 
N
r!
fr(1 f)N rr2f0;1;2;:::;Ng: (23.1)
Thebinomial distribution arises, forexample, when weipabentcoin,
withbiasf,Ntimes, andobserv ethenumberofheads,r.
ThePoisson distribution withparameter>0is:
P(rj)=e r
r!r2f0;1;2;:::g: (23.2)
ThePoisson distribution arises, forexample, when wecountthenumberof
photonsrthatarriveinapixelduring axedinterval,giventhatthemean
intensityonthepixelcorresp ondstoanaverage numberofphotons.00.050.10.150.20.25
0 5 10 15
1e-071e-061e-050.00010.0010.010.11
0 5 10 15
r
Figure 23.2.ThePoisson
distribution P(rj=2:7),ona
linear scale(top) anda
logarithmic scale(bottom).Theexponentialdistribution onintegers ,
P(rjf)=fr(1 f)r2(0;1;2;:::;1); (23.3)
arises inwaiting problems. Howlongwillyouhavetowaituntilasixisrolled,
ifafairsix-sided diceisrolled? Answer:theprobabilit ydistribution ofthe
numberofrolls,r,isexponentialoverintegers withparameterf=5=6.The
distribution mayalsobewritten
P(rjf)=(1 f)e rr2(0;1;2;:::;1); (23.4)
where=log(1=f).
311

<<<PAGE 324>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
312 23|Useful Probabilit yDistributions
23.2 Distributions overunbounded realnumbers
Gaussian, Student, Cauchy ,biexponential, inverse-cosh.
TheGaussian distribution ornormal distribution withmeanandstandard
deviationis
P(xj;)=1
Zexp 
 (x )2
22!
x2( 1;1); (23.5)
where
Z=p
22: (23.6)
Itissometimes useful toworkwiththequantity1=2,whichiscalled the
precision parameter oftheGaussian.
Asamplezfrom astandard univariate Gaussian canbegenerated by
computing
z=cos(2u1)q
2ln(1=u2); (23.7)
whereu1andu2areuniformly distributed in(0;1).Asecond samplez2=
sin(2u1)p
2ln(1=u2),independen toftherst,canthenbeobtained forfree.
TheGaussian distribution iswidely usedandoften asserted tobeavery
common distribution intherealworld,butIamsceptical aboutthisasser-
tion. Yes,unimodaldistributions maybecommon; butaGaussian isaspe-
cial,rather extreme, unimo daldistribution. Ithasverylighttails: thelog-
probabilit y-densit ydecreases quadratically .Thetypical deviation ofxfrom
is,buttherespectiveprobabilities thatxdeviates frombymorethan2,
3,4,and5,are0:046,0.003, 610 5,and610 7.Inmyexperience,
deviations fromamean fourorvetimes greater thanthetypical deviation
mayberare,butnotasrareas610 5!Itherefore urgecaution intheuseof
Gaussian distributions: ifavariable thatismodelled withaGaussian actually
hasaheavier-tailed distribution, therestofthemodelwillcontortitself to
reduce thedeviations oftheoutliers, likeasheet ofpaperbeingcrushed bya
rubberband.
.Exercise 23.1.[1]Pickavariable thatissupposedly bell-shap edinprobabilit y
distribution, gather data, andmakeaplotofthevariable's empirical
distribution. Showthedistribution asahistogram onalogscaleand
investigate whether thetailsarewell-mo delled byaGaussian distribu-
tion. [Oneexample ofavariable tostudy istheamplitude ofanaudio
signal.]
Onedistribution withheaviertailsthanaGaussian isamixture ofGaus-
sians.Amixture oftwoGaussians, forexample, isdened bytwomeans,
twostandard deviations, andtwomixing coecien ts1and2,satisfying
1+2=1,i0.
P(xj1;1;1;2;2;2)=1p
21exp
 (x 1)2
22
1
+2p
22exp
 (x 2)2
22
2
:
Ifwetakeanappropriately weightedmixture ofaninnite numberof
Gaussians, allhavingmean,weobtain aStuden t-tdistribution ,
P(xj;s;n)=1
Z1
(1+(x )2=(ns2))(n+1)=2; (23.8)
where00.10.20.30.40.5
-202468
0.00010.0010.010.1
-202468
Figure 23.3.Three unimo dal
distributions. TwoStuden t
distributions, withparameters
(m;s)=(1;1)(heavyline)(a
Cauchydistribution) and(2;4)
(lightline), andaGaussian
distribution withmean=3and
standard deviation=3(dashed
line), shownonlinear vertical
scales (top) andlogarithmic
vertical scales (bottom). Notice
thattheheavytailsoftheCauchy
distribution arescarcely eviden t
intheupper`bell-shap edcurve'.
Z=p
ns2 (n=2)
 ((n+1)=2)(23.9)

<<<PAGE 325>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
23.3: Distributions overpositiverealnumbers 313
andniscalled thenumberofdegrees offreedom and isthegamma function.
Ifn>1thentheStuden tdistribution (23.8) hasamean andthatmean is
.Ifn>2thedistribution alsohasanite variance,2=ns2=(n 2).
Asn!1,theStuden tdistribution approac hesthenormal distribution with
meanandstandard deviations.TheStuden tdistribution arises bothin
classical statistics (asthesampling-theoretic distribution ofcertain statistics)
andinBayesianinference (astheprobabilit ydistribution ofavariable coming
fromaGaussian distribution whose standard deviation wearen't sureof).
Inthespecialcasen=1,theStuden tdistribution iscalled theCauchy
distribution .
Adistribution whose tailsareintermediate inheaviness betweenStuden t
andGaussian isthebiexponentialdistribution ,
P(xj;s)=1
Zexp
 jx j
s
x2( 1;1) (23.10)
where
Z=2s: (23.11)
Theinverse-cosh distribution
P(xj)/1
[cosh(x)]1=(23.12)
isapopular modelinindependen tcomponentanalysis. Inthelimitoflarge,
theprobabilit ydistribution P(xj)becomes abiexponentialdistribution. In
thelimit!0P(xj)approac hesaGaussian withmean zeroandvariance
1=.
23.3 Distributions overpositiverealnumbers
Exponential, gamma, inverse-gamma, andlog-no rmal.
Theexponentialdistribution ,
P(xjs)=1
Zexp
 x
s
x2(0;1); (23.13)
where
Z=s; (23.14)
arises inwaiting problems. HowlongwillyouhavetowaitforabusinPois-
sonville, giventhatbuses arriveindependen tlyatrandom withoneeverys
minutesonaverage? Answer:theprobabilit ydistribution ofyourwait,x,is
exponentialwithmeans.
Thegamma distribution islikeaGaussian distribution, except whereas the
Gaussian goesfrom 1to1,gamma distributions gofrom0to1.Justas
theGaussian distribution hastwoparameters andwhichcontrolthemean
andwidth ofthedistribution, thegamma distribution hastwoparameters. It
istheproductoftheone-parameter exponentialdistribution (23.13) witha
polynomial,xc 1.Theexponentcinthepolynomial isthesecond parameter.
P(xjs;c)= (x;s;c)=1
Zx
sc 1
exp
 x
s
;0x<1 (23.15)
where
Z= (c)s: (23.16)

<<<PAGE 326>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
314 23|Useful Probabilit yDistributions
00.10.20.30.40.50.60.70.80.91
024681000.10.20.30.40.50.60.70.8
-4-2024
0.00010.0010.010.11
02468100.00010.0010.010.1
-4-2024
x l=logxFigure 23.4.Twogamma
distributions, withparameters
(s;c)=(1;3)(heavylines) and
10;0:3(lightlines), shownon
linear vertical scales (top) and
logarithmic vertical scales
(bottom); andshownasa
function ofxontheleft(23.15)
andl=logxontheright(23.18).
Thisisasimple peakeddistribution withmeanscandvariances2c.
Itisoften natural torepresen tapositiverealvariablexinterms ofits
logarithml=logx.Theprobabilit ydensit yoflis
P(l)=P(x(l))@x
@l=P(x(l))x(l) (23.17)
=1
Zlx(l)
sc
exp
 x(l)
s
; (23.18)
where
Zl= (c): (23.19)
[Thegamma distribution isnamed afteritsnormalizing constan t{anodd
convention,itseems tome!]
Figure 23.4showsacouple ofgamma distributions asafunction ofxand
ofl.Notice thatwhere theoriginal gamma distribution (23.15) mayhavea
`spike'atx=0,thedistribution overlneverhassuchaspike.Thespikeis
anartefact ofabadchoice ofbasis.
Inthelimitsc=1;c!0,weobtain thenoninformativ epriorforascale
parameter, the1=xprior. Thisimprop erprioriscalled noninformativ ebecause
ithasnoassociated length scale, nocharacteristic valueofx,soitprefers all
values ofxequally .Itisinvariantunder thereparameterization x=mx.If
wetransform the1=xprobabilit ydensit yintoadensit yoverl=logxwend
thelatter densit yisuniform.
.Exercise 23.2.[1]Imagine thatwereparameterize apositivevariablexinterms
ofitscuberoot,u=x1=3.Iftheprobabilit ydensit yofxistheimprop er
distribution 1=x,whatistheprobabilit ydensit yofu?
Thegamma distribution isalwaysaunimo daldensit yoverl=logx,and,
ascanbeseeninthegures, itisasymmetric. Ifxhasagamma distribution,
andwedecide toworkinterms oftheinverseofx,v=1=x,weobtain anew
distribution, inwhichthedensit yoverlisippedleft-for-righ t:theprobabilit y
densit yofviscalled aninverse-gamma distribution ,
P(vjs;c)=1
Zv1
svc+1
exp
 1
sv
;0v<1 (23.20)
where
Zv= (c)=s: (23.21)

<<<PAGE 327>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
23.4: Distributions overperiodicvariables 315
00.511.522.5
0 1 2 300.10.20.30.40.50.60.70.8
-4-2024
0.00010.0010.010.11
0 1 2 30.00010.0010.010.1
-4-2024
v logvFigure 23.5.Twoinversegamma
distributions, withparameters
(s;c)=(1;3)(heavylines) and
10;0:3(lightlines), shownon
linear vertical scales (top) and
logarithmic vertical scales
(bottom); andshownasa
function ofxontheleftand
l=logxontheright.
Gamma andinversegamma distributions cropupinmanyinference prob-
lemsinwhichapositivequantityisinferred fromdata. Examples include
inferring thevariance ofGaussian noise fromsome noise samples, andinfer-
ringtherateparameter ofaPoisson distribution fromthecount.
Gamma distributions alsoarisenaturally inthedistributions ofwaiting
times betweenPoisson-distributed events.GivenaPoisson processwithrate
,theprobabilit ydensit yofthearrivaltimexofthemtheventis
(x)m 1
(m 1)!e x: (23.22)
Log-normal distribution
Another distribution overapositiverealnumberxisthelog-normal distribu-
tion,whichisthedistribution thatresults whenl=lnxhasanormal distri-
bution. Wedenemtobethemedian valueofx,andstobethestandard
deviation oflnx.
P(ljm;s)=1
Zexp 
 (l lnm)2
2s2!
l2( 1;1); (23.23)
where
Z=p
2s2; (23.24)
implies
P(xjm;s)=1
xexp 
 (lnx lnm)2
2s2!
x2(0;1): (23.25)00.050.10.150.20.250.30.350.4
012345
0.00010.0010.010.1
012345
Figure 23.6.Twolog-normal
distributions, withparameters
(m;s)=(3;1:8)(heavyline)and
(3;0:7)(lightline), shownon
linear vertical scales (top) and
logarithmic vertical scales
(bottom). [Yes,theyreally do
havethesamevalueofthe
median,m=3.]
23.4 Distributions overperiodicvariables
Aperiodicvariableisarealnumber2[0;2]havingthepropertythat=0
and=2areequivalent.
Adistribution thatplaysforperiodicvariables theroleplayedbytheGaus-
siandistribution forrealvariables istheVonMises distribution :
P(j;)=1
Zexp(cos( ))2(0;2): (23.26)
Thenormalizing constan tisZ=2I0(),whereI0(x)isamodied Bessel
function.

<<<PAGE 328>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
316 23|Useful Probabilit yDistributions
Adistribution thatarises fromBrownian diusion around thecircle isthe
Gaussian distribution withwrap-around,
P(j;)=1X
n= 1Normal (;(+2n);)2(0;2): (23.27)
23.5 Distributions overprobabilities
Betadistribution, Dirichlet distribution, entropic distribution
Thebetadistribution isaprobabilit ydensit yoveravariablepthatisaprob-012345
00.250.50.75 1
00.10.20.30.40.50.6
-6-4-20246
Figure 23.7.Three beta
distributions, with
(u1;u2)=(0:3;1),(1:3;1),and
(12;2).Theuppergure shows
P(pju1;u2)asafunction ofp;the
lowershowsthecorresp onding
densit yoverthelogit,
lnp
1 p:
Notice howwell-behavedthe
densities areasafunction ofthe
logit.ability,p2(0;1):
P(pju1;u2)=1
Z(u1;u2)pu1 1(1 p)u2 1: (23.28)
Theparameters u1;u2maytakeanypositivevalue.Thenormalizing constan t
isthebetafunction.
Z(u1;u2)= (u1) (u2)
 (u1+u2)(23.29)
Specialcases include theuniform distribution {u1=1;u2=1;theJereys
prior {u1=0:5;u2=0:5;andtheimprop erLaplace prior {u1=0;u2=0.If
wetransform thebetadistribution tothecorresp onding densit yoverthelogit
llnp/(1 p),wenditisalwaysapleasan tbell-shap eddensit yoverl,while
thedensit yoverpmayhavesingularities atp=0andp=1(gure 23.7).
Moredimensions
TheDirichletdistribution isadensit yoveranI-dimensional vectorpwhose
Icomponentsarepositiveandsumto1.Thebetadistribution isaspecial
caseofaDirichletdistribution withI=2.TheDirichletdistribution is
parameterized byameasure u(avector withallcoecien tsui>0)which
Iwillwrite hereasu=m,where misanormalized measure overtheI
components(Pmi=1),andispositive:
P(pjm)=1
Z(m)IY
i=1pmi 1
i(P
ipi 1)Dirichlet(I)(pjm)(23.30)
Thefunction(x)istheDirac delta function whichrestricts thedistribution
tothesimplex suchthatpisnormalized, i.e.,P
ipi=1.Thenormalizing
constan toftheDirichletdistribution is:
Z(m)=Y
i (mi)/ (): (23.31)
Thevectormisthemean oftheprobabilit ydistribution:
Z
Dirichlet(I)(pjm)pdIp=m: (23.32)
When working withaprobabilit yvectorp,itisoften helpful toworkinthe
`softmax basis', inwhich,forexample, athree-dimensional probabilit yp=
(p1;p2;p3)isrepresen tedbythreenumbersa1;a2;a3satisfyinga1+a2+a3=0
and
pi=1
Zeai;whereZ=P
ieai. (23.33)
Thisnonlinear transformation isanalogous tothe!lntransformation
forascalevariable andthelogittransformation forasingle probabilit y,p!

<<<PAGE 329>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
23.5: Distributions overprobabilities 317
u=(20;10;7) u=(0:2;1;2) u=(0:2;0:3;0:15)
-8-4048
-8-4048-8-4048
-8-4048-8-4048
-8-4048Figure 23.8.Three Dirichlet
distributions overa
three-dimensional probabilit y
vector(p1;p2;p3).Theupper
gures show1000random draws
fromeachdistribution, showing
thevaluesofp1andp2onthetwo
axes.p3=p2+p1.Thetriangle
intherstgure isthesimplex of
legalprobabilit ydistributions.
Thelowergures showthesame
pointsinthe`softmax' basis
(equation (23.33)). Thetwoaxes
showa1anda2.a3= a1 a2.
lnp
1 p.Inthesoftmax basis, theuglyminus-ones intheexponentsinthe
Dirichletdistribution (23.30) disapp ear,andthedensit yisgivenby:
P(ajm)/1
Z(m)IY
i=1pmi
i(P
iai): (23.34)
Theroleofcanbecharacterized intwoways.First, theparametermea-
suresthesharpness ofthedistribution (gure 23.8); itmeasures howdieren t
weexpecttypical samples pfromthedistribution tobefromthemeanm,just
astheprecision=1/2ofaGaussian measures howfarsamples strayfromits
mean. Alargevalueofproduces adistribution overpthatissharply peaked
around m.Theeect ofinhigher-dimensional situations canbevisualized
bydrawingatypical sample fromthedistribution Dirichlet(I)(pjm),withm
settotheuniform vectormi=1/I,andmaking aZipfplot,thatis,aranked
plotofthevalues ofthecomponentspi.Itistraditional toplotbothpi(ver-
ticalaxis)andtherank(horizon talaxis)onlogarithmic scales sothatpower
lawrelationships appearasstraigh tlines. Figure 23.9showsthese plotsfora
single sample fromensem bleswithI=100andI=1000andwithfrom0.1
to1000. Forlarge,theplotisshallowwithmanycomponentshavingsimi-
larvalues. Forsmall,typically onecomponentpireceivesanoverwhelming
share oftheprobabilit y,andofthesmall probabilit ythatremains tobeshared
among theother components,another componentpi0receivesasimilarly large
share. Inthelimitasgoestozero,theplottends toanincreasingly steep
powerlaw.I=100
0.00010.0010.010.11
1 10 1000.1
1
10
100
1000
I=1000
1e-050.00010.0010.010.11
1 10 100 10000.1
1
10
100
1000
Figure 23.9.Zipfplotsforrandom
samples fromDirichlet
distributions withvarious values
of=0:1:::1000. Foreachvalue
ofI=100or1000andeach,
onesample pfromtheDirichlet
distribution wasgenerated. The
Zipfplotshowstheprobabilities
pi,rankedbymagnitude, versus
theirrank.Second, wecancharacterize theroleofinterms ofthepredictiv edis-
tribution thatresults when weobserv esamples frompandobtain counts
F=(F1;F2;:::;FI)ofthepossible outcomes. Thevalueofdenes the
numberofsamples frompthatarerequired inorder thatthedatadominate
overthepriorinpredictions.
Exercise 23.3.[3]TheDirichletdistribution satises aniceadditivit yproperty.
Imagine thatabiased six-sided diehastworedfacesandfourbluefaces.
ThedieisrolledNtimes andtwoBayesians examine theoutcomes in
order toinferthebiasofthedieandmakepredictions. OneBayesian
hasaccess tothered/blue colour outcomes only,andheinfers atwo-
componentprobabilit yvector (pR;pB).Theother Bayesian hasaccess
toeachfulloutcome: hecanseewhichofthesixfaces came up,and
heinfers asix-comp onentprobabilit yvector(p1;p2;p3;p4;p5;p6),where

<<<PAGE 330>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
318 23|Useful Probabilit yDistributions
pR=p1+p2andpB=p3+p4+p5+p6.Assuming thatthesec-
ondBayesianassigns aDirichletdistribution to(p1;p2;p3;p4;p5;p6)with
hyperparameters (u1;u2;u3;u4;u5;u6),showthat,inorder fortherst
Bayesian's inferences tobeconsisten twiththose ofthesecond Bayesian,
therstBayesian's priorshould beaDirichletdistribution withhyper-
parameters ((u1+u2);(u3+u4+u5+u6)).
Hint:abrute-force approac histocompute theintegralP(pR;pB)=Rd6pP(pju)(pR (p1+p2))(pB (p3+p4+p5+p6)).Acheaper
approac histocompute thepredictiv edistributions, givenarbitrary data
(F1;F2;F3;F4;F5;F6),andndthecondition forthetwopredictiv edis-
tributions tomatchforalldata.
Theentropic distribution foraprobabilit yvectorpissometimes usedin
the`maxim umentropy'image reconstruction comm unity.
P(pjm)=1
Z()exp[H(p)](P
ipi 1); (23.35)
whereH(p)=P
ipilog1=pi.
Further reading
See(MacKa yandPeto,1995) forfunwithDirichlets.
23.6 Further exercises
Exercise 23.4.[2]Ndatap ointsfxngaredrawnfrom agamma distribution
P(xjs;c)= (x;s;c)withunkno wnparameters sandc.What arethe
maxim umlikelihoodparameters sandc?

<<<PAGE 331>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
24
Exact Marginalization
Howcanweavoidtheexponentially large costofcomplete enumeration of
allhypotheses? Before westooptoapproximate metho ds,weexplore two
approac hestoexact marginalization: rst,marginalization overcontinuous
variables (sometimes knownasnuisance parameters) bydoing integrals;and
second, summation overdiscrete variables bymessage-passing.
Exact marginalization overcontinuousparameters isamachoactivit yen-
joyedbythose whoareuentindenite integration. Thischapter usesgamma
distributions; aswasexplained intheprevious chapter, gamma distributions
arealotlikeGaussian distributions, except thatwhereas theGaussian goes
from 1to1,gamma distributions gofrom0to1.
24.1 Inferring themean andvariance ofaGaussian distribution
Wediscuss again theone-dimensional Gaussian distribution, parameterized
byameanandastandard deviation:
P(xj;)=1p
2exp 
 (x )2
22!
Normal (x;;2): (24.1)
When inferring these parameters, wemustspecifytheir prior distribution.
Thepriorgivesustheopportunit ytoinclude specicknowledge thatwehave
aboutand(from independen texperimen ts,orontheoretical grounds, for
example). Ifwehavenosuchknowledge, thenwecanconstruct anappropriate
prior thatembodiesoursupposedignorance. Insection 21.2,weassumed a
uniform priorovertherange ofparameters plotted. Ifwewishtobeableto
perform exact marginalizations, itmaybeuseful toconsider conjugate priors ;
these arepriors whose functional formcombines naturally withthelikelihood
suchthattheinferences haveaconvenientform.
Conjugate priors forand
Theconjugate prior forameanisaGaussian: weintroduce two`hy-
perparameters', 0and,whichparameterize theprior on,andwrite
P(j0;)=Normal (;0;).Inthelimit0=0,!1,weobtain
thenoninformative prior foralocation parameter, theatprior. Thisis
noninformativ ebecause itisinvariant under thenatural reparameterization
0=+c.ThepriorP()=const:isalsoanimproperprior, thatis,itisnot
normalizable.
Theconjugate prior forastandard deviationisagamma distribution,
whichhastwoparameters bandc.Itismostconvenienttodene theprior
319

<<<PAGE 332>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
320 24|Exact Marginalization
densit yoftheinversevariance (theprecision parameter) =1=2:
P()= (;b;c)=1
 (c)c 1
bc
exp 
 
b!
;0<1:(24.2)
Thisisasimple peakeddistribution withmeanbcandvarianceb2
c.In
thelimitbc=1;c!0,weobtain thenoninformativ eprior forascale
parameter, the1=prior. Thisis`noninformativ e'because itisinvariant
under thereparameterization 0=c.The1=priorislessstrange-lo oking
ifweexamine theresulting densit yoverlog,orlog,whichisat.Thisis
thepriorthatexpresses ignorance aboutbysaying`well,itcould be10,or
itcould be1,oritcould be0.1,...'Scale variables suchasareusually best
represen tedinterms oftheirlogarithm. Again, thisnoninformativ e1=prior
isimprop er.
Inthefollowingexamples, Iwillusetheimprop ernoninformativ epriors
forand.Using improp erpriors isviewedasdistasteful insome circles,
soletmeexcuse myselfbysayingit'sforthesakeofreadabilit y;ifIincluded
properpriors, thecalculations could stillbedonebutthekeypointswouldbe
obscured bytheoodofextra parameters.
Maximum likeliho odandmarginalization: NandN 1
Thetaskofinferring themean andstandard deviation ofaGaussian distribu-
tionfromNsamples isafamiliar one,though maybenoteveryoneunderstands
thedierence betweentheNandN 1buttons ontheircalculator. Letus
recap theformulae,thenderivethem.
GivendataD=fxngN
n=1,an`estimator' ofis
xPN
n=1xn=N; (24.3)
andtwoestimators ofare:
NsPN
n=1(xn x)2
NandN 1sPN
n=1(xn x)2
N 1: (24.4)
There aretwoprincipal paradigms forstatistics: sampling theory andBayesian
inference. Insampling theory (alsoknownas`frequen tist'ororthodoxstatis-
tics),oneinventsestimators ofquantitiesofinterest andthenchoosesbetween
those estimators using some criterion measuring their sampling properties;
there isnoclearprinciple fordeciding whichcriterion tousetomeasure the
performance ofanestimator; nor,formost criteria, isthere anysystematic
procedure fortheconstruction ofoptimal estimators. InBayesian inference,
incontrast, oncewehavemade explicit allourassumptions aboutthemodel
andthedata, ourinferences aremechanical. Whatev erquestion wewishto
pose,therulesofprobabilit ytheory giveaunique answerwhichconsisten tly
takesintoaccoun tallthegiveninformation. Human-designed estimators and
condence intervalshavenoroleinBayesian inference; human input onlyen-
tersintotheimportanttasks ofdesigning thehypothesis space (that is,the
specication ofthemodelandallitsprobabilit ydistributions), andguring
outhowtodothecomputations thatimplemen tinference inthatspace. The
answerstoourquestions areprobabilit ydistributions overthequantities of
interest. Weoften ndthattheestimators ofsampling theory emerge auto-
matically asmodesormeans ofthese posterior distributions when wechoose
asimple hypothesis space andturnthehandle ofBayesian inference.

<<<PAGE 333>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
24.1: Inferring themean andvariance ofaGaussian distribution 321
(a1)
00.511.52
0.20.40.60.8100.010.020.030.040.050.06
meansigma
(a2)0 0.5 1 1.5 20.10.20.30.40.50.60.70.80.91
meansigma
(c)
00.010.020.030.040.050.060.070.080.09
0.2 0.4 0.60.811.21.41.61.82mu=1   
mu=1.25
mu=1.5 
(d)
00.010.020.030.040.050.060.070.080.09
0.2 0.4 0.60.811.21.41.61.82P(sigma|D)P(sigma|D,mu=1)Figure 24.1.Thelikelihood
function fortheparameters ofa
Gaussian distribution, repeated
fromgure 21.5.
(a1,a2)Surface plotandcontour
plotoftheloglikelihoodasa
function ofand.Thedataset
ofN=5pointshadmean x=1:0
andS2=P(x x)2=1:0.
Notice thatthemaxim umisskew
in.Thetwoestimators of
standard deviation havevalues
N=0:45andN 1=0:50.
(c)Theposterior probabilit yof
forvarious xedvaluesof.
(d)Theposterior probabilit yof,
P(jD),assuming aatprioron
,obtained byprojecting the
probabilit ymassin(a)ontothe
axis.Themaxim umofP(jD)is
atN 1.Bycontrast, the
maxim umofP(jD;=x;)is
atN.
Insampling theory ,theestimators abovecanbemotivatedasfollows.xis
anunbiased estimator ofwhich,outofallthepossible unbiased estimators
of,hassmallest variance (where thisvariance iscomputed byaveraging over
anensem bleofimaginary experimen tsinwhichthedatasamples areassumed
tocomefromanunkno wnGaussian distribution). Theestimator (x;N)isthe
maxim umlikelihoodestimator for(;).TheestimatorNisbiased,however:
theexpectation ofN,given,averaging overmanyimagined experimen ts,is
not.
Exercise 24.1.[2,p.323]Giveanintuitiveexplanation whytheestimatorNis
biased.
Thisbiasmotivatestheinvention,insampling theory ,ofN 1,whichcanbe
showntobeanunbiased estimator. Ortobeprecise, itis2
N 1thatisan
unbiased estimator of2.
Wenowlookatsome Bayesian inferences forthisproblem, assuming non-
informativ epriors forand.Theemphasis isthusnotonthepriors, but
rather on(a)thelikelihoodfunction, and(b)theconcept ofmarginalization.
Thejointposterior probabilit yofandisproportional tothelikelihood
function illustrated byacontourplotingure 24.1a. Theloglikelihoodis:
lnP(fxngN
n=1j;)= Nln(p
2) X
n(xn )2=(22);(24.5)
= Nln(p
2) [N( x)2+S]=(22);(24.6)
whereSP
n(xn x)2.GiventheGaussian model,thelikelihoodcanbe
expressed interms ofthetwofunctions ofthedataxandS,sothese two
quantities areknownas`sucien tstatistics'. Theposterior probabilit yof
andis,using theimprop erpriors:
P(;jfxngN
n=1)=P(fxngN
n=1j;)P(;)
P(fxngN
n=1)(24.7)
=1
(22)N=2exp
 N( x)2+S
22
1
1

P(fxngN
n=1):(24.8)

<<<PAGE 334>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
322 24|Exact Marginalization
Thisfunction describ estheanswertothequestion, `giventhedata, andthe
noninformativ epriors, what mightandbe?'Itmaybeofinterest tond
theparameter valuesthatmaximize theposterior probabilit y,though itshould
beemphasized thatposterior probabilit ymaxima havenofundamen talstatus
inBayesianinference, sincetheirlocation dependsonthechoice ofbasis. Here
wechoosethebasis(;ln),inwhichourpriorisat,sothattheposterior
probabilit ymaxim umcoincides withthemaxim umofthelikelihood.Aswe
sawinexercise 22.4(p.302),themaxim umlikelihoodsolution forandln
isf;gML=n
x;N=p
S=No
:
There ismore totheposterior distribution thanjustitsmode.Ascan
beseeningure 24.1a, thelikelihoodhasaskewpeak. Asweincrease,
thewidth oftheconditional distribution ofincreases (gure 22.1b). And
ifwextoasequence ofvalues movingawayfromthesample mean x,we
obtain asequence ofconditional distributions overwhose maxima moveto
increasing values of(gure 24.1c).
Theposterior probabilit yofgivenis
P(jfxngN
n=1;)=P(fxngN
n=1j;)P()
P(fxngN
n=1j)(24.9)
/exp( N( x)2=(22)) (24.10)
=Normal (;x;2=N): (24.11)
Wenotethefamiliar=p
Nscaling oftheerrorbarson.
Letusnowaskthequestion `giventhedata, andthenoninformativ epriors,
whatmightbe?'Thisquestion diers fromtherstoneweaskedinthatwe
arenownotinterested in.Thisparameter musttherefore bemarginalize d
over.Theposterior probabilit yofis:
P(jfxngN
n=1)=P(fxngN
n=1j)P()
P(fxngN
n=1): (24.12)
Thedata-dep enden ttermP(fxngN
n=1j)appeared earlier asthenormalizing
constan tinequation (24.9); onename forthisquantityisthe`evidence', or
marginal likelihood,for.Weobtain theevidence forbyintegrating out
;anoninformativ epriorP()=constan tisassumed; wecallthisconstan t
1=,sothatwecanthink ofthepriorasatop-hat priorofwidth.The
Gaussian integral,P(fxngN
n=1j)=RP(fxngN
n=1j;)P()d;yields:
lnP(fxngN
n=1j)= Nln(p
2) S
22+lnp
2=p
N
: (24.13)
Thersttwoterms arethebesttloglikelihood(i.e.,theloglikelihoodwith
=x).ThelasttermisthelogoftheOccam factor whichpenalizes smaller
values of.(Wewilldiscuss Occam factors more inChapter 28.)When we
dieren tiatethelogevidence withrespecttoln,tondthemost probable
,theadditional volume factor (=p
N)shifts themaxim umfromNto
N 1=q
S=(N 1): (24.14)
Intuitively,thedenominator (N 1)countsthenumberofnoisemeasuremen ts
contained inthequantityS=P
n(xn x)2.ThesumcontainsNresiduals
squared, butthere areonly(N 1)eectiv enoise measuremen tsbecause the
determination ofoneparameterfromthedatacauses onedimension ofnoise
tobegobbled upinunavoidable overtting. Intheterminology ofclassical

<<<PAGE 335>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
24.2: Exercises 323
statistics, theBayesian's bestguess forsets2(themeasure ofdeviance
dened by2P
n(xn ^)2=^2)equal tothenumberofdegrees offreedom,
N 1.
Figure 24.1d showstheposterior probabilit yof,whichisproportional
tothemarginal likelihood.Thismaybecontrasted withtheposterior prob-
abilityofwithxedtoitsmostprobable value,x=1,whichisshownin
gure 24.1c andd.
Thenalinference wemightwishtomakeis`giventhedata, whatis?'
.Exercise 24.2.[3]Marginalize overandobtain theposterior marginal distri-
bution of,whichisaStuden t-tdistribution:
P(jD)/1=
N( x)2+SN=2: (24.15)
Further reading
Abibleofexact marginalization isBretthorst's (1988) bookonBayesianspec-
trumanalysis andparameter estimation.
24.2 Exercises
.Exercise 24.3.[3][This exercise requires machointegration capabilities.] Give
aBayesian solution toexercise 22.15 (p.309),where sevenscientistsof
varying capabilities havemeasuredwithpersonal noise levelsn,
-30-20-1001020A BCD-G
andweareinterested ininferring.Lettheprior oneachnbea
broad prior, forexample agamma distribution withparameters (s;c)=
(10;0:1).Findtheposterior distribution of.Plotit,andexplore its
properties foravarietyofdatasetssuchastheonegiven,andthedata
setfxng=f13:01;7:39g.
[Hint:rstndtheposterior distribution ofngivenandxn,
P(njxn;).Note thatthenormalizing constan tforthisinference is
P(xnj).Marginalize overntondthisnormalizing constan t,then
useBayes'theorem asecond timetondP(jfxng).]
24.3 Solutions
Solution toexercise 24.1(p.321).1.Thedatapointsaredistributed withmean
squared deviation2aboutthetruemean. 2.Thesample mean isunlikely
toexactly equal thetruemean. 3.Thesample mean isthevalueofthat
minimizes thesumsquared deviation ofthedatapointsfrom.Anyother
valueof(inparticular, thetruevalueof)willhavealarger valueofthe
sum-squared deviation that=x.
Sotheexpected mean squared deviation fromthesample mean isneces-
sarily smaller thanthemean squared deviation2aboutthetruemean.

<<<PAGE 336>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
25
Exact Marginalization inTrellises
Inthischapter wewilldiscuss afewexact metho dsthatareusedinproba-
bilistic modelling. Asanexample wewilldiscuss thetaskofdecodingalinear
error-correcting code.Wewillseethatinferences canbeconducted moste-
cientlybymessage-passing algorithms ,whichtakeadvantageofthegraphical
structure oftheproblem toavoidunnecessary duplication ofcomputations
(seeChapter 16).
25.1 Decodingproblems
Acodewordtisselected fromalinear (N;K)codeC,anditistransmitted
overanoisy channel; thereceivedsignal isy.Inthischapter wewillassume
thatthechannel isamemoryless channel suchasaGaussian channel. Given
anassumed channel modelP(yjt),there aretwodecodingproblems.
Thecodeworddecodingproblem isthetaskofinferring whichcodeword
twastransmitted giventhereceivedsignal.
Thebitwisedecodingproblem isthetaskofinferring foreachtransmit-
tedbittnhowlikelyitisthatthatbitwasaonerather thanazero.
Asaconcrete example, takethe(7;4)Hamming code.InChapter 1,we
discussed thecodeworddecodingproblem forthatcode,assuming abinary
symmetric channel. Wedidn't discuss thebitwisedecodingproblem andwe
didn't discuss howtohandle moregeneral channel modelssuchasaGaussian
channel.
Solving thecodeworddecodingproblem
ByBayes'theorem, theposterior probabilit yofthecodewordtis
P(tjy)=P(yjt)P(t)
P(y): (25.1)
Likelihoodfunction .Therstfactor inthenumerator,P(yjt),isthelikeli-
hoodofthecodeword,which,foranymemoryless channel, isaseparable
function,
P(yjt)=NY
n=1P(ynjtn): (25.2)
Forexample, ifthechannel isaGaussian channel withtransmissionsx
andadditiv enoiseofstandard deviation,thentheprobabilit ydensit y
324

<<<PAGE 337>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
25.1: Decodingproblems 325
ofthereceivedsignalyninthetwocasestn=0;1is
P(ynjtn=1)=1p
22exp 
 (yn x)2
22!
(25.3)
P(ynjtn=0)=1p
22exp 
 (yn+x)2
22!
: (25.4)
Fromthepointofviewofdecoding, allthatmatters isthelikelihood
ratio,whichforthecaseoftheGaussian channel is
P(ynjtn=1)
P(ynjtn=0)=exp2xyn
2
: (25.5)
Exercise 25.1.[2]Showthatfromthepointofviewofdecoding, aGaussian
channel isequivalenttoatime-v arying binary symmetric channel with
aknownnoise levelfnwhichdependsonn.
Prior .Thesecond factor inthenumerator istheprior probabilit yofthe
codeword,P(t),whichisusually assumed tobeuniform overallvalid
codewords.
Thedenominator in(25.1) isthenormalizing constan t
P(y)=X
tP(yjt)P(t): (25.6)
Thecomplete solution tothecodeworddecodingproblem isalistofall
codewordsandtheirprobabilities asgivenbyequation (25.1). Since thenum-
berofcodewordsinalinear code,2K,isoftenverylarge, andsincewearenot
interested inknowingthedetailed probabilities ofallthecodewords,weoften
restrict attentiontoasimplied version ofthecodeworddecodingproblem.
TheMAP codeworddecodingproblem isthetaskofidentifying the
mostprobablecodewordtgiventhereceivedsignal.
Ifthepriorprobabilit yovercodewordsisuniform thenthistaskisiden-
ticaltotheproblem ofmaxim umlikelihooddecoding,thatis,identifying
thecodewordthatmaximizesP(yjt).
Example: InChapter 1,forthe(7;4)Hamming codeandabinary symmetric
channel wediscussed ametho dfordeducing themostprobable codewordfrom
thesyndrome ofthereceivedsignal, thussolving theMAP codeworddecoding
problem forthatcase. Wewouldlikeamore general solution.
TheMAP codeworddecodingproblem canbesolvedinexponentialtime
(oforder 2K)bysearchingthrough allcodewordsfortheonethatmaximizes
P(yjt)P(t).Butweareinterested inmetho dsthataremore ecien tthan
this.Insection 25.3,wewilldiscuss anexact metho dknownasthemin{sum
algorithm whichmaybeabletosolvethecodeworddecodingproblem more
ecien tly;howmuchmore ecien tlydependsontheproperties ofthecode.
Itisworthemphasizing thatMAP codeworddecodingforagenerallinear
codeisknowntobeNP-complete (whichmeans inlayman's terms thatMAP
codeworddecodinghasacomplexit ythatscales exponentially withtheblock
length, unless there isarevolution incomputer science). Sorestricting atten-
tiontotheMAP decodingproblem hasn't necessarily made thetaskmuchless
challenging; itsimply makestheanswerbriefer toreport.

<<<PAGE 338>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
326 25|Exact Marginalization inTrellises
Solving thebitwise decodingproblem
Formally ,theexact solution ofthebitwisedecodingproblem isobtained from
equation (25.1) bymarginalizing overtheother bits.
P(tnjy)=X
ftn0:n06=ngP(tjy): (25.7)
Wecanalsowrite thismarginal withtheaidofatruth function
 [S]thatis
oneifthepropositionSistrueandzerootherwise.
P(tn=1jy)=X
tP(tjy)
 [tn=1] (25.8)
P(tn=1jy)=X
tP(tjy)
 [tn=0]: (25.9)
Computing these marginal probabilities byanexplicit sumoverallcodewords
ttakesexponentialtime. But,forcertain codes,thebitwisedecodingproblem
canbesolvedmuchmore ecien tlyusing theforward{bac kwardalgorithm .
Wewilldescrib ethisalgorithm, whichisanexample ofthesum{pro duct
algorithm ,inamomen t.Both themin{sum algorithm andthesum{pro duct
algorithm havewidespread importance, andhavebeeninventedmanytimes
inmanyelds.
25.2 Codesandtrellises
InChapters 1and11,werepresen tedlinear (N;K)codesinterms oftheir
generator matrices andtheirparity-checkmatrices. Inthecaseofasystematic
blockcode,therstKtransmitted bitsineachblockofsizeNarethesource
bits,andtheremainingM=N Kbitsaretheparity-checkbits.Thismeans
thatthegenerator matrix ofthecodecanbewritten
GT="
IK
P#
; (25.10)
andtheparity-checkmatrix canbewritten
H=h
PIMi
; (25.11)
where PisanMKmatrix.
Inthissection wewillnowstudy another represen tation ofalinear code
called atrellis. Thecodesthatthese trellises represen twillnotingeneral be
systematic codes,buttheycanbemappedontosystematic codesifdesired by
areordering ofthebitsinablock.(a)
Repetition codeR3
(b)
Simple paritycodeP3
(c)
(7;4)Hamming code
Figure 25.1.Examples oftrellises.
Eachedgeinatrellis islabelled
byazero(shownbyasquare) or
aone(shownbyacross).
Denition ofatrellis
Ourdenition willbequitenarrow.Foramorecomprehensiv eviewoftrellises,
thereader should consult KschischangandSorokine (1995).
Atrellis isagraph consisting ofnodes(alsoknownasstates orvertices) and
edges.Thenodesaregroup edintovertical slices called times,andthe
times areordered suchthateachedgeconnects anodeinonetimeto
anodeinaneighbouring time. Everyedgeislabelledwithasymbol.
Theleftmost andrightmost states containonlyonenode.Apart from
these twoextreme nodes,allnodesinthetrellis haveatleastoneedge
connecting leftwardsandatleastoneconnecting rightwards.

<<<PAGE 339>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
25.3: Solving thedecodingproblems onatrellis 327
Atrellis withN+1times denes acodeofblocklengthNasfollows:a
codewordisobtained bytaking apaththatcrosses thetrellis fromlefttoright
andreading outthesymbolsontheedges thataretraversed. Eachvalidpath
through thetrellis denes acodeword.Wewillnumbertheleftmost time`time
0'andtherightmost `timeN'.Wewillnumbertheleftmost state `state 0'
andtherightmost `stateI',whereIisthetotalnumberofstates (vertices) in
thetrellis. Thenthbitofthecodewordisemitted aswemovefromtimen 1
totimen.
Thewidth ofthetrellis atagiventimeisthenumberofnodesinthat
time. Themaximal width ofatrellis iswhatitsounds like.
Atrellis iscalled alinear trellis ifthecodeitdenes isalinear code.Wewill
solely beconcerned withlinear trellises fromnowon,asnonlinear trellises are
muchmore complex beasts. Forbrevit y,wewillonlydiscuss binary trellises,
thatis,trellises whose edges arelabelledwithzeroesandones. Itisnothard
togeneralize themetho dsthatfollowtoq-arytrellises.
Figures 25.1(a{c) showthetrellises corresp onding totherepetition code
R3whichhas(N;K)=(3;1);theparitycodeP3with(N;K)=(3;2);and
the(7;4)Hamming code.
.Exercise 25.2.[2]Conrm thatthesixteen codewordslisted intable 1.14are
generated bythetrellis showningure 25.1c.
Observations aboutlineartrellises
Foranylinear codetheminimal trellis istheonethathasthesmallest number
ofnodes.Inaminimal trellis, eachnodehasatmosttwoedges entering itand
atmosttwoedges leavingit.Allnodesinatimehavethesameleftdegree as
eachother andtheyhavethesame rightdegree aseachother. Thewidth is
alwaysapoweroftwo.
Aminimal trellis foralinear (N;K)codecannot haveawidth greater than
2Ksinceeverynodehasatleastonevalidcodewordthrough it,andthere are
only2Kcodewords. Furthermore, ifwedeneM=N K,theminimal
trellis's width iseverywhere lessthan2M.Thiswillbeprovedinsection 25.4.
Notice thatforthelinear trellises ingure 25.1,allofwhichareminimal
trellises,Kisthenumberoftimes abinary branchpointisencoun tered asthe
trellis istraversed fromlefttorightorfromrighttoleft.
Wewilldiscuss theconstruction oftrellises more insection 25.4. Butwe
nowknowenough todiscuss thedecodingproblem.
25.3 Solving thedecodingproblems onatrellis
Wecanviewthetrellis ofalinear codeasgiving acausal description ofthe
probabilistic processthatgivesrisetoacodeword,withtimeowingfromleft
toright.Eachtimeadivergence isencoun tered, arandom source (thesource
ofinformation bitsforcomm unication) determines whichwaywego.
Atthereceiving end,wereceiveanoisy version ofthesequence ofedge-
labels,andwishtoinferwhichpathwastaken,ortobeprecise, (a)wewant
toidentifythemost probable pathinorder tosolvethecodeworddecoding
problem; and(b)wewanttondtheprobabilit ythatthetransmitted symbol
attimenwasazerooraone,tosolvethebitwisedecodingproblem.
Example 25.3. Consider thecaseofasingle transmission fromtheHamming
(7;4)trellis showningure 25.1c.

<<<PAGE 340>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
328 25|Exact Marginalization inTrellises
t LikelihoodPosterior probabilit y
0000000 0.0275562 0.25
0001011 0.0001458 0.0013
0010111 0.0013122 0.012
0011100 0.0030618 0.027
0100110 0.0002268 0.0020
0101101 0.0000972 0.0009
0110001 0.0708588 0.63
0111010 0.0020412 0.018
1000101 0.0001458 0.0013
1001110 0.0000042 0.0000
1010010 0.0030618 0.027
1011001 0.0013122 0.012
1100011 0.0000972 0.0009
1101000 0.0002268 0.0020
1110100 0.0020412 0.018
1111111 0.0000108 0.0001Figure 25.2.Posterior probabilities
overthesixteen codewordswhen
thereceiv edvectoryhas
normalized likelihoods
(0:1;0:4;0:9;0:1;0:1;0:1;0:3).
Letthenormalized likelihoodsbe:(0:1;0:4;0:9;0:1;0:1;0:1;0:3).That is,
theratios ofthelikelihoodsare
P(y1jx1=1)
P(y1jx1=0)=0:1
0:9;P(y2jx2=1)
P(y2jx2=0)=0:4
0:6;etc. (25.12)
Howshould thisreceivedsignal bedecoded?
1.Ifwethreshold thelikelihoodsat0.5toturnthesignal intoabi-
nary receivedvector, wehaver=(0;0;1;0;0;0;0),whichdecodes,
using thedecoderforthebinary symmetric channel (Chapter 1),into
^t=(0;0;0;0;0;0;0).
Thisisnottheoptimal decodingprocedure. Optimal inferences are
alwaysobtained byusing Bayes'theorem.
2.Wecanndtheposterior probabilit yovercodewordsbyexplicit enu-
meration ofallsixteen codewords. Thisposterior distribution isshown
ingure 25.2. Ofcourse, wearen't really interested insuchbrute-force
solutions, andtheaimofthischapter istounderstand algorithms for
getting thesame information outinlessthan2Kcomputer time.
Examining theposterior probabilities, wenotice thatthemostprobable
codewordisactually thestringt=0110001 .Thisismorethantwiceas
probable astheanswerfound bythresholding, 0000000 .
Using theposterior probabilities showningure 25.2,wecanalsocom-
putetheposterior marginal distributions ofeachofthebits.Theresult
isshowningure 25.3. Notice thatbits1,4,5and6areallquite con-
dentlyinferred tobezero. Thestrengths oftheposterior probabilities
forbits2,3,and7arenotsogreat. 2
Intheaboveexample, theMAP codewordisinagreemen twiththebitwise
decodingthatisobtained byselecting themost probable state foreachbit
using theposterior marginal distributions. Butthisisnotalwaysthecase,as
thefollowingexercise shows.

<<<PAGE 341>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
25.3: Solving thedecodingproblems onatrellis 329
n Likelihood Posterior marginals
P(ynjtn=1)P(ynjtn=0)P(tn=1jy)P(tn=0jy)
1 0:1 0:9 0:061 0:939
2 0:4 0:6 0:674 0:326
3 0:9 0:1 0:746 0:254
4 0:1 0:9 0:061 0:939
5 0:1 0:9 0:061 0:939
6 0:1 0:9 0:061 0:939
7 0:3 0:7 0:659 0:341Figure 25.3.Marginal posterior
probabilities forthe7bitsunder
theposterior distribution of
gure 25.2.
Exercise 25.4.[2,p.333]Find themost probable codewordinthecasewhere
thenormalized likelihoodis(0:2;0:2;0:9;0:2;0:2;0:2;0:2).Alsondor
estimate themarginal posterior probabilit yforeachofthesevenbits,
andgivethebit-by-bitdecoding.
[Hint:concen trateonthefewcodewordsthathavethelargest probabil-
ity.]
Wenowdiscuss howtousemessage passing onacode'strellis tosolvethe
decodingproblems.
Themin{sum algorithm
TheMAP codeworddecodingproblem canbesolvedusing themin{sum al-
gorithm thatwasintroduced insection 16.3. Eachcodewordofthecode
corresp ondstoapathacross thetrellis. Justasthecostofajourney isthe
sumofthecosts ofitsconstituen tsteps, theloglikelihoodofacodewordis
thesumofthebitwiseloglikelihoods.Byconvention,weipthesignofthe
loglikelihood(whichwewouldliketomaximize) andtalkinterms ofacost,
whichwewouldliketominimize.
Weassociatewitheachedgeacost logP(ynjtn),wheretnisthetrans-
mitted bitassociated withthatedge, andynisthereceivedsymbol.The
min{sum algorithm presen tedinsection 16.3canthenidentifythemostprob-
ablecodewordinanumberofcomputer operations equal tothenumberof
edges inthetrellis. Thisalgorithm isalsoknownastheViterbi algorithm
(Viterbi, 1967).
Thesum{pr oductalgorithm
Tosolvethebitwisedecodingproblem, wecanmakeasmall modication to
themin{sum algorithm, sothatthemessages passed through thetrellis dene
`theprobabilit yofthedatauptothecurren tpoint'instead of`thecostofthe
bestroute tothispoint'.Wereplace thecostsontheedges, logP(ynjtn),by
thelikelihoodsthemselv es,P(ynjtn).Wereplace theminandsumoperations
ofthemin{sum algorithm byasumandproductrespectively.
Letirunovernodes/states, i=0bethelabelforthestartstate,P(i)
denote thesetofstates thatareparentsofstatei,andwijbethelikelihood
associated withtheedgefromnodejtonodei.Wedene theforward-pass
messagesiby
0=1

<<<PAGE 342>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
330 25|Exact Marginalization inTrellises
i=X
j2P(i)wijj: (25.13)
These messages canbecomputed sequen tially fromlefttoright.
.Exercise 25.5.[2]Showthatforanodeiwhose time-co ordinate isn,iis
proportional tothejointprobabilit ythatthecodeword's pathpassed
through nodeiandthattherstnreceivedsymbolswerey1;:::;yn.
Themessageicomputed attheendnodeofthetrellis isproportional tothe
marginal probabilit yofthedata.
.Exercise 25.6.[2]What istheconstan tofproportionalit y?[Answ er:2K]
Wedene asecond setofbackward-pass messagesiinasimilar manner.
LetnodeIbetheendnode.
I=1
j=X
i:j2P(i)wiji: (25.14)
These messages canbecomputed sequen tially inabackwardpassfromright
toleft.
.Exercise 25.7.[2]Showthatforanodeiwhose time-co ordinate isn,iispro-
portional totheconditional probabilit y,given thatthecodeword'spath
passed through nodei,thatthesubsequen tnreceivedsymbolswere
yn+1:::yN.
Finally ,tondtheprobabilit ythatthenthbitwasa1or0,wedotwo
summations ofproducts oftheforwardandbackwardmessages. Letirunover
nodesattimenandjrunovernodesattimen 1,andlettijbethevalue
oftnassociated withthetrellis edgefromnodejtonodei.Foreachvalueof
t=0=1,wecompute
r(t)
n=X
i;j:j2P(i);tij=tjwiji: (25.15)
Then theposterior probabilit ythattnwast=0=1is
P(tn=tjy)=1
Zr(t)
n; (25.16)
where thenormalizing constan tZ=r(0)
n+r(1)
nshould beidenticaltothenal
forwardmessageIthatwascomputed earlier.
Exercise 25.8.[2]Conrm thattheabovesum{pro ductalgorithm doescom-
puteP(tn=tjy).
Other names forthesum{pro ductalgorithm presen tedhereare`theforward{
backwardalgorithm', `theBCJR algorithm', and`beliefpropagation'.
.Exercise 25.9.[2,p.333]Acodewordofthesimple paritycodeP3istransmitted,
andthereceivedsignalyhasassociated likelihoodsshownintable25.4.nP(ynjtn)
tn=0tn=1
11/41/2
21/21/4
31/81/2
Table25.4.Bitwiselikelihoodsfor
acodewordofP3.
Usethemin{sum algorithm andthesum{pro ductalgorithm inthetrellis
(gure 25.1) tosolvetheMAP codeworddecodingproblem andthe
bitwisedecodingproblem. Conrm youranswersbyenumeration of
allcodewords(000,011,110,101).[Hint:uselogstobase2anddo
themin{sum computations byhand. When working thesum{pro duct
algorithm byhand, youmayndithelpful tousethree colours ofpen,
oneforthes,oneforthews,andoneforthes.]

<<<PAGE 343>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
25.4: More ontrellises 331
25.4 More ontrellises
Wenowdiscuss various waysofmaking thetrellis ofacode.Youmaysafely
jump overthissection.
Thespanofacodewordisthesetofbitscontained betweentherstbitin
thecodewordthatisnon-zero, andthelastbitthatisnon-zero, inclusiv e.We
canindicate thespanofacodewordbyabinary vectorasshownintable25.5.
Codeword0000000 0001011 0100110 1100011 0101101
Span0000000 0001111 0111110 1111111 0111111Table25.5.Some codewordsand
theirspans.
Agenerator matrix isintrellis-orien tedformifthespans oftherowsofthe
generator matrix allstartindieren tcolumns andthespans allendindieren t
columns.
Howtomakeatrellisfromageneratormatrix
First, putthegenerator matrix intotrellis-orien tedformbyrow-manipulations
similar toGaussian elimination. Forexample, our(7;4)Hamming codecan
begenerated by
G=2
66641000101
0100110
0010111
00010113
7775(25.17)
butthismatrix isnotintrellis-orien tedform{forexample, rows1,3and4
allhavespans thatendinthesame column. Bysubtracting lowerrowsfrom
upperrows,wecanobtain anequivalentgenerator matrix (that is,onethat
generates thesame setofcodewords) asfollows:
G=2
66641101000
0100110
0011100
00010113
7775: (25.18)
Now,eachrowofthegenerator matrix canbethough tofasdening an
(N;1)subcodeofthe(N;K)code,thatis,inthiscase, acodewithtwo
codewordsoflengthN=7.Fortherstrow,thecodeconsists ofthetwo
codewords1101000 and0000000 .Thesubcodedened bythesecond row
consists of0100110 and0000000 .Itiseasytoconstruct theminimal trellises
ofthese subcodes;theyareshownintheleftcolumn ofgure 25.6.
Webuild thetrellis incremen tallyasshowningure 25.6. Westartwith
thetrellis corresp onding tothesubcodegivenbytherstrowofthegenerator
matrix. Then weaddinonesubcodeatatime. Thevertices within thespan
ofthenewsubcodeareallduplicated. Theedgesymbolsintheoriginal trellis
areleftunchanged andtheedgesymbolsinthesecond partofthetrellis are
ippedwherev erthenewsubcodehasa1andotherwise leftalone.
Another (7;4)Hamming codecanbegenerated by
G=2
66641110000
0111100
0010110
00011113
7775: (25.19)

<<<PAGE 344>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
332 25|Exact Marginalization inTrellises
+
=
+
=
+
=Figure 25.6.Trellises forfour
subcodesofthe(7;4)Hamming
code(leftcolumn), andthe
sequence oftrellises thataremade
when constructing thetrellis for
the(7;4)Hamming code(right
column).
Eachedgeinatrellis islabelled
byazero(shownbyasquare) or
aone(shownbyacross).
The(7;4)Hamming codegenerated bythismatrix diers byapermutation
ofitsbitsfromthecodegenerated bythesystematic matrix usedinChapter
1andabove.Theparity-checkmatrix corresp onding tothispermutation is:
H=2
641010101
0110011
00011113
75: (25.20)
Thetrellis obtained fromthepermutedmatrix Ggiveninequation (25.19) is
showningure 25.7a. Notice thatthenumberofnodesinthistrellis issmaller
thanthenumberofnodesintheprevious trellis fortheHamming (7;4)code
ingure 25.1c. Wethusobserv ethatrearranging theorderofthecodeword
bitscansometimes leadtosmaller,simpler trellises.(a)
(b)
Figure 25.7.Trellises forthe
permuted(7;4)Hamming code
generated from(a)thegenerator
matrix bythemetho dof
gure 25.6;(b)theparity-check
matrix bythemetho donpage
332.
Eachedgeinatrellis islabelled
byazero(shownbyasquare) or
aone(shownbyacross).Trellisesfromparity-che ckmatric es
Another wayofviewing thetrellis isinterms ofthesyndrome. Thesyndrome
ofavectorrisdened tobeHr,whereHistheparity-checkmatrix. Avector
isonlyacodewordifitssyndrome iszero. Aswegenerate acodewordwecan
describ ethecurren tstate bythepartial syndrome ,thatis,theproductof
Hwiththecodewordbitsthusfargenerated. Eachstate inthetrellis isa
partial syndrome atonetimecoordinate. Thestarting andending states are
bothconstrained tobethezerosyndrome. Eachnodeinastaterepresen tsa
dieren tpossible valueforthepartial syndrome. SinceHisanMNmatrix,
whereM=N K,thesyndrome isatmostanM-bitvector. Soweneedat
most2Mnodesineachstate. Wecanconstruct thetrellis ofacodefromits
parity-checkmatrix bywalking fromeachend,generating twotreesofpossible
syndrome sequences. Theintersection ofthese twotreesdenes thetrellis of
thecode.
Inthepictures weobtain fromthisconstruction, wecanletthevertical
coordinate represen tthesyndrome. Then anyhorizon taledgeisnecessarily
associated withazerobit(since onlyanon-zero bitchanges thesyndrome)

<<<PAGE 345>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
25.5: Solutions 333
andanynon-horizon taledgeisassociated withaonebit.(Thusinthisrep-
resentation wenolonger needtolabeltheedges inthetrellis.) Figure 25.7b
showsthetrellis corresp onding totheparity-checkmatrix ofequation (25.20).
25.5 Solutions
t LikelihoodPosterior probabilit y
0000000 0.026 0.3006
0001011 0.00041 0.0047
0010111 0.0037 0.0423
0011100 0.015 0.1691
0100110 0.00041 0.0047
0101101 0.00010 0.0012
0110001 0.015 0.1691
0111010 0.0037 0.0423
1000101 0.00041 0.0047
1001110 0.00010 0.0012
1010010 0.015 0.1691
1011001 0.0037 0.0423
1100011 0.00010 0.0012
1101000 0.00041 0.0047
1110100 0.0037 0.0423
1111111 0.000058 0.0007Table25.8.Theposterior
probabilit yovercodewordsfor
exercise 25.4.
Solution toexercise 25.4(p.329).Theposterior probabilit yovercodewordsis
shownintable 25.8. Themostprobable codewordis0000000 .Themarginal
posterior probabilities ofallsevenbitsare:
n Likelihood Posterior marginals
P(ynjtn=1)P(ynjtn=0)P(tn=1jy)P(tn=0jy)
1 0:2 0:8 0:266 0:734
2 0:2 0:8 0:266 0:734
3 0:9 0:1 0:677 0:323
4 0:2 0:8 0:266 0:734
5 0:2 0:8 0:266 0:734
6 0:2 0:8 0:266 0:734
7 0:2 0:8 0:266 0:734
Sothebitwisedecodingis0010000 ,whichisnotactually acodeword.
Solution toexercise 25.9(p.330).TheMAP codewordis101,anditslike-
lihoodis1=8.Thenormalizing constan tofthesum{pro ductalgorithm is
Z=I=3/16.Theintermediate iare(from lefttoright)1/2,1/4,5/16,4/16;
theintermediate iare(from righttoleft),1/2,1/8,9/32,3/16.Thebitwise
decodingis:P(t1=1jy)=3=4;P(t1=1jy)=1=4;P(t1=1jy)=5=6.The
codewords's probabilities are1/12,2/12,1/12,8/12for000,011,110,101.

<<<PAGE 346>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
26
Exact Marginalization inGraphs
Wenowtakeamoregeneral viewofthetasksofinference andmarginalization.
Before reading thischapter, youshould readaboutmessage passing inChapter
16.
26.1 Thegeneral problem
Assume thatafunctionPofasetofNvariables xfxngN
n=1isdened as
aproductofMfactors asfollows:
P(x)=MY
m=1fm(xm): (26.1)
Eachofthefactorsfm(xm)isafunction ofasubset xmofthevariables that
makeupx.IfPisapositivefunction thenwemaybeinterested inasecond
normalized function,
P(x)1
ZP(x)=1
ZMY
m=1fm(xm); (26.2)
where thenormalizing constan tZisdened by
Z=X
xMY
m=1fm(xm): (26.3)
Asanexample ofthenotation we'vejustintroduced, here's afunction on
three binary variablesx1,x2,x3dened bythevefactors:
f1(x1)=(
0:1x1=0
0:9x1=1
f2(x2)=(
0:1x2=0
0:9x2=1
f3(x3)=(
0:9x3=0
0:1x3=1
f4(x1;x2)=(
1(x1;x2)=(0;0)or(1;1)
0(x1;x2)=(1;0)or(0;1)
f5(x2;x3)=(
1(x2;x3)=(0;0)or(1;1)
0(x2;x3)=(1;0)or(0;1)
P(x)=f1(x1)f2(x2)f3(x3)f4(x1;x2)f5(x2;x3)
P(x)=1
Zf1(x1)f2(x2)f3(x3)f4(x1;x2)f5(x2;x3):(26.4)
334

<<<PAGE 347>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
26.1: Thegeneral problem 335
Thevesubsets offx1;x2;x3gdenoted byxminthegeneral function (26.1)
areherex1=fx1g,x2=fx2g,x3=fx3g,x4=fx1;x2g,andx5=fx2;x3g.
@@  @@  x1x2x3g g g                     
f1f2f3f4f5
Figure 26.1.Thefactor graph
associated withthefunctiong(x)
(26.4).ThefunctionP(x),bytheway,mayberecognized astheposterior prob-
abilitydistribution ofthethree transmitted bitsinarepetition code(section
1.2)when thereceivedsignal isr=(1;1;0)andthechannel isabinary sym-
metric channel withipprobabilit y0.1.Thefactorsf4andf5respectively
enforce theconstrain tsthatx1andx2mustbeidenticalandthatx2andx3
mustbeidentical. Thefactorsf1,f2,f3arethelikelihoodfunctions con-
tributed byeachcomponentofr.
Afunction ofthefactored form(26.1) canbedepicted byafactor graph ,in
whichthevariables aredepicted bycircular nodesandthefactors aredepicted
bysquare nodes.Anedgeisputbetweenvariable nodenandfactor nodem
ifthefunctionfm(xm)hasanydependence onvariablexn.Thefactor graph
fortheexample function (26.4) isshowningure 26.1.
Thenormalization problem
Thersttasktobesolvedistocompute thenormalizing constan tZ.
Themarginalization problems
Thesecond tasktobesolvedistocompute themarginal function ofany
variablexn,dened by
Zn(xn)=X
fxn0g;n06=nP(x): (26.5)
Forexample, iffisafunction ofthree variables thenthemarginal for
n=1isdened by
Z1(x1)=X
x2;x3f(x1;x2;x3): (26.6)
Thistypeofsummation, over`allthexn0except forxn'issoimportantthat
itcanbeuseful tohaveaspecialnotation forit{the`not-sum' or`summary'.
Thethirdtasktobesolvedistocompute thenormalized marginal ofany
variablexn,dened by
Pn(xn)X
fxn0g;n06=nP(x): (26.7)
[Weinclude thesux `n'inPn(xn),departing fromournormal practice inthe
restofthebook,where wewouldomitit.]
.Exercise 26.1.[1]Showthatthenormalized marginal isrelated tothemarginal
Zn(xn)by
Pn(xn)=Zn(xn)
Z: (26.8)
Wemightalsobeinterested inmarginals overasubset ofthevariables,
suchas
Z12(x1;x2)X
x3P(x1;x2;x3): (26.9)
Allthese tasks areintractable ingeneral. Evenifeveryfactor isafunction
ofonlythree variables, thecostofcomputing exact solutions forZandfor
themarginals isbelievedingeneral togrowexponentially withthenumberof
variablesN.

<<<PAGE 348>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
336 26|Exact Marginalization inGraphs
Forcertain functionsP,however,themarginals canbecomputed e-
cientlybyexploiting thefactorization ofP.Theideaofhowthiseciency
arises iswellillustrated bythemessage-passing examples ofChapter 16.The
sum{pro ductalgorithm thatwenowreview isageneralization ofmessage-
passing rule-set B(p.242).Aswasthecasethere, thesum{pro ductalgorithm
isonlyvalidifthegraph istree-lik e.
26.2 Thesum{pro ductalgorithm
Notation
Weidentifythesetofvariables thatthemthfactor dependson,xm,bytheset
oftheirindicesN(m).Forourexample function (26.4), thesetsareN(1)=
f1g(sincef1isafunction ofx1alone),N(2)=f2g,N(3)=f3g,N(4)=
f1;2g,andN(5)=f2;3g.Similarly wedene thesetoffactors inwhich
variablenparticipates, byM(n).Wedenote asetN(m)withvariablen
excluded byN(m)nn.Weintroducetheshorthand xmnnorxmnntodenote
thesetofvariables inxmwithxnexcluded, i.e.,
xmnnfxn0:n02N(m)nng: (26.10)
Thesum{pro ductalgorithm willinvolvemessages oftwotypespassing
along theedges inthefactor graph: messagesqn!mfromvariable nodesto
factor nodes,andmessagesrm!nfrom factor nodestovariable nodes. A
message (ofeither type,qorr)thatissentalong anedgeconnecting factor
fmtovariablexnisalwaysafunction ofthevariablexn.
Herearethetworulesfortheupdating ofthetwosetsofmessages.
Fromvariable tofactor:
qn!m(xn)=Y
m02M(n)nmrm0!n(xn): (26.11)
Fromfactor tovariable:
rm!n(xn)=X
xmnn0
@fm(xm)Y
n02N(m)nnqn0!m(xn0)1
A:(26.12)
Howtheserulesapplytoleavesinthefactor graph
Anodethathasonlyoneedgeconnecting ittoanother nodeiscalled aleaffmxn
rm!n(xn)=fm(xn)
Figure 26.2.Afactor nodethatis
aleafnodeperpetually sends the
messagerm!n(xn)=fm(xn)to
itsoneneighbourxn.
node.
Some factor nodesinthegraph maybeconnected toonlyonevari-
ablenode,inwhichcasethesetN(m)nnofvariables appearing inthefac-
tormessage update(26.12) isanemptyset,andtheproductoffunctionsQ
n02N(m)nnqn0!m(xn0)istheemptyproduct, whose valueis1.Suchafac-
tornodetherefore alwaysbroadcasts toitsoneneighbourxnthemessage
rm!n(xn)=fm(xn).
Similarly ,there maybevariable nodesthatareconnected toonlyone
factor node,sothesetM(n)nmin(26.11) isempty.These nodesperpetually
broadcast themessageqn!m(xn)=1.fmxn
qn!m(xn)=1
Figure 26.3.Avariable nodethat
isaleafnodeperpetually sends
themessageqn!m(xn)=1.

<<<PAGE 349>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
26.2: Thesum{pro ductalgorithm 337
Starting andnishing, method1
Thealgorithm canbeinitialized intwoways.Ifthegraph istree-lik ethen
itmusthavenodesthatareleaves.These leafnodescanbroadcast their
messages totheirrespectiveneighboursfromthestart.
Forallleafvariable nodesn:qn!m(xn)=1 (26.13)
Forallleaffactor nodesm:rm!n(xn)=fm(xn):(26.14)
Wecanthenadopt theprocedure usedinChapter 16'smessage-passing rule-
setB(p.242):amessage iscreated inaccordance withtherules(26.11, 26.12)
onlyifallthemessages onwhichitdependsarepresen t.Forexample, in
@@  @@  x1x2x3g g g                     
f1f2f3f4f5
Figure 26.4.Ourmodelfactor
graph forthefunctiong(x)(26.4).gure 26.4,themessage fromx1tof1willbesentonlyoncethemessage from
f4tox1hasbeenreceived;andthemessage fromx2tof2,q2!2,canbesent
onlyoncethemessagesr4!2andr5!2havebothbeenreceived.
Messages willthusowthrough thetree,oneineachdirection along every
edge, andafteranumberofsteps equal tothediameter ofthegraph, every
message willhavebeencreated.
Theanswerswerequire canthenbereadout.Themarginal function of
xnisobtained bymultiplying alltheincoming messages atthatnode.
Zn(xn)=Y
m2M(n)rm!n(xn): (26.15)
Thenormalizing constan tZcanbeobtained bysumming anymarginal
function,Z=P
xnZn(xn),andthenormalized marginals obtained from
Pn(xn)=Zn(xn)
Z: (26.16)
.Exercise 26.2.[2]Apply thesum{pro ductalgorithm tothefunction dened in
equation (26.4) andgure 26.1. Checkthatthenormalized marginals
areconsisten twithwhat youknowabouttherepetition codeR3.
Exercise 26.3.[3]Provethatthesum{pro ductalgorithm correctly computes
themarginal functionsZn(xn)ifthegraph istree-lik e.
Exercise 26.4.[3]Describ ehowtousethemessages computed bythesum{
productalgorithm toobtain more complicated marginal functions ina
tree-lik egraph, forexampleZ1;2(x1;x2),fortwovariablesx1andx2that
areconnected toonecommon factor node.
Starting andnishing, method2
Alternativ ely,thealgorithm canbeinitialized bysetting alltheinitial mes-
sages fromvariables to1:
foralln,m:qn!m(xn)=1; (26.17)
thenproceeding withthefactor message updaterule(26.12), alternating with
thevariable message updaterule(26.11). Compared withmetho d1,thislazy
initialization metho dleads toaloadofwasted computations, whose results
aregradually ushed outbythecorrect answerscomputed bymetho d1.
After anumberofiterations equal tothediameter ofthefactor graph,
thealgorithm willconvergetoasetofmessages satisfying thesum{pro duct
relationships (26.11, 26.12).

<<<PAGE 350>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
338 26|Exact Marginalization inGraphs
Exercise 26.5.[2]Apply thissecond version ofthesum{pro ductalgorithm to
thefunction dened inequation (26.4) andgure 26.1.
Thereason forintroducing thislazymetho disthat(unlik emetho d1)itcan
beapplied tographs thatarenottree-lik e.When thesum{pro ductalgorithm
isrunonagraph withcycles, thealgorithm doesnotnecessarily converge,
andcertainly doesnotingeneral compute thecorrect marginal functions; but
itisnevertheless analgorithm ofgreat practical importance, especially inthe
decodingofsparse graph codes.
Sum{pr oductalgorithm withon-the-y normalization
Ifweareinterested inonlythenormalize dmarginals, thenanother version
ofthesum{pro ductalgorithm maybeuseful. Thefactor-to-v ariable messages
rm!narecomputed injustthesame way(26.12), butthevariable-to-factor
messages arenormalized thus:
qn!m(xn)=nmY
m02M(n)nmrm0!n(xn) (26.18)
wherenmisascalar chosen suchthat
X
xnqn!m(xn)=1: (26.19)
Exercise 26.6.[2]Apply thisnormalized version ofthesum{pro ductalgorithm
tothefunction dened inequation (26.4) andgure 26.1.
Afactorization viewofthesum{pr oductalgorithm
Onewaytoviewthesum{pro ductalgorithm isthatitreexpresses theoriginal
factored function, theproductofMfactorsP(x)=QM
m=1fm(xm),asanother
factored function whichistheproductofM+Nfactors,
P(x)=MY
m=1m(xm)NY
n=1 n(xn): (26.20)
Eachfactormisassociated withafactor nodem,andeachfactor n(xn)is
associated withavariable node.Initiallym(xm)=fm(xm)and n(xn)=1.
Eachtimeafactor-to-v ariable messagerm!n(xn)issent,thefactorization
isupdated thus:
 n(xn)=Y
m2M(n)rm!n(xn) (26.21)
m(xm)=f(xm)Q
n2N(m)rm!n(xn): (26.22)
Andeachmessage canbecomputed interms ofand using
rm!n(xn)=X
xmnn0
@m(xm)Y
n02N(m) n0(xn0)1
A (26.23)
whichdiers fromtheassignmen t(26.12) inthattheproductisoveralln02
N(m).
Exercise 26.7.[2]Conrm thattheupdaterules(26.21{26.23) areequivalent
tothesum{pro ductrules(26.11{26.12). So n(xn)eventually becomes
themarginalZn(xn).
Thisfactorization viewp ointapplies whether ornotthegraph istree-lik e.

<<<PAGE 351>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
26.3: Themin{sum algorithm 339
Computational tricks
On-the-y normalization isagoodideafromacomputational pointofview
because ifPisaproductofmanyfactors, itsvaluesarelikelytobeverylarge
orverysmall.
Another useful computational trickinvolvespassing thelogarithms ofthe
messagesqandrinstead ofqandrthemselv es;thecomputations ofthe
products inthealgorithm (26.11, 26.12) arethenreplaced bysimpler additions.
Thesummations in(26.12) ofcourse become moredicult: tocarry them out
andreturn thelogarithm, weneedtocompute softmax functions like
l=ln(el1+el2+el3): (26.24)
Butthiscomputation canbedoneecien tlyusing look-up tables along with
theobserv ation thatthevalueoftheanswerlistypically justalittlelarger
thanmax ili.Ifwestoreinlook-up tables values ofthefunction
ln(1+e) (26.25)
(fornegativ e)thenlcanbecomputed exactly inanumberoflook-ups and
additions scaling asthenumberofterms inthesum. Iflook-ups andsorting
operations arecheaperthanexp() thenthisapproac hcosts lessthanthe
direct evaluation (26.24). Thenumberofoperations canbefurther reduced
byomitting negligible contributions fromthesmallest oftheflig.
Athirdcomputational trickapplicable tocertain error-correcting codesis
topassnotthemessages buttheFourier transform ofthemessages. This
again makesthecomputations ofthefactor-to-v ariable messages quicker.A
simple example ofthisFourier transform trickisgiveninChapter 47atequa-
tion(47.9).
26.3 Themin{sum algorithm
Thesum{pro ductalgorithm solvestheproblem ofnding themarginal func-
tionofagivenproductP(x).Thisisanalogous tosolving thebitwisedecod-
ingproblem ofsection 25.1. Andjustasthere wereother decodingproblems
(forexample, thecodeworddecodingproblem), wecandene other tasks
involvingP(x)thatcanbesolvedbymodications ofthesum{pro ductalgo-
rithm. Forexample, consider thistask,analogous tothecodeworddecoding
problem:
Themaximization problem .Find thesetting ofxthatmaximizes the
productP(x).
Thisproblem canbesolvedbyreplacing thetwooperations addandmul-
tiplyeverywhere theyappearinthesum{pro ductalgorithm byanother pair
ofoperations thatsatisfy thedistributiv elaw,namely maxandmultiply .If
wereplace summation (+,P)bymaximization, wenotice thatthequantity
formerly knownasthenormalizing constan t,
Z=X
xP(x); (26.26)
becomes maxxP(x).
Thusthesum{pro ductalgorithm canbeturned intoamax{pro ductalgo-
rithm thatcomputes maxxP(x),andfromwhichthesolution ofthemax-
imization problem canbededuced. Each`marginal'Zn(xn)thenliststhe
maxim umvaluethatP(x)canattain foreachvalueofxn.

<<<PAGE 352>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
340 26|Exact Marginalization inGraphs
Inpractice, themax{pro ductalgorithm ismost often carried outinthe
negativ eloglikelihooddomain, where maxandproductandreplaced bymin
andsum. Themin{sum algorithm isalsoknownastheViterbi algorithm.
26.4 Thejunction treealgorithm
What should onedowhen thefactor graph oneisinterested inisnotatree?
There areseveraloptions, andtheydivide intoexact metho dsandapprox-
imate metho ds.Themostwidely usedexact metho dforhandling marginaliza-
tionongraphs withcycles iscalled thejunction treealgorithm. Thisalgorithm
worksbyagglomerating variables together untiltheagglomerated graph has
nocycles. Youcanprobably gure outthedetails foryourself; thecomplexit y
ofthemarginalization growsexponentially withthenumberofagglomerated
variables. Read more aboutthejunction treealgorithm in(Lauritzen, 1996;
Jordan, 1998).
There aremanyapproximate metho ds,andwe'llvisitsome ofthem over
thenextfewchapters {MonteCarlo metho dsandvariational metho ds,to
name acouple. However,themost amusing wayofhandling factor graphs
towhichthesum{pro ductalgorithm maynotbeapplied is,aswealready
mentioned, toapply thesum{pro ductalgorithm! Wesimply compute the
messages foreachnodeinthegraph, asifthegraph wereatree,iterate, and
crossourngers. Thisso-called `loopy'message passing hasgreat importance
inthedecodingoferror-correcting codes,andwe'llcome backtoitinsection
33.8andPartVI.
Further reading
Forfurther reading aboutfactor graphs andthesum{pro ductalgorithm, see
Kschischangetal.(2001), Yedidia etal.(2000c), Yedidia etal.(2000a), Yedidia
etal.(2002), Wainwrightetal.(2002), andForney (2001).
SeealsoPearl(1988). Agoodreference forthefundamen taltheory of
graphical modelsisLauritzen (1996). Areadable introduction toBayesian
networksisgivenbyJensen (1996).
Interesting message-passing algorithms thathavedieren tcapabilities from
thesum{pro ductalgorithm include expectation propagation (Mink a,2001)
andsurveypropagation (Braunstein etal.,2003). Seealsosection 33.8.
26.5 Exercises
.Exercise 26.8.[2]Express thejointprobabilit ydistribution fromtheburglar
alarm andearthquak eproblem (example 21.1(p.293))asafactor graph,
andndthemarginal probabilities ofallthevariables aseachpiece of
information comes toFred'sattention,using thesum{pro ductalgorithm
withon-the-y normalization.

<<<PAGE 353>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
27
Laplace's Metho d
Theideabehind theLaplace approximation issimple. Weassume thatan
unnormalized probabilit ydensit yP(x),whose normalizing constan t
ZPZ
P(x)dx (27.1)
isofinterest, hasapeakatapointx0.WeTaylor-expand thelogarithm ofP(x)
P(x)around thispeak:
lnP(x)
lnP(x)
&lnQ(x)lnP(x)'lnP(x0) c
2(x x0)2+; (27.2)
where
c= @2
@x2lnP(x)
x=x0: (27.3)
WethenapproximateP(x)byanunnormalized Gaussian,
Q(x)P(x0)exp
 c
2(x x0)2
; (27.4)
andweapproximate thenormalizing constan tZPbythenormalizing constan t
P(x)
&Q(x)ofthisGaussian,
ZQ=P(x0)r2
c: (27.5)
Wecangeneralize thisintegral toapproximateZPforadensit yP(x)over
aK-dimensional spacex.Ifthematrix ofsecond derivativesof lnP(x)at
themaxim umx0isA,dened by:
Aij= @2
@xi@xjlnP(x)
x=x0; (27.6)
sothattheexpansion (27.2) isgeneralized to
lnP(x)'lnP(x0) 1
2(x x0)TA(x x0)+; (27.7)
thenthenormalizing constan tcanbeapproximated by:
ZP'ZQ=P(x0)1q
det1
2A=P(x0)s
(2)K
detA: (27.8)
Predictions canbemade using theapproximationQ.Physicists alsocallthis
widely-used approximation thesaddle-p ointapproximation .
341

<<<PAGE 354>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
342 27|Laplace's Metho d
Thefactthatthenormalizing constan tofaGaussian isgivenby
Z
dKxexph
 1
2xTAxi
=r
(2)K
detA(27.9)
canbeprovedbymaking anorthogonal transformation intothebasisuinwhichA
istransformed intoadiagonal matrix. Theintegral thenseparates intoaproductof
one-dimensional integrals, eachoftheform
Z
duiexph
 1
2iu2
ii
=r
2
i: (27.10)
Theproductoftheeigenvalues iisthedeterminan tofA.
TheLaplace approximation isbasis-dep enden t:ifxistransformed toa
nonlinear functionu(x)andthedensit yistransformed toP(u)=P(x)jdx=duj
theningeneral theapproximate normalizing constan tsZQwillbedieren t.
Thiscanbeviewedasadefect {sincethetruevalueZPisbasis-indep enden t
{oranopportunit y{because wecanhuntforachoice ofbasisinwhichthe
Laplace approximation ismostaccurate.
27.1 Exercises
Exercise 27.1.[2](Seealsoexercise 22.8(p.307).)Aphoton counterispointed
ataremote starforoneminute,inorder toinfertherateofphotons
arriving atthecounterperminute,.Assuming thenumberofphotons
collectedrhasaPoisson distribution withmean,
P(rj)=exp( )r
r!; (27.11)
andassuming theimprop erpriorP()=1=,makeLaplace approxima-
tionstotheposterior distribution
(a)over
(b)overlog.[Note theimprop erprior transforms toP(log)=
constan t.]
.Exercise 27.2.[2]UseLaplace's metho dtoapproximate theintegral
Z(u1;u2)=Z1
 1daf(a)u1(1 f(a))u2; (27.12)
wheref(a)=1=(1+e a)andu1;u2arepositive.Checktheaccuracy of
theapproximation against theexact answer(23.29, p.316)for(u1;u2)=
(1/2;1/2)and(u1;u2)=(1;1).Measure theaccuracy (logZP logZQ)
inbits.
.Exercise 27.3.[3]Linearregression.Ndatap ointsf(x(n);t(n))garegenerated by
theexperimen terchoosing eachx(n),thentheworlddelivering anoisy
version ofthelinear function
y(x)=w0+w1x; (27.13)
t(n)Normal (y(x(n));2
): (27.14)
Assuming Gaussian priors onw0andw1,maketheLaplace approxima-
tiontotheposterior distribution ofw0andw1(whichisexact, infact)
andobtain thepredictiv edistribution forthenextdatap ointt(N+1),given
x(N+1).
(SeeMacKa y(1992a) forfurther reading.)

<<<PAGE 355>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
28
ModelComparison andOccam's Razor                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          
Figure 28.1.Apicture tobe
interpreted. Itcontainsatreeand
someboxes.
28.1 Occam's razor
Howmanyboxesareinthepicture (gure 28.1)? Inparticular, howmany
boxesareinthevicinit yofthetree? Ifwelookedwithx-rayspectacles,
wouldweseeoneortwoboxesbehind thetrunk (gure 28.2)? (Oreven
more?) Occam's razor istheprinciple thatstates apreference forsimple1?
or2?
Figure 28.2.Howmanyboxesare
behind thetree?
theories. `Accept thesimplest explanation thattsthedata'. Thusaccording
toOccam's razor, weshould deduce thatthereisonlyoneboxbehind thetree.
Isthisanadhocruleofthumb?Oristhere aconvincing reason forbelieving
there ismostlikelyonebox?Perhaps yourintuition likestheargumen t`well,
itwouldbearemark ablecoincidenc eforthetwoboxestobejustthesame
heightandcolour aseachother'. Ifwewishtomakeartical intelligences that
interpret datacorrectly ,wemusttranslate thisintuitivefeeling intoaconcrete
theory .
Motivations forOccam'srazor
Ifseveralexplanations arecompatible withasetofobserv ations, Occam's
razor advises ustobuythesimplest. Thisprinciple isoften advocated forone
oftworeasons: therstisaesthetic (`Atheory withmathematical beautyis
more likelytobecorrect thananuglyonethattssome experimen taldata'
343

<<<PAGE 356>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
344 28|ModelComparison andOccam's Razor
P(D|H )2P(D|H )1Evidence
CD
1Figure 28.3.WhyBayesian
inference embodiesOccam's razor.
Thisgure givesthebasic
intuition forwhycomplex models
canturnouttobelessprobable.
Thehorizon talaxisrepresen tsthe
space ofpossible datasetsD.
Bayes'theorem rewardsmodelsin
proportion tohowmuchthey
predictedthedatathatoccurred.
These predictions arequantied
byanormalized probabilit y
distribution onD.This
probabilit yofthedatagiven
modelHi,P(DjHi),iscalled the
evidence forHi.
Asimple modelH1makesonlya
limited range ofpredictions,
shownbyP(DjH1);amore
powerfulmodelH2,thathas,for
example, morefreeparameters
thanH1,isabletopredict a
greater varietyofdatasets.This
means, however,thatH2doesnot
predict thedatasetsinregionC1
asstrongly asH1.Supposethat
equal priorprobabilities havebeen
assigned tothetwomodels.Then,
ifthedatasetfallsinregionC1,
thelesspowerful modelH1willbe
themoreprobablemodel.(PaulDirac)); thesecond reason isthepastempirical success ofOccam's razor.
Howeverthere isadieren tjustication forOccam's razor, namely:
Coheren tinference (asembodiedbyBayesian probabilit y)auto-
matically embodiesOccam's razor, quantitativ ely.
Itisindeed moreprobablethatthere's oneboxbehind thetree,andwecan
compute howmuchmore probable oneisthantwo.
Modelcomparison andOccam'srazor
Weevaluate theplausibilit yoftwoalternativ etheoriesH1andH2inthelight
ofdataDasfollows:using Bayes'theorem, werelate theplausibilit yofmodel
H1giventhedata,P(H1jD),tothepredictions made bythemodelabout
thedata,P(DjH1),andtheprior plausibilit yofH1,P(H1).Thisgivesthe
followingprobabilit yratiobetweentheoryH1andtheoryH2:
P(H1jD)
P(H2jD)=P(H1)
P(H2)P(DjH1)
P(DjH2): (28.1)
Therstratio(P(H1)=P(H2))ontheright-hand sidemeasures howmuchour
initial beliefs favouredH1overH2.Thesecond ratioexpresses howwellthe
observ eddatawerepredicted byH1,compared toH2.
Howdoesthisrelate toOccam's razor, whenH1isasimpler modelthan
H2?Therstratio(P(H1)=P(H2))givesustheopportunit y,ifwewish, to
insert aprior biasinfavourofH1onaesthetic grounds, oronthebasis of
experience. Thiswouldcorresp ondtotheaesthetic andempirical motivations
forOccam's razor mentioned earlier. Butsuchapriorbiasisnotnecessary:
thesecond ratio, thedata-dep enden tfactor, embodiesOccam's razorauto-
matically.Simple modelstendtomakeprecise predictions. Complex models,
bytheirnature, arecapable ofmaking agreater varietyofpredictions (gure
28.3). SoifH2isamore complex model,itmustspread itspredictiv eprob-
abilityP(DjH2)more thinly overthedataspace thanH1.Thus,inthecase
where thedataarecompatible withboththeories, thesimplerH1willturnout
more probable thanH2,without ourhavingtoexpress anysubjectivedislike
forcomplex models.Oursubjectivepriorjustneeds toassign equal priorprob-
abilities tothepossibilities ofsimplicit yandcomplexit y.Probabilit ytheory
thenallowstheobserv eddatatoexpress theiropinion.
Letusturntoasimple example. Hereisasequence ofnumbers:
 1;3;7;11:
Thetaskistopredict thenexttwonumbers,andinfertheunderlying process
thatgaverisetothissequence. Apopular answertothisquestion isthe
prediction `15,19',withtheexplanation `add4totheprevious number'.
What aboutthealternativ eanswer` 19:9;1043:8'withtheunderlying
rulebeing: `getthenextnumberfromtheprevious number,x,byevaluating

<<<PAGE 357>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
28.1: Occam's razor 345
 x3=11+9=11x2+23=11'?Iassume thatthisprediction seems rather less
plausible. Butthesecond ruletsthedata( 1,3,7,11)justaswellasthe
rule`add4'.Sowhyshould wenditlessplausible? Letusgivelabelstothe
twogeneral theories:
Ha{thesequence isanarithmetic progression, `addn',wherenisaninteger.
Hc{thesequence isgenerated byacubic function oftheformx!cx3+
dx2+e,wherec,dandearefractions.
Onereason fornding thesecond explanation,Hc,lessplausible, mightbe
thatarithmetic progressions aremorefrequen tlyencoun tered thancubic func-
tions. Thiswouldputabiasintheprior probabilit yratioP(Ha)=P(Hc)in
equation (28.1). Butletusgivethetwotheories equal priorprobabilities, and
concen trateonwhat thedatahavetosay.Howwelldideachtheory predict
thedata?
ToobtainP(DjHa)wemustspecifytheprobabilit ydistribution thateach
modelassigns toitsparameters. First,Hadependsontheadded integern,
andtherstnumberinthesequence. Letussaythatthese numberscould
eachhavebeenanywhere between 50and50.Then since onlythepairof
valuesfn=4,rstnumber= 1ggiverisetotheobserv eddataD=( 1,3,
7,11),theprobabilit yofthedata, givenHa,is:
P(DjHa)=1
1011
101=0:00010: (28.2)
ToevaluateP(DjHc),wemustsimilarly saywhat values thefractionsc;d
andemighttakeon.[Ichoosetorepresen tthese numbersasfractions rather
thanrealnumbersbecause ifweusedrealnumbers,themodelwouldassign,
relativ etoHa,aninnitesimal probabilit ytoD.Realparameters arethe
norm however,andareassumed intherestofthischapter.] Areasonable
priormightstate thatforeachfraction thenumerator could beanynumber
between 50and50,andthedenominator isanynumberbetween1and50.
Asfortheinitial valueinthesequence, letusleaveitsprobabilit ydistribution
thesameasinHa.There arefourwaysofexpressing thefractionc= 1=11=
 2=22= 3=33= 4=44under thisprior, andsimilarly therearefourandtwo
possible solutions fordande,respectively.Sotheprobabilit yoftheobserv ed
data, givenHc,isfound tobe:
P(DjHc)=1
1014
1011
504
1011
502
1011
50
=0:0000000000025 =2:510 12: (28.3)
Thuscomparing P(DjHc)withP(DjHa)=0:00010, evenifourpriorproba-
bilities forHaandHcareequal, theodds,P(DjHa):P(DjHc),infavourof
HaoverHc,giventhesequenceD=( 1,3,7,11),areaboutfortymillion to
one. 2
Thisanswerdependsonseveralsubjectiveassumptions; inparticular, the
probabilit yassigned tothefreeparameters n,c,d,eofthetheories. Bayesians
makenoapologies forthis:there isnosuchthing asinference orprediction
without assumptions. However,thequantitativ edetails oftheprior proba-
bilities havenoeect onthequalitativ eOccam's razor eect; thecomplex
theoryHcalwayssuers an`Occam factor' because ithasmore parameters,
andsocanpredict agreater varietyofdatasets(gure 28.3). Thiswasonly
asmall example, andthere wereonlyfourdatapoints;aswemovetolarger
andmore sophisticated problems themagnitude oftheOccam factors typi-
callyincreases, andthedegree towhichourinferences areinuenced bythe
quantitativ edetails ofoursubjectiveassumptions becomes smaller.

<<<PAGE 358>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
346 28|ModelComparison andOccam's Razor
Gather
DATACreate
alternativ e
MODELS
  @@
FiteachMODEL
totheDATA
  @@
Assign preferences tothe
alternativ eMODELS
Choosewhat
datato
gather nextGather
moredata
Decide whether
tocreate new
modelsCreate new
models
Choosefuture
actions6 -
6 
@
@@R 
  	
?Figure 28.4.Where Bayesian
inference tsintothedata
modelling process.
Thisgure illustrates an
abstraction ofthepartofthe
scienticprocessinwhichdata
arecollected andmodelled. In
particular, thisgure applies to
pattern classication, learning,
interpolation, etc.Thetwo
double-framed boxesdenote the
twosteps whichinvolveinference.
Itisonlyinthose twosteps that
Bayes'theorem canbeused.
Bayesdoesnottellyouhowto
inventmodels,forexample.
Therstbox,`tting eachmodel
tothedata', isthetaskof
inferring what themodel
parameters mightbegiventhe
modelandthedata. Bayesian
metho dsmaybeusedtondthe
mostprobable parameter values,
anderrorbarsonthose
parameters. Theresult of
applying Bayesianmetho dstothis
problem isoften littledieren t
fromtheanswersgivenby
orthodoxstatistics.
Thesecond inference task,model
comparison inthelightofthe
data, iswhere Bayesianmetho ds
areinaclassoftheirown.This
second inference problem requires
aquantitativ eOccam's razor to
penalise over-complex models.
Bayesianmetho dscanassign
objectivepreferences tothe
alternativ emodelsinawaythat
automatically embodiesOccam's
razor.Bayesian methodsanddataanalysis
Letusnowrelate thediscussion abovetorealproblems indataanalysis.
There arecountlessproblems inscience, statistics andtechnology which
require that, givenalimited dataset,preferences beassigned toalternativ e
modelsofdiering complexities. Forexample, twoalternativ ehypotheses
accoun tingforplanetary motion areMr.Inquisition's geocentricmodelbased
on`epicycles', andMr.Copernicus's simpler modelofthesolarsystem with
thesunatthecentre.Theepicyclic modeltsdataonplanetary motion at
leastaswellastheCopernican model,butdoessousing more parameters.
Coinciden tallyforMr.Inquisition, twooftheextra epicyclic parameters for
everyplanet arefound tobeidenticaltotheperiodandradius ofthesun's
`cycle around theearth'. IntuitivelywendMr.Copernicus's theory more
probable.
Themechanism oftheBayesian razor:theevidenc eandtheOccamfactor
Twolevelsofinference canoften bedistinguished intheprocessofdatamod-
elling. Attherstlevelofinference, weassume thataparticular modelistrue,
andwetthatmodeltothedata, i.e.,weinferwhat values itsfreeparam-
etersshould plausibly take,giventhedata. Theresults ofthisinference are
often summarized bythemostprobable parameter values, anderrorbarson
those parameters. Thisanalysis isrepeated foreachmodel.Thesecond level
ofinference isthetaskofmodelcomparison. Herewewishtocompare the
modelsinthelightofthedata, andassign some sortofpreference orranking
tothealternativ es.
Note thatbothlevelsofinferencearedistinct fromdecision theory.Thegoalof
inference is,givenadened hypothesis space andaparticular dataset,toassign
probabilities tohypotheses. Decision theory typically choosesbetweenalternativ e
actions onthebasisofthese probabilities soastominimize theexpectation ofa`loss
function'. Thischapter concerns inference alone andnolossfunctions areinvolved.
When wediscuss modelcomparison, thisshould notbeconstrued asimplying model
choice.Ideal Bayesian predictions donotinvolvechoice betweenmodels; rather,
predictions aremade bysumming overallthealternativ emodels,weightedbytheir
probabilities.

<<<PAGE 359>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
28.1: Occam's razor 347
Bayesian metho dsareableconsisten tlyandquantitativ elytosolveboth
theinference tasks. There isapopular myththatstates thatBayesian meth-
odsonlydier fromorthodoxstatistical metho dsbytheinclusion ofsubjective
priors, whicharedicult toassign, andwhichusually don't makemuchdif-
ference totheconclusions. Itistruethat, attherstlevelofinference, a
Bayesian's results willoften dier littlefromtheoutcome ofanorthodoxat-
tack.What isnotwidely appreciated ishowaBayesian performs thesecond
levelofinference; thissection willtherefore focusonBayesian modelcompar-
ison.
Modelcomparison isadicult taskbecause itisnotpossible simply to
choosethemodelthattsthedatabest:more complex modelscanalways
tthedatabetter, sothemaxim umlikelihoodmodelchoice wouldleadus
inevitably toimplausible, over-parameterized models,whichgeneralize poorly.
Occam's razor isneeded.
Letuswrite downBayes'theorem forthetwolevelsofinference describ ed
above,soastoseeexplicitly howBayesian modelcomparison works. Each
modelHiisassumed tohaveavector ofparameters w.Amodelisdened
byacollection ofprobabilit ydistributions: a`prior' distribution P(wjHi),
whichstates what values themodel'sparameters mightbeexpected totake;
andasetofconditional distributions, oneforeachvalueofw,dening the
predictions P(Djw;Hi)thatthemodelmakesaboutthedataD.
1.Modeltting. Attherstlevelofinference, weassume thatonemodel,
theith,say,istrue,andweinferwhatthemodel'sparameters wmight
be,giventhedataD.Using Bayes'theorem, theposterior probabilit y
oftheparameters wis:
P(wjD;Hi)=P(Djw;Hi)P(wjHi)
P(DjHi); (28.4)
thatis,
Posterior =LikelihoodPrior
Evidence:
Thenormalizing constan tP(DjHi)iscommonly ignored sinceitisirrele-
vanttotherstlevelofinference, i.e.,theinference ofw;butitbecomes
importantinthesecond levelofinference, andwename ittheevidence
forHi.Itiscommon practice tousegradien t-based metho dstondthe
maxim umoftheposterior, whichdenes themostprobable valueforthe
parameters, wMP;itisthenusual tosummarize theposterior distribu-
tionbythevalueofwMP,anderrorbarsorcondence intervalsonthese
besttparameters. Error barscanbeobtained fromthecurvature ofthe
posterior; evaluating theHessian atwMP,A= rr lnP(wjD;Hi)jwMP,
andTaylor-expanding thelogposterior probabilit ywithw=w wMP:
P(wjD;Hi)'P(wMPjD;Hi)exp
 1/2wTAw
; (28.5)
weseethattheposterior canbelocally approximated asaGaussian
withcovariance matrix (equiv alenttoerror bars)A 1.[Whether this
approximation isgoodornotwilldependontheproblem wearesolv-
ing.Indeed, themaxim umandmean oftheposterior distribution have
nofundamen talstatus inBayesian inference {theybothchange under
nonlinear reparameterizations. Maximization ofaposterior probabil-
ityisonlyuseful ifanapproximation likeequation (28.5) givesagood
summary ofthedistribution.]

<<<PAGE 360>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
348 28|ModelComparison andOccam's Razor
wMPwjD
wwP(wjHi)P(wjD;Hi)Figure 28.5.TheOccam factor.
Thisgure showsthequantities
thatdetermine theOccam factor
forahypothesisHihavinga
single parameter w.Theprior
distribution (solid line)forthe
parameter haswidthw.The
posterior distribution (dashed
line)hasasingle peakatwMP
withcharacteristic widthwjD.
TheOccam factor is
wjDP(wMPjHi)=wjD
w:2.Modelcomparison. Atthesecond levelofinference, wewishtoinfer
whichmodelismostplausible giventhedata. Theposterior probabilit y
ofeachmodelis:
P(HijD)/P(DjHi)P(Hi): (28.6)
Notice thatthedata-dep enden ttermP(DjHi)istheevidence forHi,
whichappeared asthenormalizing constan tin(28.4). Thesecond term,
P(Hi),isthesubjectiveprioroverourhypothesis space, whichexpresses
howplausible wethough tthealternativ emodelswerebeforethedata
arrived.Assuming thatwechoosetoassign equal priorsP(Hi)tothe
alternativ emodels,modelsHiarerankedbyevaluating theevidenc e.The
normalizing constan tP(D)=P
iP(DjHi)P(Hi)hasbeenomitted from
equation (28.6) because inthedatamodelling processwemaydevelop
newmodelsafterthedatahavearrived,when aninadequacy oftherst
modelsisdetected, forexample. Inference isopenended: wecontinually
seekmore probable modelstoaccoun tforthedatawegather.
Torepeatthekeyidea: torankalternativ emodelsHi,aBayesian eval-
uates theevidenceP(DjHi).Thisconcept isverygeneral: theevidence
canbeevaluated forparametric and`non-parametric' modelsalike;what-
everourdatamodelling task,aregression problem, aclassication prob-
lem,oradensit yestimation problem, theevidence isatransp ortable
quantityforcomparing alternativ emodels. Inallthese cases theevi-
dence naturally embodiesOccam's razor.
Evaluating theevidenc e
Letusnowstudy theevidence more closely togaininsigh tintohowthe
Bayesian Occam's razor works. Theevidence isthenormalizing constan tfor
equation (28.4):
P(DjHi)=Z
P(Djw;Hi)P(wjHi)dw: (28.7)
Formanyproblems theposteriorP(wjD;Hi)/P(Djw;Hi)P(wjHi)has
astrong peakatthemost probable parameters wMP(gure 28.5). Then,
taking forsimplicit ytheone-dimensional case, theevidence canbeapprox-
imated, using Laplace's metho d,bytheheightofthepeakoftheintegrand
P(Djw;Hi)P(wjHi)times itswidth,wjD:
P(DjHi)'P(DjwMP;Hi)|{z}P(wMPjHi)wjD|{z}:
Evidence'BesttlikelihoodOccam factor(28.8)
Thustheevidence isfound bytaking thebesttlikelihoodthatthemodel
canachieveandmultiplying itbyan`Occam factor', whichisatermwith
magnitude lessthanonethatpenalizesHiforhavingtheparameter w.

<<<PAGE 361>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
28.1: Occam's razor 349
wwjDP(wjH3)P(wjD;H3)
P(wjH2)P(wjD;H2)P(wjH1)P(wjD;H1)P(DjH1)P(DjH2)P(DjH3)
DD
w w wFigure 28.6.Ahypothesis space
consisting ofthree exclusiv e
models,eachhavingone
parameter w,anda
one-dimensional datasetD.The
`data set'isasingle measured
valuewhichdiers fromthe
parameter wbyasmall amoun t
ofadditiv enoise. Typical samples
fromthejointdistribution
P(D;w;H)areshownbydots.
(NB,these arenotdatapoints.)
Theobserv ed`data set'isasingle
particular valueforDshownby
thedashed horizon talline.The
dashed curvesbelowshowthe
posterior probabilit yofwforeach
modelgiventhisdataset(c.f.
gure 28.3). Theevidence forthe
dieren tmodelsisobtained by
marginalizing ontotheDaxisat
theleft-hand side(c.f.gure
28.5).
Interpr etation oftheOccamfactor
ThequantitywjDistheposterior uncertain tyinw.Supposeforsimplicit y
thatthepriorP(wjHi)isuniform onsomelargeintervalw,represen tingthe
range ofvaluesofwthatwerepossible apriori,according toHi(gure 28.5).
ThenP(wMPjHi)=1=w,and
Occam factor =wjD
w; (28.9)
i.e.,theOccamfactor isequaltotheratiooftheposterior accessible volume
ofHi'sparameter spacetotheprioraccessible volume, orthefactor bywhich
Hi'shypothesis space collapses when thedataarrive.ThemodelHicanbe
viewedasconsisting ofacertain numberofexclusiv esubmo dels,ofwhichonly
onesurviv eswhen thedataarrive.TheOccam factor istheinverseofthat
number.Thelogarithm oftheOccam factor isameasure oftheamoun tof
information wegainaboutthemodel'sparameters when thedataarrive.
Acomplex modelhavingmanyparameters, eachofwhichisfreetovary
overalargerangew,willtypically bepenalized byastronger Occam factor
thanasimpler model.TheOccam factor alsopenalizes modelsthathaveto
benely tuned totthedata, favouring modelsforwhichtherequired pre-
cision oftheparameters wjDiscoarse. Themagnitude oftheOccam factor
isthusameasure ofcomplexit yofthemodel;itrelates tothecomplexit yof
thepredictions thatthemodelmakesindataspace. Thisdependsnotonly
onthenumberofparameters inthemodel,butalsoonthepriorprobabilit y
thatthemodelassigns tothem. Whichmodelachievesthegreatest evidence
isdetermined byatrade-o betweenminimizing thisnatural complexit ymea-
sureandminimizing thedatamist. Incontrasttoalternativ emeasures of
modelcomplexit y,theOccam factor foramodelisstraigh tforwardtoevalu-
ate:itsimply dependsontheerrorbarsontheparameters, whichwealready
evaluated when tting themodeltothedata.
Figure 28.6displa ysanentirehypothesis space soastoillustrate thevar-
iousprobabilities intheanalysis. There arethree models,H1;H2;H3,which

<<<PAGE 362>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
350 28|ModelComparison andOccam's Razor
haveequal priorprobabilities. Eachmodelhasoneparameter w(eachshown
onahorizon talaxis), butassigns adieren tpriorrangeWtothatparame-
ter.H3isthemost`exible' or`complex' model,assigning thebroadest prior
range. Aone-dimensional dataspace isshownbythevertical axis. Each
modelassigns ajointprobabilit ydistribution P(D;wjHi)tothedataandthe
parameters, illustrated byacloud ofdots. These dotsrepresen trandom sam-
plesfromthefullprobabilit ydistribution. Thetotalnumberofdotsineach
ofthethree modelsubspaces isthesame, because weassigned equal prior
probabilities tothemodels.
When aparticular datasetDisreceived(horizon talline), weinferthepos-
terior distribution ofwforamodel(H3,say)byreading outthedensit yalong
thathorizon talline,andnormalizing. Theposterior probabilit yP(wjD;H3)
isshownbythedotted curveatthebottom. Alsoshownisthepriordistribu-
tionP(wjH3)(c.f.gure 28.5). [InthecaseofmodelH1whichisverypoorly
matchedtothedata, theshapeoftheposterior distribution willdependon
thedetails ofthetailsofthepriorP(wjH1)andthelikelihoodP(Djw;H1);
thecurveshownisforthecasewhere thepriorfallsomore strongly .]
Weobtain gure 28.3bymarginalizing thejointdistributions P(D;wjHi)
ontotheDaxisattheleft-hand side. ForthedatasetDshownbythe
dotted horizon talline,theevidenceP(DjH3)forthemore exible modelH3
hasasmaller valuethantheevidence forH2.ThisisbecauseH3placed less
predictiv eprobabilit y(fewerdots) onthatline.Interms ofthedistributions
overw,modelH3hassmaller evidence because theOccam factorwjD=wis
smaller forH3thanforH2.Thesimplest modelH1hasthesmallest evidence
ofall,because thebesttthatitcanachievetothedataDisverypoor.
Giventhisdataset,themostprobable modelisH2.
Occamfactor forseveralparameters
Iftheposterior iswellapproximated byaGaussian, thentheOccam factor
isobtained fromthedeterminan tofthecorresp onding covariance matrix (c.f.
equation (28.8) andChapter 27):
P(DjHi)'P(DjwMP;Hi)|{z}P(wMPjHi)det 1
2(A=2)|{z };
Evidence'Besttlikelihood Occam factor(28.10)
where A= rr lnP(wjD;Hi),theHessian whichweevaluated when we
calculated theerror barsonwMP(equation 28.5andChapter 27).Asthe
amoun tofdatacollected increases, thisGaussian approximation isexpected
tobecome increasingly accurate.
Insummary ,Bayesianmodelcomparison isasimple extension ofmaxim um
likelihoodmodelselection: theevidenc eisobtaine dbymultiplying thebestt
likeliho odbytheOccamfactor.
Toevaluate theOccam factor weneedonlytheHessian A,iftheGaussian
approximation isgood.ThustheBayesian metho dofmodelcomparison by
evaluating theevidence isnomore demanding computationally thanthetask
ofnding foreachmodelthebesttparameters andtheirerrorbars.
28.2 Example
Let's return totheexample thatopenedthischapter. Arethere oneortwo
boxesbehind thetreeingure 28.1? Whydocoincidences makeussuspicious?
Let's assume theimage ofthearearound thetrunk andboxhasasize
of50pixels, thatthetrunk is10pixels wide, andthat16dieren tcolours of

<<<PAGE 363>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
28.3: Minim umdescription length (MDL) 351
boxescanbedistinguished. ThetheoryH1thatsaysthere isoneboxnear
thetrunk hasfourfreeparameters: three coordinates dening thetopthree
edges ofthebox,andoneparameter giving thebox'scolour. (Ifboxescould
levitate, there wouldbevefreeparameters.)
ThetheoryH2thatsaysthere aretwoboxesnearthetrunk haseightfree
parameters (twicefour), plusaninth,abinary variable thatindicates which
ofthetwoboxesistheclosest totheviewer.1?
or2?
Figure 28.7.Howmanyboxesare
behind thetree?What istheevidence foreachmodel?We'lldoH1rst.Weneedaprioron
theparameters toevaluate theevidence. Forconvenience, let'sworkinpixels.
Let'sassign aseparable priortothehorizon tallocation ofthebox,itswidth,
itsheight,anditscolour. Theheightcould haveanyof,say,20distinguishable
values, socould thewidth, andsocould thelocation. Thecolour could have
anyof16values. We'llputuniform priors overthese variables. We'llignore
alltheparameters associated withother objectsintheimage, sincetheydon't
come intothemodelcomparison betweenH1andH2.Theevidence is
P(DjH1)=1
201
201
201
16(28.11)
sinceonlyonesetting oftheparameters tsthedata, anditpredicts thedata
perfectly .
AsformodelH2,sixofitsnineparameters arewell-determined, andthree
ofthem arepartly-constrained bythedata. Iftheleft-hand boxisfurthest
away,forexample, thenitswidth isatleast8pixels andatmost 30;ifit's
thecloser ofthetwoboxes,thenitswidth isbetween8and18pixels. (I'm
assuming herethatthevisible portion oftheleft-hand boxisabout8pixels
wide.) Togettheevidence weneedtosumuptheprior probabilities ofall
viable hypotheses. Todoanexact calculation, weneedtobemore specic
aboutthedataandthepriors, butlet'sjustgettheballpark answer,assuming
thatthetwounconstrained realvariables havehalftheirvaluesavailable, and
thatthebinary variable iscompletely undetermined. (Asanexercise, youcan
makeanexplicit modelandworkouttheexact answer.)
P(DjH2)'1
201
2010
201
161
201
2010
201
162
2: (28.12)
Thustheposterior probabilit yratiois(assuming equal priorprobabilit y):
P(DjH1)P(H1)
P(DjH2)P(H2)=1
1
2010
2010
201
16(28.13)
=202216'1000=1: (28.14)
Sothedataareroughly 1000to1infavourofthesimpler hypothesis. The
fourfactors canbeinterpreted interms ofOccam factors. Themore complex
modelhasfourextra parameters forsizesandcolours {three forsizes, and
oneforcolour. IthastopaytwobigOccam factors (1/20and1/16)forthe
highly suspicious coincidences thatthetwoboxheightsmatchexactly andthe
twocolours matchexactly; anditalsopaystwolesser Occam factors forthe
twolesser coincidences thatbothboxeshappenedtohaveoneoftheiredges
convenientlyhidden behind atreeorbehind eachother.
28.3 Minim umdescription length (MDL)
Acomplemen taryviewofBayesianmodelcomparison isobtained byreplacing
probabilities ofeventsbythelengths inbitsofmessages thatcomm unicate

<<<PAGE 364>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
352 28|ModelComparison andOccam's Razor
H1:L(H1)L(w
(1)jH1) L(Djw
(1);H1)
H2:L(H2)L(w
(2)jH2) L(Djw
(2);H2)
H3:L(H3)L(w
(3)jH3) L(Djw
(3);H3)Figure 28.8.Apopular viewof
modelcomparison byminim um
description length. Eachmodel
Hicomm unicates thedataDby
sending theidentityofthemodel,
sending thebesttparameters of
themodelw,thensending the
datarelativ etothose parameters.
Asweproceedtomorecomplex
modelsthelength ofthe
parameter message increases. On
theother hand, thelength ofthe
datamessage decreases, because a
complex modelisabletotthe
databetter, making theresiduals
smaller. Inthisexample the
intermediate modelH2achieves
theoptim umtrade-o between
these twotrends.theeventswithout losstoareceiver.Message lengthsL(x)corresp ondtoa
probabilistic modelovereventsxviatherelations:
P(x)=2 L(x);L(x)= log2P(x): (28.15)
TheMDL principle (Wallace andBoulton, 1968) states thatoneshould
prefer modelsthatcancomm unicate thedatainthesmallest numberofbits.
Consider atwo-part message thatstates whichmodel,H,istobeused, and
thencomm unicates thedataDwithin thatmodel,tosome pre-arranged pre-
cisionD.Thisproduces amessage oflengthL(D;H)=L(H)+L(DjH).The
lengthsL(H)fordieren tHdene animplicit priorP(H)overthealternativ e
models.SimilarlyL(DjH)corresp ondstoadensit yP(DjH).Thus,aproce-
dureforassigning message lengths canbemappedontoposterior probabilities:
L(D;H)= logP(H) log(P(DjH)D) (28.16)
= logP(HjD)+const: (28.17)
Inprinciple, then, MDL canalwaysbeinterpreted asBayesian modelcom-
parison andviceversa.However,thissimple discussion hasnotaddressed
howonewouldactually evaluate thekeydata-dep enden ttermL(DjH),which
corresp ondstotheevidence forH.Often, thismessage isimagined asbeing
subdivided intoaparameter blockandadatablock(gure 28.8). Modelswith
asmall numberofparameters haveonlyashort parameter blockbutdonot
tthedatawell,andsothedatamessage (alistoflargeresiduals) islong. As
thenumberofparameters increases, theparameter blocklengthens, andthe
datamessage becomes shorter. There isanoptim ummodelcomplexit y(H2
inthegure) forwhichthesumisminimized.
Thispicture glosses oversome subtle issues. Wehavenotspecied the
precision towhichtheparameters wshould besent.Thisprecision hasan
importanteect (unlik etheprecisionDtowhichreal-valued dataDare
sent,which,assumingDissmall relativ etothenoise level,justintroduces
anadditiv econstan t).Aswedecrease theprecision towhichwissent,the
parameter message shortens, butthedatamessage typically lengthens because
thetruncated parameters donotmatchthedatasowell.There isanon-trivial
optimal precision. Insimple Gaussian cases itispossible tosolveforthis
optimal precision (Wallace andFreeman, 1987), anditisclosely related tothe
posterior errorbarsontheparameters, A 1,where A= rr lnP(wjD;H).
Itturns outthattheoptimal parameter message length isvirtually identicalto
thelogoftheOccam factor inequation (28.10). (Therandom elemen tinvolved
inparameter truncation means thattheencodingisslightlysub-optimal.)
With care, therefore, onecanreplicate Bayesian results inMDL terms.
Although some oftheearliest workoncomplex modelcomparison involved
theMDL framew ork(PatrickandWallace, 1982), MDL hasnoapparen tad-
vantages overthedirect probabilistic approac h.
MDL doeshaveitsusesasapedagogical tool.Thedescription length
concept isuseful formotivating priorprobabilit ydistributions. Also, dieren t
waysofbreaking downthetaskofcomm unicating datausing amodelcangive
helpful insigh tsintothemodelling process,aswillnowbeillustrated.

<<<PAGE 365>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
28.3: Minim umdescription length (MDL) 353
On-line learning andcross-validation.
Incaseswhere thedataconsist ofasequence ofpointsD=t(1);t(2);:::;t(N),
thelogevidence canbedecomp osedasasumof`on-line' predictiv eperfor-
mances:
logP(DjH)=logP(t(1)jH)+logP(t(2)jt(1);H)
+logP(t(3)jt(1);t(2);H)++logP(t(N)jt(1):::t(N 1);H):(28.18)
Thisdecomp osition canbeusedtoexplain thedierence betweentheev-
idence and`leave-one-out cross-v alidation' asmeasures ofpredictiv eabil-
ity.Cross-v alidation examines theaverage valueofjustthelastterm,
logP(t(N)jt(1):::t(N 1);H),under random re-orderings ofthedata. Theevi-
dence, ontheother hand, sumsuphowwellthemodelpredicted allthedata,
starting fromscratc h.
The`bitsback'encodingmethod.
Another MDL though texperimen t(HintonandvanCamp, 1993) involvesin-
corporating random bitsintoourmessage. Thedataarecomm unicated using a
parameter blockandadatablock.Theparameter vectorsentisarandom sam-
plefromtheposterior distribution P(wjD;H)=P(Djw;H)P(wjH)=P(DjH).
Thissample wissenttoanarbitrary small granularitywusing amessage
lengthL(wjH)= log[P(wjH)w].Thedataareencodedrelativ etowwith
amessage oflengthL(Djw;H)= log[P(Djw;H)D].Once thedatames-
sagehasbeenreceived,therandom bitsusedtogenerate thesample wfrom
theposterior canbededuced bythereceiver.Thenumberofbitssorecov-
eredis log[P(wjD;H)w].These recoveredbitsneednotcounttowardsthe
message length, sincewemightusesome other optimally encodedmessage as
arandom bitstring, thereb ycomm unicating thatmessage atthesame time.
Thenetdescription costistherefore:
L(wjH)+L(Djw;H) `Bitsback'= logP(wjH)P(Djw;H)D
P(wjD;H)
= logP(DjH) logD:(28.19)
Thusthisthough texperimen thasyielded theoptimal description length. Bits
backencodinghasbeenturned intoapractical compression metho dfordata
modelled withlatentvariable modelsbyFrey(1998).
Further reading
Bayesianmetho dsareintroduced andcontrasted withsampling-theory statis-
ticsin(Jaynes, 1983; Gull, 1988; Loredo, 1990). TheBayesian Occam's razor
isdemonstrated onmodelproblems in(Gull, 1988; MacKa y,1992a). Useful
textbooksare(BoxandTiao, 1973; Berger, 1985).
Onedebate worthunderstanding isthequestion ofwhether it'spermis-
sibletouseimprop erpriors inBayesian inference (Dawidetal.,1996). If
wewanttodomodelcomparison (asdiscussed inthischapter), itisessen-
tialtouseproperpriors {otherwise theevidences andtheOccam factors are
meaningless. Only when onehasnointentiontodomodelcomparison may
itbesafetouseimprop erpriors, andeveninsuchcases there arepitfalls, as
Dawidetal.explain. Iwouldagree withtheiradvice toalways useproper
priors ,temperedbyanencouragemen ttobesmart when making calculations,
recognizing opportunities forapproximation.

<<<PAGE 366>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
354 28|ModelComparison andOccam's Razor
28.4 Exercises
Exercise 28.1.[3]Random variablesxcome independen tlyfromaprobabilit y
distribution P(x).According tomodelH0,P(x)isauniform distribu-
tion
P(xjH0)=1
2x2( 1;1): (28.20)
According tomodelH1,P(x)isanonuniform distribution withanun-x 1 1P(xjH0)
 1 1xP(xjm= 0:4;H1) knownparameterm2( 1;1):
P(xjm;H1)=1
2(1+mx)x2( 1;1): (28.21)
GiventhedataD=f0:3;0:5;0:7;0:8;0:9g,what istheevidence forH0
andH1?
Exercise 28.2.[3]Datap oints(x;t)arebelievedtocome fromastraigh tline.
Theexperimen terchoosesx,andtisGaussian-distributed about
y=w0+w1x (28.22)
withvariance2
.According tomodelH1,thestraigh tlineishorizon tal, xy=w0+w1x
sow1=0.According tomodelH2,w1isaparameter withpriordistribu-
tionNormal (0;1).Both modelsassign apriordistribution Normal(0;1)
tow0.GiventhedatasetD=f( 8;8);( 2;10);(6;11)g,andassuming
thenoise levelis=1,whatistheevidence foreachmodel?
Exercise 28.3.[3]Asix-sided dieisrolled 30times andthenumbersoftimes
eachfacecame upwereF=f3;3;2;2;9;11g.What istheprobabilit y
thatthedieisaperfectly fairdie(`H0'),assuming thealternativ ehy-
pothesisH1saysthatthediehasabiased distribution p,andtheprior
densit yforpisuniform overthesimplexpi0,P
ipi=1?
Solvethisproblem twoways:exactly ,using thehelpful Dirichletformu-
lae(23.30, 23.31), andapproximately ,using Laplace's metho d.Notice
thatyourchoice ofbasis fortheLaplace approximation isimportant.
SeeMacKa y(1998a) fordiscussion ofthisexercise.
Exercise 28.4.[3]Theinuence ofraceontheimposition ofthedeath penalty
formurder inAmerica hasbeenmuchstudied. Thefollowingthree-w ay
table classies 326cases inwhichthedefendan twasconvicted ofmur-
der.Thethree variables arethedefendan t'srace,thevictim's race,and
whether thedefendan twassentenced todeath. (Data fromM.Radelet,
`Racial characteristics andimposition ofthedeath penalty,'American
SociologicalReview,46(1981), pp.918-927.)
White defendan t Blackdefendan t
Death penalty Death penalty
Yes No Yes No
White victim 19 132 White victim 11 52
Blackvictim 0 9 Blackvictim 6 97
Itseems thatthedeath penaltywasapplied muchmore often when the
victim waswhite thenwhen thevictim wasblack.When thevictim was
white 14%ofdefendan tsgotthedeath penalty,butwhen thevictim was
black6%ofdefendan tsgotthedeath penalty.[Inciden tally,these data

<<<PAGE 367>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
28.4: Exercises 355
provideanexample ofaphenomenon knownasSimpson's parado x:a
higher fraction ofwhite defendan tsaresentenced todeath overall, but
incasesinvolving blackvictims ahigher fraction ofblackdefendan tsare
sentenced todeath andincasesinvolving white victims ahigher fraction
ofblackdefendan tsaresentenced todeath.]vm
dH11
vm
dH11
vm
dH10
vm
dH10
vm
dH01
vm
dH01
vm
dH00
vm
dH00
Figure 28.9.Fourhypotheses
concerning thedependence ofthe
imposition ofthedeath penaltyd
ontheraceofthevictimvand
theraceoftheconvicted murderer
m.H01,forexample, asserts that
theprobabilit yofreceiving the
death penaltydoesdependonthe
murderer's race,butnotonthe
victim's.Quantifytheevidence forthefouralternativ ehypotheses showning-
ure28.9. Ishould mentionthatIdon't believeanyofthese modelsis
adequate: severaladditional variables areimportantinmurder cases,
suchaswhether thevictim andmurderer knew eachother, whether the
murder waspremeditated, andwhether thedefendan thadapriorcrim-
inalrecord; noneofthese variables isincluded inthetable. Sothisis
anacademic exercise inmodelcomparison rather thanaserious study
ofracial biasinthestateofFlorida.
Thehypotheses areshownasgraphical models,witharrowsshowing
dependencies betweenthevariablesv(victim race),m(murderer race),
andd(whether death penaltygiven). ModelH00hasonlyonefree
parameter, theprobabilit yofreceiving thedeath penalty;modelH11has
foursuchparameters, oneforeachstateofthevariablesvandm.Assign
uniform priors tothese variables. Howsensitiv earetheconclusions to
thechoice ofprior?

<<<PAGE 368>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
AboutChapter 29
Thelastcouple ofchapters haveassumed thataGaussian approximation to
theprobabilit ydistribution weareinterested inisadequate. What ifitisnot?
Wehavealready seenanexample {clustering {where thelikelihoodfunction
ismultimo dal,andhasnastyunboundedly highspikesincertain locations in
theparameter space; somaximizing theposterior probabilit yandtting a
Gaussian isnotalwaysgoing towork.Thisdicult ywithLaplace's metho dis
onemotivation forbeinginterested inMonteCarlo metho ds.Infact,Monte
Carlo metho dsprovideageneral-purp osesetoftoolswithapplications in
Bayesian datamodelling andmanyother elds.
Thischapter describ esasequence ofmetho ds:importance sampling ,re-
jection sampling ,theMetrop olismetho d,Gibbs sampling andslicesampling .
Foreachmetho d,wediscuss whether themetho disexpected tobeuseful for
high-dimensional problems suchasariseininference withgraphical models.
[Agraphical modelisaprobabilistic modelinwhichdependencies andinde-
pendencies ofvariables arerepresen tedbyedges inagraph whose nodesare
thevariables.] Along theway,theterminology ofMarkovchainMonteCarlo
metho dsispresen ted.Thesubsequen tchapter givesadiscussion ofadvanced
metho dsforreducing random walkbehaviour.
Fordetails ofMonteCarlo metho ds,theorems andproofsandafulllist
ofreferences, thereader isdirected toNeal(1993b), Gilksetal.(1996), and
Tanner (1996).
Inthischapter Iwillusetheword`sample' inthefollowingsense: asample
fromadistribution P(x)isasingle realization xwhose probabilit ydistribution
isP(x).Thiscontrasts withthealternativ eusage instatistics, where `sample'
refers toacollection ofrealizationsfxg.
When wediscuss transition probabilit ymatrices, Iwillusearight-multipli-
cation convention:Ilikemymatrices toacttotheright,preferring
u=Mv (29.1)
to
uT=vTMT: (29.2)
Atransition probabilit ymatrixTijorTijjspecies theprobabilit y,giventhe
curren tstateisj,ofmaking thetransition fromjtoi.Thecolumns ofTare
probabilit yvectors. Ifwewrite downatransition probabilit ydensit y,weuse
thesame conventionfortheorder ofitsargumen ts:T(x0;x)isatransition
probabilit ydensit yfromxtox0.Thisunfortunately means thatyouhave
togetusedtoreading fromrighttoleft{thesequencexyzhasprobabilit y
T(z;y)T(y;x)(x).
356

<<<PAGE 369>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
29
MonteCarlo Metho ds
29.1 Theproblems tobesolved
MonteCarlo metho dsarecomputational techniques thatmakeuseofrandom
numbers.TheaimsofMonteCarlo metho dsaretosolveoneorbothofthe
followingproblems.
Problem 1:togenerate samplesfx(r)gR
r=1fromagivenprobabilit ydistribu-
tionP(x).
Problem 2:toestimate expectations offunctions under thisdistribution, for
example
=h(x)iZ
dNxP(x)(x): (29.3)
Theprobabilit ydistribution P(x),whichwecallthetarget densit y,might
beadistribution fromstatistical physicsoraconditional distribution arising
indatamodelling {forexample, theposterior probabilit yofamodel'spa-
rameters givensome observ eddata. Wewillgenerally assume thatxisan
N-dimensional vector withrealcomponentsxn,butwewillsometimes con-
siderdiscrete spaces also.
Simple examples offunctions(x)whose expectations wemightbeinter-
ested ininclude therstandsecond momen tsofquantities thatwewishto
predict, fromwhichwecancompute means andvariances; forexample ifsome
quantitytdependsonx,wecanndthemean andvariance oftunderP(x)
bynding theexpectations ofthefunctions1(x)=t(x)and2(x)=(t(x))2,
1E[1(x)]and2E[2(x)]; (29.4)
thenusing
t=1andvar(t)=2 2
1: (29.5)
Itisassumed thatP(x)issucien tlycomplex thatwecannot evaluate these
expectations byexact metho ds;soweareinterested inMonteCarlo metho ds.
Wewillconcen trate ontherstproblem (sampling), because ifwehave
solvedit,thenwecansolvethesecond problem byusing therandom samples
fx(r)gR
r=1togivetheestimator
^1
RX
r(x(r)): (29.6)
Ifthevectorsfx(r)gR
r=1aregenerated fromP(x)thentheexpectation of^is
.Also, asthenumberofsamplesRincreases, thevariance of^willdecrease
as2/R,where2isthevariance of,
2=Z
dNxP(x)((x) )2: (29.7)
357

<<<PAGE 370>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
358 29|MonteCarlo Metho ds
(a)00.511.522.53
-4 -2 0 2 4P*(x)
(b)00.511.522.53
-4 -2 0 2 4P*(x)Figure 29.1.(a)Thefunction
P(x)=
exp
0:4(x 0:4)2 0:08x4
.How
todrawsamples fromthis
densit y?(b)ThefunctionP(x)
evaluated atadiscrete setof
uniformly spaced pointsfxig.
Howtodrawsamples fromthis
discrete distribution?
Thisisoneoftheimportantproperties ofMonteCarlo metho ds.
Theaccuracy oftheMonteCarlo estimate (29.6) dependsonlyon
thevariance of,notonthedimensionalit yofthespace sampled.
Tobeprecise, thevariance of^goesas2=R.Soregardless ofthe
dimensionalit yofx,itmaybethatasfewasadozen independen t
samplesfx(r)gsuce toestimate satisfactorily .
Wewillndlater, however,thathighdimensionalit ycancause other di-
culties forMonteCarlo metho ds.Obtaining independen tsamples fromagiven
distribution P(x)isoften noteasy.
Whyissampling fromP(x)hard?
Wewillassume thatthedensit yfromwhichwewishtodrawsamples,P(x),
canbeevaluated, atleasttowithin amultiplicativ econstan t;thatis,wecan
evaluate afunctionP(x)suchthat
P(x)=P(x)=Z: (29.8)
IfwecanevaluateP(x),whycanwenoteasily solveproblem 1?Whyisitin
general dicult toobtain samples fromP(x)?There aretwodiculties. The
rstisthatwetypically donotknowthenormalizing constan t
Z=Z
dNxP(x): (29.9)
Thesecond isthat, evenifwedidknowZ,theproblem ofdrawingsamples
fromP(x)isstillachallenging one,especially inhigh-dimensional spaces,
because there isnoobvious waytosample fromPwithout enumerating most
orallofthepossible states. Correct samples fromPwillbydenition tend
tocome fromplaces inx-space whereP(x)isbig;howcanweidentifythose
places whereP(x)isbig,without evaluatingP(x)everywher e?There areonly
afewhigh-dimensional densities fromwhichitiseasytodrawsamples, for
example theGaussian distribution.
Letusstartwithasimple one-dimensional example. Imagine thatwewish
todrawsamples fromthedensit yP(x)=P(x)=Zwhere
P(x)=exph
0:4(x 0:4)2 0:08x4i
;x2( 1;1): (29.10)
Wecanplotthisfunction (gure 29.1a). Butthatdoesnotmean wecandraw
samples fromit.Tostartwith, wedon't knowthenormalizing constan tZ.

<<<PAGE 371>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
29.1: Theproblems tobesolved 359
Togiveourselv esasimpler problem, wecould discretize thevariablexand
askforsamples fromthediscrete probabilit ydistribution overanite setof
uniformly spaced pointsfxig(gure 29.1b). Howcould wesolvethisproblem?
Ifweevaluatep
i=P(xi)ateachpointxi,wecancompute
Z=X
ip
i (29.11)
and
pi=p
i=Z (29.12)
andwecanthensample fromtheprobabilit ydistributionfpigusing various
metho dsbased onasource ofrandom bits(seesection 6.3).Butwhat isthe
costofthisprocedure, andhowdoesitscalewiththedimensionalit yofthe
space,N?Letusconcen trateontheinitial costofevaluatingZ(29.11). To
computeZwehavetovisiteverypointinthespace. Ingure 29.1b there are
50uniformly spaced pointsinonedimension. Ifoursystem hadNdimensions,
N=1000say,thenthecorresp onding numberofpointswouldbe501000,an
unimaginable numberofevaluations ofP.Evenifeachcomponentxntook
onlytwodiscrete values, thenumberofevaluations ofPwouldbe21000,a
numberthatisstillhorribly huge.Ifeveryelectron intheuniverse(there are
about2266ofthem) werea1000gigahertz computer thatcould evaluateP
foratrillion (240)states everysecond, andifweranthose 2266computers for
atimeequal totheageoftheuniverse(258seconds), theywouldstillonly
visit2364states. We'dhavetowaitformore than2636'10190universeages
toelapse beforeall21000states hadbeenvisited.
Systems with21000states aretwoapenny.?Oneexample isacollection?Translation forAmerican readers:
`suchsystems areadimeadozen'; in-
cidentally,thisequivalence (10c=6p)
showsthatthecorrect exchange rate
is$1.00=$1.67.of1000spinssuchasa3030fragmen tofanIsingmodelwhose probabilit y
distribution isproportional to
P(x)=exp[ E(x)] (29.13)
wherexn2f1gand
E(x)= "
1
2X
m;nJmnxmxn+X
nHnxn#
: (29.14)
Theenergy functionE(x)isreadily evaluated foranyx.Butifwewishto
evaluate thisfunction atallstates x,thecomputer timerequired wouldbe
21000function evaluations.
TheIsingmodelisasimple modelwhichhasbeenaround foralongtime,
butthetaskofgenerating samples fromthedistribution P(x)=P(x)=Zis
stillanactiveresearc harea; therst`exact' samples fromthisdistribution
werecreated inthepioneering workofPropp andWilson (1996), aswe'll
describ einChapter 32.
Auseful analogyP(x)
Figure 29.2.Alakewhose depth
atx=(x;y)isP(x).
Imagine thetasks ofdrawingrandom watersamples fromalakeandnding
theaverage plankton concen tration (gure 29.2). Thedepth ofthelakeat
x=(x;y)isP(x),andweassert (inorder tomaketheanalogy work)that
theplankton concen tration isafunction ofx,(x).Therequired average
concen tration isanintegral like(29.3), namely
=h(x)i1
ZZ
dNxP(x)(x); (29.15)

<<<PAGE 372>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
360 29|MonteCarlo Metho ds
whereZ=RdxdyP(x)isthevolume ofthelake.Youareprovided with
aboat,asatellite navigation system, andaplumbline. Using thenavigator,
youcancantakeyourboattoanydesired location xonthemap; using the
plumbline youcanmeasureP(x)atthatpoint.Youcanalsomeasure the
plankton concen tration there.
Problem 1istodraw1cm3watersamples atrandom fromthelake,in
suchawaythateachsample isequally likelytocome fromanypointwithin
thelake.Problem 2istondtheaverage plankton concen tration.
These aredicult problems tosolvebecause attheoutset weknownothing
aboutthedepthP(x).Perhaps muchofthevolume ofthelakeiscontainedFigure 29.3.Aslicethrough alake
thatincludes somecanyons.
innarrow,deepunderw atercanyons(gure 29.3), inwhichcase,tocorrectly
sample fromthelakeandcorrectly estimate ourmetho dmustimplicitly
discoverthecanyonsandndtheirvolume relativ etotherestofthelake.
Dicult problems, yes;nevertheless, we'llseethatcleverMonteCarlo metho ds
cansolvethem.
Uniform sampling
Havingaccepted thatwecannot exhaustiv elyvisiteverylocation xinthe
statespace, wemightconsider trying tosolvethesecond problem (estimating
theexpectation ofafunction(x))bydrawing random samplesfx(r)gR
r=1
uniformly fromthestate space andevaluatingP(x)atthose points.Then
wecould introduceanormalizing constan tZR,dened by
ZR=RX
r=1P(x(r)); (29.16)
andestimate =RdNx(x)P(x)by
^=RX
r=1(x(r))P(x(r))
ZR: (29.17)
Isanything wrong withthisstrategy? Well,itdependsonthefunctions(x)
andP(x).Letusassume that(x)isabenign, smoothly varying function
andconcen trate onthenature ofP(x).AswelearntinChapter 4,ahigh-
dimensional distribution isoften concen trated inasmall region ofthestate
space knownasitstypical setT,whose volume isgivenbyjTj'2H(X),where
H(X)istheentropyoftheprobabilit ydistribution P(x).Ifalmost allthe
probabilit ymass islocated inthetypical setand(x)isabenign function,
thevalueof=RdNx(x)P(x)willbeprincipally determined bythevalues
that(x)takesoninthetypical set.Souniform sampling willonlystand
achance ofgiving agoodestimate ofifwemakethenumberofsamples
Rsucien tlylarge thatwearelikelytohitthetypical setatleastonceor
twice. So,howmanysamples arerequired? LetustakethecaseoftheIsing
modelagain. (Strictly ,theIsingmodelmaynotbeagoodexample, sinceit
doesn'tnecessarily haveatypical set,asdened inChapter 4;thedenition
ofatypical setwasthatallstates hadlogprobabilit yclosetotheentropy,
whichforanIsing modelwouldmean thattheenergyisveryclose tothe
meanenergy;butinthevicinit yofphase transitions, thevariance ofenergy ,
alsoknownastheheatcapacit y,maydiverge,whichmeans thattheenergy
ofarandom state isnotnecessarily expected tobeveryclose tothemean
energy .)Thetotalsizeofthestatespace is2Nstates, andthetypical sethas
size2H.Soeachsample hasachance of2H=2Noffalling inthetypical set.
Thenumberofsamples required tohitthetypical setonceisthusoforder
Rmin'2N H: (29.18)

<<<PAGE 373>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
29.2: Importance sampling 361
64 log(2)
0
0123456Entropy
Temperature
(a) (b)Figure 29.4.(a)Entropyofa
64-spin Isingmodelasafunction
oftemperature. (b)Onestateofa
1024-spin Isingmodel.
So,whatisH?Athightemperatures, theprobabilit ydistribution ofanIsing
modeltends toauniform distribution andtheentropytends toHmax=N
bits,whichmeansRminisoforder 1.Under theseconditions, uniform sampling
maywellbeasatisfactory technique forestimating .Buthightemperatures
arenotofgreat interest. Considerably moreinteresting areintermediate tem-
peratures suchasthecritical temperature atwhichtheIsingmodelmelts from
anordered phase toadisordered phase. Thecritical temperature ofaninnite
Isingmodel,atwhichitmelts, isc=2:27.Atthistemperature theentropy
ofanIsingmodelisroughlyN=2bits(gure 29.4). Forthisprobabilit ydis-
tribution thenumberofsamples required simply tohitthetypical setonceis
oforder
Rmin'2N N=2=2N=2; (29.19)
whichforN=1000isabout10150.Thisisroughly thesquare ofthenumber
ofparticles intheuniverse. Thusuniform sampling isutterly useless forthe
study ofIsingmodelsofmodestsize.Andinmosthigh-dimensional problems,
ifthedistribution P(x)isnotactually uniform, uniform sampling isunlikely
tobeuseful.
Overview
Havingestablished thatdrawingsamples fromahigh-dimensional distribution
P(x)=P(x)=Zisdicult evenifP(x)iseasytoevaluate, wewillnow
study asequence ofmore sophisticated MonteCarlo metho ds:importance
sampling ,rejection sampling ,theMetrop olismetho d,Gibbs sampling ,and
slicesampling .
29.2 Importance sampling
Importance sampling isnotametho dforgenerating samples fromP(x)(prob-
lem1);itisjustametho dforestimating theexpectation ofafunction(x)
(problem 2).Itcanbeviewedasageneralization oftheuniform sampling
metho d.
Forillustrativ epurposes,letusimagine thatthetarget distribution isa
one-dimensional densit yP(x).Letusassume thatweareabletoevaluate this
densit yatanychosen pointx,atleasttowithin amultiplicativ econstan t;
thuswecanevaluate afunctionP(x)suchthat
P(x)=P(x)=Z: (29.20)
ButP(x)istoocomplicated afunction forustobeabletosample fromit
directly .Wenowassume thatwehaveasimpler densit yQ(x)fromwhichwe
cangenerate samples andwhichwecanevaluate towithin amultiplicativ e

<<<PAGE 374>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
362 29|MonteCarlo Metho ds
constan t(that is,wecanevaluateQ(x),whereQ(x)=Q(x)=ZQ).An
example ofthefunctionsP,Qandisshowningure 29.5. WecallQthe
xP(x)Q(x)(x)
Figure 29.5.Functions involvedin
importance sampling. Wewishto
estimate theexpectation of(x)
underP(x)/P(x).Wecan
generate samples fromthesimpler
distribution Q(x)/Q(x).We
canevaluateQandPatany
point.sampler densit y.
Inimportance sampling, wegenerateRsamplesfx(r)gR
r=1fromQ(x).If
these pointsweresamples fromP(x)thenwecould estimate byequa-
tion(29.6). Butwhen wegenerate samples fromQ,valuesofxwhereQ(x)is
greater thanP(x)willbeover-representedinthisestimator, andpointswhere
Q(x)islessthanP(x)willbeunder-r epresented.Totakeintoaccoun tthe
factthatwehavesampled fromthewrong distribution, weintroduceweights
wrP(x(r))
Q(x(r))(29.21)
whichweusetoadjust the`importance' ofeachpointinourestimator thus:
^P
rwr(x(r))P
rwr: (29.22)
.Exercise 29.1.[2,p.384]Provethat,ifQ(x)isnon-zero forallxwhereP(x)is
non-zero, theestimator ^converges to,themean valueof(x),asR
increases. What isthevariance ofthisestimator, asymptotically? Hint:
consider thestatistics ofthenumerator andthedenominator separately .
Istheestimator ^anunbiased estimator forsmallR?
Apractical dicult ywithimportance sampling isthatitishardtoestimate
howreliable theestimator ^is.Thevariance oftheestimator isunkno wn
beforehand, because itdependsonanintegral overxofafunction involving
P(x).Andthevariance of^ishardtoestimate, because theempirical
variances ofthequantitieswrandwr(x(r))arenotnecessarily agoodguide
tothetruevariances ofthenumerator anddenominator inequation (29.22).
Iftheproposaldensit yQ(x)issmall inaregion wherej(x)P(x)jislarge
thenitisquitepossible, evenaftermanypointsx(r)havebeengenerated, that
noneofthem willhavefallen inthatregion. Inthiscasetheestimate of
wouldbedrastically wrong, andthere wouldbenoindication intheempiric al
variance thatthetruevariance oftheestimator ^islarge.
(a)-7.2-7-6.8-6.6-6.4-6.2
10 100 1000 10000 100000 1000000
(b)-7.2-7-6.8-6.6-6.4-6.2
10 100 1000 10000 100000 1000000Figure 29.6.Importance sampling
inaction: (a)using aGaussian
sampler densit y;(b)using a
Cauchysampler densit y.Vertical
axisshowstheestimate ^.The
horizon tallineindicates thetrue
valueof.Horizon talaxisshows
numberofsamples onalogscale.
Cautionary illustrationofimportanc esampling
Inatoyproblem related tothemodelling ofamino acidprobabilit ydistribu-
tionswithaone-dimensional variablex,Ievaluated aquantityofinterest us-
ingimportance sampling. Theresults using aGaussian sampler andaCauchy
sampler areshowningure 29.6. Thehorizon talaxisshowsthenumberof

<<<PAGE 375>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
29.2: Importance sampling 363
samples onalogscale. InthecaseoftheGaussian sampler, afterabout500
samples hadbeenevaluated onemightbetempted tocallahalt;buteviden tly
thereareinfrequen tsamples thatmakeahugecontribution to^,andthevalue
oftheestimate at500samples iswrong. Evenafteramillion samples have
beentaken,theestimate hasstillnotsettled downclosetothetruevalue. In
contrast, theCauchysampler doesnotsuer fromglitches;itconverges (on
thescaleshownhere) afterabout5000samples.
Thisexample illustrates thefactthatanimportance sampler should have
heavytails.
Exercise 29.2.[2,p.385]Consider thesituation whereP(x)ismultimo dal,con-
sisting ofseveralwidely separated peaks. (Probabilit ydistributions like
thisarisefrequen tlyinstatistical datamodelling.) Discuss whether itis
awisestrategy todoimportance sampling using asamplerQ(x)that
isaunimo daldistribution tted tooneofthese peaks. Assume that-5 0 510 15P(x)
Q(x)
phi(x)
Figure 29.7.Amultimo dal
distribution P(x)andaunimo dal
samplerQ(x).
thefunction(x)whose mean istobeestimated isasmoothly vary-
ingfunction ofxsuchasmx+c.Describ ethetypical evolution ofthe
estimator ^asafunction ofthenumberofsamplesR.
Importanc esampling inmany dimensions
Wehavealready observ edthatcareisneeded inone-dimensional importance
sampling problems. Isimportance sampling auseful technique inspaces of
higher dimensionalit y,sayN=1000?
Consider asimple case-study where thetarget densit yP(x)isauniform
distribution inside asphere,
P(x)=(
10(x)RP
0(x)>RP;(29.23)
where(x)(P
ix2
i)1=2,andtheproposaldensit yisaGaussian centredon
theorigin,
Q(x)=Y
iNormal (xi;0;2): (29.24)
Animportance sampling metho dwillbeintrouble iftheestimator ^isdom-
inated byafewlargeweightswr.What willbethetypical range ofvalues of
theweightswr?Weknowfromourdiscussions oftypical sequences inPartI{
seeexercise 6.14(p.124),forexample {thatifisthedistance fromtheorigin
ofasample fromQ,thequantity2hasaroughly Gaussian distribution with
mean andstandard deviation:
2N2p
2N2: (29.25)
Thusalmost allsamples fromQlieinatypical setwithdistance fromtheorigin
veryclosetop
N.Letusassume thatischosen suchthatthetypical set
ofQliesinside thesphere ofradiusRP.[Ifitdoesnot,thenthelawoflarge
numbersimplies thatalmost allthesamples generated fromQwillfalloutside
RPandwillhaveweightzero.] Then weknowthatmostsamples fromQwill
haveavalueofQthatliesintherange
1
(22)N=2exp 
 N
2p
2N
2!
: (29.26)
Thustheweightswr=P=Qwilltypically havevalues intherange
(22)N=2exp 
N
2p
2N
2!
: (29.27)

<<<PAGE 376>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
364 29|MonteCarlo Metho ds
(a)
xP(x)cQ(x)(b)
xu
xP(x)cQ(x)Figure 29.8.Rejection sampling.
(a)Thefunctions involvedin
rejection sampling. Wedesire
samples fromP(x)/P(x).We
areabletodrawsamples from
Q(x)/Q(x),andweknowa
valuecsuchthatcQ(x)>P(x)
forallx.(b)Apoint(x;u)is
generated atrandom inthelightly
shaded areaunder thecurve
cQ(x).Ifthispointalsolies
belowP(x)thenitisaccepted.Soifwedrawahundred samples, what willthetypical range ofweightsbe?
Wecanroughly estimate theratioofthelargest weighttothemedian weight
bydoubling thestandard deviation inequation (29.27). Thelargest weight
andthemedian weightwilltypically beintheratio:
wmax
r
wmedr=expp
2N
: (29.28)
InN=1000dimensions therefore, thelargest weightafteronehundred sam-
plesislikelytoberoughly 1019times greater thanthemedian weight.Thusan
importance sampling estimate forahigh-dimensional problem willverylikely
beutterly dominated byafewsamples withhugeweights.
Inconclusion, importance sampling inhighdimensions often suers from
twodiculties. First, weneedtoobtain samples thatlieinthetypical setofP,
andthismaytakealongtimeunlessQisagoodapproximation toP.Second,
evenifweobtain samples inthetypical set,theweightsassociated withthose
samples arelikelytovarybylargefactors, because theprobabilities ofpoints
inatypical set,although similar toeachother, stilldier byfactors oforder
exp(p
N),sotheweightswilltoo,unlessQisanear-p erfect approximation to
P.
29.3 Rejection sampling
Weassume again aone-dimensional densit yP(x)=P(x)=Zthatistoocom-
plicated afunction forustobeabletosample fromitdirectly .Weassume
thatwehaveasimpler proposaldensityQ(x)whichwecanevaluate (within a
multiplicativ efactorZQ,asbefore), andfromwhichwecangenerate samples.
Wefurther assume thatweknowthevalueofaconstan tcsuchthat
cQ(x)>P(x);forallx: (29.29)
Aschematic picture ofthetwofunctions isshowningure 29.8a.
Wegenerate tworandom numbers.Therst,x,isgenerated fromthe
proposaldensit yQ(x).WethenevaluatecQ(x)andgenerate auniformly
distributed random variableufromtheinterval[0;cQ(x)].These tworandom
numberscanbeviewedasselecting apointinthetwo-dimensional plane as
showningure 29.8b.
WenowevaluateP(x)andaccept orreject thesamplexbycomparing the
valueofuwiththevalueofP(x).Ifu>P(x)thenxisrejected; otherwise
itisaccepted, whichmeans thatweaddxtooursetofsamplesfx(r)g.The
valueofuisdiscarded.
Whydoesthisprocedure generate samples fromP(x)?Theproposedpoint
(x;u)comes withuniform probabilit yfromthelightlyshaded areaunderneath
thecurvecQ(x)asshowningure 29.8b. Therejection rulerejects allthe
pointsthatlieabovethecurveP(x).Sothepoints(x;u)thatareaccepted
areuniformly distributed intheheavilyshaded areaunderP(x).Thisimplies

<<<PAGE 377>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
29.4: TheMetrop olis{Hastings metho d 365
thattheprobabilit ydensit yofthex-coordinates oftheaccepted pointsmust
beproportional toP(x),sothesamples mustbeindependen tsamples from
P(x).
Rejection sampling willworkbestifQisagoodapproximation toP.IfQ
isverydieren tfromPthen,forcQtoexceedPeverywhere,cwillnecessarily
havetobelargeandthefrequency ofrejection willbelarge.
-4-3-2-101234P(x)
cQ(x)
Figure 29.9.AGaussianP(x)and
aslightlybroader GaussianQ(x)
scaled upbyafactorcsuchthat
cQ(x)P(x).Rejectionsampling inmany dimensions
Inahigh-dimensional problem itisverylikelythattherequiremen tthatcQ
beanupperboundforPwillforcectobesohugethatacceptances willbe
veryrareindeed. Finding suchavalueofcmaybedicult too,sinceinmany
problems weknowneither where themodesofParelocated norhowhigh
theyare.
Asacasestudy,consider apairofN-dimensional Gaussian distributions
withmean zero(gure 29.9). Imagine generating samples fromonewithstan-
darddeviationQandusing rejection sampling toobtain samples fromthe
other whose standard deviation isP.Letusassume thatthese twostandard
deviations arecloseinvalue{say,Qis1%larger thanP.[Qmustbelarger
thanPbecause ifthisisnotthecase,there isnocsuchthatcQexceedsP
forallx.]So,what valueofcisrequired ifthedimensionalit yisN=1000?
Thedensit yofQ(x)attheorigin is1=(22
Q)N=2,soforcQtoexceedPwe
needtoset
c=(22
Q)N=2
(22
P)N=2=exp
NlnQ
P
: (29.30)
WithN=1000andQ
P=1:01,wendc=exp(10)'20,000. What willthe
acceptance ratebeforthisvalueofc?Theanswerisimmediate: since the
acceptance rateistheratioofthevolume under thecurveP(x)tothevolume
undercQ(x),thefactthatPandQarebothnormalized hereimplies that
theacceptance ratewillbe1=c,forexample, 1/20,000. Ingeneral,cgrows
exponentially withthedimensionalit yN,sotheacceptance rateisexpected
tobeexponentially small inN.
Rejection sampling, therefore, whilst auseful metho dforone-dimensional
problems, isnotexpected tobeapractical technique forgenerating samples
fromhigh-dimensional distributions P(x).
29.4 TheMetrop olis{Hastings metho d
Importance sampling andrejection sampling workwellonlyiftheproposal
densit yQ(x)issimilar toP(x).Inlargeandcomplex problems itisdicult
tocreate asingle densit yQ(x)thathasthisproperty.xx(1)Q(x;x(1))
P(x)
xx(2)Q(x;x(2))
P(x)
Figure 29.10.Metrop olis{Hastings
metho dinonedimension. The
proposaldistribution Q(x0;x)is
hereshownashavingashapethat
changes asxchanges, though this
isnottypical oftheproposal
densities usedinpractice.TheMetrop olis{Hastings algorithm instead makesuseofaproposalden-
sityQwhich dependsonthecurrentstatex(t).Thedensit yQ(x0;x(t))might
beasimple distribution suchasaGaussian centredonthecurren tx(t).The
proposaldensit yQ(x0;x)canbeanyxeddensit yfromwhichwecandraw
samples. Incontrasttoimportance sampling andrejection sampling, itisnot
necessary thatQ(x0;x(t))lookatallsimilar toP(x)inorder forthealgorithm
tobepractically useful. Anexample ofaproposaldensit yisshowning-
ure29.10; thisgure showsthedensit yQ(x0;x(t))fortwodieren tstatesx(1)
andx(2).
Asbefore, weassume thatwecanevaluateP(x)foranyx.Atentative
newstatex0isgenerated fromtheproposaldensit yQ(x0;x(t)).Todecide

<<<PAGE 378>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
366 29|MonteCarlo Metho ds
whether toaccept thenewstate, wecompute thequantity
a=P(x0)
P(x(t))Q(x(t);x0)
Q(x0;x(t)): (29.31)
Ifa1thenthenewstateisaccepted.
Otherwise ,thenewstateisaccepted withprobabilit ya.
Ifthestepisaccepted, wesetx(t+1)=x0.
Ifthestepisrejected, thenwesetx(t+1)=x(t).
Note thedierence fromrejection sampling: inrejection sampling, rejected
pointsarediscarded andhavenoinuence onthelistofsamplesfx(r)gthat
wecollected. Here, arejection causes thecurren tstate tobewritten again
ontothelist.
Notation. Ihaveusedthesuperscriptr=1;:::;Rtolabelpointsthat
areindependent samples fromadistribution, andthesuperscriptt=1;:::;T
tolabelthesequence ofstates inaMarkovchain. Itisimportanttonotethat
aMetrop olis{Hastings simulation ofTiterations doesnotproduceTindepen-
dentsamples fromthetarget distribution P.Thesamples arecorrelated.
Tocompute theacceptance probabilit y(29.31) weneedtobeabletocom-
putetheprobabilit yratiosP(x0)=P(x(t))andQ(x(t);x0)=Q(x0;x(t)).Ifthe
proposaldensit yisasimple symmetrical densit ysuchasaGaussian centredon
thecurren tpoint,thenthelatter factor isunity,andtheMetrop olis{Hastings
metho dsimply involvescomparing thevalueofthetarget densit yatthetwo
points.Thisspecialcaseissometimes called theMetrop olismetho d.However,
withapologies toHastings, Iwillcallthegeneral Metrop olis{Hastings algo-
rithm forasymmetric Q`theMetrop olisalgorithm' sinceIbelieveimportant
ideas deserv eshort names.
Conver genceoftheMetropolismethodtothetargetdensity
ItcanbeshownthatforanypositiveQ(that is,anyQsuchthatQ(x0;x)>0
forallx;x0),ast!1,theprobabilit ydistribution ofx(t)tends toP(x)=
P(x)=Z.[This statemen tshould notbeseenasimplying thatQhastoassign
positiveprobabilit ytoeverypointx0{wewilldiscuss examples laterwhere
Q(x0;x)=0forsomex;x0;notice alsothatwehavesaidnothing abouthow
rapidly theconvergence toP(x)takesplace.]
TheMetrop olismetho disanexample ofaMarkovchainMonteCarlo
metho d(abbreviated MCMC). Incontrasttorejection sampling, where the
accepted pointsfx(r)gareindependent samples fromthedesired distribution,
MarkovchainMonteCarlo metho dsinvolveaMarkovprocessinwhichase-
quence ofstatesfx(t)gisgenerated, eachsamplex(t)havingaprobabilit y
distribution thatdependsontheprevious value,x(t 1).Since successiv esam-
plesarecorrelated witheachother, theMarkovchainmayhavetoberunfora
considerable timeinorder togenerate samples thatareeectiv elyindependen t
samples fromP.
Justasitwasdicult toestimate thevariance ofanimportance sampling
estimator, soitisdicult toassess whether aMarkovchainMonteCarlo
metho dhas`converged', andtoquantifyhowlongonehastowaittoobtain
samples thatareeectiv elyindependen tsamples fromP.
Demonstr ationoftheMetropolismethod
TheMetrop olismetho diswidely usedforhigh-dimensional problems. Many
implemen tations oftheMetrop olismetho demplo yaproposaldistribution

<<<PAGE 379>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
29.4: TheMetrop olis{Hastings metho d 367
x(1)
Q(x;x(1))P(x)
LFigure 29.11.Metrop olismetho d
intwodimensions, showinga
traditional proposaldensit ythat
hasasucien tlysmall stepsize
thattheacceptance frequency will
beabout0.5.
withalength scalethatisshort relativ etothelongest length scaleLofthe
probable region (gure 29.11). Areason forchoosing asmall length scaleis
thatformosthigh-dimensional problems, alargerandom stepfromatypical
point(that is,asample fromP(x))isverylikelytoendinastate thathas
verylowprobabilit y;suchsteps areunlikelytobeaccepted. Ifislarge,
movementaround thestatespace willonlyoccurwhen suchatransition toa
low-probabilit ystateisactually accepted, orwhen alargerandom stepchances
tolandinanother probable state. Sotherateofprogress willbeslowiflarge
steps areused.
Thedisadv antageofsmall steps, ontheother hand, isthattheMetrop olis
metho dwillexplore theprobabilit ydistribution byarandom walk,anda
random walktakesalongtimetogetanywhere, especially ifthewalkismade
ofsmall steps.
Exercise 29.3.[1]Consider aone-dimensional random walk,oneachstepof
whichthestate movesrandomly totheleftortotherightwithequal
probabilit y.ShowthatafterTsteps ofsize,thestateislikelytohave
movedonlyadistance aboutp
T.(Compute therootmean square
distance travelled.)
Recall thattherstaimofMonteCarlo sampling istogenerate anumberof
independent samples fromthegivendistribution (adozen, say).Ifthelargest
length scaleofthestatespace isL,thenwehavetosimulate arandom-w alk
Metrop olismetho dforatimeT'(L=)2beforewecanexpecttogetasample
thatisroughly independen toftheinitial condition {andthat's assuming that
everystepisaccepted: ifonlyafractionfofthestepsareaccepted onaverage,
thenthistimeisincreased byafactor 1=f.
Ruleofthumb:lowerbound onnumberofiterations ofa
Metrop olismetho d.Ifthelargest length scaleofthespace of
probable states isL,aMetrop olismetho dwhose proposaldistribu-
tiongenerates arandom walkwithstepsizemustberunforat
least
T'(L=)2(29.32)
iterations toobtain anindependen tsample.
Thisruleofthumbgivesonlyalowerbound; thesituation maybemuch
worse,if,forexample, theprobabilit ydistribution consists ofseveralislands
ofhighprobabilit yseparated byregions oflowprobabilit y.

<<<PAGE 380>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
368 29|MonteCarlo Metho ds
(a)
(b)Metrop olis
024681012
0 5 10 15 20100 iterations
0510152025303540
0 5 10 15 20400 iterations
0102030405060708090
0 5 10 15 201200 iterations(c)Independen tsampling
024681012
0 5 10 15 20100 iterations
0510152025303540
0 5 10 15 20400 iterations
0102030405060708090
0 5 10 15 201200 iterationsFigure 29.12.Metrop olismetho d
foratoyproblem. (a)Thestate
sequence fort=1;:::;600.
Horizon taldirection =states from
0to20;vertical direction =time
from1to600;thecrossbarsmark
timeintervalsofduration 50.(b)
Histogram ofoccupancy ofthe
states after100,400and1200
iterations. (c)Forcomparison,
histograms resulting when
successiv epointsaredrawn
independently fromthetarget
distribution.

<<<PAGE 381>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
29.4: TheMetrop olis{Hastings metho d 369
Toillustrate howslowlyarandom walkexplores astatespace, gure 29.12
showsasimulation ofaMetrop olisalgorithm forgenerating samples fromthe
distribution:
P(x)=(
1/21x2f0;1;2;:::;20g
0otherwise.(29.33)
Theproposaldistribution is
Q(x0;x)=(
1/2x0=x1
0otherwise.(29.34)
Because thetarget distribution P(x)isuniform, rejections occuronlywhen
theproposaltakesthestatetox0= 1orx0=21.
Thesimulation wasstarted inthestatex0=10anditsevolution isshown
ingure 29.12a. Howlongdoesittaketoreachoneoftheendstatesx=0
andx=20?Since thedistance is10steps, theruleofthumb(29.32) predicts
thatitwilltypically takeatimeT'100iterations toreachanendstate. This
isconrmed inthepresen texample: therststepintoanendstateoccurs on
the178th iteration. Howlongdoesittaketovisitbothendstates? Therule
ofthumbpredicts about400iterations arerequired totraversethewhole state
space; andindeed therstencoun terwiththeother endstatetakesplace on
the540th iteration. Thuseectiv ely-indep enden tsamples areonlygenerated
bysimulating foraboutfourhundred iterations perindependen tsample.
Thissimple example showsthatitisimportanttotrytoabolishrandom
walkbehaviour inMonteCarlo metho ds.Asystematic exploration ofthetoy
statespacef0;1;2;:::;20gcould getaround it,using thesame stepsizes, in
abouttwentysteps instead offourhundred. Metho dsforreducing random
walkbehaviour arediscussed inthenextchapter.
Metropolismethodinhighdimensions
Theruleofthumb(29.32), whichgivesalowerboundonthenumberofitera-
tionsofarandom walkMetrop olismetho d,alsoapplies tohigher dimensional
problems. Consider thesimple caseofatarget distribution thatisanN-
dimensional Gaussian, andaproposaldistribution thatisaspherical Gaussian
ofstandard deviationineachdirection. Without lossofgeneralit y,wecan
assume thatthetarget distribution isaseparable distribution aligned withthe
axesfxng,andthatithasstandard deviationnindirectionn.Letmaxand
minbethelargest andsmallest ofthese standard deviations. Letusassume
thatisadjusted suchthattheacceptance frequency iscloseto1.Under this
assumption, eachvariablexnevolvesindependen tlyofalltheothers, executing
arandom walkwithstepsizeabout.Thetimetakentogenerate eectiv ely
independen tsamples fromthetarget distribution willbecontrolled bythe
largest lengthscale max.Justasintheprevious section, where weneeded at
leastT'(L=)2iterations toobtain anindependen tsample, hereweneed
T'(max=)2.
Now,howbigcanbe?Thebigger itis,thesmaller thisnumberTbe-
comes, butifistoobig{bigger thanmin{thentheacceptance ratewill
fallsharply .Itseems plausible thattheoptimalmustbesimilar tomin.
Strictly ,thismaynotbetrue;inspecialcases where thesecond smallestn
issignican tlygreater thanmin,theoptimalmaybecloser tothatsecond
smallestn.Butourrough conclusion isthis: where simple spherical pro-
posaldistributions areused, wewillneedatleastT'(max=min)2iterations
toobtain anindependen tsample, wheremaxandminarethelongest and
shortest lengthscales ofthetarget distribution.

<<<PAGE 382>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
370 29|MonteCarlo Metho ds
(a)x1x2
P(x)
(b)x1x2
P(x1jx(t)
2)
x(t)
(c)x1x2
P(x2jx1)
(d)x1x2
x(t)x(t+1)x(t+2)Figure 29.13.Gibbs sampling.
(a)Thejointdensit yP(x)from
whichsamples arerequired. (b)
Starting fromastatex(t),x1is
sampled fromtheconditional
densit yP(x1jx(t)
2).(c)Asample
isthenmade fromtheconditional
densit yP(x2jx1).(d)Acouple of
iterations ofGibbs sampling.
Thisisgoodnews andbadnews. Itisgoodnews because, unlikethe
casesofrejection sampling andimportance sampling, there isnocatastrophic
dependence onthedimensionalit yN.Ourcomputer willgiveuseful answers
inatimeshorter thantheageoftheuniverse.Butitisbadnewsallthesame,
because thisquadratic dependence onthelengthscale-ratio maystillforceus
tomakeverylength ysimulations.
Fortunately ,there aremetho dsforsuppressing random walksinMonte
Carlo simulations, whichwewilldiscuss inthenextchapter.
29.5 Gibbs sampling
Weintroduced importance sampling, rejection sampling andtheMetrop olis
metho dusing one-dimensional examples. Gibbs sampling, alsoknownasthe
heatbathmethodor`Glaub erdynamics', isametho dforsampling fromdis-
tributions overatleasttwodimensions. Gibbs sampling canbeviewedasa
Metrop olismetho dinwhichasequence ofproposaldistributions Qaredened
interms oftheconditional distributions ofthejointdistribution P(x).Itis
assumed that, whilstP(x)istoocomplex todrawsamples fromdirectly ,its
conditional distributions P(xijfxjgj6=i)aretractable toworkwith. Formany
graphical models(butnotall)these one-dimensional conditional distributions
arestraigh tforwardtosample from. Forexample, ifaGaussian distribution
forsomevariables dhasanunkno wnmeanm,andthepriordistribution ofm
isGaussian, thentheconditional distribution ofmgivendisalsoGaussian.
Conditional distributions thatarenotofstandard formmaystillbesampled
frombyadaptiv erejection sampling iftheconditional distribution satises
certain convexityproperties (Gilks andWild, 1992).
Gibbs sampling isillustrated foracasewithtwovariables (x1;x2)=xin
gure 29.13. Oneachiteration, westartfromthecurren tstatex(t),andx1is
sampled fromtheconditional densit yP(x1jx2),withx2xedtox(t)
2.Asample
x2isthenmade fromtheconditional densit yP(x2jx1),using thenewvalueof

<<<PAGE 383>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
29.5: Gibbs sampling 371
x1.Thisbrings ustothenewstatex(t+1),andcompletes theiteration.
Inthegeneral caseofasystem withKvariables, asingle iteration involves
sampling oneparameter atatime:
x(t+1)
1P(x1jx(t)
2;x(t)
3;:::;x(t)
K) (29.35)
x(t+1)
2P(x2jx(t+1)
1;x(t)
3;:::;x(t)
K) (29.36)
x(t+1)
3P(x3jx(t+1)
1;x(t+1)
2;:::;x(t)
K);etc. (29.37)
Conver genceofGibbs sampling tothetargetdensity
.Exercise 29.4.[2]Showthatasingle variable-up dateofGibbs sampling can
beviewedasaMetrop olismetho dwithtarget densit yP(x),andthat
thisMetrop olismetho dhasthepropertythateveryproposalisalways
accepted.
Because Gibbs sampling isaMetrop olismetho d,theprobabilit ydistribution
ofx(t)tends toP(x)ast!1,aslongasP(x)doesnothavepathological
properties.
.Exercise 29.5.[2,p.385]Discuss whether thesyndrome decodingproblem fora
(7;4)Hamming codecanbesolvedusing Gibbs sampling. Thesyndrome
decodingproblem, ifwearetosolveitwithaMonteCarlo approac h,
istodrawsamples fromtheposterior distribution ofthenoise vector
n=(n1;:::;nn;:::;nN),
P(njf;z)=1
ZNY
n=1fnnn(1 fn)(1 nn)
 [Hn=z]; (29.38)
wherefnisthenormalized likelihoodforthenthtransmitted bitandz
istheobserv edsyndrome. Thefactor
 [Hn=z]is1ifnhasthecorrect
syndrome zand0otherwise.
What aboutthesyndrome decodingproblem foranylinear error-correcting
code?
Gibbs sampling inhighdimensions
Gibbs sampling suers fromthesame defect assimple Metrop olisalgorithms
{thestate space isexplored byaslowrandom walk,unless afortuitous pa-
rameterization hasbeenchosen thatmakestheprobabilit ydistribution P(x)
separable. If,say,twovariablesx1andx2arestrongly correlated, having
marginal densities ofwidthLandconditional densities ofwidth,thenitwill
takeatleastabout(L=)2iterations togenerate anindependen tsample from
thetarget densit y.Figure 30.3,p.390,illustrates theslowprogress made by
Gibbs sampling whenL.
HoweverGibbs sampling involvesnoadjustable parameters, soitisanat-
tractiv estrategy when onewantstogetamodelrunning quickly.Anexcellen t
softwarepackage,BUGS,makesiteasytosetupalmost arbitrary probabilistic
modelsandsimulate them byGibbs sampling (Thomas etal.,1992).1
1http://www.mrc-bsu.cam.ac.uk/b ugs/

<<<PAGE 384>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
372 29|MonteCarlo Metho ds
29.6 Terminology forMarkovchainMonteCarlo metho ds
Wenowspendafewmomen tssketchingthetheory onwhichtheMetrop olis
metho dandGibbs sampling arebased. Wedenote byp(t)(x)theprobabil-
itydistribution ofthestate ofaMarkovchainsimulator. (Tovisualize this
distribution, imagine running aninnite collection ofidenticalsimulators in
parallel.) OuraimistondaMarkovchainsuchthatast!1,p(t)(x)tends
tothedesired distribution P(x).
AMarkovchaincanbespecied byaninitial probabilit ydistribution
p(0)(x)andatransition probabilit yT(x0;x).
Theprobabilit ydistribution ofthestate atthe(t+1)thiteration ofthe
Markovchain,p(t+1)(x),isgivenby
p(t+1)(x0)=Z
dNxT(x0;x)p(t)(x): (29.39)
Example 29.6. Anexample ofaMarkovchainisgivenbytheMetrop olis
demonstration ofsection 29.4(gure 29.12), forwhichthetransition proba-
bilityis
T=2
6666666666666666666666666641/21/21/21/2
1/21/2
1/21/2
1/21/2
1/21/2
1/21/2
1/21/2
1/21/2
1/21/2
1/21/2
1/21/2
1/21/2
1/21/2
1/21/2
1/21/2
1/21/2
1/21/2
1/21/2
1/21/2
1/21/23
777777777777777777777777775
andtheinitial distribution was
p(0)(x)=h
1i
: (29.40)
Theprobabilit ydistribution p(t)(x)ofthestate atthetthiteration isshown
fort=0;1,2,3,5,10,100,200,400ingure 29.14; anequivalentsequence of
distributions isshowningure 29.15 forthechainthatbegins ininitial state
x0=17.Both chains convergetothetarget densit y,theuniform densit y,as
t!1.p(0)(x)
05101520
p(1)(x)
05101520
p(2)(x)
05101520
p(3)(x)
05101520
p(10)(x)
05101520
p(100)(x)
05101520
p(200)(x)
05101520
p(400)(x)
05101520
Figure 29.14.Theprobabilit y
distribution ofthestateofthe
Markovchainofexample 29.6.
Requiredproperties
When designing aMarkovchainMonteCarlo metho d,weconstruct achain
withthefollowingproperties:
1.Thedesired distribution P(x)isaninvariantdistribution ofthechain.
Adistribution (x)isaninvariantdistribution ofthetransition proba-
bilityT(x0;x)if
(x0)=Z
dNxT(x0;x)(x): (29.41)
Aninvariantdistribution isaneigenvector ofthetransition probabilit y
matrix thathaseigenvalue1.

<<<PAGE 385>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
29.6: Terminology forMarkovchainMonteCarlo metho ds 373
2.Thechainmustalsobeergodic,thatis,
p(t)(x)!(x)ast!1,foranyp(0)(x). (29.42)
Acouple ofreasons whyachainmightnotbeergodicare:
(a)Itsmatrix mightbereducible ,whichmeans thatthestate space
contains twoormore subsets ofstates thatcanneverbereached
fromeachother. Suchachainhasmanyinvariantdistributions;
whichonep(t)(x)wouldtendtoast!1woulddependonthe
initial conditionp(0)(x).p(0)(x)
05101520
p(1)(x)
05101520
p(2)(x)
05101520
p(3)(x)
05101520
p(10)(x)
05101520
p(100)(x)
05101520
p(200)(x)
05101520
p(400)(x)
05101520
Figure 29.15.Theprobabilit y
distribution ofthestateofthe
Markovchainforinitial condition
x0=17(example 29.6(p.372)).Thetransition probabilit ymatrix ofsuchachainhasmore than
oneeigenvalueequal to1.
(b)Thechainmighthaveaperiodicset,whichmeans that, forsome
initial conditions, p(t)(x)doesn'ttendtoaninvariantdistribution,
butinstead tends toaperiodiclimit-cycle.
Asimple Markovchainwiththispropertyistherandom walkonthe
N-dimensional hypercube.ThechainTtakesthestate fromone
corner toarandomly chosen adjacen tcorner. Theunique invariant
distribution ofthischainistheuniform distribution overall2N
states, butthechainisnotergodic;itisperiodicwithperiodtwo:
ifwedivide thestates intostates withoddparityandstates with
evenparity,wenotice thateveryoddstate issurrounded byeven
states andviceversa.Soiftheinitial condition attimet=0isa
state withevenparity,thenattimet=1{andatalloddtimes
{thestate musthaveoddparity,andatalleventimes, thestate
willbeofevenparity.
Thetransition probabilit ymatrix ofsuchachainhasmore than
oneeigenvaluewithmagnitude equal to1.Therandom walkon
thehypercube,forexample, haseigenvalues equal to+1and 1.
Methodsofconstruction ofMarkov chains
Itisoften convenienttoconstructTbymixing orconcatenating simple base
transitions Ballofwhichsatisfy
P(x0)=Z
dNxB(x0;x)P(x); (29.43)
forthedesired densit yP(x),i.e.,theyallhavethedesired densit yasan
invariantdistribution. These basetransitions neednotindividually beergodic.
Tisamixture ofseveralbasetransitionsBb(x0;x)ifwemakethetransition
bypickingoneofthebasetransitions atrandom, andallowingittodetermine
thetransition, i.e.,
T(x0;x)=X
bpbBb(x0;x); (29.44)
wherefpbgisaprobabilit ydistribution overthebasetransitions.
Tisaconcatenation oftwobasetransitions B1(x0;x)andB2(x0;x)ifwe
rstmakeatransition toanintermediate statex00usingB1,andthenmakea
transition fromstatex00tox0usingB2.
T(x0;x)=Z
dNx00B2(x0;x00)B1(x00;x): (29.45)

<<<PAGE 386>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
374 29|MonteCarlo Metho ds
Detaile dbalance
Manyuseful transition probabilities satisfy thedetailed balance property:
T(xa;xb)P(xb)=T(xb;xa)P(xa);forallxbandxa: (29.46)
Thisequation saysthatifwepick(bymagic) astatefromthetarget densit y
Pandmakeatransition underTtoanother state, itisjustaslikelythatwe
willpickxbandgofromxbtoxaasitisthatwewillpickxaandgofromxa
toxb.Markovchains thatsatisfy detailed balance arealsocalled reversible
Markovchains. Thereason whythedetailed balance propertyisofinterest
isthatdetailed balance implies invariance ofthedistribution P(x)under the
MarkovchainT,whichisanecessary condition forthekeypropertythatwe
wantfromourMCMC simulation {thattheprobabilit ydistribution ofthe
chainshould convergetoP(x).
.Exercise 29.7.[2]Provethatdetailed balance implies invariance ofthedistri-
butionP(x)under theMarkovchainT.
Provingthatdetailed balance holds isoften akeystepwhen provingthata
MarkovchainMonteCarlo simulation willconvergetothedesired distribu-
tion.TheMetrop olismetho dsatises detailed balance, forexample. Detailed
balance isnotanessentialcondition, however,andwewillseelaterthatir-
reversible Markovchains canbeuseful inpractice, because theymayhave
dieren trandom walkproperties.
.Exercise 29.8.[2]Showthat,ifweconcatenate twobasetransitions B1andB2
whichsatisfy detailed balance, itisnotnecessarily thecasethattheT
thusdened (29.45) satises detailed balance.
Exercise 29.9.[2]DoesGibbs sampling, withseveralvariables allupdated ina
deterministic sequence, satisfy detailed balance?
29.7 Slicesampling
Slice sampling (Neal, 1997a; Neal, 2003) isaMarkovchainMonteCarlo
metho dthathassimilarities torejection sampling, Gibbs sampling andthe
Metrop olismetho d.Itcanbeapplied wherev ertheMetrop olismetho dcan
beapplied, thatis,toanysystem forwhichthetarget densit yP(x)canbe
evaluated atanypointx;ithastheadvantageoversimple Metrop olismetho ds
thatitismore robust tothechoice ofparameters likestepsizes. Thesim-
plestversion ofslicesampling issimilar toGibbs sampling inthatitconsists of
one-dimensional transitions inthestatespace; howeverthereisnorequiremen t
thattheone-dimensional conditional distributions beeasytosample from, nor
thattheyhaveanyconvexityproperties suchasarerequired foradaptiv ere-
jection sampling. Andslicesampling issimilar torejection sampling inthat
itisametho dthatasymptotically drawssamples fromthevolume under the
curvedescrib edbyP(x);butthere isnorequiremen tforanupper-bounding
function.
Iwilldescrib eslicesampling bygiving asketchofaone-dimensional sam-
pling algorithm, thengiving apictorial description thatincludes thedetails
thatmakethemetho dvalid.

<<<PAGE 387>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
29.7: Slicesampling 375
Theskeleton ofslicesampling
Letusassume thatwewanttodrawsamples fromP(x)/P(x)wherex
isarealnumber.Aone-dimensional slicesampling algorithm isametho d
formaking transitions fromatwo-dimensional point(x;u)lying under the
curveP(x)toanother point(x0;u0)lying under thesame curve,suchthat
theprobabilit ydistribution of(x;u)tends toauniform distribution overthe
areaunder thecurveP(x),whatev erinitial pointwestartfrom{likethe
uniform distribution under thecurveP(x)produced byrejection sampling
(section 29.3).
Asingle transition (x;u)!(x0;u0)ofaone-dimensional slicesampling
algorithm hasthefollowingsteps, ofwhichsteps3and8willrequire further
elaboration.
1:evaluateP(x)
2:drawavertical coordinateu0Uniform(0;P(x))
3:create ahorizontal interval (xl;xr)enclosingx
4:loopf
5: drawx0Uniform(xl;xr)
6: evaluateP(x0)
7: ifP(x0)>u0breakoutofloop4-9
8: elsemodifytheinterval (xl;xr)
9:g
There areseveralmetho dsforcreating theinterval(xl;xr)instep3,and
severalmetho dsformodifying itatstep8.Theimportantpointisthatthe
overallmetho dmustsatisfy detailed balance, sothattheuniform distribution
for(x;u)under thecurveP(x)isinvariant.
The`stepping out'methodforstep3
Inthe`stepping out'metho dforcreating aninterval(xl;xr)enclosingx,we
stepoutinstepsoflengthwuntilwendendpointsxlandxratwhichPis
smaller thanu.Thealgorithm isshowningure 29.16.
3a:drawrUniform(0;1)
3b:xl:=x rw
3c:xr:=x+(1 r)w
3d:while(P(xl)>u)fxl:=xl wg
3e:while(P(xr)>u)fxr:=xr+wg
The`shrinking' methodforstep8
Whenev erapointx0isdrawnsuchthat(x0;u0)liesabovethecurveP(x),
weshrink theintervalsothatoneoftheendpointsisx0,andsuchthatthe
original pointxisstillenclosed intheinterval.
8a:if(x0>x)fxr:=x0g
8b:elsefxl:=x0g
Properties ofslicesampling
Likeastandard Metrop olismetho d,slicesampling getsaround byarandom
walk,butwhereas intheMetrop olismetho d,thechoice ofthestepsizeis

<<<PAGE 388>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
376 29|MonteCarlo Metho ds
1 2
3a,3b,3c 3d,3e
5,6 8
5,6,7Figure 29.16.Slicesampling. Each
panel islabelledbythesteps of
thealgorithm thatareexecuted in
it.Atstep1,P(x)isevaluated
atthecurren tpointx.Atstep2,
avertical coordinate isselected
giving thepoint(x;u0)shownby
thebox;Atsteps3a-c,an
intervalofsizewcontaining
(x;u0)iscreated atrandom. At
step3d,Pisevaluated attheleft
endoftheintervalandisfound to
belarger thanu0,soasteptothe
leftofsizewismade. Atstep3e,
Pisevaluated attherightendof
theintervalandisfound tobe
smaller thanu0,sonostepping
outtotherightisneeded. When
step3disrepeated,Pisfound to
besmaller thanu0,sothe
stepping outhalts. Atstep5a
pointisdrawnfromtheinterval,
shownbya.Step6establishes
thatthispointisabovePand
step8shrinks theintervaltothe
rejected pointinsuchawaythat
theoriginal pointxisstillinthe
interval.When step5isrepeated,
thenewcoordinatex0(whichisto
theright-hand sideofthe
interval)givesavalueofP
greater thanu0,sothispointx0is
theoutcome atstep7.

<<<PAGE 389>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
29.7: Slicesampling 377
critical totherateofprogress, inslicesampling thestepsizeisself-tuning. If
theinitial intervalsizewistoosmall byafactorfcompared withthewidth of
theprobable region thenthestepping-out procedure expands theintervalsize.
Thecostofthisstepping-out isonlylinear inf,whereas intheMetrop olis
metho dthecomputer-time scales asthesquare offifthestepsizeistoo
small.
Ifthechosen valueofwistoolarge byafactorFthenthealgorithm
spendsatimeproportional tothelogarithm ofFshrinking theintervaldown
totherightsize,sincetheintervaltypically shrinks byafactor intheballpark
of0:6eachtimeapointisrejected. Incontrast, theMetrop olisalgorithm
respondstoatoo-large stepsizebyrejecting almost allproposals, sotherate
ofprogress isexponentially badinF.There arenorejections inslicesampling.
Theprobabilit yofstayinginexactly thesame place isverysmall.1234567891011 0110
Figure 29.17.P(x).
.Exercise 29.10.[2]Investigate theproperties ofslicesampling applied tothe
densit yshowningure 29.17.xisarealvariable between0.0and11.0.
Howlongdoesittaketypically forslicesampling togetfromanxin
thepeakregionx2(0;1)toanxinthetailregionx2(1;11),andvice
versa?Conrm thattheprobabilities ofthese transitions doyield an
asymptotic probabilit ydensit ythatiscorrect.
Howslicesampling isusedinrealproblems
AnN-dimensional densit yP(x)/P(x)maybesampled withthehelpof
one-dimensional slicesampling metho dpresen tedabovebypickingasequence
ofdirections y(1);y(2);:::anddening x=x(t)+xy(t).ThefunctionP(x)
aboveisreplaced byP(x)=P(x(t)+xy(t)).Thedirections maybechosen
invarious ways;forexample, asinGibbs sampling, thedirections could bethe
coordinate axes; alternativ ely,thedirections y(t)maybeselected atrandom
inanymanner suchthattheoverallprocedure satises detailed balance.
Computer-friend lyslicesampling
Therealvariables ofaprobabilistic modelwillalwaysberepresen tedina
computer using anite numberofbits. Inthefollowingimplemen tation of
slicesampling duetoSkilling, thestepping-out, randomization, andshrinking
operations, describ edaboveinterms ofoating-p ointoperations, arereplaced
bybinary andinteger operations.
Weassume thatthevariablexthatisbeingslice-sampled isrepresen tedby
ab-bitintegerXtaking ononeofB=2bvalues, 0;1;2;:::;B 1,manyorall
ofwhichcorresp ondtovalidvaluesofx.Using anintegergrideliminates any
errors indetailed balance thatmightensue fromvariable-precision rounding of
oating-p ointnumbers.Themapping fromXtoxneednotbelinear; ifitis
nonlinear, weassume thatthefunctionP(x)isreplaced byanappropriately
transformed function {forexample,P(X)/P(x)jdx=dXj.
Weassume thefollowingoperators onb-bitintegers areavailable:
X+N arithmetic sum,moduloB,ofXandN.
X N dierence, moduloB,ofXandN.
XN bitwiseexclusiv e-orofXandN.
N:=randbits (l) setsNtoarandoml-bitinteger.
Aslice-sampling procedure forintegers isthenasfollows:

<<<PAGE 390>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
378 29|MonteCarlo Metho ds
Given: acurrent pointXandaheightY=P(X)Uniform(0;1)P(X)
1:U:=randbits (b) Dene arandom translation Uofthebinary coor-
dinate system.
2:setltoavaluelb Setinitiall-bitsampling range.
3:dof
4:N:=randbits (l) Dene arandom movewithin thecurren tintervalof
width 2l.
5:X0:=((X U)N)+U Randomize thelowestlbitsofX(inthetranslated
coordinate system).
6:l:=l 1 IfX0isnotacceptable, decreaselandtryagain
7:guntil(X0=X)or(P(X)Y) withasmaller perturbation ofX;termination ator
beforel=0isassured.
Thetranslation Uisintroduced toavoidpermanen tsharp edges, where
forexample theadjacen tbinary integers0111111111 and1000000000 would
otherwise bepermanen tlyindieren tsectors, making itdicult forXtomove
fromonetotheother.
0 B−1 X
Figure 29.18.Thesequence of
intervalsfromwhichthenew
candidate pointsaredrawn.Thesequence ofintervalsfromwhichthenewcandidate pointsaredrawn
isillustrated ingure 29.18. First, apointisdrawnfromtheentireinterval,
shownbythetophorizon talline. Ateachsubsequen tdraw,theintervalis
halvedinsuchawayastocontaintheprevious pointX.
Ifpreliminary stepping-out fromtheinitial range isrequired, step2above
canbereplaced bythefollowingsimilar procedure:
2a:setltoavaluel<b lsetstheinitial width
2b:dof
2c:N:=randbits (l)
2d:X0:=((X U)N)+U
2e:l:=l+1
2f:guntil(l=b)or(P(X)<Y)
These shrinking andstepping outmetho dsshrink andexpand byafactor
oftwoperevaluation. Avariantistoshrink orexpand bymore thanonebit
eachtime, settingl:=llwithl>1.Taking lateachstepfromany
pre-assigned distribution (whichmayinclude l=0)allowsextra exibilit y.
Exercise 29.11.[4]Intheshrinking phase, afteranunacceptable X0hasbeen
produced, thechoiceoflisallowedtodependonthedierence between
theslice's heightYandthevalueofP(X0),without spoiling thealgo-
rithm's validity.(Provethis.) Itmightbeagoodideatochoosealarger
valueoflwhenY P(X)islarge. Investigate thisideatheoretically
orempirically .
Afeature ofusing theinteger represen tation isthat, withasuitably ex-
tended numberofbits,thesingle integerXcanrepresen ttwoormore real
parameters {forexample, bymappingXto(x1;x2;x3)through aspace-lling
curvesuchasaPeanocurve.Thusmulti-dimensional slicesampling canbe
performed using thesame softwareasforonedimension.
29.8 Practicalities
Canwepredict howlongaMarkovchainMonteCarlo simulation

<<<PAGE 391>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
29.9: Further practical issues 379
willtaketoequilibrate? Byconsidering therandom walksinvolvedina
MarkovchainMonteCarlo simulation wecanobtain simple lower bounds on
thetimerequired forconvergence. Butpredicting thistimemoreprecisely isa
dicult problem, andmostofthetheoretical results giving upperbounds on
theconvergence timeareoflittlepractical use.Theexact sampling metho ds
ofChapter 32oerasolution tothisproblem forcertain Markovchains.
Canwediagnose ordetect convergence inarunning simulation?
Thisisalsoadicult problem. There areafewpractical toolsavailable, but
noneofthem isperfect (CowlesandCarlin, 1996).
Canwespeeduptheconvergence timeandtimebetweenindepen-
dentsamples ofaMarkovchainMonteCarlo metho d?Here, there is
goodnews, asdescrib edinthenextchapter, whichdescrib estheHamiltonian
MonteCarlo metho d,overrelaxation, andsimulated annealing.
29.9 Further practical issues
Canthenormalizing constant beevaluate d?
Ifthetarget densit yP(x)isgivenintheformofanunnormalized densit y
P(x)withP(x)=1
ZP(x),thevalueofZmaywellbeofinterest. Monte
Carlo metho dsdonotreadily yieldanestimate ofthisquantity,anditisan
areaofactiveresearc htondwaysofevaluating it.Techniques forevaluating
Zinclude:
1.Importance sampling (review edbyNeal(1993b)) andannealed impor-
tance sampling (Neal, 1998).
2.`Thermo dynamic integration' during simulated annealing, the`accep-
tance ratio' metho d,and`umbrella sampling' (review edbyNeal(1993b)).
3.`Reversible jump MarkovchainMonteCarlo' (Green, 1995).
Onewayofdealing withZ,however,maybetondasolution toone's
taskthatdoesnotrequire thatZbeevaluated. InBayesian datamodelling
onemightbeabletoavoidtheneedtoevaluateZ{whichwouldbeimportant
formodelcomparison {bynothavingmorethanonemodel.Instead ofusing
severalmodels(diering incomplexit y,forexample) andevaluating theirrel-
ativeposterior probabilities, onecanmakeasingle hierarc hicalmodelhaving,
forexample, various continuoushyperparameters whichplayarolesimilar to
thatplayedbythedistinct models(Neal, 1996). Innoting thepossibilit yof
notcomputingZ,Iamnotendorsing thisapproac h.Thenormalizing constan t
Zisoftenthesingle mostimportantnumberintheproblem, andIthink every
eort should bedevotedtocalculating it.
TheMetropolismethodforbigmodels
Ouroriginal description oftheMetrop olismetho dinvolvedajointupdating
ofallthevariables using aproposaldensit yQ(x0;x).Forbigproblems it
maybemoreecien ttouseseveralproposaldistributions Q(b)(x0;x),eachof
whichupdates onlysomeofthecomponentsofx.Eachproposalisindividually
accepted orrejected, andtheproposaldistributions arerepeatedly runthrough
insequence.
.Exercise 29.12.[2,p.385]Explain whytherateofmovementthrough thestate
space willbegreater whenBproposalsQ(1);:::;Q(B)areconsidered

<<<PAGE 392>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
380 29|MonteCarlo Metho ds
individual lyinsequence, compared withthecaseofasingle proposal
Qdened bytheconcatenation ofQ(1);:::;Q(B).Assume thateach
proposaldistribution Q(b)(x0;x)hasanacceptance ratef<1=2.
IntheMetrop olismetho d,theproposaldensit yQ(x0;x)typically hasa
numberofparameters thatcontrol,forexample, its`width'. These parameters
areusually setbytrialanderror withtheruleofthumbbeingtoaimfora
rejection frequency ofabout0.5.Itisnotvalidtohavethewidth parameters
bedynamically updated during thesimulation inawaythatdependsonthe
history ofthesimulation. Suchamodication oftheproposaldensit ywould
violate thedetailed balance condition whichguaran teesthattheMarkovchain
hasthecorrect invariantdistribution.
Gibbs sampling inbigmodels
Ourdescription ofGibbs sampling involvedsampling oneparameter atatime,
asdescrib edinequations (29.35{29.37). Forbigproblems itmaybemore
ecien ttosample groupsofvariables jointly,thatistouseseveralproposal
distributions:
x(t+1)
1;:::;x(t+1)
aP(x1;:::;xajx(t)
a+1;:::;x(t)
K) (29.47)
x(t+1)
a+1;:::;x(t+1)
bP(xa+1;:::;xbjx(t+1)
1;:::;x(t+1)
a;x(t)
b+1;:::;x(t)
K);etc.
Howmany samples areneeded?
Atthestartofthischapter, weobserv edthatthevariance ofanestimator ^
dependsonlyonthenumberofindependen tsamplesRandthevalueof
2=Z
dNxP(x)((x) )2: (29.48)
Wehavenowdiscussed avarietyofmetho dsforgenerating samples fromP(x).
Howmanyindependen tsamplesRshould weaimfor?
Inmanyproblems, wereally onlyneedabouttwelveindependen tsamples
fromP(x).Imagine thatxisanunkno wnvector suchastheamoun tof
corrosion presen tineachof10000underground pipelines around Cambridge,
and(x)isthetotalcostofrepairing those pipelines. Thedistribution P(x)
describ estheprobabilit yofastatexgiventheteststhathavebeencarried out
onsome pipelines andtheassumptions aboutthephysicsofcorrosion. The
quantityistheexpected costoftherepairs. Thequantity2isthevariance
ofthecost{measures byhowmuchweshould expecttheactual costto
dier fromtheexpectation .
Now,howaccurately wouldamanager liketoknow?Iwouldsuggest
there islittlepointinknowingtoaprecision nerthanabout=3.After
all,thetruecostislikelytodier byfrom .IfweobtainR=12
independen tsamples fromP(x),wecanestimate toaprecision of=p
12{
whichissmaller than=3.Sotwelvesamples suce.
Allocationofresources
Assuming wehavedecided howmanyindependen tsamplesRarerequired,
animportantquestion ishowoneshould makeuseofone's limited computer
resources toobtain these samples.
Atypical MarkovchainMonteCarlo experimen tinvolvesaninitial pe-
riodinwhichcontrolparameters ofthesimulation suchasstepsizesmaybe

<<<PAGE 393>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
29.10: Summary 381
(1)
(2)
(3)Figure 29.19.Three possible
MarkovchainMonteCarlo
strategies forobtaining twelve
samples inaxedamoun tof
computer time. Time is
represen tedbyhorizon tallines;
samples bywhite circles. (1)A
single runconsisting ofonelong
`burn in'periodfollowedbya
sampling period.(2)Four
medium-length runswithdieren t
initial conditions anda
medium-length burninperiod.
(3)Twelveshort runs. adjusted. Thisisfollowedbya`burn in'periodduring whichwehopethe
simulation `converges' tothedesired distribution. Finally ,asthesimulation
continues,werecord thestatevectoroccasionally soastocreate alistofstates
fx(r)gR
r=1thatwehopeareroughly independen tsamples fromP(x).
There areseveralpossible strategies (gure 29.19):
1.Makeonelongrun,obtaining allRsamples fromit.
2.Makeafewmedium length runswithdieren tinitial conditions, obtain-
ingsome samples fromeach.
3.MakeRshort runs, eachstarting fromadieren trandom initial condi-
tion,withtheonlystate thatisrecorded beingthenalstate ofeach
simulation.
Therststrategy hasthebestchance ofattaining `convergence'. Thelast
strategy mayhavetheadvantagethatthecorrelations betweentherecorded
samples aresmaller. Themiddle pathispopular withMarkovchainMonte
Carlo experts(Gilks etal.,1996) because itavoidstheineciency ofdiscarding
burn-in iterations inmanyruns, while stillallowingonetodetect problems
withlackofconvergence thatwouldnotbeapparen tfromasingle run.
Finally ,Ishould emphasize thatthereisnoneedtomakethepointsnearly-
independen t.Averaging overdependen tpointsisne{itwon'tleadtoany
biasintheestimates. Forexample, when youusestrategy 1or2,youmay,if
youwish, include allthepointsbetweentherstandlastsample ineachrun.
Ofcourse, estimating theaccuracy oftheestimate isharder when thepoints
aredependen t.
29.10 Summary
MonteCarlo metho dsareapowerfultoolthatallowonetosample from
anyprobabilit ydistribution thatcanbeexpressed intheformP(x)=
1
ZP(x).
MonteCarlo metho dscananswervirtually anyquery related toP(x)by
putting thequery intheform
Z
(x)P(x)'1
RX
r(x(r)): (29.49)
Inhigh-dimensional problems theonlysatisfactory metho dsarethose
based onMarkovchains, suchastheMetrop olismetho d,Gibbs sam-
pling andslicesampling. Gibbs sampling isanattractiv emetho dbe-
cause ithasnoadjustable parameters butitsuseisrestricted tocases

<<<PAGE 394>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
382 29|MonteCarlo Metho ds
where samples canbegenerated fromtheconditional distributions. Slice
sampling isattractiv ebecause, whilst ithassteplength parameters, its
performance isnotverysensitiv etotheirvalues.
Simple Metrop olisalgorithms andGibbs sampling algorithms, although
widely used, perform poorlybecause theyexplore thespace byaslow
random walk.Thenextchapter willdiscuss metho dsforspeeding up
MarkovchainMonteCarlo simulations.
Slicesampling doesnotavoidrandom walkbehaviour, butitautomat-
ically choosesthelargest appropriate stepsize,thusreducing thebad
eects oftherandom walkcompared with, say,aMetrop olismetho d
withatinystepsize.
29.11 Exercises
Exercise 29.13.[2C,p.386]Astudy ofimportance sampling. Wealready estab-
lished insection 29.2thatimportance sampling islikelytobeuseless in
high-dimensional problems. Thisexercise explores afurther cautionary
tale,showingthatimportance sampling canfaileveninonedimension,
evenwithfriendly Gaussian distributions.
Imagine thatwewanttoknowtheexpectation ofafunction(x)under
adistribution P(x),
=Z
dxP(x)(x); (29.50)
andthatthisexpectation isestimated byimportance sampling with
adistribution Q(x).Alternativ ely,perhaps wewishtoestimate the
normalizing constan tZinP(x)=P(x)=Zusing
Z=Z
dxP(x)=Z
dxQ(x)P(x)
Q(x)=P(x)
Q(x)
xQ: (29.51)
Now,letP(x)andQ(x)beGaussian distributions withmean zeroand
standard deviationspandq.EachpointxdrawnfromQwillhave
anassociated weightP(x)=Q(x).What isthevariance oftheweights?
[Assume thatP=P,soPisactually normalized, andZ=1,though
wecanpretend thatwedidn't knowthat.] What happenstothevariance
oftheweightsas2
q!2
p=2?
Checkyourtheory bysimulating thisimportance-sampling problem on
acomputer.
Exercise 29.14.[2]Consider theMetrop olisalgorithm fortheone-dimensional
toyproblem ofsection 29.4, sampling fromf0;1;:::;20g.Whenev er
thecurren tstateisoneoftheendstates, theproposaldensit ygivenin
equation (29.34) willproposewithprobabilit y50%astate thatwillbe
rejected.
Toreduce this`waste', Fredmodies thesoftwareresponsible forgen-
erating samples fromQsothatwhenx=0,theproposaldensit yis
100% onx0=1,andsimilarly whenx=20,x0=19isalwaysproposed.
Fredsetsthesoftwarethatimplemen tstheacceptance rulesothatthe
softwareaccepts allproposedmoves.What probabilit yP0(x)willFred's
modied softwaregenerate samples from?
What isthecorrect acceptance ruleforFred'sproposaldensit y,inorder
toobtain samples fromP(x)?

<<<PAGE 395>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
29.11: Exercises 383
.Exercise 29.15.[3C]Implemen tGibbs sampling fortheinference ofasingle
one-dimensional Gaussian, whichwestudied using maxim umlikelihood
insection 22.1. Assign abroad Gaussian priortoandabroad gamma
prior(24.2) totheprecision parameter=1=2.Eachupdateofwill
involveasample fromaGaussian distribution, andeachupdateof
requires asample fromagamma distribution.
Exercise 29.16.[3C]Gibbs sampling forclustering. Implemen tGibbs sampling
fortheinference ofamixture ofKone-dimensional Gaussians, whichwe
studied using maxim umlikelihoodinsection 22.2. Allowtheclusters to
havedieren tstandard deviationsk.Assign priors tothemeans and
standard deviations inthesamewayastheprevious exercise. Either x
thepriorprobabilities oftheclassesfkgtobeequal orputauniform
priorovertheparameters andinclude them intheGibbs sampling.
Notice thesimilarit yofGibbs sampling tothesoftK-means clustering
algorithm (algorithm 22.2). Wecanalternately assign theclasslabels
fknggiventheparametersfk;kg,thenupdatetheparameters given
theclasslabels.Theassignmen tstepinvolvessampling fromtheprob-
abilitydistributions dened bytheresponsiblities (refeq.assignI I),and
theupdatestepupdates themeans andvariances using probabilit ydis-
tributions centredontheK-means algorithm's values (22.23, 22.24).
Doyourexperimen tsconrm thatMonteCarlo metho dsbypass theover-
tting diculties ofmaxim umlikelihooddiscussed insection 22.4?
Asolution tothisexercise andtheprevious one,written inoctave ,is
available.2
.Exercise 29.17.[3C]Implemen tGibbs sampling forthesevenscientists inference
problem, whichweencoun tered inexercise 22.15 (p.309),andwhichyou
mayhavesolvedbyexact marginalization (exercise 24.3(p.323))[it's
notessentialtohavedonethelatter].
.Exercise 29.18.[2]AMetrop olismetho disusedtoexplore adistribution P(x)
thatisactually a1000-dimensional spherical Gaussian distribution of
standard deviation 1inalldimensions. Theproposaldensit yQisa
1000-dimensional spherical Gaussian distribution ofstandard deviation
.Roughly whatisthestepsizeiftheacceptance rateis0.5?Assuming
thisvalueof,
(a)roughly howlongwouldthemetho dtaketotraversethedistribution
andgenerate asample independen toftheinitial condition?
(b)ByhowmuchdoeslnP(x)change inatypical step? Byhowmuch
should lnP(x)varywhenxisdrawnfromP(x)?
(c)What happensif,rather thanusing aMetrop olismetho dthattries
tochange allcomponentsatonce, oneinstead usesaconcatenation
ofMetrop olisupdates changing onecomponentatatime?
.Exercise 29.19.[2]When discussing thetimetakenbytheMetrop olisalgo-
rithm togenerate independen tsamples weconsidered adistribution with
longest spatial length scaleLbeingexplored using aproposaldistribu-
tionwithstepsize.Another dimension thataMCMC metho dmust
explore istherange ofpossible values ofthelogprobabilit ylnP(x).
2http://www.inference.phy.cam.a c.uk/mackay/itila/

<<<PAGE 396>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
384 29|MonteCarlo Metho ds
Assuming thatthestatexcontains anumberofindependen trandom
variables proportional toN,when samples aredrawnfromP(x),the
`asymptotic equipartition' principle tellusthatthevalueof lnP(x)is
likelytobeclosetotheentropyofx,varying either sidewithastandard
deviation thatscales asp
N.Consider aMetrop olismetho dwithasym-
metrical proposaldensit y,thatis,onethatsatisesQ(x;x0)=Q(x0;x).
Assuming thataccepted jumps either increase lnP(x)bysomeamoun t
ordecrease itbyasmallamoun t,e.g.lne=1(isthisareasonable
assumption?), discuss howlongitmusttaketogenerate roughly inde-
penden tsamples fromP(x).Discuss whether Gibbs sampling hassimilar
properties.
Exercise 29.20.[3]MarkovchainMonteCarlo metho dsdonotcompute parti-
tionfunctionsZ,yettheyallowratios ofquantities likeZtobeesti-
mated. Forexample, consider arandom-w alkMetrop olisalgorithm ina
statespace where theenergy iszeroinaconnected accessible region, and
innitely largeeverywhere else;andimagine thattheaccessible space can
bechoppedintotworegions connected byoneormore corridor states.
Thefraction oftimes spentineachregion atequilibrium isproportional
tothevolume oftheregion. HowdoestheMonteCarlo metho dmanage
todothiswithout measuring thevolumes?
Exercise 29.21.[5]Philosophy .
Onecurious defect oftheseMonteCarlo metho ds{whicharewidely used
byBayesian statisticians {isthattheyareallnon-Ba yesian (O'Hagan,
1987). They involvecomputer experimen tsfromwhichestimators of
quantities ofinterest arederived.These estimators dependonthepro-
posaldistributions thatwereusedtogenerate thesamples andonthe
random numbersthathappenedtocome outofourrandom number
generator. Incontrast, analternativ eBayesianapproac htotheproblem
wouldusetheresults ofourcomputer experimen tstoinfertheproper-
tiesofthetarget functionP(x)andgenerate predictiv edistributions for
quantities ofinterest suchas.Thisapproac hwouldgiveanswersthat
woulddependonlyonthecomputed values ofP(x(r))atthepoints
fx(r)g;theanswerswouldnotdependonhowthose pointswerechosen.
CanyoumakeaBayesian MonteCarlo metho d?(SeeRasmussen and
Ghahramani (2003) forapractical attempt.)
29.12 Solutions
Solution toexercise 29.1(p.362).Wewishtoshowthat
^P
rwr(x(r))P
rwr(29.52)
convergestotheexpectation ofunderP.Weconsider thenumerator andthe
denominator separately .First, thedenominator. Consider asingle importance
weight.
wrP(x(r))
Q(x(r)): (29.53)
What isitsexpectation, averaged under thedistribution Q=Q=ZQofthe
pointx(r)?
hwri=Z
dxQ(x)P(x)
Q(x)=Z
dx1
ZQP(x)=ZP
ZQ: (29.54)

<<<PAGE 397>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
29.12: Solutions 385
Sotheexpectation ofthedenominator is
*X
rwr+
=RZP
ZQ: (29.55)
Aslongasthevariance ofwrisnite, thedenominator, divided byR,will
convergetoZP=ZQasRincreases. [Infact,theestimate converges tothe
rightanswerevenifthisvariance isinnite, aslongastheexpectation is
well-dened.] Similarly ,theexpectation ofoneterminthenumerator is
hwr(x)i=Z
dxQ(x)P(x)
Q(x)(x)=Z
dx1
ZQP(x)(x)=ZP
ZQ;(29.56)
where istheexpectation ofunderP.Sothenumerator, divided byR,
converges toZP
ZQwithincreasingR.Thus^converged to.
Thenumerator andthedenominator areunbiased estimators ofRZP=ZQ
andRZP=ZQrespectively,buttheirratio ^isnotnecessarily anunbiased
estimator forniteR.
Solution toexercise 29.2(p.363).When thetruedensit yPismultimo dal,itis
unwisetouseimportance sampling withasampler densit ytted toonemode,
because ontherareoccasions thatapointisproduced thatlands inoneof
theother modes,theweightassociated withthatpointwillbeenormous. The
estimates willhaveenormous variance, butthisenormous variance maynot
beeviden ttotheuserifnopointsintheother modehavebeenseen.
Solution toexercise 29.5(p.371).Theposterior distribution forthesyndrome
decodingproblem isapathological distribution fromthepointofviewofGibbs
sampling. Thefactor
 [Hn=z]isonly1onasmall fraction ofthespace of
possible vectors n,namely the2Kpointsthatcorresp ondtothevalidcode-
words. Notwocodewordsareadjacen t,sosimilarly ,anysingle bitipfrom
aviable statenwilltakeustoastatewithzeroprobabilit yandsothestate
willnevermoveinGibbs sampling.
Ageneral codehasexactly thesame problem. Thepointscorresp onding
tovalidcodewordsarerelativ elyfewinnumberandtheyarenotadjacen t(at
leastforanyuseful code).SoGibbs sampling isnouseforsyndrome decoding
fortworeasons. First, nding anyreasonably goodhypothesis isdicult, and
aslongasthestateisnotnearavalidcodeword,Gibbs sampling cannot help
sincenoneoftheconditional distributions isdened; andsecond, onceweare
inavalidhypothesis, Gibbs sampling willnevertakeusoutofit.
Onecould attempt toperform Gibbs sampling using thebitsoftheoriginal
message sasthevariables. Thisapproac hwouldnotgetlockedupintheway
justdescrib ed,but,foragoodcode,anysingle bitipwouldsubstan tially
alterthereconstructed codeword,soifonehadfound astatewithreasonably
large likelihood,Gibbs sampling wouldtakeanimpractically large timeto
escapefromit.
Solution toexercise 29.12 (p.379).EachMetrop olisproposalwilltakethe
energy ofthestateupordownbysome amoun t.Thetotalchange inenergy
whenBproposalsareconcatenated willbetheend-pointofarandom walk
withBsteps init.Thiswalkmighthavemean zero, oritmighthavea
tendency todriftupwards(ifmostmovesincrease theenergy andonlyafew
decrease it).Ingeneral thelatter willhold, iftheacceptance ratefissmall:
themean change inenergy fromanyonemovewillbesome E>0andso
theacceptance probabilit yfortheconcatenation ofBmoveswillbeoforder

<<<PAGE 398>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
386 29|MonteCarlo Metho ds
1=(1+exp( BE)),whichscales roughly asfB.Themean-square-distance
movedwillbeoforderfBB2,whereisthetypical stepsize.Incontrast,
themean-square-distance movedwhen themovesareconsidered individually
willbeoforderfB2.
0.20.30.40.50.60.70.80.911.1
00.20.40.60.811.21.41.61000
10000
100000
theory
0246810
00.20.40.60.811.21.41.61000
10000
100000
theory
00.511.522.533.5
00.20.40.60.811.21.41.6
Figure 29.20.Importance
sampling inonedimension. For
R=1000;104,and105,the
normalizing constan tofa
Gaussian distribution (knownin
facttobe1)wasestimated using
importance sampling witha
sampler densit yofstandard
deviationq(horizon talaxis).
Thesamerandom numberseed
wasusedforallruns. Thethree
plotsshow(a)theestimated
normalizing constan t;(b)the
empiric alstandard deviation of
theRweights;(c)30ofthe
weights.Solution toexercise 29.13 (p.382).Theweightsarew=P(x)=Q(x)andxis
drawnfromQ.Themean weightis
Z
dxQ(x)[P(x)=Q(x)]=Z
dxP(x)=1; (29.57)
assuming theintegral converges. Thevariance is
var(w)=Z
dxQ(x)P(x)
Q(x) 12
(29.58)
=Z
dxP(x)2
Q(x) 2P(x)+Q(x) (29.59)
="Z
dxZQ
Z2
Pexp 
 x2
2 
2
2p 1
2q!!#
 1; (29.60)
whereZQ=Z2
P=q=(p
22
p).Theintegral in(29.60) isnite onlyifthe
coecien tofx2intheexponentispositive,i.e.,if
2
q>1
22
p: (29.61)
Ifthiscondition issatised, thevariance is
var(w)=qp
22pp
2 
2
2p 1
2q! 1
2
 1=2
q
p
22q 2p1=2 1:(29.62)
Asqapproac hesthecritical value{about0:7p{thevariance becomes
innite. Figure 29.20 illustrates these phenomena forp=1withqvarying
from0.1to1.5.Thesame random numberseedwasusedforallruns, so
theweightsandestimates followsmoothcurves.Notice thattheempiric al
standard deviation oftheRweightscanlookquite small andwell-behaved
(say,atq'0:3)when thetruestandard deviation isnevertheless innite.

<<<PAGE 399>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
30
Ecien tMonteCarlo Metho ds
Thischapter discusses severalmetho dsforreducing random walkbehaviour
inMetrop olismetho ds.Theaimistoreduce thetimerequired toobtain
eectiv elyindependen tsamples. Forbrevit y,wewillsay`indep enden tsamples'
when wemean `eectiv elyindependen tsamples'.
30.1 Hamiltonian MonteCarlo
TheHamiltonian MonteCarlo metho disaMetrop olismetho d,applicable
tocontinuousstatespaces, thatmakesuseofgradien tinformation toreduce
random walkbehaviour. [TheHamiltonian MonteCarlo metho dwasoriginally
called hybrid MonteCarlo, forhistorical reasons.]
Formanysystems whose probabilit yP(x)canbewritten intheform
P(x)=e E(x)
Z; (30.1)
notonlyE(x)butalsoitsgradien twithrespecttoxcanbereadily evaluated.
Itseems wasteful touseasimple random-w alkMetrop olismetho dwhen this
gradien tisavailable {thegradien tindicates whichdirection oneshould goin
tondstates withhigher probabilit y!
Overview ofHamiltonian Monte Carlo
IntheHamiltonian MonteCarlo metho d,thestatespacexisaugmen tedby
momentum variables p,andthere isanalternation oftwotypesofproposal.
Therstproposalrandomizes themomen tumvariable, leavingthestatexun-
changed. Thesecond proposalchanges bothxandpusing simulated Hamil-
tonian dynamics asdened bytheHamiltonian
H(x;p)=E(x)+K(p); (30.2)
whereK(p)isa`kinetic energy' suchasK(p)=pTp=2.These twoproposals
areusedtocreate (asymptotically) samples fromthejointdensit y
PH(x;p)=1
ZHexp[ H(x;p)]=1
ZHexp[ E(x)]exp[ K(p)]:(30.3)
Thisdensit yisseparable, sothemarginal distribution ofxisthedesired
distribution exp[ E(x)]=Z.So,simply discarding themomen tumvariables,
weobtain asequence ofsamplesfx(t)gthatasymptotically come fromP(x).
387

<<<PAGE 400>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
388 30|Ecien tMonteCarlo Metho ds
Algorithm 30.1.Octave source
codefortheHamiltonian Monte
Carlo metho d.g=gradE(x); #setgradient usinginitial x
E=findE(x); #setobjective function too
forl=1:L #loopLtimes
p=randn(size(x) ); #initial momentum isNormal(0,1)
H=p'*p/2+E; #evaluate H(x,p)
xnew=x;gnew=g;
fortau=1:Tau #makeTau`leapfrog' steps
p=p-epsilon *gnew/2;#makehalf-step inp
xnew=xnew+epsilon *p;#makestepinx
gnew=gradE(xnew);#findnewgradient
p=p-epsilon *gnew/2;#makehalf-step inp
endfor
Enew=findE(xnew); #findnewvalueofH
Hnew=p'*p/2+Enew;
dH=Hnew-H; #Decidewhether toaccept
if(dH<0) accept=1;
elseif(rand()<exp(-dH) )accept=1;
else accept=0;
endif
if(accept)
g=gnew;x=xnew;E=Enew;
endif
endfor
Hamiltonian MonteCarlo Simple Metrop olis
(a)
-1-0.500.51
-1-0.500.51(c)
-1-0.500.51
-1-0.500.51
(b)
-1.5-1-0.500.51
-1.5-1-0.500.51(d)
-1-0.500.51
-1-0.500.51Figure 30.2.(a,b)Hamiltonian
MonteCarlo usedtogenerate
samples fromabivariate Gaussian
withcorrelation =0:998.(c,d)
Forcomparison, asimple
random-w alkMetrop olismetho d,
givenequal computer time.

<<<PAGE 401>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
30.1: Hamiltonian MonteCarlo 389
Details ofHamiltonian Monte Carlo
Therstproposal,whichcanbeviewedasaGibbs sampling update,drawsa
newmomen tumfromtheGaussian densit yexp[ K(p)]=ZK.Thisproposalis
alwaysaccepted. During thesecond, dynamical proposal,themomen tumvari-
abledetermines where thestatexgoes,andthegradient ofE(x)determines
howthemomen tumpchanges, inaccordance withtheequations
_x=p (30.4)
_p= @E(x)
@x: (30.5)
Because ofthepersisten tmotion ofxinthedirection ofthemomen tump
during eachdynamical proposal,thestate ofthesystem tends tomovea
distance thatgoeslinearlywiththecomputer time, rather thanasthesquare
root.
Thesecond proposalisaccepted inaccordance withtheMetrop olisrule.
Ifthesimulation oftheHamiltonian dynamics isnumerically perfect then
theproposalsareaccepted everytime, because thetotalenergyH(x;p)isa
constan tofthemotion andsoainequation (29.31) isequal toone.Ifthe
simulation isimperfect, because ofnite stepsizesforexample, thensome of
thedynamical proposalswillberejected. Therejection rulemakesuseofthe
change inH(x;p),whichiszeroifthesimulation isperfect. Theoccasional
rejections ensure that,asymptotically ,weobtain samples (x(t);p(t))fromthe
required jointdensit yPH(x;p).
Thesource codeingure 30.1describ esaHamiltonian MonteCarlo metho d
thatusesthe`leapfrog' algorithm tosimulate thedynamics onthefunction
findE(x) ,whose gradien tisfound bythefunction gradE(x) .Figure 30.2
showsthisalgorithm generating samples fromabivariate Gaussian whose en-
ergyfunction isE(x)=1
2xTAxwith
A="
250:25 249:75
 249:75 250:25#
; (30.6)
corresp onding toavariance{co variance matrix of
"
1 0:998
0:9981#
: (30.7)
Ingure 30.2a, starting fromthestate markedbythearrow,thesolidline
represen tstwosuccessiv etrajectories generated bytheHamiltonian dynamics.
Thesquares showtheendpointsofthese twotrajectories. Eachtrajectory
consists ofTau=19`leapfrog' steps withepsilon =0:055.These steps are
indicated bythecrosses onthetrajectory inthemagnied inset. After each
trajectory ,themomen tumisrandomized. Here, bothtrajectories areaccepted;
theerrors intheHamiltonian wereonly+0:016and 0:06respectively.
Figure 30.2b showshowasequence offourtrajectories converges froman
initial condition, indicated bythearrow,thatisnotclosetothetypical set
ofthetarget distribution. Thetrajectory parameters Tauandepsilon were
randomized foreachtrajectory using uniform distributions withmeans 19and
0.055 respectively.Thersttrajectory takesustoanewstate, ( 1:5; 0:5),
similar inenergy totherststate. Thesecond trajectory happenstoendin
astate nearer thebottom oftheenergy landscap e.Here, sincethepotential
energyEissmaller, thekinetic energyK=p2=2isnecessarily larger thanit
wasatthestartofthetrajectory .When themomen tumisrandomized before

<<<PAGE 402>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
390 30|Ecien tMonteCarlo Metho ds
Gibbs sampling Overrelaxation
(a)
-1-0.500.51
-1-0.500.51-1-0.500.51
-1-0.500.51
(b)
-1-0.8-0.6-0.4-0.20
-1-0.8-0.6-0.4-0.20
(c)
Gibbs sampling
-3-2-10123
0200 400 600 800 1000 1200 1400 1600 1800 2000
Overrelaxation
-3-2-10123
0200 400 600 800 1000 1200 1400 1600 1800 2000Figure 30.3.Overrelaxation
contrasted withGibbs sampling
forabivariate Gaussian with
correlation =0:998.(a)The
statesequence for40iterations,
eachiteration involving one
updateofbothvariables. The
overrelaxation metho dhad
= 0:98.(This excessiv elylarge
valueischosen tomakeiteasyto
seehowtheoverrelaxation metho d
reduces random walkbehaviour.)
Thedotted lineshowsthecontour
xT 1x=1.(b)Detail of(a),
showingthetwosteps making up
eachiteration. (c)Time-course of
thevariablex1during 2000
iterations ofthetwometho ds.
Theoverrelaxation metho dhad
= 0:89.(After Neal(1995).)
thethirdtrajectory ,itskinetic energy becomes muchsmaller. After thefourth
trajectory hasbeensimulated, thestateappearstohavebecome typical ofthe
target densit y.
Figures 30.2(c) and(d)showarandom-w alkMetrop olismetho dusing a
Gaussian proposaldensit ytosample fromthesame Gaussian distribution,
starting fromtheinitial conditions of(a)and(b)respectively.In(c)thestep
sizewasadjusted suchthattheacceptance ratewas58%. Thenumberof
proposalswas38sothetotalamoun tofcomputer timeusedwassimilar to
thatin(a).Thedistance movedissmall because ofrandom walkbehaviour.
In(d)therandom-w alkMetrop olismetho dwasusedandstarted fromthe
same initial condition as(b)andgivenasimilar amoun tofcomputer time.
30.2 Overrelaxation
Themetho dofoverrelaxation isametho dforreducing random walkbehaviour
inGibbs sampling. Overrelaxation wasoriginally introduced forsystems in
whichalltheconditional distributions areGaussian.
Anexample ofajointdistribution thatisnotGaussian butwhose conditional distri-

<<<PAGE 403>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
30.2: Overrelaxation 391
butions areallGaussian isP(x;y)=exp( x2y2 x2 y2)=Z.
Overrelaxation forGaussian conditional distributions
Inordinary Gibbs sampling, onedrawsthenewvaluex(t+1)
iofthecurren t
variablexifromitsconditional distribution, ignoring theoldvaluex(t)
i.The
state makeslength yrandom walksincases where thevariables arestrongly
correlated, asillustrated intheleft-hand panel ofgure 30.3. Thisgure uses
acorrelated Gaussian distribution asthetarget densit y.
InAdler's (1981) overrelaxation metho d,oneinstead samplesx(t+1)
ifrom
aGaussian thatisbiased totheoppositesideoftheconditional distribution.
Iftheconditional distribution ofxiisNormal (;2)andthecurren tvalueof
xiisx(t)
i,thenAdler's metho dsetsxito
x(t+1)
i=+(x(t)
i )+(1 2)1=2; (30.8)
whereNormal (0;1)andisaparameter between 1and1,usually setto
anegativ evalue.(Ifispositive,thenthemetho discalled under-relaxation.)
Exercise 30.1.[2]Showthatthisindividual transition leavesinvariantthecon-
ditional distribution xiNormal (;2).
Asingle iteration ofAdler's overrelaxation, likeoneofGibbs sampling, updates
eachvariable inturnasindicated inequation (30.8). Thetransition matrix
T(x0;x)dened byacomplete updateofallvariables insomexedorder does
notsatisfy detailed balance. Eachindividual transition foronecoordinate
justdescrib eddoessatisfy detailed balance {sotheoverallchaingivesavalid
sampling strategy whichconverges tothetarget densit yP(x){butwhen we
formachainbyapplying theindividual transitions inaxedsequence, the
overallchainisnotreversible. Thistemporalasymmetry isthekeytowhy
overrelaxation canbebenecial. If,say,twovariables arepositivelycorrelated,
thentheywill(onashort timescale) evolveinadirected manner instead ofby
random walk,asshowningure 30.3. Thismaysignican tlyreduce thetime
required toobtain independen tsamples.
Exercise 30.2.[3]Thetransition matrixT(x0;x)dened byacomplete update
ofallvariables insome xedorder doesnotsatisfy detailed balance. If
theupdates wereinarandom order,thenTwouldbesymmetric. Inves-
tigate, forthetoytwo-dimensional Gaussian distribution, theassertion
thattheadvantages ofoverrelaxation arelostiftheoverrelaxed updates
aremade inarandom order.
OrderedOverrelaxation
Theoverrelaxation metho dhasbeengeneralized byNeal(1995) whose ordered
overrelaxation metho disapplicable toanysystem where Gibbs sampling is
used. Inordered overrelaxation, instead oftaking onesample fromthecondi-
tional distribution P(xijfxjgj6=i),wecreateKsuchsamplesx(1)
i;x(2)
i;:::;x(K)
i,
whereKmightbesettotwentyorso.Often generating K 1extra samples
addsanegligible computational costtotheinitial computations required for
making therstsample. Thepointsfx(k)
igarethensorted numerically ,and
thecurren tvalueofxiisinserted intothesorted list,giving alistofK+1
points.Wegivethem ranks 0;1;2;:::;K.Letbetherankofthecurren t
valueofxiinthelist.Wesetx0
itothevaluethatisanequal distance from

<<<PAGE 404>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
392 30|Ecien tMonteCarlo Metho ds
theother endofthelist,thatis,thevaluewithrankK .Theroleplayed
byAdler'sparameter ishereplayedbytheparameterK.WhenK=1,we
obtain ordinary Gibbs sampling. Forpractical purposesNealestimates that
ordered overrelaxation mayspeedupasimulation byafactor oftenortwenty.
30.3 Simulated annealing
Athird technique forspeeding convergence issimulated annealing .Insimu-
lated annealing, a`temp erature' parameter isintroduced which,when large,
allowsthesystem tomaketransitions thatwouldbeimprobable attemper-
ature 1.Thetemperature issettoalarge valueandgradually reduced to
1.Thisprocedure issupposedtoreduce thechance thatthesimulation gets
stuckinanunrepresen tativeprobabilit yisland.
Weasssume thatwewishtosample fromadistribution oftheform
P(x)=e E(x)
Z(30.9)
whereE(x)canbeevaluated. Inthesimplest simulated annealing metho d,
weinstead sample fromthedistribution
PT(x)=1
Z(T)e E(x)
T (30.10)
anddecreaseTgradually to1.
Often theenergy function canbeseparated intotwoterms,
E(x)=E0(x)+E1(x); (30.11)
ofwhichthersttermis`nice' (forexample, aseparable function ofx)andthe
second is`nasty'.Inthese cases, abettersimulated annealing metho dmight
makeuseofthedistribution
P0
T(x)=1
Z0(T)e E0(x) E1(x)/T(30.12)
withTgradually decreasing to1.Inthisway,thedistribution athightem-
peratures revertstoawell-behaveddistribution dened byE0.
Simulated annealing isoften usedasanoptimization metho d,where the
aimistondanxthatminimizesE(x),inwhichcasethetemperature is
decreased tozerorather thanto1.
AsaMonteCarlo metho d,simulated annealing asdescrib edabovedoesn't
sample exactly fromtherightdistribution, because there isnoguaran teethat
theprobabilit yoffalling intoonebasin oftheenergy isequal tothetotalprob-
abilityofallthestates inthatbasin. Theclosely related `simulated tempering'
metho d(Marinari andParisi,1992) corrects thebiases introduced bythean-
nealing processbymaking thetemperature itself arandom variable thatis
updated inMetrop olisfashion during thesimulation. Neal's (1998) `annealed
importance sampling' metho dremovesthebiases introduced byannealing by
computing importance weightsforeachgenerated point.
30.4 Skilling's multi-state leapfrog metho d
Afourth metho dforspeeding upMonteCarlo simulations, duetoJohn
Skilling, hasasimilar spirit tooverrelaxation, butworksinmore dimensions.
Thismetho disapplicable tosampling fromadistribution overacontinuous

<<<PAGE 405>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
30.4: Skilling's multi-state leapfrog metho d 393
statespace, andthesolerequiremen tisthattheenergyE(x)should beeasy
toevaluate. Thegradien tisnotused. Thisleapfrog metho disnotintended to
beusedonitsownbutrather insequence withother MonteCarlo operators.
Instead ofmovingjustonestate vectorxaround thestate space, aswas
thecaseforalltheMonteCarlo metho dsdiscussed thusfar,Skilling's leapfrog
metho dsimultaneously maintains asetofSstate vectorsfx(s)g,whereS
mightbesixortwelve.TheaimisthatallSofthese vectors willrepresen t
independen tsamples fromthesame distribution P(x).
Skilling's leapfrog makesaproposalforthenewstatex(s)0,whichisac-
cepted orrejected inaccordance withtheMetrop olismetho d,byleapfrogging
x(s)x(t)x(s)0thecurren tstatex(s)overanother statevectorx(t):
x(s)0=x(t)+(x(t) x(s))=2x(t) x(s): (30.13)
Alltheother statevectors areleftwhere theyare,sotheacceptance probabilit y
dependsonlyonthechange inenergy ofx(s).
Whichvector,t,isthepartner fortheleapfrog eventcanbechosen in
various ways.Thesimplest metho distoselect thepartner atrandom from
theother vectors. Itmightbebetter tochoosetbyselecting oneofthe
nearest neighboursx(s){nearest byanychosen distance function {aslong
asonethenusesanacceptance rulethatensures detailed balance bychecking
whether pointtisstillamong thenearest neighboursofthenewpoint,x(s)0.
Whytheleapfrogisagoodidea
Imagine thatthetarget densit yP(x)hasstrong correlations {forexample,
thedensit ymightbeaneedle-lik eGaussian withwidthandlengthL,where
L1.Aswehaveemphasized, motion around suchadensit ybystandard
metho dsproceeds byaslowrandom walk.
Imagine nowthatoursetofSpointsislurking initially inalocation that
isprobable under thedensit y,butinaninappropriately small ballofsize.
Now,under Skilling's leapfrog metho d,atypical rstmovewilltakethepoint
alittleoutside thecurren tball,perhaps doubling itsdistance fromthecentre
oftheball.After allthepointshavehadachance tomove,theballwillhave
increased insize;ifallthemovesareaccepted, theballwillbebigger bya
factor oftwoorsoinalldimensions. Therejection ofsome moveswillmean
thattheballcontaining thepointswillprobably haveelongated intheneedle's
longdirection byafactor of,say,two.After another cyclethrough thepoints,
theballwillhavegrowninthelongdirection byanother factor oftwo.Sothe
typical distance travelledinthelongdimension growsexponential lywiththe
numberofiterations.
Now,maybeafactor oftwogrowthperiteration isontheoptimistic side;
buteveniftheballonlygrowsbyafactor of,let'ssay,1.1periteration, the
growthisnevertheless exponential.Itwillonlytakeanumberofiterations
proportional tologL=log(1:1)forthelongdimension tobeexplored.
.Exercise 30.3.[2,p.398]Discuss howtheeectiv enessofSkilling's metho dscales
withdimensionalit y,using acorrelatedN-dimensional Gaussian distri-
bution asanexample. Findanexpression fortherejection probabilit y,
assuming theMarkovchainisatequilibrium. Alsodiscuss howitscales
withthestrength ofcorrelation among theGaussian variables. [Hint:
Skilling's metho disinvariantunder ane transformations, sotherejec-
tionprobabilit yatequilibrium canbefound bylooking atthecaseofa
separableGaussian.]

<<<PAGE 406>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
394 30|Ecien tMonteCarlo Metho ds
Thismetho dhassomesimilarit ytothe`adaptiv edirection sampling' metho d
ofGilksetal.(1994) buttheleapfrog metho dissimpler andcanbeapplied
toagreater varietyofdistributions.
30.5 MonteCarlo algorithms ascomm unication channels
Itmaybeahelpful perspective,when thinking aboutspeeding upMonteCarlo
metho ds,tothink abouttheinformation thatisbeingcomm unicated. Two
comm unications takeplace when asample fromP(x)isbeinggenerated.
First, theselection ofaparticular xfromP(x)necessarily requires that
atleastlog1=P(x)random bitsbeconsumed. [Recall theuseofinversearith-
metic codingasametho dforgenerating samples from givendistributions
(section 6.3).]
Second, thegeneration ofasample conveysinformation aboutP(x)from
thesubroutine thatisabletoevaluateP(x)(andfromanyother subroutines
thathaveaccess toproperties ofP(x)).
Consider adumbMetrop olismetho d,forexample. InadumbMetrop olis
metho d,theproposalsQ(x0;x)havenothing todowithP(x).Properties
ofP(x)areonlyinvolvedinthealgorithm attheacceptance step,when the
ratioP(x0)=P(x)iscomputed. Thechannel fromthetruedistribution P(x)
totheuserwhoisinterested incomputing properties ofP(x)thuspasses
through abottlenec k:alltheinformation aboutPisconveyedbythestring of
acceptances andrejections. IfP(x)werereplaced byadieren tdistribution
P2(x),theonlywayinwhichthischange wouldhaveaninuence isthatthe
string ofacceptances andrejections wouldbechanged. Iamnotawareofmuch
usebeingmade ofthisinformation-theoretic viewofMonteCarlo algorithms,
butIthink itisaninstructiv eviewp oint:iftheaimistoobtain information
aboutproperties ofP(x)thenpresumably itishelpful toidentifythechannel
through whichthisinformation ows,andmaximize therateofinformation
transfer.
Example 30.4. Theinformation-theoretic viewp ointoers asimple justication
forthewidely-adopted ruleofthumb,whichstates thattheparameters of
adumbMetrop olismetho dshould beadjusted suchthattheacceptance
rateisaboutonehalf. Let's calltheacceptance history ,thatis,the
binary string ofaccept orreject decisions, a.Theinformation learned
aboutP(x)afterthealgorithm hasrunforTstepsislessthanorequal to
theinformation contentofa,sinceallinformation aboutPismediated
bya.Andtheinformation contentofaisupper-bounded byTH2(f),
wherefistheacceptance rate. Thisbound oninformation acquired
aboutPismaximized bysettingf=1=2.
Another helpful analogy foradumbMetrop olismetho disanevolutionary
one.Eachproposalgenerates aprogen yx0fromthecurren tstatex.These two
individuals thencompetewitheachother, andtheMetrop olismetho dusesa
noisysurviv al-of-the-ttest rule.Iftheprogen yx0istterthantheparent(i.e.,
P(x0)>P(x),assuming theQ=Qfactor isunity)thentheprogen yreplaces
theparent.Thesurviv alrulealsoallowsless-t progen ytoreplace theparent,
sometimes. Insigh tsabouttherateofevolution canthusbeapplied toMonte
Carlo metho ds.
Exercise 30.5.[3]Letx2f0;1gGandletP(x)beaseparable distribution,
P(x)=Y
gp(xg); (30.14)

<<<PAGE 407>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
30.6: Multi-state metho ds 395
withp(0)=p0andp(1)=p1,forexamplep1=0:1.Lettheproposal
densit yofadumbMetrop olisalgorithmQinvolveipping afractionm
oftheGbitsinthestatex.Analyze howlongittakesforthechainto
convergetothetarget densit yasafunction ofm.Findtheoptimalm
anddeduce howlongtheMetrop olismetho dmustrunfor.
Compare theresult withtheresults foranevolving population under
natural selection found inChapter 19.
Theinsigh tthatthefastest progress thatastandard Metrop olismetho d
canmake,ininformation terms, isaboutonebitperiteration, givesastrong
motivation forspeeding upthealgorithm. Thischapter hasalready review ed
severalmetho dsforreducing random-w alkbehaviour. Dothese metho dsalso
speeduptherateatwhichinformation isacquired?
Exercise 30.6.[4]DoesGibbs sampling, whichisasmart Metrop olismetho d
whose proposaldistributions dodependonP(x),allowinformation about
P(x)toleakoutataratefaster thanonebitperiteration? Findtoy
examples inwhichthisquestion canbeprecisely investigated.
Exercise 30.7.[4]Hamiltonian MonteCarlo isanother smart Metrop olismetho d
inwhichtheproposaldistributions dependonP(x).CanHamiltonian
MonteCarlo extract information aboutP(x)ataratefaster thanone
bitperiteration?
Exercise 30.8.[5]Inimportance sampling, theweightwr=P(x(r))=Q(x(r)),
aoating-p ointnumber,iscomputed andretained untiltheendofthe
computation. Incontrast, inthedumbMetrop olismetho d,theratio
a=P(x0)=P(x)isreduced toasingle bit(`isabigger thanorsmaller
thantherandom numberu?').Thusinprinciple importance sampling
preserv esmore information aboutPthandoesdumbMetrop olis.Can
youndatoyexample inwhichthisextra information doesindeed lead
tofaster convergence ofimportance sampling thanMetrop olis? Can
youdesign aMarkovchainMonteCarlo algorithm thatmovesaround
adaptiv ely,likeaMetrop olismetho d,andthatretains more useful in-
formation aboutthevalueofP,likeimportance sampling?
InChapter 19wenoticed thatanevolving population ofNindividuals can
makefaster evolutionary progress iftheindividuals engage insexual reproduc-
tion.Thisobserv ation motivateslooking atMonteCarlo algorithms inwhich
multiple parameter vectors xareevolvedandinteract.
30.6 Multi-state metho ds
Inamulti-state metho d,multiple parameter vectors xaremaintained; they
evolveindividually under movessuchasMetrop olisandGibbs; there arealso
interactions among thevectors. Theintentioniseither thateventually allthe
vectors xshould besamples fromP(x)(asillustrated bySkilling's leapfrog
metho d),orthatinformation associated withthenalvectors xshould allow
ustoapproximate expectations underP(x),asinimportance sampling.
Genetic methods
Genetic algorithms arenotoftendescrib edbytheirproponentsasMonteCarlo
algorithms, butIthink thisisthecorrect categorization, andanidealgenetic

<<<PAGE 408>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
396 30|Ecien tMonteCarlo Metho ds
algorithm wouldbeonethatcanbeprovedtobeavalidMonteCarlo algorithm
thatconverges toaspecied densit y.
I'lluseRtodenote thenumberofvectors inthepopulation. Weaimto
haveP(fx(r)gR
1)=QP(x(r)).Agenetic algorithm involvesmovesoftwoor
three types.
First, individual movesinwhichonestatevectorisperturb ed,x(r)!x(r)0,
whichcould beperformed using anyoftheMonteCarlo metho dswehave
mentioned sofar.
Second, weallowcrosso vermovesoftheformx;y!x0;y0;inatypical
crosso vermove,theprogen yx0receiveshalfhisstatevector fromoneparent,
x,andhalffromtheother, y;thesecret ofsuccess inagenetic algorithm is
thattheparameter xmustbeencodedinsuchawaythatthecrosso verof
twoindependen tstates xandy,bothofwhichhavegoodtnessP,should
haveareasonably goodchance ofproducing progen ywhoareequally t.This
constrain tisahardonetosatisfy inmanyproblems, whichiswhygenetic
algorithms aremainly talkedaboutandhypedup,andrarely usedbyserious
experts.Havingintroduced acrosso vermovex;y!x0;y0,weneedtochoose
anacceptance rule.Oneeasywaytoobtain avalidalgorithm istoaccept or
reject thecrosso verproposalusing theMetrop olisrulewithP(fx(r)gR
1)as
thetarget densit y{thisinvolvescomparing thetnesses beforeandafterthe
crosso verusing theratio
P(x0)P(y0)
P(x)P(y): (30.15)
Ifthecrosso veroperator isreversible thenwehaveaneasyproofthatthis
procedure satises detailed balance andsoisavalidcomponentinachain
converging toP(fx(r)gR
1).
.Exercise 30.9.[3]Discuss whether theabovetwooperators, individual varia-
tionandcrosso verwiththeMetrop olisacceptance rule,willgiveamore
ecien tMonteCarlo metho dthanastandard metho dwithonlyone
statevector andnocrosso ver.
Thereason whythesexual comm unitycould acquire information faster than
theasexual comm unityinChapter 19wasbecause thecrosso veroperation
produced diversitywithstandard deviationp
G,thentheBlind Watchmaker
wasabletoconveylotsofinformation aboutthetness function bykilling
othelesstospring. Theabovetwooperators donotoeraspeed-up ofp
Gcompared withstandard MonteCarlo metho dsbecause thereisnokilling.
What's required, inorder toobtain aspeed-up, istwothings: multiplication
anddeath; andatleastoneofthese mustoperateselectively .Either wemust
killotheless-t statevectors, orwemustallowthemore-t statevectors to
giverisetomore ospring. While it'seasytosketchthese ideas, itishardto
dene avalidmetho dfordoing it.
Exercise 30.10.[5]Design abirth ruleandadeath rulesuchthatthechain
converges toP(fx(r)gR
1).
Ibelievethisisstillanopenresearc hproblem.
Particle lters
Particle lters, whichareparticularly popular ininference problems involving
temporaltracking, aremultistate metho dsthatmixtheideas ofimportance
sampling andMarkovchainMonteCarlo. SeeIsard andBlake(1996), Isard
andBlake(1998), Berzuini etal.(1997), Berzuini andGilks (2001), Doucet
etal.(2001).

<<<PAGE 409>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
30.7: Metho dsthatdonotnecessarily help 397
30.7 Metho dsthatdonotnecessarily help
Itiscommon practice tousemany initial conditions foraparticular Markov
chain(gure 29.19). Ifyouareworried aboutsampling wellfromacomplicated
densit yP(x),canyouensure thestates produced bythesimulations arewell
distributed aboutthetypical setofP(x)byensuring thattheinitial points
are`welldistributed aboutthewhole statespace' ?
Theansweris,unfortunately ,no.Inhierarc hical Bayesian models,for
example, alargenumberofparametersfxngmaybecoupled together viaan-
other parameter(knownasahyperparameter). Forexample, thequantities
fxngmightbeindependen tnoisesignals, andmightbetheinverse-variance
ofthenoise source. Thejointdistribution ofandfxngmightbe
P(;fxng)=P()NY
n=1P(xnj)
=P()NY
n=11
Z()e x2
n=2;
whereZ()=p
2=andP()isabroad distribution describing ourigno-
rance aboutthenoiselevel.Forsimplicit y,let'sleaveoutalltheother variables
{dataandsuch{thatmightbeinvolvedinarealistic problem. Let'simagine
thatwewanttosample eectiv elyfromP(;fxng)byGibbs sampling {alter-
nately samplingfromtheconditional distribution P(jxn)thensampling all
thexnfromtheirconditional distributions P(xnj).[Theresulting marginal
distribution ofshould asymptotically bethebroad distribution P().]
IfNislarge thentheconditional distribution ofgivenanyparticular
setting offxngwillbetightlyconcen trated onaparticular most-probable value
of,withwidth proportional to1=p
N.Progress upanddownthe-axiswill
therefore takeplace byaslowrandom walkwithsteps ofsize/1=p
N.
So,totheinitialization strategy .Canwenesse ourslowconvergence
problem byusing initial conditions located `alloverthestate space' ?Sadly,
no.Ifwedistribute thepointsfxngwidely ,what weareactually doing is
favouring aninitial valueofthenoise level1=thatislarge.Therandom
walkoftheparameter willthustend, after therstdrawing offrom
P(jxn),alwaystostartofromoneendofthe-axis.
Further reading
TheHamiltonian MonteCarlo metho disreview edinNeal(1993b). Thisexcel-
lenttomealsoreviews ahugerange ofother MonteCarlo metho ds,including
therelated topics ofsimulated annealing andfreeenergy estimation.
30.8 Further exercises
Exercise 30.11.[4]Animportantdetail oftheHamiltonian MonteCarlo metho d
isthatthesimulation oftheHamiltonian dynamics, while itmaybe
inaccurate, mustbeperfectly reversible, inthesense thatiftheini-
tialcondition (x;p)!(x0;p0),thenthesame simulator musttake
(x0; p0)!(x; p),andtheinaccurate dynamics mustconserv estate-
space volume. [Theleapfrog metho dinalgorithm 30.1satises these
rules.]
Explain whythese rulesmustbesatised andcreate anexample illus-
trating theproblems thatariseiftheyarenot.

<<<PAGE 410>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
398 30|Ecien tMonteCarlo Metho ds
Exercise 30.12.[4]Amulti-state ideaforslicesampling. Investigate thefollow-
ingmulti-state metho dforslicesampling. AsinSkilling's multi-state
leapfrog metho d(section 30.4), maintainasetofSstatevectorsfx(s)g.
Updateonestatevectorx(s)byone-dimensional slicesampling inadi-
rection ydetermined bypickingtwoother state vectors x(v)andx(w)
atrandom andsetting y=x(v) x(w).Investigate thismetho dontoy
x(s)x(v)
x(w)problems suchasahighly-correlated multivariate Gaussian distribution.
Bearinmind thatifS 1issmaller thanthenumberofdimensions
Nthenthismetho dwillnotbeergodicbyitself, soitmayneedtobe
mixed withother metho ds.Arethereclasses ofproblems thatarebetter
solvedbythisslicesampling metho dthanbythestandard metho dsfor
pickingysuchascycling through thecoordinate axesorpickinguat
random fromaGaussian distribution?
30.9 Solutions
Solution toexercise 30.3(p.393).Consider thespherical Gaussian distribution
where allcomponentshavemean zeroandvariance 1.Inonedimension, the
nth,ifx(1)
nleapfrogs overx(2)
n,weobtain theproposedcoordinate
(x(1)
n)0=2x(2)
n x(1)
n: (30.16)
Assuming thatx(1)
nandx(2)
nareGaussian random variables fromNormal(0;1),
(x(1)
n)0isGaussian fromNormal (0;2),where2=22+( 1)2=5.Thechange
inenergy contributed bythisonedimension willbe
1
2h
(2x(2)
n x(1)
n)2 (x(1)
n)2i
=2(x(2)
n)2 2x(2)
nx(1)
n (30.17)
sothetypical change inenergy is2h(x(2)
n)2i=2.Thispositivechange isbad
news. InNdimensions, thetypical change inenergy when aleapfrog moveis
made, atequilibrium, isthus+2N.Theprobabilit yofacceptance ofthemove
scales as
e 2N: (30.18)
Thisimplies thatSkilling's metho d,asdescrib ed,isnoteectiv einveryhigh-
dimensional problems {atleast, notonceconvergence hasoccured. Never-
theless ithastheimpressiv eadvantagethatitsconvergence properties are
independen tofthestrength ofcorrelations betweenthevariables {aproperty
thatnoteventheHamiltonian MonteCarlo andoverrelaxation metho dsoer.

<<<PAGE 411>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
AboutChapter 31
Some oftheneural networkmodelsthatwewillencoun terarerelated toIsing
models,whichareidealized magnetic systems. Itisnotessentialtounderstand
thestatistical physicsofIsingmodelstounderstand theseneural networks,but
Ihopeyou'llndthem helpful.
Isingmodelsarealsorelated toseveralother topics inthisbook.Wewill
useexact tree-based computation metho dslikethose introduced inChapter
25toevaluate properties ofinterest inIsingmodels.Isingmodelsoercrude
modelsforbinary images. AndIsingmodelsrelate totwo-dimensional con-
strained channels (c.f.Chapter 17):atwo-dimensional bar-co deinwhicha
blackdotmaynotbecompletely surrounded byblackdotsandawhite dot
maynotbecompletely surrounded bywhite dotsissimilar toanantiferro-
magnetic Isingmodelatlowtemperature. Evaluating theentropyofthisIsing
modelisequivalenttoevaluating thecapacit yoftheconstrained channel for
conveying bits.
Ifyouwouldliketojogyourmemory onstatistical physicsandthermo dy-
namics, youmightndAppendix Bhelpful. Ialsorecommend thebookby
Reif(1965).
399

<<<PAGE 412>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
31
IsingModels
AnIsingmodelisanarrayofspins(e.g., atoms thatcantakestates1)that
aremagnetically coupled toeachother. Ifonespinis,say,inthe+1state
thenitisenergetically favourable foritsimmediate neighbourstobeinthe
samestate, inthecaseofaferromagnetic model,andintheoppositestate, in
thecaseofanantiferromagnet. Inthischapter wediscuss twocomputational
techniques forstudying Isingmodels.
LetthestatexofanIsingmodelwithNspins beavector inwhicheach
componentxntakesvalues 1or+1.Iftwospinsmandnareneighbourswe
write (m;n)2N.Thecoupling betweenneighbouring spins isJ.Wedene
Jmn=JifmandnareneighboursandJmn=0otherwise. Theenergy ofa
statexis
E(x;J;H)= "
1
2X
m;nJmnxmxn+X
nHxn#
; (31.1)
whereHistheapplied eld. IfJ>0thenthemodelisferromagnetic, and
ifJ<0itisantiferromagnetic. We'veincluded thefactor of1/2because each
pairiscountedtwiceintherstsum,onceas(m;n)andonceas(n;m).At
equilibrium attemperatureT,theprobabilit ythatthestateisxis
P(xj;J;H)=1
Z(;J;H)exp[ E(x;J;H)]; (31.2)
where=1=kBT,kBisBoltzmann's constan t,and
Z(;J;H)X
xexp[ E(x;J;H)]: (31.3)
Relevanc eofIsingmodels
Isingmodelsarerelevantforthree reasons.
Isingmodelsareimportantrstasmodelsofmagnetic systems thathave
aphase transition. Thetheory ofuniversalit yinstatistical physicsshowsthat
allsystems withthesame dimension (here, two),andthesame symmetries,
haveequivalentcritical properties, i.e.,thescaling lawsshownbytheirphase
transitions areidentical. Sobystudying Isingmodelswecanndoutnotonly
aboutmagnetic phase transitions butalsoaboutphase transitions inmany
other systems.
Second, ifwegeneralize theenergy function to
E(x;J;h)= "
1
2X
m;nJmnxmxn+X
nhnxn#
; (31.4)
where thecouplingsJmnandapplied eldshnarenotconstan t,weobtain
afamily ofmodelsknownas`spin glasses' tophysicists, andas`Hopeld
400

<<<PAGE 413>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
31|IsingModels 401
networks'or`Boltzmann machines' totheneural networkcomm unity.Insome
ofthese models,allspinsaredeclared tobeneighboursofeachother, inwhich
casephysicists callthesystem an`innite range' spinglass, andnetworkers
callita`fully connected' network.
Third, theIsingmodelisalsouseful asastatistical modelinitsownright.
Inthischapter wewillstudy Isingmodelsusing twodieren tcomputational
techniques.
Some remarkable relationships instatistic alphysics
Wewouldliketogetasmuchinformation aspossible outofourcomputations.
Consider forexample theheatcapacit yofasystem, whichisdened tobe
C@
@TE; (31.5)
where
E=1
ZX
xexp( E(x))E(x): (31.6)
Toworkouttheheatcapacit yofasystem, wemightnaivelyguess thatwehave
toincrease thetemperature andmeasure theenergy change. Heatcapacit y,
however,isintimately related toenergy uctuations atconstan ttemperature.
Let'sstartfromthepartition function,
Z=X
xexp( E(x)): (31.7)
Themean energy isobtained bydieren tiation withrespectto:
@lnZ
@=1
ZX
x E(x)exp( E(x))= E: (31.8)
Afurther dieren tiation spitsoutthevariance oftheenergy:
@2lnZ
@2=1
ZX
xE(x)2exp( E(x)) E2=hE2i E2=var(E):(31.9)
Buttheheatcapacit yisalsothederivativeofEwithrespecttotemperature:
@E
@T= @
@T@lnZ
@= @2lnZ
@2@
@T= var(E)( 1=kBT2): (31.10)
Soforanysystem attemperatureT,
C=var(E)
kBT2=kB2var(E): (31.11)
Thusifwecanobserv ethevariance oftheenergy ofasystem atequilibrium,
wecanestimate itsheatcapacit y.
Indthisanalmost parado xical relationship. Consider asystem with
anite setofstates, andimagine heating itup.Athightemperature, all
states willbeequiprobable, sothemean energy willbeessentially constan t
andtheheatcapacit ywillbeessentially zero. Butontheother hand, with
allstates beingequiprobable, there willcertainly beuctuations inenergy .
Sohowcantheheatcapacit yberelated totheuctuations? Theansweris
inthewords`essentially zero'above.Theheatcapacit yisnotquite zeroat
hightemperature, itjusttends tozero. Andittends tozeroasvar(E)
kBT2,with
thequantityvar(E)tending toaconstan tathightemperatures. This1=T2

<<<PAGE 414>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
402 31|IsingModels
behaviour oftheheatcapacit yofnite systems athightemperatures isthus
verygeneral.
The1=T2factor canbeviewedasanacciden tofhistory .Ifonlytem-
perature scales hadbeendened using=1
kBT,thenthedenition ofheat
capacit ywouldbe
C()@E
@=var(E); (31.12)
andheatcapacit yanductuations wouldbeidenticalquantities.
.Exercise 31.1.[2][Wewillcalltheentropyofaphysical systemSrather than
H,while weareinastatistical physicschapter; forconvenience wewill
setkB=1.]
Theentropyofasystem whose states arex,attemperatureT=1=,is
S=X
p(x)[ln1=p(x)] (31.13)
where
p(x)=1
Z()exp[ E(x)]: (31.14)
(a)Showthat
S=lnZ()+E() (31.15)
where E()isthemean energy ofthesystem.
(b)Showthat
S= @F
@T; (31.16)
where thefreeenergyF= kTlnZandkT=1=.
31.1 Isingmodels{MonteCarlo simulation
Inthissection westudy two-dimensional planar Isingmodelsusing asimple
Gibbs sampling metho d.Starting fromsome initial state, aspinnisselected
atrandom, andtheprobabilit ythatitshould be+1giventhestate ofthe
other spins andthetemperature iscomputed,
P(+1jbn)=1
1+exp( 2bn); (31.17)
where=1=kBTandbnisthelocaleld
bn=X
m:(m;n)2NJxm+H: (31.18)
[Thefactor of2appearsinequation (31.17) because thetwospinstates are
f+1; 1grather thanf+1;0g.]Spinnissetto+1withthatprobabilit y,
andotherwise to 1;thenthenextspintoupdateisselected atrandom.
After sucien tlymanyiterations, thisprocedure convergestotheequilibrium
distribution (31.2). Analternativ etotheGibbs sampling formula(31.17) is
theMetrop olisalgorithm, inwhichweconsider thechange inenergy that
results fromipping thechosen spinfromitscurren tstatexn,
E=2xnbn; (31.19)
andadopt thischange inconguration withprobabilit y
P(accept ;E;)=(
1 E0
exp( E)E>0:(31.20)

<<<PAGE 415>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
31.1: Isingmodels{MonteCarlo simulation 403
Thisprocedure hasroughly double theprobabilit yofaccepting energetically
unfavourable moves,somaybeamoreecien tsampler {butatverylowtem-
peratures therelativ emerits ofGibbs sampling andtheMetrop olisalgorithm
maybesubtle.bbbb
bbbb
bbbb
bbbb
bbbb
bbbb
Figure 31.1.Rectangular Ising
model.Rectangular geometry
Irstsimulated anIsingmodelwiththerectangular geometry showning-
ure31.1, andwithperiodicboundary conditions. Alinebetweentwospins
indicates thattheyareneighbours. Isettheexternal eldH=0andcon-
sidered thetwocasesJ=1whichareaferromagnet andantiferromagnet
respectively.
Istarted atalargetemperature (T=33;=0:03)andchanged thetemper-
ature everyIiterations, rstdecreasing itgradually toT=0:1;=10,then
increasing itgradually backtoalarge temperature again. Thisprocedure
givesacrude checkonwhether `equilibrium hasbeenreached'ateachtem-
perature; ifnot,we'dexpecttoseesome hysteresis inthegraphs weplot. It
alsogivesanideaofthereproducibilit yoftheresults, ifweassume thatthetwo
runs,withdecreasing andincreasing temperature, areeectiv elyindependen t
ofeachother.
Ateachtemperature Irecorded themean energy perspinandthestandard
deviation oftheenergy ,andthemean square valueofthemagnetization m,
m=1
NX
nxn: (31.21)
Onetrickydecision thathastobemade ishowsoontostart taking theseT
5
2.5
2.4
2.3
2
Figure 31.2.Sample states of
rectangular Isingmodelswith
J=1atasequence of
temperaturesT.measuremen tsafteranewtemperature hasbeenestablished; itisdicult to
detect `equilibrium' {oreventogiveacleardenition ofasystem's being`at
equilibrium' ![ButinChapter 32wewillseeasolution tothisproblem.] My
crude strategy wastoletthenumberofiterations ateachtemperature,I,be
afewhundred times thenumberofspinsN,andtodiscard therst1/3of
those iterations. WithN=100,Ifound Ineeded morethan100000iterations
toreachequilibrium atanygiventemperature.
Results forsmallNwithJ=1.
Isimulated anllgridforl=4;5;:::;10;40;64.Let's haveaquickthink
aboutwhat results weexpect.Atlowtemperatures thesystem isexpected
tobeinaground state. Therectangular Ising modelwithJ=1hastwo
ground states, theall+1state andtheall 1state. Theenergy perspinof
either ground state is 2.Athightemperatures, thespins areindependen t,
allstates areequally probable, andtheenergy isexpected touctuate around
amean of0withastandard deviation proportional to1=p
N.
Let's lookatsome results. Inallgures temperatureTisshownwith
kB=1.Thebasic picture emerges withasfewas16spins (gure 31.3,
top): theenergy risesmonotonically .Asweincrease thenumberofspins to
100(gure 31.3, bottom) some newdetails emerge. First, asexpected, the
uctuations atlargetemperature decrease as1=p
N.Second, theuctuations
atintermediate temperature become relativ elybigger .Thisisthesignature
ofa`collectiv ephenomenon', inthiscase,aphase transition. Only systems
withinniteNshowtruephase transitions, butwithN=100wearegetting
ahintofthecritical uctuations. Figure 31.5showsdetails ofthegraphs for
N=100andN=4096. Figure 31.2showsasequence oftypical states from
thesimulation ofN=4096spins atasequence ofdecreasing temperatures.

<<<PAGE 416>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
404 31|IsingModels
N Mean energy anductuations Mean square magnetization
16
-2-1.5-1-0.500.5
1 10Energy
Temperature00.20.40.60.81
1 10Mean Square Magnetization
Temperature
100
-2-1.5-1-0.500.5
1 10Energy
Temperature00.20.40.60.81
1 10Mean Square Magnetization
TemperatureFigure 31.3.MonteCarlo
simulations ofrectangular Ising
modelswithJ=1.Mean energy
anductuations inenergy asa
function oftemperature (left).
Mean square magnetization asa
function oftemperature (right).
Inthetoprow,N=16,andthe
bottom,N=100.Forevenlarger
N,seelatergures.
ContrastwithSchottky anomaly
T
Figure 31.4.Schematic diagram to
explain themeaning ofaSchottky
anomaly. Thecurveshowsthe
heatcapacit yoftwogases asa
function oftemperature. The
lowercurveshowsanormal gas
whose heatcapacit yisan
increasing function of
temperature. Theuppercurvehas
asmall peakintheheatcapacit y,
whichisknownasaSchottky
anomaly (atleastinCambridge).
Thepeakisproduced bythegas
havingmagnetic degrees of
freedom withanite numberof
accessible states.Apeakintheheatcapacit y,asafunction oftemperature, occursinanysystem
thathasanite numberofenergy levels;apeakisnotinitselfevidence ofa
phase transition. Suchpeakswereviewedasanomalies inclassical thermo dy-
namics, since`normal' systems withinnite numbersofenergy levels(suchas
aparticle inabox)haveheatcapacities thatareeither constan torincreasing
functions oftemperature. Incontrast, systems withanite numberoflevels
produced small blipsintheheatcapacit ygraph (gure 31.4).
Letusrefresh ourmemory ofthesimplest suchsystem, atwo-levelsystem
withstatesx=0(energy 0)andx=1(energy).Themean energy is
E()=exp( )
1+exp( )=1
1+exp()(31.22)
andthederivativewithrespecttois
dE=d= 2exp()
[1+exp()]2: (31.23)
Sotheheatcapacit yis
C=dE=dT= dE
d1
kBT2=2
kBT2exp()
[1+exp()]2(31.24)
andtheuctuations inenergy aregivenbyvar(E)=CkBT2= dE=d,
whichwasevaluated in(31.23). Theheatcapacit yanductuations areplotted
ingure 31.6. Thetake-home message atthispointisthatwhilst Schottky
anomalies dohaveapeakintheheatcapacit y,there isnopeakintheir
uctuations ;thevariance oftheenergy simply increases monotonically with
temperature toavalueproportional tothenumberofindependen tspins. Thus
itisapeakintheuctuations thatisinteresting, rather thanapeakinthe
heatcapacit y.TheIsingmodelhassuchapeakinitsuctuations, ascanbe
seeninthesecond rowofgure 31.5.
Rectangular IsingmodelwithJ= 1
What doweexpecttohappeninthecaseJ= 1?Theground states ofan
innite system arethetwocheckerboardpatterns (gure 31.7), andtheyhave

<<<PAGE 417>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
31.1: Isingmodels{MonteCarlo simulation 405
N=100 N=4096
(a)-2-1.5-1-0.50
2 2.5 33.544.55Energy
-2-1.5-1-0.50
2 2.5 33.544.55
(b)0.080.10.120.140.160.180.20.220.240.260.28
2 2.5 33.544.55sd of Energy
0.0150.020.0250.030.0350.040.0450.05
2 2.5 33.544.55
(c)00.20.40.60.81
2 2.5 33.544.55Mean Square Magnetization
00.20.40.60.81
2 2.5 33.544.55
(d)00.20.40.60.811.21.41.6
2 2.5 33.544.55Heat Capacity
00.20.40.60.811.21.41.61.8
2 2.5 33.544.55Figure 31.5.Detail ofMonteCarlo
simulations ofrectangular Ising
modelswithJ=1.(a)Mean
energy anductuations inenergy
asafunction oftemperature. (b)
Fluctuations inenergy (standard
deviation). (c)Mean square
magnetization. (d)Heatcapacit y.
00.050.10.150.20.250.30.350.40.45
0.1 1 10
TemperatureHeat Capacity
Var(E)Figure 31.6.Schottky anomaly {
Heatcapacit yanductuations in
energy asafunction of
temperature foratwo-levelsystem
withseparation=1andkB=1.

<<<PAGE 418>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
406 31|IsingModels
energy perspin 2,liketheground states oftheJ=1model.Canthisanalogy
bepressed further?A momen t'sreection willconrm thatthetwosystems
Figure 31.7.Thetwoground
states ofarectangular Isingmodel
withJ= 1.
J= 1J=+1
Figure 31.8.Twostates of
rectangular Isingmodelswith
J=1thathaveidenticalenergy .areequivalenttoeachother under acheckerboardsymmetry operation. Ifyou
takeaninniteJ=1system insome state andipallthespins thatlieon
theblacksquares ofaninnite checkerboard, andsetJ= 1(gure 31.8),
thentheenergy isunchanged. (Themagnetization changes, ofcourse.) Soall
thermo dynamic properties ofthetwosystems areexpected tobeidenticalin
thecaseofzeroapplied eld.
Butthere isasubtlet ylurking here. Haveyouspotted it?Wearesimu-
lating nite gridswithperiodicboundary conditions. Ifthesizeofthegridin
anydirection isodd,thenthecheckerboardoperation isnolonger asymme-
tryoperation relatingJ=+1toJ= 1,because thecheckerboarddoesn't
matchupattheboundaries. Thismeans thatforsystems ofoddsize,the
ground state ofasystem withJ= 1willhavedegeneracy greater than2,
andtheenergy ofthose ground states willnotbeaslowas 2perspin.Sowe
expectqualitativ edierences betweenthecasesJ=1inoddsizedsystems.
These dierences areexpected tobemostprominen tforsmall systems. The
frustrations areintroduced bytheboundaries, andthelength oftheboundary
growsasthesquare rootofthesystem size,sothefractional inuence ofthis
boundary-related frustration ontheenergy andentropyofthesystem willde-
crease as1=p
N.Figure 31.9compares theenergies oftheferromagnetic and
antiferromagnetic modelswithN=25.Here, thedierence isstriking.
J=+1 J= 1
-2-1.5-1-0.500.5
1 10Energy
Temperature-2-1.5-1-0.500.5
1 10
TemperatureFigure 31.9.MonteCarlo
simulations ofrectangular Ising
modelswithJ=1andN=25.
Mean energy anductuations in
energy asafunction of
temperature. (a)J=1.(b)
J= 1.
Triangular Isingmodel
bbbb

HHHHHHHH
bbbb

HHHHHHHH
bbbb

HHHHHHHH
bbbb

HHHHHHHH
bbbb

HHHHHHHH
bbbb

HHHHHHHH
+1 +1-1
+1 +1+1
(a) (b)
Figure 31.10.Inan
antiferromagnetic triangular Ising
model,anythree neighbouring
spins arefrustrated. Oftheeight
possible congurations ofthree
spins, sixhaveenergy jJj(a),
andtwohaveenergy 3jJj(b).Wecanrepeatthese computations foratriangular Isingmodel.Doweexpect
thetriangular IsingmodelwithJ=1toshowdieren tphysical properties
fromtherectangular Ising model?Presumably theJ=1modelwillhave
broadly similar properties toitsrectangular counterpart. ButthecaseJ= 1
isradically dieren tfromwhat's gonebefore. Think aboutit:thereisno
unfrustr atedgroundstate;inanystate, theremust befrustrations {pairsof
neighbourswhohavethesame signaseachother. Unlikethecaseofthe
rectangular modelwithoddsize,thefrustrations arenotintroduced bythe
periodicboundary conditions. Every setofthreemutual lyneighb ouring spins
mustbeinastateoffrustration, asshowningure 31.10. (Solid linesshow
`happy'couplings whichcontribute jJjtotheenergy; dashed linesshow
`unhapp y'couplings whichcontributejJj.)Thuswecertainly expectdieren t
behaviour atlowtemperatures. Infactwemightexpectthissystem tohave
anon-zero entropyatabsolute zero. (`Triangular modelviolates third lawof
thermo dynamics!')
Let's lookatsome results. Sample states areshowningure 31.12, and
gure 31.11 showstheenergy ,uctuations, andheatcapacit yforN=4096.

<<<PAGE 419>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
31.2: Direct computation ofpartition function ofIsingmodels 407
Note howdieren ttheresults forJ=1are.There isnopeakatallin
thestandard deviation oftheenergy inthecaseJ= 1.Thisindicates that
theantiferromagnetic system doesnothaveaphase transition toastatewith
long-range order.
J=+1 J= 1
(a)-3-2.5-2-1.5-1-0.500.5
1 10Energy
Temperature (d)-3-2.5-2-1.5-1-0.500.5
1 10Energy
Temperature
(b)00.010.020.030.040.050.060.070.08
1 10sd of Energy
Temperature (e)00.0050.010.0150.020.0250.03
1 10sd of Energy
Temperature
(c)00.20.40.60.811.21.41.6
1 10Heat Capacity
Temperature (f)00.050.10.150.20.25
1 10Heat Capacity
TemperatureFigure 31.11.MonteCarlo
simulations oftriangular Ising
modelswithJ=1and
N=4096. (a{c)J=1.(d{f)
J= 1.(a,d)Mean energy and
uctuations inenergy asa
function oftemperature. (b,e)
Fluctuations inenergy (standard
deviation). (c,f)Heatcapacit y.
31.2 Direct computation ofpartition function ofIsingmodels
Wenowexamine acompletely dieren tapproac htoIsingmodels.Thetrans-
fermatrix metho disanexact andabstract approac hthatobtains physical
properties ofthemodelfromthepartition function
Z(;J;b)X
xexp[ E(x;J;b)] (31.25)
where thesummation isoverallstates x,andtheinversetemperature is
=1=T.[Asusual, LetkB=1.]Thefreeenergy isgivenbyF= 1
lnZ.
Thenumberofstates is2N,sodirect computation ofthepartition function
isnotpossible forlargeN.Toavoidenumerating allglobal states explicitly ,
wecanuseatricksimilar tothesum{pro ductalgorithm discussed inChapter
25.Weconcen trateonmodelsthathavetheformofalongthinstripofwidth
Wwithperiodicboundary conditions inbothdirections, andweiterate along
thelength ofourmodel,working outasetofpartial partition functions atone
locationlinterms ofpartial partition functions attheprevious locationl 1.
Eachiteration involvesasummation overallthestates attheboundary .This
operation isexponentialinthewidth ofthestrip,W.Thenalclevertrick

<<<PAGE 420>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
408 31|IsingModels
TJ=+1
20
6
4
3
2
TJ= 1
50
5
2
0.5
Figure 31.12.Sample states of
triangular Isingmodelswith
J=1andJ= 1.

<<<PAGE 421>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
31.2: Direct computation ofpartition function ofIsingmodels 409
istonotethatifthesystem istranslation-in variantalong itslength thenwe
onlyneedtodooneiteration inorder tondtheproperties ofasystem ofany
length.
Thecomputational taskbecomes theevaluation ofanSSmatrix, where
Sisthenumberofmicrostates thatneedtobeconsidered attheboundary ,
andthecomputation ofitseigenvalues. Theeigenvalueoflargest magnitude
givesthepartition function foraninnite-length thinstrip.
Hereisamore detailed explanation. Labelthestates oftheCcolumns of
thethinstrips1;s2;:::;sC,witheachsaninteger from0to2W 1.Therth
bitofscindicates whether thespininrowr,columncisupordown.The
partition function is
Z=X
xexp( E(x)) (31.26)
=X
s1X
s2X
sCexp 
 CX
c=1E(sc;sc+1)!
(31.27)
whereE(sc;sc+1)isanappropriately dened energy ,and,ifwewantperiodic
boundary conditions, sC+1isdened tobes1.Onedenition forEis:  +
 ++
+ +
  +
+++
s2s3
Figure 31.13.Illustration tohelp
explain thedenition (31.28).
E(s2;s3)countsallthe
contributions totheenergy inthe
rectangle. Thetotalenergy is
givenbystepping therectangle
along. Eachhorizon talbond
inside therectangle iscounted
once; eachvertical bondis
half-inside therectangle (andwill
behalf-inside anadjacen t
rectangle) sohalfitsenergy is
included inE(s2;s3);thefactor of
1=4appearsinthesecond term
becausemandnbothrunoverall
nodesincolumnc,soeachbondis
visited twice.
Forthestateshownhere,
s2=(100) 2,s3=(110) 2,the
horizon talbondscontribute +Jto
E(s2;s3),andthevertical bonds
contribute J=2ontheleftand
 J=2ontheright,assuming
periodicboundary conditions
betweentopandbottom. So
E(s2;s3)=0.E(sc;sc+1)=X
(m;n)2N:
m2c;n2c+1Jxmxn+1
4X
(m;n)2N:
m2c;n2cJxmxn+1
4X
(m;n)2N:
m2c+1;n2c+1Jxmxn:(31.28)
Thisdenition oftheenergy hasthenicepropertythat(fortherectangular
Isingmodel)itdenes amatrix thatissymmetric initstwoindicessc;sc+1.
Thefactors of1=4areneeded because vertical linksarecountedfourtimes.
Letusdene
Mss0=exp  E(s;s0): (31.29)
Then continuingfromequation (31.27),
Z=X
s1X
s2X
sC"CY
c=1Msc;sc+1#
(31.30)
=Traceh
MCi
(31.31)
=X
aC
a; (31.32)
wherefag2W
a=1aretheeigenvaluesofM.Asthelength ofthestripCincreases,
Zbecomes dominated bythelargest eigenvaluemax:
Z!C
max: (31.33)
Sothefreeenergy perspininthelimitofaninnite thinstripisgivenby:
f= kTlnZ=(WC)= kTClnmax=(WC)= kTlnmax=W:(31.34)
It'sreally neatthatallthethermo dynamic properties ofalongthinstripcan
beobtained fromjustthelargest eigenvalueofthismatrix M!
Computations
Icomputed thepartition functions oflongthinstripIsing modelswiththe
geometries showningure 31.14.
Asinthelastsection, Isettheapplied eldHtozeroandconsidered the
twocasesJ=1whichareaferromagnet andantiferromagnet respectively.I
computed thefreeenergy perspin,f(;J;H)=F=Nforwidths fromW=2
to8asafunction offorH=0.

<<<PAGE 422>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
410 31|IsingModels
Rectangular: Triangular:
bbbb
bbbb
bbbb
bbbb
bbbb
bbbb
bbbb
bbbb
bbbb
bbbb
bbbb
bbbb
W6
?
bbbb

HHHHHHHH
bbbb

HHHHHHHH
bbbb

HHHHHHHH
bbbb

HHHHHHHH
bbbb

HHHHHHHH
bbbb

HHHHHHHH
bbbb

HHHHHHHH
bbbb

HHHHHHHH
bbbb

HHHHHHHH
bbbb

HHHHHHHH
bbbb

HHHHHHHH
bbbb

HHHHHHHH
W6
?
Figure 31.14.Twolongthinstrip
Isingmodels.Alinebetweentwo
spins indicates thattheyare
neighbours. Thestrips havewidth
Wandinnite length.Computational ideas:
Onlythelargest eigenvalueisneeded. There areseveralwaysofgetting this
quantity,forexample, iterativ emultiplication ofthematrix byaninitial vec-
tor.Because thematrix isallpositiveweknowthattheprincipal eigenvector
isallpositivetoo(Frobenius{P erron theorem), soareasonable initial vectoris
(1;1;:::;1).Thisiterativ eprocedure maybefaster thanexplicit computation
ofalleigenvalues. Icomputed them allanyway,whichhastheadvantagethat
wecanndthefreeenergy ofnite length strips {using equation (31.32) {as
wellasinnite ones.
-7-6-5-4-3-2-1
0 2 4 6 8 10Free Energy
TemperatureFerromagnets of width 8
Triangular
Rectangular
-8-7-6-5-4-3-2-1
0 2 4 6 8 10
TemperatureAntiferromagnets of width 8
Triangular
RectangularFigure 31.15.Freeenergy perspin
oflong-thin-strip Isingmodels.
Notethenon-zero gradien tat
T=0inthecaseofthetriangular
antiferromagnet.
Comments ongraphs:
Forlargetemperatures allIsingmodelsshould showthesame behaviour: the
freeenergy isentropy-dominated, andtheentropyperspinisln(2).Themean
energy perspingoestozero.Thefreeenergy perspinshould tendto ln(2)=.
Thefreeenergies areshowningure 31.15.
Oneoftheinteresting properties wecanobtain fromthefreeenergy is
00.10.20.30.40.50.60.7
0 2 4 6 8 10Entropy
TemperatureTriangular(-)
Rectangular
Triangular(+)Figure 31.16.Entropies (innats)
ofwidth 8Isingsystems asa
function oftemperature, obtained
bydieren tiating thefreeenergy
curvesingure 31.15. The
rectangular ferromagnet and
antiferromagnet haveidentical
thermal properties. Forthe
triangular systems, theupper
curve( )denotes the
antiferromagnet andthelower
curve(+)theferromagnet.

<<<PAGE 423>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
31.2: Direct computation ofpartition function ofIsingmodels 411
-3.5-3-2.5-2-1.5-1-0.50
1 10Triangular(-)
Rectangular(+/-)
Triangular(+)Figure 31.17.Mean energy versus
temperature oflongthinstrip
Isingmodelswithwidth 8.
Compare withgure 31.3.
-0.200.20.40.60.811.2
1 10Heat Capacity
TemperatureRectangular Ferromagnet
width 4
width 8
-0.200.20.40.60.811.2
1 10
TemperatureTriangular Ising Models
width 4 (-)
width 8 (-)
width 4 (+)
width 8 (+)Figure 31.18.Heatcapacities of
(a)rectangular model;(b)
triangular modelswithdieren t
widths, (+)and( )denoting
ferromagnet andantiferromagnet.
Compare withgure 31.11.
thedegeneracy oftheground state. Asthetemperature goestozero, the
Boltzmann distribution becomes concen trated intheground state. Ifthe
ground stateisdegenerate (i.e.,therearemultiple ground states withidentical
energy) thentheentropyasT!0isnon-zero. Wecanndtheentropyfrom
thefreeenergy usingS= @F=@T.
Theentropyofthetriangular antiferromagnet atabsolute zeroappearsto
beabout0.3,thatis,abouthalfitshightemperature value(gure 31.16).
Themean energy asafunction oftemperature isplotted ingure 31.17. Itis
evaluated using theidentityhEi= @lnZ=@.
Figure 31.18 showstheestimated heatcapacit y(taking rawderivativesof
themean energy) asafunction oftemperature forthetriangular modelswith
widths 4and8.Figure 31.19 showstheuctuations inenergy asafunction of
temperature. Allofthesegures should showsmoothgraphs; theroughness of
thecurvesisduetoinaccurate numerics. Thenature ofanyphase transition
isnotobvious, butthegraphs seem compatible withtheassertion thatthe
ferromagnet shows,andtheantiferromagnet doesnotshowaphase transition.
-101234567
1 10var(E)
TemperatureRectangular Ferromagnet
width 4
width 8
-20246810121416
1 10
TemperatureTriangular Ising Models
width 4 (-)
width 8 (-)
width 4 (+)
width 8 (+)Figure 31.19.Energy variances,
perspin,of(a)rectangular model;
(b)triangular modelswith
dieren twidths, (+)and( )
denoting ferromagnet and
antiferromagnet. Compare with
gure 31.11.

<<<PAGE 424>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
412 31|IsingModels
Thepictures ofthefreeenergy ingure 31.15 givesome insigh tintohow
wecould predict thetransition temperature. Wecanseehowthetwophases
oftheferromagnetic systems eachhavesimple freeenergies: astraigh tsloping
linethroughF=0,T=0forthehightemperature phase, andahorizon tal
lineforthelowtemperature phase. (Theslopeofeachlineshowswhat the
entropyperspinofthatphase is.)Thephase transition occurs roughly at
theintersection ofthese lines. Sowepredict thetransition temperature tobe
linearly related totheground stateenergy .
Comparison withtheMonte Carlo results
Theagreemen tbetweentheresults ofthetwoexperimen tsseems verygood.
Thetwosystems simulated (thelongthinstripandtheperiodicsquare) are
notquite identical. Onecould amore accurate comparison bynding all
eigenvaluesforthestripofwidthWandcomputingPWtogetthepartitition
function ofaWWpatch.
31.3 Exercises
.Exercise 31.2.[4]What wouldbethebestwaytoextract theentropyfromthe
MonteCarlo simulations? What wouldbethebestwaytoobtain the
entropyandtheheatcapacit yfromthepartition function computation?
Exercise 31.3.[3]AnIsingmodelmaybegeneralized tohaveacouplingJmn
betweenanyspinsmandn,andthevalueofJmncould bedieren tforeach
mandn.Inthespecialcasewhere allthecouplings arepositiveweknow
thatthesystem hastwoground states, theall-up andall-downstates. Fora
moregeneral setting ofJmnitisconceiv ablethatthere could bemany ground
states.
Imagine thatitisrequired tomakeaspinsystem whose localminima are
agivenlistofstates x(1);x(2);:::;x(S).Canyouthink ofawayofsetting J
suchthatthechosen states arelowenergy states? Youareallowedtoadjust
allthefJmngtowhatev ervalues youwish.

<<<PAGE 425>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
32
Exact MonteCarlo Sampling
32.1 Theproblem withMonteCarlo metho ds
Forhigh-dimensional problems, themostwidely usedrandom sampling meth-
odsareMarkovchainMonteCarlo metho dsliketheMetrop olismetho d,Gibbs
sampling, andslicesampling.
Theproblem withallthese metho dsisthis:yes,agivenalgorithm canbe
guaran teedtoproducesamples fromthetarget densit yP(x)asymptotically ,
`once thechainhasconverged totheequilibrium distribution'. Butifoneruns
thechainfortooshort atimeT,thenthesamples willcome fromsomeother
distribution P(T)(x).ForhowlongmusttheMarkovchainberunbeforeithas
`converged' ?Aswasmentioned inChapter 29,thisquestion isusually very
hardtoanswer.However,thepioneering workofPropp andWilson (1996)
allowsone,forcertain chains, toanswerthisveryquestion; furthermore Propp
andWilson showhowtoobtain `exact' samples fromthetarget densit y.
32.2 Exact sampling concepts
Propp andWilson's exact sampling metho d(alsoknownas`perfect simulation'
or`coupling fromthepast') dependsonthree ideas.
CoalescenceofcoupledMarkov chains
First, ifseveralMarkovchains starting fromdieren tinitial conditions share
asingle random-n umbergenerator, thentheirtrajectories instatespace may
coalesce ;and,having, coalesced, willnotseparate again. Ifallinitial condi-
tionsleadtotrajectories thatcoalesce intoasingle trajectory ,thenwecanbe
surethattheMarkovchainhas`forgotten' itsinitial condition. Figure 32.1a-
ishowstwenty-one Markovchains identicaltotheonedescrib edinsection
29.4, whichsamples fromf0;1;:::;20gusing theMetrop olisalgorithm (g-
ure29.12, p.368);eachofthechains hasadieren tinitial condition butthey
arealldrivenbyasingle random numbergenerator; thechains coalesce after
about80steps. Figure 32.1(a-ii) showsthesame Markovchains withadif-
ferentrandom numberseed;inthiscase,coalescence doesnotoccuruntil400
steps haveelapsed (notshown). Figure 32.1b showssimilar Markovchains,
eachofwhichhasidenticalproposaldensit ytothose insection 29.4andg-
ure32.1a; butingure 32.1b, theproposedmoveateachstep,`left'or`right',
isobtained inthesamewaybyallthechains atanytimestep, independen tof
thecurren tstate. Thiscoupling ofthechains changes thestatistics ofcoales-
cence. Because twoneighbouring paths onlymerge when arejection occurs,
andrejections onlyoccuratthewalls(forthisparticular Markovchain), coa-
413

<<<PAGE 426>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
414 32|Exact MonteCarlo Sampling
lescence willoccuronlywhen thechains areallintheleftmost stateorallin
therightmost state.
Coupling fromthepast
Howcanweusethecoalescence propertytondanexact sample fromthe
equilibrium distribution ofthechain? Thestateofthesystem atthemomen t
when complete coalescence occurs isnotavalidsample fromtheequilibrium
distribution; forexample ingure 32.1b, nalcoalescence alwaysoccurs when
thestate isagainst oneofthetwowalls,because trajectories onlymerge at
thewalls.Sosampling forwardintimeuntilcoalescence occurs isnotavalid
metho d.
Thesecond keyideaofexact sampling isthatwecanobtain exact samples
bysampling fromatimeT0inthepast,uptothepresent.Ifcoalescence
hasoccured, thepresen tsample isanunbiased sample fromtheequilibrium
distribution; ifnot,werestart thesimulation from atimeT0further into
thepast,reusing thesamerandom numbers.Thesimulation isrepeated ata
sequence ofevermoredistan ttimesT0,withadoubling ofT0fromonerunto
thenextbeingaconvenientchoice. When coalescence occurs atatimebefore
`thepresen t',wecanrecordx(0)asanexact sample fromtheequilibrium
distribution oftheMarkovchain.
Figure 32.2showstwoexact samples produced inthisway.Intheleftmost
panel ofgure 32.2a, westarttwenty-one chains inallpossible initial condi-
tionsatT0= 50andrunthem forwardintime. Coalescence doesnotoccur.
Werestart thesimulation fromallpossible initial conditions atT0= 100,and
resettherandom numbergenerator insuchawaythattherandom numbers
generated ateachtimet(inparticular, fromt= 50tot=0)willbeidentical
towhattheywereintherstrun.Notice thatthetrajectories produced from
t= 50tot=0bythese runsthatstarted fromT0= 100areidenticaltoa
subset ofthetrajectories intherstsimulation withT0= 50.Coalescence
stilldoesnotoccur,sowedoubleT0again toT0= 200.Thistime, allthe
trajectories coalesce andweobtain anexact sample, shownbythearrow.If
wepickanearlier timesuchasT0= 500,allthetrajectories muststillend
inthesamepointatt=0,sincealltrajectories mustpassthrough somestate
att= 200,andallthose states leadtothesame nalpoint.Soifweran
theMarkovchainforaninnite timeinthepast,fromanyinitial condition,
itwouldendinthesamestate. Figure 32.2b showsanexact sample produced
inthesame waywiththeMarkovchains ofgure 32.1b.
Thismetho d,called coupling fromthepast,isimportantbecause itallows
ustoobtain exact samples fromtheequilibrium distribution; but,asdescrib ed
here,itisoflittlepractical use,sinceweareobliged tosimulatechains starting
inallinitial states. Intheexamples shown,there areonlytwenty-one states,
butinanyrealistic sampling problem therewillbeanutterly enormous number
ofstates {think ofthe21000states ofasystem of1000 binary spins, for
example. Thewhole pointofintroducing MonteCarlo metho dswastotryto
avoidhavingtovisitallthestates ofsuchasystem!
Monotonicity
Havingestablished thatwecanobtain validsamples bysimulating forward
fromtimes inthepast,starting inallpossible states atthose times, thethird
trickofPropp andWilson, whichmakestheexact sampling metho duseful
inpractice, istheideathat, forsome Markovchains, itmaybepossible to
detect coalescence ofalltrajectories without simulating allthosetrajectories .

<<<PAGE 427>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
32.2: Exact sampling concepts 415
050100150200250
05101520050100150200250
05101520050100150200250
05101520050100150200250
05101520
(i) (ii) (i) (ii)
(a) (b)Figure 32.1.Coalescence, therst
ideabehind theexact sampling
metho d.Intheleftmost panel,
coalescence occurred within 100
steps. Dieren tcoalescence
properties areobtained depending
onthewayeachstateusesthe
random numbersitissupplied
with. (a)Tworunsofa
Metrop olissimulator inwhichthe
random bitsthatdetermine the
proposedstepdependonthe
curren tstate; adieren trandom
numberseedwasusedineach
case.(b)Inthissimulator the
random proposal(`left' or`right')
isthesameforallstates. Ineach
panel, oneofthepaths, theone
starting atlocationx=8,has
beenhighligh ted.

<<<PAGE 428>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
416 32|Exact MonteCarlo Sampling
Figure 32.2.`Coupling fromthepast', thesecond ideabehind theexact sampling metho d.-250-200-150-100-500
01020-250-200-150-100-500
01020-250-200-150-100-500
01020-250-200-150-100-500
01020-250-200-150-100-500
01020-250-200-150-100-500
01020
T0= 50 T0= 100 T0= 200 T0= 50 T0= 100 T0= 200
(a) (b)

<<<PAGE 429>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
32.2: Exact sampling concepts 417
Figure 32.3.(a)Ordering ofstates, thethirdideabehind theexact sampling metho d.Thetrajectories
shownherearetheleft-most andright-most trajectories ofgure 32.2b. Inorder toestablish
whatthestateattimezerois,weonlyneedtorunsimulations fromT0= 50,T0= 100,
andT0= 200,afterwhichpointcoalescence occurs.
(b,c)Twomore exact samples fromthetarget densit y,generated bythismetho d,and
dieren trandom numberseeds. Theinitial times required wereT0= 50andT0= 1000,
respectively.-250-200-150-100-500
01020-250-200-150-100-500
01020-250-200-150-100-500
01020-250-200-150-100-500
01020-250-200-150-100-500
01020
T0= 50 T0= 100 T0= 200 T0= 50 T0= 1000
(a) (b) (c)

<<<PAGE 430>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
418 32|Exact MonteCarlo Sampling
Thispropertyholds, forexample, inthechainofgure 32.1b, whichhasthe
propertythattwotrajectories never cross.Soifwesimply trackthetwotra-
jectories starting fromtheleftmost andrightmost states, wewillknowthat
coalescence ofalltrajectories hasoccurred when those twotrajectories co-
alesce. Figure 32.3b illustrates thisideabyshowingonlytheleft-most and
right-most trajectories ofgure 32.2b. Figure 32.3(c,d) showstwomore ex-
actsamples fromthesame equilibrium distribution generated byrunning the
`coupling fromthepast' metho dstarting fromthetwoend-states alone. In
(c),tworunscoalesced starting fromT0= 50;in(d),itwasnecessary totry
times uptoT0= 1000toachievecoalescence.
32.3 Exact sampling frominteresting distributions
Inthetoyproblem westudied, thestates could beputinaone-dimensional
order suchthatnotwotrajectories crossed. Thestates ofmanyinteresting
state spaces canalsobeputintoapartial order andcoupled Markovchains
canbefound thatrespectthispartial order. [Anexample ofapartial order
onthefourpossible states oftwospins isthis: (+;+)>(+; )>( ; );
and(+;+)>( ;+)>( ; );andthestates (+; )and( ;+)arenot
ordered.] Forsuchsystems, wecanshowthatcoalescence hasoccurred merely
byverifying thatcoalescence hasoccurred forallthehistories whose initial
states were`maximal' and`minimal' states ofthestatespace.Computeai:=P
jJijxj
DrawufromUniform(0;1)
Ifu<1=(1+e ai)
xi:=+1
Else
xi:= 1
Algorithm 32.4.Gibbs sampling
coupling metho d.TheMarkov
chains arecoupled together by
havingallchains updatethesame
spiniateachtimestepand
havingallchains sharing a
common sequence ofrandom
numbersu.
Asanexample, consider theGibbs sampling metho dapplied toaferro-
magnetic Isingspinsystem, withthepartial ordering ofstates beingdened
thus:statexis`greater thanorequal to'stateyifxiyiforallspinsi.
Themaximal andminimal states arethetheall-up andall-downstates. The
Markovchains arecoupled together asshowninalgorithm 32.4. Propp and
Wilson (1996) showthatexact samples canbegenerated forthissystem, al-
though thetimetondexact samples islargeiftheIsingmodelisbelowits
critical temperature, sincetheGibbs sampling metho ditselfisslowly-mixing
under these conditions. Propp andWilson haveimpro vedonthismetho dfor
theIsing modelbyusing aMarkovchaincalled thesingle-b ondheatbath
algorithm tosample fromarelated modelcalled therandom cluster model;
theyshowthatexact samples fromtherandom cluster modelcanbeobtained
rapidly andcanbeconverted intoexact samples fromtheIsingmodel.Their
ground-breaking paperincludes anexact sample froma16-million-spin Ising
modelatitscritical temperature. Asample forasmaller Isingmodelisshown
ingure 32.5.Figure 32.5.Anexact sample from
theIsingmodelatitscritical
temperature, produced by
D.B.Wilson. Suchsamples canbe
produced within seconds onan
ordinary computer byexact
sampling.Ageneralization oftheexactsampling methodfor`non-attr active' distri-
butions
Themetho dofPropp andWilson fortheIsingmodel,sketchedabove,can
onlybeapplied toprobabilit ydistributions thatare,astheycallthem, `at-
tractiv e'.Rather thandene thisterm, let'ssaywhat itmeans, forpractical
purposes: themetho dcanbeapplied tospinsystems inwhichallthecou-
plings arepositive(e.g., theferromagnet), andtoafewspecialspinsystems
withnegativ ecouplings (e.g., aswealready observ edinChapter 31,therect-
angular ferromagnet andantiferromagnet areequivalent);butitcannot be
applied togeneral spinsystems inwhichsomecouplings arenegativ e,because
insuchsystems thetrajectories followedbytheall-up andall-downstates are
notguaran teedtobeupperandlowerbounds forthesetofalltrajectories.
Fortunately ,however,wedonotneedtobesostrict. Itispossible tore-

<<<PAGE 431>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
32.3: Exact sampling frominteresting distributions 419
express thePropp andWilson algorithm inawaythatgeneralizes tothecase
ofspinsystems withnegativ ecouplings. Theideathesummary stateversion
oftheexact sampling metho disstillthatwekeeptrackofbounds ontheset
ofalltrajectories, anddetect when these bounds areequal, soastondexact
samples. Butthebounds willnotthemselv esbeactual trajectories, andthey
willnotnecessarily betightbounds.
Instead ofsimulating twotrajectories, eachofwhichmovesinastatespace
f 1;+1gN,wesimulate onetrajectory envelopeinanaugmen tedstatespace
f 1;+1;?gN,where thesymbol?denotes `either 1or+1'.Wecallthestate
ofthisaugmen tedsystem the`summary state'. Anexample summary stateof
asix-spin system is++-?+? .Thissummary state isshorthand forthesetof
states
++-+++ ,++-++- ,++--++ ,++--+- .
TheupdateruleateachstepoftheMarkovchaintakesasingle spin,enu-
merates allpossible states oftheneighbouring spinsthatarecompatible with
thecurren tsummary state, and,foreachofthese localscenarios, computes
thenewvalue(+or-)ofthespinusing Gibbs sampling (coupled toarandom
numberuasinalgorithm 32.4). Ifallthese newvalues agree, thenthenew
valueoftheupdated spininthesummary stateissettotheunanimous value
(+or-).Otherwise, thenewvalueofthespininthesummary stateis`?'.The
initial condition, attimeT0,isgivenbysetting allthespins inthesummary
stateto`?',whichcorresp ondstoconsidering allpossible startcongurations.
Inthecaseofaspinsystem withpositivecouplings, thissummary state
simulation willbeidenticaltothesimulation oftheuppermost stateandlow-
ermost states, inthestyleofPropp andWilson, withcoalescence occuring
when allthe`?'symbolshavedisapp eared. Thesummary statemetho dcan
beapplied togeneral spinsystems withanycouplings. Theonlyshortcoming
ofthismetho disthattheenvelopemaydescrib eanunnecessarily largesetof
states, sothere isnoguaran teethatthesummary state algorithm willcon-
verge;thetimeforcoalescence tobedetectedmaybeconsiderably larger than
theactual timetakenfortheunderlying Markovchaintocoalesce.
Thesummary state scheme hasbeenapplied toexact sampling inbelief
networksbyHarveyandNeal(2000), andtothetriangular antiferromagnetic
IsingmodelbyChilds etal.(2001).
Further reading
Forfurther reading, impressiv epictures ofexact samples fromother distribu-
tions, andgeneralizations oftheexact sampling metho d,browsetheperfectly-
random sampling website.1
Forbeautiful exact-sampling demonstrations running liveinyourweb-
browser, seeJimPropp's website.2
Other usesforcoupling
Theideaofcoupling together Markovchains byhavingthem share arandom
numbergenerator hasother applications beyondexact sampling. Pintoand
Neal(2001) haveshownthattheaccuracy ofestimates obtained fromaMarkov
chainMonteCarlo simulation (thesecond problem discussed insection 29.1,
1http://www.dbwilson.com/exact/
2http://www.math.wisc.edu/ propp/tiling/www/applets/

<<<PAGE 432>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
420 32|Exact MonteCarlo Sampling
Figure 32.6.Aperfectly random
tiling ofahexagon bylozenges,
provided byJ.G.Propp and
D.B.Wilson.
p.357),using theestimator
^P1
TX
t(x(t)); (32.1)
canbeimpro vedbycoupling thechainofinterest, whichconverges toP,toa
second chain,whichgenerates samples fromasecond, simpler distribution, Q.
Thecoupling mustbesetupinsuchawaythatthestates ofthetwochains
arestrongly correlated. Theideaisthatwerstestimate theexpectations of
afunction ofinterest,,underPandunderQinthenormal way(32.1) and
compare theestimate underQ,^Q,withthetruevalueoftheexpectation
underQ,Qwhichweassume canbeevaluated exactly .If^Qisanoveres-
timate thenitislikelythat^Pwillbeanoverestimate too.Thedierence
(^Q Q)canthusbeusedtocorrect ^P.
32.4 Exercises
.Exercise 32.1.[2,p.421]Isthere anyrelationship betweentheprobabilit ydis-
tribution ofthetimetakenforalltrajectories tocoalesce, andtheequi-
libration timeofaMarkovchain? Provethatthere isarelationship, or
ndasingle chainthatcanberealized intwodieren twaysthathave
dieren tcoalescence times.
.Exercise 32.2.[2]Imagine thatFredignores therequiremen tthattherandom
bitsusedatsome timet,ineveryrunfromincreasingly distan ttimes
T0,mustbeidentical,andmakesacoupled-Mark ov-chainsimulator that
usesfresh random numberseverytimeT0ischanged. Describ ewhat
happensifFredapplies hismetho dtotheMarkovchainthatisintended
tosample fromtheuniform distribution overthestates 0,1,and2,using
theMetrop olismetho d,drivenbyarandom bitsource asingure 32.1b.

<<<PAGE 433>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
32.5: Solutions 421
Exercise 32.3.[5]Investigate theapplication ofperfect sampling tolinear re-
gression inHolmes andMallic k(1998) andtrytogeneralize it.
Exercise 32.4.[3]Theconcept ofcoalescence hasmanyapplications. Some sur-
names aremorefrequen tthanothers, andsomedieoutaltogether. Make
amodelofthisprocess;howlongwillittakeuntileveryonehasthesame
surname?
Similarly ,variabilit yinanyparticular portion ofthehuman genome
(whichforms thebasisofforensic DNA ngerprin ting)isinherited likea
surname. ADNA ngerprin tislikeastring ofsurnames. Should thefact
thatthese surnames aresubjecttocoalescences, sothatsome surnames
arebychance moreprevalentthanothers, aect thewayinwhichDNA
ngerprin tevidence isusedincourt?
.Exercise 32.5.[2]Howcanyouuseacointocreate arandom ranking of3
people? Construct asolution thatusesexact sampling. Forexample,
youcould apply exact sampling toaMarkovchaininwhichthecoinis
repeatedly usedalternately todecide whether toswitchrstandsecond,
thenwhether toswitchsecond andthird.
Exercise 32.6.[5]Finding thepartition functionZofaprobabilit ydistribution
isadicult problem. ManyMarkovchainMonteCarlo metho dsproduce
validsamples fromadistribution without evernding outwhatZis.
Isthere anyprobabilit ydistribution andMarkovchainsuchthateither
thetimetakentoproduceaperfect sample orthenumberofrandom bits
usedtocreate aperfect sample arerelated tothevalueofZ?Arethere
some situations inwhichthetimetocoalescence conveysinformation
aboutZ?
32.5 Solutions
Solution toexercise 32.1(p.420).Itisperhaps surprising thatthere isno
direct relationship betweentheequilibration timeandthetimetocoalescence.
Asimple example thatprovesthisisthecaseoftheuniform distribution over
theintegersA=f0;1;2;:::;20g.AMarkovchainthatconverges tothis
distribution inexactly oneiteration isthechainforwhichtheprobabilit yof
statest+1givenstistheuniform distribution, forallst.Suchachaincan
becoupled toarandom numbergenerator intwoways:(a)wecould drawa
random integeru2A,andsetst+1equal touregardless ofst;or(b)wecould
drawarandom integeru2A,andsetst+1equal to(st+u)mod21.Metho d
(b)wouldproduceacohort oftrajectories lockedtogether, similar tothe
trajectories ingure 32.1,except thatnocoalescence everoccurs. Thus,while
theequilibration times ofmetho ds(a)and(b)arebothone,thecoalescence
times arerespectivelyoneandinnit y.
Itseems plausible ontheother handthatcoalescence timeprovides some
sortofupperboundonequilibration time.

<<<PAGE 434>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
33
Variational Metho ds
Variational metho dsareanimportanttechnique fortheapproximation ofcom-
plicated probabilit ydistributions, havingapplications instatistical physics,
datamodelling andneural networks.
33.1 Variational freeenergy minimization
Onemetho dforapproximating acomplex distribution inaphysical system is
mean eldtheory .Mean eldtheory isaspecialcaseofageneral variational
freeenergy approac hofFeynman andBogoliub ovwhichwewillnowstudy.
Thekeypiece ofmathematics needed tounderstand thismetho disGibbs'
inequalit y,whichwerepeathere. Gibbs' inequalit yrstappeared in
equation (1.24); seealsoexercise 2.26
(p.37). Therelativ eentropybetweentwoprobabilit ydistributions Q(x)andP(x)
thataredened overthesame alphab etAXis
DKL(QjjP)=X
xQ(x)logQ(x)
P(x): (33.1)
Therelativ eentropysatisesDKL(QjjP)0(Gibbs' inequalit y)with
equalit yonlyifQ=P.IngeneralDKL(QjjP)6=DKL(PjjQ).
Inthischapter wewillreplace thelogbyln,andmeasure thedivergence
innats.
Probability distributions instatistic alphysics
Instatistical physicsoneoftenencoun tersprobabilit ydistributions oftheform
P(xj;J)=1
Z(;J)exp[ E(x;J)]; (33.2)
where forexample thestate vector isx2f 1;+1gN,andE(x;J)issome
energy function suchas
E(x;J)= 1
2X
m;nJmnxmxn X
nhnxn: (33.3)
Thepartition function (normalizing constan t)is
Z(;J)X
xexp[ E(x;J)]: (33.4)
Theprobabilit ydistribution ofequation (33.2) iscomplex. Notunbearably
complex {wecan,afterall,evaluateE(x;J)foranyparticular xinatime
422

<<<PAGE 435>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
33.1: Variational freeenergy minimization 423
polynomial inthenumberofspins. Butevaluating thenormalizing constan t
Z(;J)isdicult, aswesawinChapter 29,anddescribing theproperties of
theprobabilit ydistribution isalsohard. KnowingthevalueofE(x;J)ata
fewarbitrary pointsx,forexample, givesnouseful information aboutwhat
theaverage properties ofthesystem are.
Anevaluation ofZ(;J)wouldbeparticularly desirable because fromZ
wecanderiveallthethermo dynamic properties ofthesystem.
Variational freeenergy minimization isametho dforapproximating the
complex distribution P(x)byasimpler ensem bleQ(x;)thatisparameterized
byadjustable parameters .Weadjust these parameters soastogetQto
bestapproximateP,insome sense. Aby-productofthisapproximation isa
lowerboundonZ(;J).
Thevariational freeenergy
Theobjectivefunction chosen tomeasure thequalityoftheapproximation is
thevariational freeenergy
~F()=X
xQ(x;)lnQ(x;)
exp[ E(x;J)]: (33.5)
Thisexpression canbemanipulated intoacouple ofinteresting forms: rst,
~F()=X
xQ(x;)E(x;J) X
xQ(x;)ln1
Q(x;)(33.6)
hE(x;J)iQ SQ; (33.7)
wherehE(x;J)iQistheaverage oftheenergy function under thedistribution
Q(x;),andSQistheentropyofthedistribution Q(x;)(wesetkBtoone
inthedenition ofSsothatitisidenticaltothedenition oftheentropyH
inPartI).
Second, wecanusethedenition ofP(xj;J)towrite:
~F()=X
xQ(x;)lnQ(x;)
P(xj;J) lnZ(;J) (33.8)
=DKL(QjjP)+F; (33.9)
whereFisthetruefreeenergy ,dened by
F lnZ(;J); (33.10)
andDKL(QjjP)istherelativ eentropybetweentheapproximating distribution
Q(x;)andthetruedistribution P(xj;J).ThusbyGibbs' inequalit y,the
variational freeenergy ~F()isbounded belowbyFandonlyattains thisvalue
forQ(x;)=P(xj;J).
Ourstrategy isthustovaryinsuchawaythat~F()isminimized.
Theapproximating distribution thengivesasimplied approximation tothe
truedistribution thatmaybeuseful, andthevalueof~F()willbeanupper
boundforF.Equiv alently,~Ze ~F(
 )isalowerboundforZ.
Can~Fbeevaluate d?
Wehavealready agreed thattheevaluation ofvarious interesting sumsoverx
isintractable. Forexample, thepartition function
Z=X
xexp( E(x;J)); (33.11)

<<<PAGE 436>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
424 33|Variational Metho ds
theenergy
hEiP=1
ZX
xE(x;J)exp( E(x;J)); (33.12)
andtheentropy
SX
xP(xj;J)ln1
P(xj;J)(33.13)
areallpresumed tobeimpossible toevaluate. Sowhyshould wesuppose
thatthisobjectivefunction~F(),whichisalsodened interms ofasum
overallx(33.5), should beaconvenientquantitytodealwith? Well,fora
range ofinteresting energy functions, andforsucien tlysimple approximating
distributions, thevariational freeenergy canbeecien tlyevaluated.
33.2 Variational freeenergy minimization forspinsystems
Anexample ofatractable variational freeenergy isgivenbythespinsystem
whose energy function wasgiveninequation (33.3), whichwecanapproximate
withaseparableapproximating distribution,
Q(x;a)=1
ZQexp X
nanxn!
: (33.14)
Thevariational parameters ofthevariational freeenergy (33.5) arethe
componentsofthevectora.Toevaluate thevariational freeenergy weneed
theentropyofthisdistribution,
SQ=X
xQ(x;a)ln1
Q(x;a)(33.15)
andthemean oftheenergy ,
hE(x;J)iQ=X
xQ(x;a)E(x;J): (33.16)
Theentropyoftheseparable approximating distribution issimply thesumof
theentropies oftheindividual spins(exercise 4.2,p.68),
SQ=X
nH(e)
2(qn); (33.17)
whereqnistheprobabilit ythatspinnis+1,
qn=ean
ean+e an=1
1+exp( 2an); (33.18)
and
H(e)
2(q)=qln1
q+(1 q)ln1
(1 q): (33.19)
Themean energy underQiseasytoobtain becauseP
m;nJmnxmxnisasumof
terms eachinvolving theproductoftwoindependent random variables. (There
arenoself-couplings, soJmn=0whenm=n.)Ifwedene themean value
ofxntobexn,whichisgivenby
xn=ean e an
ean+e an=tanh(an)=2qn 1; (33.20)

<<<PAGE 437>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
33.2: Variational freeenergy minimization forspinsystems 425
weobtain
hE(x;J)iQ=X
xQ(x;a)"
 1
2X
m;nJmnxmxn X
nhnxn#
(33.21)
= 1
2X
m;nJmnxmxn X
nhnxn: (33.22)
Sothevariational freeenergy isgivenby
~F(a)=hE(x;J)iQ SQ= 
 1
2X
m;nJmnxmxn X
nhnxn!
 X
nH(e)
2(qn):
(33.23)
00.51
00.51
00.51
00.51
Figure 33.1.Thevariational free
energy ofthetwo-spin system
whose energy isE(x)= x1x2,as
afunction ofthetwovariational
parameters q1andq2.The
inverse-temp erature is=1:44.
Thefunction plotted is
~F= x1x2 H(e)
2(q1) H(e)
2(q2);
where xn=2qn 1.Notice that
forxedq2thefunction is
convex^withrespecttoq1,and
forxedq1itisconvex^with
respecttoq2.Wenowconsider minimizing thisfunction withrespecttothevariational
parameters a.Ifq=1=(1+e 2a),thederivativeoftheentropyis
@
@qHe
2(q)=ln1 q
q= 2a: (33.24)
Soweobtain
@
@am~F(a)="
 X
nJmnxn hm#
2@qm
@am
 ln1 qm
qm@qm
@am
=2@qm
@am"
  X
nJmnxn+hm!
+am#
: (33.25)
Thisderivativeisequal tozerowhen
am= X
nJmnxn+hm!
: (33.26)
So~F(a)isextremized atanypointthatsatises equation (33.26) and
xn=tanh(an): (33.27)
Thevariational freeenergy ~F(a)maybeamultimo dalfunction, inwhich
caseeachstationary point(maxim um,minim umorsaddle) willsatisfy equa-
tions(33.26) and(33.27). Onewayofusing these equations, inthecaseofa
system withanarbitrary coupling matrix J,istoupdateeachparameteram
andthecorresp onding valueofxmusing equation (33.26), oneatatime. This
asynchronous updating oftheparameters isguaran teedtodecrease~F(a).
Equations (33.26) and(33.27) mayberecognized asthemean eldequa-
tionsforaspinsystem. Thevariational parameteranmaybethough tofas
thestrength ofactitious eldapplied toanisolated spinn.Equation (33.27)
describ esthemean responseofspinn,andequation (33.26) describ eshowthe
eldamissetinresponsetothemean stateofalltheother spins.
Thevariational freeenergy derivation isahelpful viewp ointformean eld
theory fortworeasons.
1.Thisapproac hassociates anobjectivefunction~Fwiththemean eld
equations; suchanobjectivefunction isuseful because itcanhelpidentify
alternativ edynamical systems thatminimize thesame function.
2.Thetheory isreadily generalized toother approximating distributions.
Wecanimagine introducing amorecomplex approximationQ(x;)that

<<<PAGE 438>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
426 33|Variational Metho ds
-1-0.500.51
012345678h = 0.00
h = 0.40
h = 0.80Figure 33.2.Solutions ofthe
variational freeenergy
extremization problem forthe
Isingmodel.Horizon talaxis:
temperatureT=1=.Vertical
axis:magnetization x.The
critical temperature found by
mean eldtheory isTmft
c=4.
mightforexample capture correlations among thespinsinstead ofmod-
elling thespinsasindependen t.Onecould thenevaluate thevariational
freeenergy andoptimize theparameters ofthismorecomplex approx-
imation. Themore degrees offreedom theapproximating distribution
has,thetightertheboundonthefreeenergy becomes. However,ifthe
complexit yofanapproximation isincreased, theevaluation ofeither the
mean energy ortheentropytypically becomes more challenging.
33.3 Example: mean eldtheory fortheferromagnetic Isingmodel
Inthesimple Isingmodelstudied inChapter 31,everycouplingJmnisequal
toJifmandnareneighboursandzerootherwise. There isanapplied
eldhn=hthatisthesame forallspins. Averysimple approximating
distribution isonewithjustasingle variational parametera,whichdenes a
separable distribution
Q(x;a)=1
ZQexp X
naxn!
(33.28)
inwhichallspinsareindependen tandhavethesame probabilit y
qn=1
1+exp( 2a)(33.29)
ofbeingup.Themean magnetization is
x=tanh(a) (33.30)
andtheequation (33.26) whichdenes theminim umofthevariational free
energy becomes
a=(CJx+h); (33.31)
whereCisthenumberofcouplings thataspinisinvolvedin,C=4inthe
caseofarectangular two-dimensional Ising model.Wecansolveequations
(33.30) and(33.31) forxnumerically {infact,itiseasiest tovaryxandsolve
for{andobtain graphs ofthefreeenergy minima andmaxima asafunction
oftemperature asshowningure 33.2. ThesolidlineshowsxversusT=1=
forthecaseC=4;J=1.
Whenh=0,thereisapitchfork bifurcation atacritical temperatureTmft
c.
[Apitchfork bifurcation isatransition liketheoneshownbythesolidlinesin
gure 33.2,fromasystem withoneminim umasafunction ofa(ontheright)

<<<PAGE 439>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
33.4: Variational metho dsininference anddatamodelling 427
toasystem (ontheleft)withtwominima andonemaxim um;themaxim um
isthemiddle oneofthethree lines. Thesolidlineslooklikeapitchfork.]
Abovethistemperature, there isonlyoneminim uminthevariational free
energy ,ata=0andx=0;thisminim umcorresp ondstoanapproximating
distribution thatisuniform overallstates. Belowthecritical temperature,
there aretwominima corresp onding toapproximating distributions thatare
symmetry-brok en,withallspinsmore likelytobeup,orallspinsmore likely
tobedown.Thestate x=0persists asastationary pointofthevariational
freeenergy ,butnowitisalocalmaximum ofthevariational freeenergy .
Whenh>0,there isaglobal variational freeenergy minim umatany
temperature forapositivevalueofx,shownbytheupperdotted curvesin
gure 33.2. Aslongash<JC,there isalsoasecond localminim uminthe
freeenergy ,ifthetemperature issucien tlysmall. Thissecond minim umcor-
respondstoaself-preserving stateofmagnetization intheoppositedirection
totheapplied eld. Thetemperature atwhichthesecond minim umappears
issmaller thanTmft
c,andwhen itappears,itisaccompanied byasaddle point
located betweenthetwominima. Aname giventothistypeofbifurcation is
asaddle-no debifurcation.
Thevariational freeenergy perspinisgivenby
~F=
 C
2Jx2 hx
 H(e)
2x+1
2
: (33.32)
Exercise 33.1.[2]Sketchthevariational freeenergy asafunction ofitsone
parameter xforavarietyofvaluesofthetemperatureTandtheapplied
eldh.
Figure 33.2reproduces thekeyproperties oftherealIsingsystem {that,
forh=0,there isacritical temperature belowwhichthesystem haslong-
range order, andthatitcanadopt oneoftwomacroscopic states. However,
byprobing alittlemore wecanrevealsome inadequacies ofthevariational
approximation. Tostart with, thecritical temperatureTmft
cis4,whichis
nearly afactor of2greater thanthetruecritical temperatureTc=2:27.Also,
thevariational modelhasequivalentproperties inanynumberofdimensions,
includingd=1,where thetruesystem doesnothaveaphase transition. So
thebifurcation atTmft
cshould notbedescrib edasaphase transition.
Forthecaseh=0wecanfollowthetrajectory oftheglobal minim umas
afunction ofandndtheentropy,heatcapacit yanductuations oftheap-
proximating distribution andcompare themwiththose ofareal88fragmen t
using thematrix metho dofChapter 31.Asshowningure 33.3,oneofthe
biggest dierences isintheuctuations inenergy .Therealsystem haslarge
uctuations nearthecritical temperature, whereas theapproximating distri-
bution hasnocorrelations among itsspins andthushasanenergy-v ariance
whichscales simply linearly withthenumberofspins.
33.4 Variational metho dsininference anddatamodelling
Instatistical datamodelling weareinterested intheposterior probabilit y
distribution ofaparameter vectorwgivendataDandmodelassumptionsH,
P(wjD;H).
P(wjD;H)=P(Djw;H)P(wjH)
P(DjH): (33.33)
Intraditional approac hestomodeltting, asingle parameter vectorwisop-
timized tondthemodeofthisdistribution. What isreally ofinterest is

<<<PAGE 440>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
428 33|Variational Metho ds
FreeEnergy Energy
-6-5.5-5-4.5-4-3.5-3-2.5-2
012345678mean field theory
real 8x8 system
-2-1.5-1-0.50
012345678mean field theory
real 8x8 system
Entropy HeatCapacit y,dE=dT
00.10.20.30.40.50.60.7
012345678mean field theory
real 8x8 system
-0.200.20.40.60.811.21.41.6
012345678mean field theory
real 8x8 system
Fluctuations, var(E)
-10123456
012345678mean field theory
real 8x8 systemFigure 33.3.Comparison of
approximating distribution's
properties withthose ofareal
88fragmen t.Notice thatthe
variational freeenergy ofthe
approximating distribution is
indeed anupperbound onthe
freeenergy oftherealsystem. All
quantitiesareshown`perspin'.

<<<PAGE 441>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
33.5: Thecaseofanunkno wnGaussian 429
thewhole distribution. Wemayalsobeinterested initsnormalizing constan t
P(DjH)ifwewishtodomodelcomparison. Theprobabilit ydistribution
P(wjD;H)isoften acomplex distribution. Inavariational approac htoin-
ference, weintroduceanapproximating probabilit ydistribution overthepa-
rameters,Q(w;),andoptimize thisdistribution (byvarying itsownparam-
eters)sothatitapproximates theposterior distribution oftheparameters
P(wjD;H)well.
Oneobjectivefunction wemaychoosetomeasure thequalityoftheap-
proximation isthevariational freeenergy
~F()=Z
dkwQ(w;)lnQ(w;)
P(Djw;H)P(wjH): (33.34)
Thedenominator P(Djw;H)P(wjH)is,within amultiplicativ econstan t,
equal totheposterior probabilit yP(wjD;H)=P(Djw;H)P(wjH)=P(DjH):
Sothevariational freeenergy ~F()canbeviewedasthesumof lnP(DjH)
andtherelativ eentropybetweenQ(w;)andP(wjD;H).~F()isbounded
belowby lnP(DjH)andonlyattains thisvalueforQ(w;)=P(wjD;H).
Forcertain modelsandcertain approximating distributions, thisfreeenergy ,
anditsderivativeswithrespecttotheapproximating distribution's parame-
ters,canbeevaluated.
Theapproximation ofposterior probabilit ydistributions using variational
freeenergy minimization providesauseful approac htoapproximating Bayesian
inference inanumberofeldsranging fromneural networkstothedecodingof
error-correcting codes(HintonandvanCamp, 1993; HintonandZemel, 1994;
Dayanetal.,1995; NealandHinton,1998; MacKa y,1995a). Themetho d
issometimes called ensem blelearning tocontrastitwithtraditional learning
processes inwhichasingle parameter vector isoptimized. Another name for
itisvariational Bayes.Letusexamine howensem blelearning worksinthe
simple caseofaGaussian distribution.
33.5 Thecaseofanunkno wnGaussian: approximating theposterior
distribution ofand
Wewilltanapproximating ensem bleQ(;)totheposterior distribution
thatwestudied inChapter 24,
P(;jfxngN
n=1)=P(fxngN
n=1j;)P(;)
P(fxngN
n=1)(33.35)
=1
(22)N=2exp
 N( x)2+S
221
1

P(fxngN
n=1):(33.36)
Wemakethesingle assumption thattheapproximating ensem bleisseparable
intheformQ(;)=Q()Q().Norestrictions onthefunctional formof
Q()andQ()aremade.
Wewrite downavariational freeenergy ,
~F(Q)=Z
ddQ()Q()lnQ()Q()
P(Dj;)P(;): (33.37)
Wecanndtheoptimal separable distribution Qbyconsidering separately
theoptimization of~FoverQ()forxedQ(),andthentheoptimization
ofQ()forxedQ().

<<<PAGE 442>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
430 33|Variational Metho ds
(a)
0.20.30.40.50.60.70.80.91
00.5 11.5 2(b)
0.20.30.40.50.60.70.80.91
00.5 11.5 2(c)
0.20.30.40.50.60.70.80.91
00.5 11.5 2
(d)
0.20.30.40.50.60.70.80.91
00.5 11.5 2(e)
0.20.30.40.50.60.70.80.91
00.5 11.5 2
:::(f)
0.20.30.40.50.60.70.80.91
00.5 11.5 2Figure 33.4.Optimization ofan
approximating distribution. The
posterior distribution
P(;jfxng),whichisthesame
asthatingure 24.1,isshownby
solidcontours. (a)Initial
condition. Theapproximating
distribution Q(;)(dotted
contours) isanarbitrary separable
distribution. (b)Qhasbeen
updated, using equation (33.41).
(c)Qhasbeenupdated, using
equation (33.44). (d)Qupdated
again. (e)Qupdated again. (f)
Converged approximation (after
15iterations). Thearrowspoint
tothepeaksofthetwo
distributions, whichareat
N=0:45(forP)andN 1=0:5
(forQ).
Optimization ofQ()
Asafunctional ofQ(),~Fis:
~F= Z
dQ()Z
dQ()lnP(Dj;)+ln[P()=Q()]
+(33.38)
=Z
dQ()Z
dQ()N1
2( x)2+lnQ()
+0; (33.39)
where1=2anddenotes constan tsthatdonotdependonQ().The
dependence onQthuscollapses downtoasimple dependence onthemean
Z
dQ()1=2: (33.40)
Nowwecanrecognize thefunction N1
2( x)2asthelogarithm of
aGaussian identicaltotheposterior distribution foraparticular valueof
=.Since adivergenceRQln(Q=P)isminimized bysettingQ=P,we
canimmediately write downthedistribution Qopt
()thatminimizes ~Ffor
xedQ:
Qopt
()=P(jD;;H)=Normal(;x;2
jD): (33.41)
where2
jD=1=(N).
Optimization ofQ()
Asafunctional ofQ(),~Fis(neglecting additiv econstan ts):
~F= Z
dQ()Z
dQ()lnP(Dj;)+ln[P()=Q()]
(33.42)
=Z
dQ()h
(N2
jD+S)=2 
N
2 1
ln+lnQ()i
(33.43)

<<<PAGE 443>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
33.6: Interlude 431
where theintegral overisperformed assumingQ()=Qopt
():Here, the-
dependen texpression insquare bracketscanberecognized asthelogarithm of
agamma distribution over{seeequation (23.15) {giving asthedistribution
thatminimizes ~FforxedQ:
Qopt
()= (;b0;c0); (33.44)
with1
b0=1
2(N2
jD+S)andc0=N
2: (33.45)
Ingure 33.4,these twoupdaterules(33.41, 33.44) areapplied alternately ,
starting fromanarbitrary initial condition. Thealgorithm converges tothe
optimal approximating ensem bleinafewiterations.
Directsolution forthejointoptimum Q()Q()
Inthisproblem, wedonotneedtoresort toiterativ ecomputation tond
theoptimal approximating ensem ble.Equations (33.41) and(33.44) dene
theoptim umimplicitly .Wemustsimultaneously have2
jD=1=(N),and
=b0c0.Thesolution is:
1==S=(N 1): (33.46)
Thisissimilar tothetrueposterior distribution of,whichisagamma distri-
bution withc0=N 1
2and1=b0=S=2(seeequation 24.13). Thistrueposterior
alsohasamean valueofsatisfying 1==S=(N 1);theonlydierence is
thattheapproximating distribution's parameterc0istoolargeby1=2.
Theapproximations givenbyvariational freeenergy minimization
alwaystendtobemore compact thanthetruedistribution.
Inconclusion, ensem blelearning givesanapproximation totheposterior
thatagrees nicely withtheconventional estimators. Theapproximate poste-
riordistribution overisagamma distribution withmean corresp onding
toavariance of2=S=(N 1)=2
N 1.Andtheapproximate posterior dis-
tribution overisaGaussian withmean xandstandard deviationN 1=p
N.
Thevariational freeenergy minimization approac hhastheniceprop-
ertythatitisparameterization-indep enden t;itavoidstheproblem ofbasis-
dependence fromwhichMAP metho dsandLaplace's metho dsuer.
Aconvenientsoftwarepackageforautomatic implemen tation ofvariational
inference ingraphical modelsisVIBES (Bishop andWinn, 2000; Bishop etal.,
2002; Bishop andWinn, 2003). Itplaysthesameroleforvariational inference
asBUGSplaysforMonteCarlo inference.
33.6 Interlude
Oneofmystuden tsasked:
Howdoyouevercomeupwithauseful approximating distribution,
giventhatthetruedistribution issocomplex youcan't compute
itdirectly?
Let'sanswerthisquestion inthecontextofBayesian datamodelling. Letthe
`true' distribution ofinterest betheposterior probabilit ydistribution overa
setofparameters x,P(xjD).Astandard datamodelling practice istond
asingle, `best-t' setting oftheparameters, x,forexample, bynding the

<<<PAGE 444>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
432 33|Variational Metho ds
maxim umofthelikelihoodfunctionP(Djx),oroftheposterior distribution.
Oneinterpretation ofthisstandard practice isthatthefulldescription of
ourknowledge aboutx,P(xjD),isbeingapproximated byadelta-function,
aprobabilit ydistribution concen trated onx.Fromthisperspective,any
approximating distribution Q(x;),nomatter howcrumm yitis,hastobe
animpro vementonthespikeproduced bythestandard metho d!Soevenif
weuseonlyasimple Gaussian approximation, wearedoing well.
Wenowstudy anapplication ofthevariational approac htoarealistic
example {dataclustering.
33.7 K-means clustering andtheexpectation{maximization algorithm
asavariational metho d
InChapter 20,weintroduced thesoftK-means clustering algorithm, version 1.
InChapter 22,weintroduced versions 2and3ofthisalgorithm, andmotivated
thealgorithm asamaxim umlikelihoodalgorithm.
K-means clustering isanexample ofan`expectation{maximization' (EM)
algorithm, withthetwosteps, whichwecalled `assignmen t'and`update',
beingknownasthe`E-step' andthe`M-step' respectively.
Wenowgiveamore general viewofK-means clustering, duetoNeal
andHinton(1998), inwhichthealgorithm isshowntooptimize avariational
objectivefunction. NealandHinton'sderivationapplies toanyEMalgorithm.
Theprobability ofeverything
Lettheparameters ofthemixture model{themeans, standard deviations, and
weights{bedenoted by.Foreachdatapoint,thereisamissing variable (also
knownasalatentvariable), theclasslabelknforthatpoint.Theprobabilit y
ofeverything, givenourassumed modelH,is
P(fx(n);kngN
n=1;jH)=P(jH)NY
n=1h
P(x(n)jkn;)P(knj)i
:(33.47)
Theposterior probabilit yofeverything, giventhedata, isproportional tothe
probabilit yofeverything:
P(fkngN
n=1;jfx(n)gN
n=1;H)=P(fx(n);kngN
n=1;jH)
P(fx(n)gN
n=1jH): (33.48)
Wenowapproximate thisposterior distribution byaseparable distribution
Qk(fkngN
n=1)Q
 (); (33.49)
anddene avariational freeenergy intheusual way:
~F(Qk;Q
 )=X
fkngZ
dDQk(fkngN
n=1)Q
 ()lnQk(fkngN
n=1)Q
 ()
P(fx(n);kngN
n=1;jH):
(33.50)
~Fisbounded belowbyminustheevidence, lnP(fx(n)gN
n=1jH).Wecannow
makeaniterativ ealgorithm withan`assignmen t'stepandan`update' step.
Intheassignmen tstep,Qk(fkngN
n=1)isadjusted toreduce ~F,forxedQ
 ;in
theupdatestep,Q
 isadjusted toreduce ~F,forxedQk.
Ifwewishtoobtain exactly thesoftK-means algorithm, weimposea
further constrain tonourapproximating distribution: Q
 isconstrained tobe
adelta function centredonapointestimate of,=:
Q
 ()=( ): (33.51)

<<<PAGE 445>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
33.8: Variational metho dsother thanfreeenergy minimization 433
012
-5 0 5Upperbound
1
1+e aexp(a He
2())2[0;1]
Lowerbound
1
1+e ag()exp
(a )=2 ()(a2 2)
where()=[g() 1=2]=2.
Figure 33.5.Illustration ofthe
Jaakk ola{Jordan variational
metho d.Upperandlowerbounds
onthelogistic function (solid line)
g(a)1
1+e a:
These upperandlowerbounds are
exponentialorGaussian functions
ofa,andsoeasier tointegrate
over.Thegraph showsthe
sigmoid function andupperand
lowerbounds with=0:505and
= 2:015.Unfortunately ,thisdistribution contributes tothevariational freeenergy an
innitely large integralRdDQ
 ()lnQ
 (),sowe'dbetterleavethatterm
outof~F,treating itasanadditiv econstan t.[Using adeltafunctionQ
 isnot
agoodideaifouraimistominimize ~F!]Movingon,ouraimistoderivethe
softK-means algorithm.
.Exercise 33.2.[2]Showthat,givenQ
 ()=( ),theoptimalQk,inthe
sense ofminimizing ~F,isaseparable distribution inwhichtheprobabil-
itythatkn=kisgivenbytheresponsibilit yr(n)
k.
.Exercise 33.3.[4]Showthat,givenaseparableQkasdescrib edabove,theop-
timal ,inthesense ofminimizing ~F,isobtained bytheupdatestep
ofthesoftK-means algorithm. (Assume auniform prioron.)
Exercise 33.4.[4]Wecaninstan tlyimpro veontheinnitely large valueof~F
achievedbysoftK-means clustering byallowingQ
 tobeamoregeneral
distribution thanadelta-function. DeriveanupdatestepinwhichQ
 is
allowedtobeaseparable distribution, aproductofQ(fg),Q(fg),
andQ().Discuss whether thisgeneralized algorithm stillsuers from
softK-means's `kaboom'problem, where thealgorithm glues anever-
shrinking Gaussian toonedatapoint.
Sadly,while itsounds likeapromising generalization ofthealgorithm
toallowQ
 tobeanon-delta-function, andthe`kaboom'problem goes
away,other artefacts canariseinthisapproximate inference metho d,
involving localminima of~F.Forfurther reading, see(MacKa y,1997a;
MacKa y,2001).
33.8 Variational metho dsother thanfreeenergy minimization
There areother strategies forapproximating acomplicated distribution P(x),
inaddition tothose based onminimizing therelativ eentropybetweenan
approximating distribution QandP.Oneapproac hpioneered byJaakk ola
andJordan istocreate adjustable upperandlowerboundsQUandQLtoP,
asillustrated ingure 33.5. These bounds (whichareunnormalized densities)
areparameterized byvariational parameters whichareadjusted inorder to
obtain thetightestpossible t.Thelowerboundcanbeadjusted tomaximize
X
xQL(x); (33.52)
andtheupperboundcanbeadjusted tominimize
X
xQU(x): (33.53)

<<<PAGE 446>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
434 33|Variational Metho ds
Using thenormalized versions oftheoptimized bounds wethencompute ap-
proximations tothepredictiv edistributions. Further reading onsuchmetho ds
canbefound inthereferences (Jaakk olaandJordan, 2000a; Jaakk olaandJor-
dan,2000b; Jaakk olaandJordan, 1996; Gibbs andMacKa y,2000).
Further reading
TheBethe andKikuchi freeenergies
InChapter 26wediscussed thesum{pro ductalgorithm forfunctions ofthe
factor-graph form(26.1). Ifthefactor graph istree-lik e,thesum{pro ductalgo-
rithm converges andcorrectly computes themarginal function ofanyvariable
xnandcanalsoyieldthejointmarginal function ofsubsets ofvariables that
appearinacommon factor, suchasxm.
Thesum{pro ductalgorithm mayalsobeapplied tofactor graphs thatare
nottree-lik e.Ifthealgorithm converges toaxedpoint,ithasbeenshown
thatthatxedpointisastationary point(usually aminim um)ofafunction
ofthemessages called theKikuc hifreeenergy .Inthespecialcasewhere all
factors infactor graph arefunctions ofoneortwovariables, theKikuc hifree
energy iscalled theBethe freeenergy.
Forarticles onthisidea,andnewapproximate inference algorithms mo-
tivatedbyit,seeYedidia (2000); Yedidia etal.(2000c); Welling andTeh
(2001); Yuille(2001); Yedidia etal.(2000b); Yedidia etal.(2000a).
33.9 Further exercises
Exercise 33.5.[2,p.435]Thisexercise explores theassertion, made above,that
theapproximations givenbyvariational freeenergy minimization al-
waystendtobemore compact thanthetruedistribution. Consider a
twodimensional Gaussian distribution P(x)withaxesaligned withthe
directions e(1)=(1;1)ande(2)=(1; 1).Letthevariances inthese two
directions be2
1and2
2.What istheoptimal variance ifthisdistribution
isapproximated byaspheric alGaussian withvariance2
Q,optimized by
variational freeenergy minimization? Ifweinstead optimized theobjec-
tivefunction
G=Z
dxP(x)lnP(x)
Q(x;2); (33.54)
what wouldbetheoptimal valueof2?Sketchacontourofthetrue
distribution P(x)andthetwoapproximating distributions inthecase
1=2=10.
[Note thatingeneral itisnotpossible toevaluate theobjectivefunc-
tionG,because integrals under thetruedistribution P(x)areusually
intractable.]
Exercise 33.6.[2,p.436]What doyouthink oftheideaofusing avariational
metho dtooptimize anapproximating distribution Qwhichwethenuse
asaproposaldensit yforimportance sampling?
Exercise 33.7.[2]Dene therelative entropyorKullback{Leiblerdivergencebe-
tweentwoprobabilit ydistributions PandQ,andstateGibbs' inequalit y.
Consider theproblem ofapproximating ajointdistribution P(x;y)bya
separable distribution Q(x;y)=QX(x)QY(y).Showthatiftheobjec-

<<<PAGE 447>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
33.10: Solutions 435
tivefunction forthisapproximation is
G(QX;QY)=X
x;yP(x;y)log2P(x;y)
QX(x)QY(y)
thattheminimal valueofGisachievedwhenQXandQYareequal to
themarginal distributions overxandy.
Nowconsider thealternativ eobjectivefunction
F(QX;QY)=X
x;yQX(x)QY(y)log2QX(x)QY(y)
P(x;y);
theprobabilit ydistribution P(x;y)showninthemargin istobeap-
P(x;y) x
1234
11/81/800
y21/81/800
3001/40
40001/4proximated byaseparable distribution Q(x;y)=QX(x)QY(y).State
thevalueofF(QX;QY)ifQXandQYaresettothemarginal distribu-
tionsoverxandy.
ShowthatF(QX;QY)hasthreedistinct minima, identifythose minima,
andevaluateFateachofthem.
33.10 Solutions
Solution toexercise 33.5(p.434).Weneedtoknowtherelativ eentropybe-
tweentwoone-dimensional Gaussian distributions:
Z
dxNormal(x;0;Q)lnNormal (x;0;Q)
Normal (x;0;P)
=Z
dxNormal(x;0;Q)"
lnP
Q 1
2x2 
1
2
Q 1
2
P!#
(33.55)
=1
2 
ln2
P
2
Q 1+2
Q
2
P!
: (33.56)
So,ifweapproximateP,whose variances are2
1and2
2,byQ,whose variances
areboth2
Q,wend
F(2
Q)=1
2 
ln2
1
2
Q 1+2
Q
2
1+ln2
2
2
Q 1+2
Q
2
2!
; (33.57)
dieren tiating,
d
dln(2
Q)F=1
2"
 2+ 2
Q
2
1+2
Q
2
2!#
; (33.58)
whichiszerowhen
1
2
Q=1
21
2
1+1
2
2
: (33.59)
Thuswesettheapproximating distribution's inversevariance tothemean
inversevariance ofthetarget distribution P.
Inthecase1=10and2=1,weobtainQ'p
2,whichisjustafactor
ofp
2larger than2,prettymuchindependent ofthevalueofthelarger
standard deviation1.Variational freeenergyminimization typicallyleadsto
approximating distributions whose length scalesmatch theshortest length scale
ofthetargetdistribution. Theapproximating distribution mightbeviewedas
toocompact.

<<<PAGE 448>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
436 33|Variational Metho ds
(a) (b)Figure 33.6.Twoseparable
Gaussian approximations (dotted
lines) toabivariate Gaussian
distribution (solid line). (a)The
approximation thatminimizes the
variational freeenergy .(b)The
approximation thatminimizes the
objectivefunctionG.Ineach
gure, thelinesshowthecontours
atwhichxTAx=1,where Ais
theinversecovariance matrix of
theGaussian.
Incontrast, ifweusetheobjectivefunctionGthenwend:
G(2
Q)=1
2 
ln2
Q+2
1
2
Q+ln2
Q+2
2
2
Q!
+constan t; (33.60)
where theconstan tdependson1and2only.Dieren tiating,
d
dln2
QG=1
2 
2 2
1
2
Q+2
2
2
Q!
; (33.61)
whichiszerowhen
2
Q=1
2
2
1+2
2
: (33.62)
Thuswesettheapproximating distribution's variance tothemean variance
ofthetarget distribution P.
Inthecase1=10and2=1,weobtainQ'10=p
2,whichisjusta
factor ofp
2smaller than1,independen tofthevalueof2.
Thetwoapproximations areshowntoscaleingure 33.6.
Solution toexercise 33.6(p.434).Thebestpossible variational approximation
isofcourse thetarget distribution P.Assuming thatthisisnotpossible, a
goodvariational approximation ismorecompactthanthetruedistribution.
Incontrast, agoodsampler ismoreheavytailedthanthetruedistribution.
Anover-compact distribution wouldbealousy sampler withalargevariance.

<<<PAGE 449>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
34
Indep enden tComp onentAnalysis and
LatentVariable Modelling
34.1 Latentvariable models
Manystatistical modelsaregenerativ emodels(that is,modelsthatspecify
afullprobabilit ydensit yoverallvariables inthesituation) thatmakeuseof
latentvariables todescrib eaprobabilit ydistribution overobserv ables.
Examples oflatentvariable modelsinclude Chapter 22'smixture models,
whichmodeltheobserv ables ascoming fromasuperposedmixture ofsimple
probabilit ydistributions (thelatentvariables aretheunkno wnclasslabels
oftheexamples); hidden Markovmodels(Rabiner andJuang, 1986; Durbin
etal.,1998); andfactor analysis.
Thedecodingproblem forerror-correcting codescanalsobeviewedin
terms ofalatentvariable model{gure 34.1. Inthatcase, theencoding
matrix Gisnormally knowninadvance. Inlatentvariable modelling, the
parameters equivalenttoGareusually notknown,andmustbeinferred from
thedataalong withthelatentvariables s.yN y1GsK s1
Figure 34.1.Error-correcting
codesaslatentvariable models.
TheKlatentvariables arethe
independen tsource bits
s1;:::;sK;these giverisetothe
observ ablesviathegenerator
matrix G.
Usually ,thelatentvariables haveasimple distribution, often aseparable
distribution. Thuswhen wetalatentvariable model,wearending ade-
scription ofthedatainterms of`indep enden tcomponents'.The`indep enden t
componentanalysis' algorithm corresp ondstoperhaps thesimplest possible
latentvariable modelwithcontinuouslatentvariables.
34.2 Thegenerativ emodelforindependen tcomponentanalysis
AsetofNobserv ationsD=fx(n)gN
n=1areassumed tobegenerated asfollows.
EachJ-dimensional vectorxisalinear mixture ofIunderlying source signals,
s:
x=Gs; (34.1)
where thematrix ofmixing coecien tsGisnotknown.
Thesimplest algorithm results ifweassume thatthenumberofsources
isequal tothenumberofobserv ations, i.e.,I=J.Ouraimistorecover
thesource variables s(within some multiplicativ efactors, andpossibly per-
muted). Toputitanother way,weaimtocreate theinverseofG(within a
post-multiplicativ efactor) givenonlyasetofexamplesfxg.Weassume that
thelatentvariables areindependen tlydistributed, withmarginal distributions
P(sijH)pi(si).HereHdenotes theassumed formofthismodelandthe
assumed probabilit ydistributions piofthelatentvariables.
Theprobabilit yoftheobserv ables andthehidden variables, givenGand
437

<<<PAGE 450>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
438 34|Indep enden tComp onentAnalysis andLatentVariable Modelling
H,is:
P(fx(n);s(n)gN
n=1jG;H)=NY
n=1h
P(x(n)js(n);G;H)P(s(n)jH)i
(34.2)
=NY
n=12
40
@Y
j
x(n)
j P
iGjis(n)
i1
A Y
ipi(s(n)
i)!3
5: (34.3)
Weassume thatthevectorxisgenerated without noise.Thisassumption is
notusually made inlatentvariable modelling, sincenoise-free dataarerare;
butitmakestheinference problem farsimpler tosolve.
Thelikeliho odfunction
Forlearning aboutGfromthedataD,therelevantquantityisthelikelihood
function
P(DjG;H)=Y
nP(x(n)jG;H) (34.4)
whichisaproductoffactors eachofwhichisobtained bymarginalizing over
thelatentvariables. When wemarginalize overdelta functions, remem ber
thatRds(x vs)f(s)=1
vf(x=v):Weadopt summation conventionatthis
point,suchthat, forexample,Gjis(n)
iP
iGjis(n)
i.Asingle factor inthe
likelihoodisgivenby
P(x(n)jG;H)=Z
dIs(n)P(x(n)js(n);G;H)P(s(n)jH)(34.5)
=Z
dIs(n)Y
j
x(n)
j Gjis(n)
iY
ipi(s(n)
i)(34.6)
=1
jdetGjY
ipi(G 1
ijxj) (34.7)
)lnP(x(n)jG;H)= lnjdetGj+X
ilnpi(G 1
ijxj): (34.8)
Toobtain amaxim umlikelihoodalgorithm wendthegradien tofthelog
likelihood.IfweintroduceWG 1,theloglikelihoodcontributed bya
single example maybewritten:
lnP(x(n)jG;H)=lnjdetWj+X
ilnpi(Wijxj): (34.9)
We'llassume fromnowonthatdetWispositive,sothatwecanomitthe
absolute valuesign.Wewillneedthefollowingidentities:
@
@GjilndetG=G 1
ij=Wij (34.10)
@
@GjiG 1
lm= G 1
ljG 1
im= WljWim (34.11)
@
@Wijf= Gjm@
@Glmf
Gli: (34.12)
LetusdeneaiWijxj,
i(ai)dlnpi(ai)=dai; (34.13)

<<<PAGE 451>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
34.2: Thegenerativ emodelforindependen tcomponentanalysis 439
Repeatforeachdatap ointx:
1.Putxthrough alinear mapping:
a=Wx:
2.Putathrough anonlinear map:
zi=i(ai);
where apopular choice foris= tanh(ai).
3.Adjust theweightsinaccordance with
W/[WT] 1+zxT:Algorithm 34.2.Independen t
componentanalysis {online
steepestascentsversion.
Seealsoalgorithm 34.4,whichis
tobepreferred.
andzi=i(ai),whichindicates inwhichdirectionaineeds tochange tomake
theprobabilit yofthedatagreater. Wemaythenobtain thegradien twith
respecttoGjiusing equations (34.10) and(34.11):
@
@GjilnP(x(n)jG;H)= Wij aizi0Wi0j: (34.14)
Oralternativ ely,thederivativewithrespecttoWij:
@
@WijlnP(x(n)jG;H)=Gji+xjzi: (34.15)
IfwechoosetochangeWsoastoascend thisgradien t,weobtain thelearning
rule
W/[WT] 1+zxT: (34.16)
Thealgorithm sofarissummarized inalgorithm 34.2.
Choicesof
Thechoice ofthefunctiondenes theassumed prior distribution ofthe
latentvariables.
Let's rstconsider thelinear choicei(ai)= ai,whichimplicitly (via
equation 34.13) assumes aGaussian distribution onthelatentvariables. The
Gaussian distribution onthelatentvariables isinvariantunder rotation ofthe
latentvariables, sotherecanbenoevidence favouring anyparticular alignmen t
ofthelatentvariable space. Thelinear algorithm isthusuninteresting inthat
itwillneverrecoverthematrix Gortheoriginal sources. Ouronlyhopeis
thusthatthesources arenon-Gaussian. Thankfully ,most realsources have
non-Gaussian distributions; often theyhaveheaviertailsthanGaussians.
Wethusmoveontothepopular tanhnonlinearit y.If
i(ai)= tanh(ai) (34.17)
thenimplicitly weareassuming
pi(si)/1=cosh(si)/1
esi+e si: (34.18)
Thisisaheavier-tailed distribution forthelatentvariables thantheGaussian
distribution.

<<<PAGE 452>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
440 34|Indep enden tComp onentAnalysis andLatentVariable Modelling
(a)x204
2
0
-2
-4x1
0 4 2 0 -2 -4
(b)x204
2
0
-2
-4x1
0 4 2 0 -2 -4
(c)x208
6
4
2
0
-2
-4
-6
-8x1
0 86420-2-4-6-8
(d) x10 3020100 -10-20-30x2030
20
10
0
-10
-20
-30Figure 34.3.Illustration ofthe
generativ emodelsimplicit inthe
learning algorithm. (a)
Distributions overtwoobserv ables
generated by1=coshdistributions
onthelatentvariables, for
G=3=41=2
1=21
(compact
distribution) and
G=2 1
 13=2
(broader
distribution). (b)Contours ofthe
generativ edistributions when the
latentvariables haveCauchy
distributions. Thelearning
algorithm tsthisamoeboid
objecttotheempirical datain
suchawayastomaximize the
likelihood.Thecontourplotin
(b)doesnotadequately represen t
thisheavy-tailed distribution. (c)
PartofthetailsoftheCauchy
distribution, giving thecontours
0:01:::0:1times thedensit yat
theorigin. (d)Some datafrom
oneofthegenerativ edistributions
illustrated in(b)and(c).Canyou
tellwhich?200samples were
created, ofwhich196fellinthe
plotted region.Wecould alsouseatanh nonlinearit ywithgain,thatis,i(ai)=
 tanh(ai),whose implicit probabilistic modelispi(si)/1=[cosh(si)]1=.In
thelimitoflarge,thenonlinearit ybecomes astepfunction andtheprobabil-
itydistribution pi(si)becomes abiexponentialdistribution, pi(si)/exp( jsj).
Inthelimit!0,pi(si)approac hesaGaussian withmean zeroandvariance
1=.Heavier-tailed distributions thanthese mayalsobeused. TheStuden t
andCauchydistributions spring tomind.
Example distributions
Figures 34.3(a{c) illustrate typical distributions generated bytheindependen t
componentsmodelwhen thecomponentshave1=coshandCauchydistribu-
tions. Figure 34.3d showssomesamples fromtheCauchymodel.TheCauchy
distribution, beingthemoreheavy-tailed, givestheclearest picture ofhowthe
predictiv edistribution dependsontheassumed generativ eparameters G.
34.3 Acovariant,simpler, andfaster learning algorithm
Wehavethusderivedalearning algorithm thatperforms steepestdescen ts
onthelikelihoodfunction. Thealgorithm doesnotworkveryquickly,even
ontoydata; thealgorithm isill-conditioned andillustrates nicely thegeneral
advice that,whilending thegradient ofanobjectivefunction isasplendid
idea,ascending thegradient directlymaynotbe.Thefactthatthealgorithm is
ill-conditioned canbeseeninthefactthatitinvolvesamatrix inverse,which
canbearbitrarily largeorevenundened.

<<<PAGE 453>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
34.3: Acovariant,simpler, andfaster learning algorithm 441
Covariant optimization ingeneral
Theprinciple ofcovariance saysthataconsisten talgorithm should givethe
sameresults independen toftheunitsinwhichquantitiesaremeasured (Knuth,
1968). Aprime example ofanon-covariantalgorithm isthepopular steepest
descen tsrule.Adimensionless objectivefunctionL(w)isdened, itsderiva-
tivewithrespecttosome parameters wiscomputed, andthenwischanged
bytherule
wi=@L
@wi: (34.19)
Thispopular equation isdimensionally inconsisten t:theleft-hand sideofthis
equation hasdimensions of[wi]andtheright-hand sidehasdimensions 1=[wi].
Thebehaviour ofthelearning algorithm (34.19) isnotcovariantwithrespect
tolinear rescaling ofthevectorw.Dimensional inconsistency isnottheendof
theworld,asthesuccess ofnumerous gradien tdescen talgorithms hasdemon-
strated, andindeed ifdecreases withn(during on-line learning) as1=nthen
theMunro{Robbins theorem (Bishop, 1992, p.41)showsthattheparameters
willasymptotically convergetothemaxim umlikelihoodparameters. Butthe
non-co variantalgorithm maytakeaverylargenumberofiterations toachieve
thisconvergence; indeed manyformer users ofsteepestdescen tsalgorithms
prefer tousealgorithms suchasconjugate gradien tsthatadaptiv elygure
outthecurvature oftheobjectivefunction. Thedefense ofequation (34.19)
thatpointsoutcould beadimensional constan tisuntenable ifnotallthe
parameters wihavethesame dimensions.
Thealgorithm wouldbecovariantifithadtheform
wi=X
i0Mii0@L
@wi; (34.20)
whereMisapositive-denite matrix whosei;i0elemen thasdimensions [wiwi0].
Fromwhere canweobtain suchamatrix? Twosources ofsuchmatrices are
metrics andcurvatur es.
Metrics andcurvatur es
Ifthere isanatural metric thatdenes distanc esinourparameter spacew,
thenamatrix Mcanbeobtained fromthemetric. There isoften anatural
choice. Inthespecialcasewhere there isaknownquadratic metric dening
thelength ofavectorw,thenthematrix canbeobtained fromthequadratic
form. Forexample ifthelength isw2thenthenatural matrix isM=I,and
steepestdescen tsisappropriate.
Another wayofnding ametric istolookatthecurvature oftheobjective
function, dening A rrL(wherer@=@w).Then thematrix M=
A 1willgiveacovariantalgorithm; whatismore, thisalgorithm istheNewton
algorithm, sowerecognize thatitwillalleviate oneoftheprincipal diculties
withsteepestdescen ts,namely itsslowconvergence toaminim umwhen the
objectivefunction isatallill-conditioned. TheNewton algorithm converges
totheminim uminasingle stepifLisquadratic.
Insome problems itmaybethatthecurvatureAconsists ofbothdata-
dependen tterms anddata-indep enden tterms; inthiscase,onemightchoose
todene themetric using thedata-indep enden tterms only(Gull, 1989). The
resulting algorithm willstillbecovariantbutitwillnotimplemen tanexact
Newton step. Obviously there aremanycovariantalgorithms; there isno
unique choice. Butcovariantalgorithms areasmall subset ofthesetofall
algorithms!

<<<PAGE 454>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
442 34|Indep enden tComp onentAnalysis andLatentVariable Modelling
Backtoindependent component analysis
Forthepresen tmaxim umlikelihoodproblem wehaveevaluated thegradien t
withrespecttoGandthegradien twithrespecttoW=G 1.Steepest
ascentsinWisnotcovariant.Letusconstruct analternativ e,covariant
algorithm withthehelpofthecurvature oftheloglikelihood.Taking the
second derivativeoftheloglikelihoodwithrespecttoWweobtain twoterms,
therstofwhichisdata-indep enden t:
@Gji
@Wkl= GjkGli; (34.21)
andthesecond ofwhichisdata-dep enden t:
@(zixj)
@Wkl=xjxlikz0
i;(nosumoveri) (34.22)
wherez0isthederivativeofz.Itistempting todropthedata-dep enden tterm
anddene thematrix Mby[M 1](ij)(kl)=[GjkGli].However,thismatrix
isnotpositivedenite (ithasatleastonenon-p ositiveeigenvalue), soitis
apoorapproximation tothecurvature oftheloglikelihood,whichmustbe
positivedenite intheneighbourho odofamaxim umlikelihoodsolution. We
musttherefore consult thedata-dep enden ttermforinspiration. Theaimis
tondaconvenientapproximation tothecurvature andtoobtain acovariant
algorithm, notnecessarily toimplemen tanexact Newton step. What isthe
average valueofxjxlikz0
i?IfthetruevalueofGisG,then

xjxlikz0
i=D
G
jmsmsnG
lnikz0
iE
: (34.23)
Wenowmakeseveralsevereapproximations: wereplace Gbythepresen t
valueofG,andreplace thecorrelated averagehsmsnz0
iibyhsmsnihz0
ii
mnDi.Hereisthevariance{co variance matrix ofthelatentvariables
(whichisassumed toexist), andDiisthetypical valueofthecurvature
d2lnpi(a)=da2.Giventhatthesources areassumed tobeindependen t,
andDarebothdiagonal matrices. These approximations motivatethema-
trixMgivenby:
[M 1](ij)(kl)=GjmmnGlnikDi; (34.24)
thatis,
M(ij)(kl)=Wmj 1
mnWnlikD 1
i: (34.25)
Forsimplicit y,wefurther assume thatthesources aresimilar toeachother so
thatandDarebothhomogeneous andthatD=1.Thiswillleadusto
analgorithm thatiscovariantwithrespecttolinear rescaling ofthedatax,
butnotwithrespecttolinear rescaling ofthelatentvariables. Wethususe:
M(ij)(kl)=WmjWmlik: (34.26)
Multiplying thismatrix bythegradien tinequation (34.15) weobtain the
followingcovariantlearning algorithm:
Wij= Wij+Wi0jai0zi: (34.27)
Notice thatthisexpression doesnotrequire anyinversion ofthematrix W.
Theonlyadditional computation oncezhasbeencomputed isasingle back-
wardpassthrough theweightstocompute thequantity
x0
j=Wi0jai0 (34.28)

<<<PAGE 455>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
34.3: Acovariant,simpler, andfaster learning algorithm 443
Repeatforeachdatap ointx:
1.Putxthrough alinear mapping:
a=Wx:
2.Putathrough anonlinear map:
zi=i(ai);
where apopular choice foris= tanh(ai).
3.Putabackthrough W:
x0=WTa:
4.Adjust theweightsinaccordance with
W/W+zx0T:Algorithm 34.4.Independen t
componentanalysis {covariant
version.
interms ofwhichthecovariantalgorithm reads:
Wij=
Wij+x0
jzi
: (34.29)
Thequantity
Wij+x0
jzi
ontheright-hand sideissometimes called the
natural gradien t.Thecovariantindependen tcomponentanalysis algorithm is
summarized inalgorithm 34.4.
Further reading
ICAwasoriginally derivedusing aninformation maximization approac h(Bell
andSejnowski, 1995). Another viewofICA, interms ofenergy functions,
whichmotivatesmore general models,isgivenbyHintonetal.(2001). An-
other generalization ofICAcanbefound inPearlmutter andParra(1996,
1997).There isnowanenormous literature onapplications ofICA. Avari-
ational freeenergy minimization approac htoICA-lik emodelsisgivenin
(Miskin, 2001; Miskin andMacKa y,2000; Miskin andMacKa y,2001). Further
reading onblind separation, including non-ICA algorithms, canbefound in
(Jutten andHerault, 1991; Comon etal.,1991; Hendin etal.,1994; Amari
etal.,1996; Hojen-Sorensen etal.,2002).
Innite models
While latentvariable modelswithanitenumberoflatentvariables arewidely
used, itisoften thecasethatourbeliefs aboutthesituation wouldbemost
accurately captured byaverylargenumberoflatentvariables.
Consider clustering, forexample. Ifweattackspeechrecognition bymod-
elling wordsusing acluster model,howmanyclusters should weuse? The
numberofpossible wordsisunbounded (section 18.2), sowewouldreally like
touseamodelinwhichit'salwayspossible fornewclusters toarise.
Furthermore, ifwedoacareful jobofmodelling thecluster corresp onding
tojustoneEnglish word,wewillprobably ndthatthecluster foroneword
should itself bemodelled ascomposedofclusters {indeed, ahierarc hyof

<<<PAGE 456>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
444 34|Indep enden tComp onentAnalysis andLatentVariable Modelling
clusters within clusters. Therstlevelsofthehierarc hywoulddivide male
speakersfromfemale, andwouldseparate speakersfromdieren tregions {
India, Britain, Europ e,andsoforth. Within eachofthose clusters wouldbe
subclusters forthedieren taccentswithin eachregion. Thesubclusters could
havesubsub clusters rightdowntothelevelofvillages, streets, orfamilies.
Thuswewouldoften liketohaveinnite numbersofclusters; insome
cases theclusters wouldhaveahierarc hicalstructure, andinother cases the
hierarc hywouldbeat.So,howshould suchinnite modelsbeimplemen ted
innite computers? Andhowshould wesetupourBayesian modelssoasto
avoidgetting sillyanswers?
Innite mixture modelsforcategorical dataarepresen tedinNeal(1991),
along withaMonteCarlo metho dforsimulating inferences andpredictions.
Innite Gaussian mixture modelswithaathierarc hical structure arepre-
sentedinRasmussen (2000). Neal(2001) showshowtouseDirichletdiusion
treestodene modelsofhierarc hicalclusters. Most ofthese ideas build on
theDirichletprocess(section 18.2). Thisremains anactiveresearc harea
(Rasm ussen andGhahramani, 2002; Bealetal.,2002).
34.4 Exercises
Exercise 34.1.[3]Repeatthederivation ofthealgorithm, butassume asmall
amoun tofnoise inx:x=Gs+n;sotheterm
x(n)
j P
iGjis(n)
i
inthejointprobabilit y(34.3) isreplaced byaprobabilit ydistribution
overx(n)
jwithmeanP
iGjis(n)
i.Showthat,ifthisnoisedistribution has
sucien tlysmall standard deviation, theidenticalalgorithm results.
Exercise 34.2.[3]Implemen tthecovariantICAalgorithm andapply ittotoy
data.
Exercise 34.3.[4-5]Create algorithms appropriate forthesituations: (a)xin-
cludes substan tialGaussian noise; (b)more measuremen tsthanlatent
variables (J>I);(c)fewermeasuremen tsthanlatentvariables (J<I).
Factor analysis assumes thattheobserv ations xcanbedescrib edinterms of
independen tlatentvariablesfskgandindependen tadditiv enoise. Thusthe
observ ablexisgivenby
x=Gs+n; (34.30)
wherenisanoisevectorwhose componentshaveaseparable probabilit ydistri-
bution. Infactor analysis itisoftenassumed thattheprobabilit ydistributions
offskgandfnigarezero-mean Gaussians; thenoiseterms mayhavedieren t
variances2
i.
Exercise 34.4.[4]Makeamaxim umlikelihoodalgorithm forinferring Gfrom
data, assuming thegenerativ emodelx=Gs+niscorrect andthats
andnhaveindependen tGaussian distributions. Include parameters 2
j
todescrib ethevariance ofeachnj,andmaximize thelikelihoodwith
respecttothem too.Letthevariance ofeachsibe1.
Exercise 34.5.[4C]Implemen ttheinnite Gaussian mixture modelofRasmussen
(2000).

<<<PAGE 457>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
35
Random Inference Topics
35.1 What doyouknowifyouareignoran t?
Example 35.1. Arealvariablexismeasured inanaccurate experimen t.For
example,xmightbethehalf-life oftheneutron, thewavelength oflight
emitted byarey,thedepth ofLakeVostok, orthemassofJupiter's
moonIo.
What istheprobabilit ythatthevalueofxstarts witha`1',likethe
charge oftheelectron (inS.I.units),
e=1:602:::10 19C;
andtheBoltzmann constan t,
k=1:38066:::10 23JK 1?
Andwhat istheprobabilit ythatitstarts witha`9',liketheFaraday
constan t,
F=9:648:::104Cmol 1?
What aboutthesecond digit? What istheprobabilit ythatthemantissa
ofxstarts `1.1... ',andwhatistheprobabilit ythatxstarts `9.9... '?
Solution. Anexpertonneutrons, reies, Antarctica, orJovemightbeableto
predict thevalueofx,andthuspredict therstdigitwithsomecondence, but
whataboutsomeone withnoknowledge ofthetopic? What istheprobabilit y
distribution corresp onding to`knowingnothing' ?
Onewaytoattackthisquestion istonotice thattheunits ofxhavenot
beenspecied. Ifthehalf-life oftheneutron weremeasured infortnigh ts
instead ofseconds, thenumberxwouldbedivided by1209600;ifitwere
measured inyears,itwouldbedivided by3107.Now,isourknowledge
aboutx,and,inparticular, ourknowledge ofitsrstdigit, aected bythe
change inunits? Fortheexpert,theanswerisyes;butletustakesomeone
trulyignoran t,forwhom theanswerisno;theirpredictions abouttherstdigit
ofxareindependen toftheunits. Thearbitrariness oftheunitscorresp ondsto
invariance oftheprobabilit ydistribution whenxismultiplie dbyanynumber.metres6
1234567891020304050607080
inches6
405060708090100200300400500600700800900100020003000
feet6
3456789102030405060708090100200
Figure 35.1.When viewedona
logarithmic scale, scales using
dieren tunits aretranslated
relativ etoeachother.
Ifyoudon't knowtheunitsthataquantityismeasured in,theprobabilit y
oftherstdigitmustbeproportional tothelength ofthecorresp onding piece
oflogarithmic scale. Theprobabilit ythattherstdigitofanumberis1is
thus
p1=log2 log1
log10 log1=log2
log10: (35.1)
445

<<<PAGE 458>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
446 35|Random Inference Topics
Now,210=1024'103=1000, sowithout needing acalculator, wehave
12345678910
6
?P(1)6
?P(3)6? P(9)10log2'3log10and
p1'3
10: (35.2)
More generally ,theprobabilit ythattherstdigitisdis
(log(d+1) log(d))=(log10 log1)=log10(1+1=d): (35.3)
Thisobserv ation aboutinitial digits isknownasBenford's law.Ignorance
doesnotcorresp ondtoauniform probabilit ydistribution. 2
.Exercise 35.2.[2]Apinisthrowntumblingintheair.What istheprobabilit y
distribution oftheangle1betweenthepinandthevertical atamomen t
while itisintheair?Thetumbling pinisphotographed. What isthe
probabilit ydistribution oftheangle3betweenthepinandthevertical
asimaged inthephotograph?
.Exercise 35.3.[2]Recordbreaking .Consider keeping trackoftheworldrecord
forsome quantityx,sayearthquak emagnitude, orlongjump distances
jumpedatworldchampionships. Ifweassume thatattempts tobreak
therecord takeplace atasteady rate,andifweassume thattheunder-
lyingprobabilit ydistribution oftheoutcomex,P(x),isnotchanging {
anassumption thatIthink isunlikelytobetrueinthecaseofsports
endeavours,butaninteresting assumption toconsider nonetheless {and
assuming noknowledge atallaboutP(x),whatcanbepredicted about
successiv eintervalsbetweenthedates when records arebroken?
35.2 TheLuria{Delbr uckdistribution
Exercise 35.4.[3C,p.449]Intheirlandmark paperdemonstrating thatbacteria
could mutate fromvirus sensitivit ytovirus resistance, Luria andDelbruck
(1943) wantedtoestimate themutation rateinanexponentially-gro wingpop-
ulation fromthetotal numberofmutantsfound attheendoftheexperi-
ment.Thisproblem isdicult because thequantitymeasured (thenumber
ofmutated bacteria) hasaheavy-tailed probabilit ydistribution: amutation
occuring early intheexperimen tcangiverisetoahugenumberofmutants.
Unfortunately ,Luria andDelbruckdidn't knowBayes'theorem, andtheirway
ofcoping withtheheavy-tailed distribution involvesarbitrary hacksleading to
twodieren testimators ofthemutation rate.Oneofthese estimators (based
onthemean numberofmutated bacteria, averaging overseveralexperimen ts)
hasappallingly large variance, yetsampling theorists continuetouseitand
basecondence intervalsaround it(Kepler andOprea, 2001). Inthisexercise
you'lldotheinference right.
Ineachculture, asingle bacterium thatisnotresistant givesrise,afterg
generations, toN=2gdescendan ts,allclones except fordierences arising
frommutations. Thenalculture isthenexposedtoavirus, andthenumber
ofresistan tbacterianismeasured. According tothenowaccepted mutation
hypothesis, theseresistan tbacteria gottheirresistance fromrandom mutations
thattookplace during thegrowthofthecolony.Themutation rate(percell
pergeneration), a,isaboutoneinahundred million. Thetotalnumberof
opportunities tomutate isN,sincePg 1
i=02i'2g=N.Ifabacterium mutates
attheithgeneration, itsdescendan tsallinherit themutation, andthenal
numberofresistan tbacteria contributed bythatoneancestor is2g i.

<<<PAGE 459>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
35.3: Inferring causation 447
GivenMseparate experimen ts,ineachofwhichacolonyofsizeNis
created, andwhere themeasured numbersofresistan tbacteria arefnmgM
m=1,
whatcanweinferaboutthemutation rate,a?
Maketheinference giventhefollowingdataset fromLuria andDelbruck,
forN=2:4108:fnmg=f1;0;3;0;0;5;0;5;0;6;107;0;0;0;1;0;0;64;0;35g.
[Asmall amoun tofcomputation isrequired tosolvethisproblem.]
35.3 Inferring causation
Exercise 35.5.[2,p.450]IntheBayesian graphical modelcomm unity,thetask
ofinferring whichwaythearrowspoint{thatis,whichnodesareparents,
andwhichchildren {isoneonwhichmuchhasbeenwritten.
Inferring causation istrickybecause of`likelihoodequivalence'. Twograph-
icalmodelsarelikelihood-equiv alentifforanysetting oftheparameters of
either, there exists asetting oftheparameters oftheothers suchthatthetwo
jointprobabilit ydistributions ofallobserv ables areidentical. Anexample of
apairoflikelihood-equiv alentmodelsareA!BandB!A.Themodel
A!Basserts thatAistheparentofB,or,inverysloppyterminology ,`A
causesB'.Anexample ofasituation where `B!A'istrueisthecasewhere
Bisthevariable `burglar inhouse' andAisthevariable `alarm isringing'.
Hereitisliterally truethatBcausesA.Butthischoice ofwordsisconfusing if
applied toanother example,R!D,whereRdenotes `itrained thismorning'
andDdenotes `thepavementisdry'.`RcausesD'isconfusing. I'lltherefore
usethewords`BisaparentofA'todenote causation. Some statistical meth-
odsthatusethelikelihoodalone areunable tousedatatodistinguish between
likelihood-equiv alentmodels.InaBayesian approac h,ontheother hand, two
likelihood-equiv alentmodelsmaynevertheless besomewhat distinguished, in
thelightofdata, sincelikelihood-equiv alence doesnotforceaBayesian touse
priors thatassign equivalentdensities overthetwoparameter spaces ofthe
models.
However,manyBayesian graphical modelling folks, perhaps outofsym-
pathyfortheirnon-Ba yesian colleagues, orfromalatenturgenottoappear
dieren tfromthem, deliberately discard thispotentialadvantageofBayesian
metho ds{theabilitytoinfercausation fromdata{byskewing theirmodels
sothattheabilitygoesaway;awidespread orthodoxyholds thatoneshould
identifythechoices ofpriorforwhich`prior equivalence' holds, i.e.,thepriors
suchthatmodelsthatarelikelihood-equiv alentalsohaveidenticalposterior
probabilities, andthenoneshould useoneofthose priors ininference and
prediction. Thisargumen tmotivatestheuse,astheprioroverallprobabilit y
vectors, ofspecially-constructed Dirichletdistributions.
Inmyviewitisaphilosophical error touseonlythose priors suchthat
causation cannot beinferred. Priors should besettodescrib eone's assump-
tions; when thisisdone, it'slikelythatinteresting inferences aboutcausation
canbemade fromdata.
Inthisexercise, you'llmakeanexample ofsuchaninference.
Consider thetoyproblem whereAandBarebinary variables. Thetwo
modelsareHA!BandHB!A.HA!Basserts thatthemarginal probabil-
ityofAcomes fromabetadistribution withparameters (1;1),i.e.,theuni-
formdistribution; andthatthetwoconditional distributions P(bja=0)and
P(bja=1)alsocome independen tlyfrombetadistributions withparameters
(1;1).Theother modelassigns similar priors tothemarginal probabilit yof
Bandtheconditional distributions ofAgivenB.Dataaregathered, andthe

<<<PAGE 460>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
448 35|Random Inference Topics
counts,givenF=1000outcomes, are
a=0a=1
b=0760 5765
b=1190 45235
950 50(35.4)
What aretheposterior probabilities ofthetwohypotheses?
Hint:it'sagoodideatoworkthisexercise outsymbolically inorder tospotallthe
simplications thatemerge.
	(x)=d
dxln (x)'ln(x) 1
2x+O(1=x2): (35.5)
Thetopic ofinferring causation isacomplex one.ThefactthatBayesian
inference cansensibly beusedtoinferthedirections ofarrowsingraphs seems
tobeaneglected view, butitiscertainly notthewhole story.SeePearl(2000)
fordiscussion ofmanyother aspectsofcausalit y.
35.4 Further exercises
Exercise 35.6.[3]Photons arriving ataphoton detector arebelievedtobeemit-
tedasaPoisson processwithatime-v arying rate,
(t)=exp(a+bsin(!t+)); (35.6)
where theparameters a,b,!,andareknown.Dataarecollected during
thetimet=0:::T.GiventhatNphotons arrivedattimesftngN
n=1,
discuss theinference ofa,b,!,and.[Further reading: Gregory and
Loredo (1992).]
.Exercise 35.7.[2]Adataleconsisting oftwocolumns ofnumbershasbeen
printedinsuchawaythattheboundaries betweenthecolumns are
unclear. Herearetheresulting strings.
891.10.0 912.20.0 874.10.0 870.20.0 836.10.0 861.20.0
903.10.0 937.10.0 850.20.0 916.20.0 899.10.0 907.10.0
924.20.0 861.10.0 899.20.0 849.10.0 887.20.0 840.10.0
849.20.0 891.10.0 916.20.0 891.10.0 912.20.0 875.10.0
898.20.0 924.10.0 950.20.0 958.10.0 971.20.0 933.10.0
966.20.0 908.10.0 924.20.0 983.10.0 924.20.0 908.10.0
950.20.0 911.10.0 913.20.0 921.25.0 912.20.0 917.30.0
923.50.0
Discuss howprobable itis,giventhese data, thatthecorrect parsing of
eachitemis:
(a)891:10:0!891:10:0,etc.
(b)891:10:0!891:10:0,etc.
[Aparsing ofastring isagrammatical interpretation ofthestring. For
example, `Punchbores'could beparsed as`Punch(noun) bores(verb)',
or`Punch(imperativeverb)bores(plural noun)'.]
.Exercise 35.8.[2]Inanexperimen t,themeasured quantitiesfxngcome inde-
penden tlyfromabiexponentialdistribution withmean,
P(xj)=1
Zexp( jx j);

<<<PAGE 461>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
35.5: Solutions 449
whereZisthenormalizing constan t,Z=2.Themeanisnotknown.
Anexample ofthisdistribution, with=1,isshowningure 35.2.
-3-2-10123
Figure 35.2.Thebiexponential
distribution P(xj=1).Assuming thefourdatap ointsare
fxng=f0;0:9;2;6g;012345678
what dothese datatellusabout?Include detailed sketchesinyour
answer.Givearange ofplausible values of.
35.5 Solutions
Solution toexercise 35.4(p.446).Apopulation ofsizeNhasNopportunities
tomutate. Theprobabilit yofthenumberofmutations thatoccured,r,is
roughly Poisson
P(rja;N)=e aN(aN)r
r!: (35.7)
(This isslightlyinaccurate because thedescendan tsofamutantcannot them-
selvesundergo thesame mutation.) Eachmutation givesrisetoanumberof
nalmutantcellsnithatdependsonthegeneration timeofthemutation. If
multiplication wentlikeclockworkthentheprobabilit yofnibeing1would
be1=2,theprobabilit yof2wouldbe1=4,theprobabilit yof4wouldbe1=8,
andP(ni)=1=(2n)forallnithatarepowersoftwo.Butwedon't expect
themutantprogen ytodivide inexact synchrony,andwedon't knowthepre-
cisetiming oftheendoftheexperimen tcompared tothedivision times. A
smoothed version ofthisdistribution thatpermits allintegers tooccuris
P(ni)=1
Z1
n2
i; (35.8)
whereZ=2=6=1:645.[This distribution's momen tsareallwrong, since
nicanneverexceedN,butwhocares aboutmomen ts?{onlysampling
theory statisticians whoarebarking upthewrong tree,constructing `unbiased
estimators' suchas^a=n=ln(N).Theerrorthatweintroduceinthelikelihood
function byusing theapproximation toP(ni)isnegligible.]
Theobserv ednumberofmutantsnisthesum
n=rX
i=1ni: (35.9)
Theprobabilit ydistribution ofngivenristheconvolution ofridentical
distributions oftheform(35.8). Forexample,
P(njr=2)=n 1X
n1=11
Z21
n2
11
(n n1)2forn2: (35.10)
Theprobabilit ydistribution ofngivena,whichiswhat weneed forthe
Bayesian inference, isgivenbysumming overr.
P(nja)=NX
r=0P(njr)P(rja;N): (35.11)
Thisquantitycan't beevaluated analytically ,butforsmalla,it'seasyto
evaluate toanydesired numerical precision byexplicitly summing overrfrom
r=0tosomermax,withP(njr)alsobeingfound foreachrbyrmaxexplicit

<<<PAGE 462>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
450 35|Random Inference Topics
convolutions forallrequired values ofn;ifrmax=nmax,thelargest value
ofnencoun tered inthedata, thenP(nja)iscomputed exactly; butforthis
question's data,rmax=9isplentyforanaccurate result; Iusedrmax=
74tomakethegraphs ingure 35.3.Octave source codeisavailable.1
00.20.40.60.811.2
1e-10 1e-09 1e-08 1e-07
1e-101e-081e-060.00010.011
1e-10 1e-09 1e-08 1e-07
Figure 35.3.Likelihoodofthe
mutation rateaonalinear scale
andlogscale, givenLuria and
Delbruc k'sdata. Vertical axis:
likelihood/10 23;horizon talaxis:
a.Inciden tally,fordatasetsliketheoneinthisexercise, whichhaveasubstan tial
numberofzerocounts,verylittleislostbymaking Luria andDelbruc k'ssecond
approximation, whichistoretain onlythecountofhowmanynwereequal to
zero,andhowmanywerenon-zero. Thelikelihoodfunction found using this
weakeneddataset,
L(a)=(e aN)11(1 e aN)9; (35.12)
isscarcely distinguishable fromthelikelihoodcomputed using fullinformation.
Solution toexercise 35.5(p.447).Fromthesixterms oftheform
P(Fjm)=Q
i (Fi+mi)
 (P
iFi+) ()Q
i (mi); (35.13)
mostfactors cancel andallthatremains is
P(HA!BjData)
P(HB!AjData)=(765+1)(235 +1)
(950+1)(50 +1)=3:8
1: (35.14)
There ismodestevidence infavourofHA!Bbecause thethree probabilities
inferred forthathypothesis (roughly 0.95, 0.8,and0.1)aremore typical of
theprior thanarethethree probabilities inferred fortheother (0.24, 0.008,
and0.19). Thisstatemen tsounds absurd ifwethink ofthepriors as`uniform'
overthethree probabilities {surely ,under auniform prior, anysettings ofthe
probabilities areequally probable? Butinthenatural basis, thelogitbasis,
thepriorisproportional top(1 p),andtheposterior probabilit yratiocan
beestimated by
0:950:050:80:20:10:9
0:240:760:0080:9920:190:81'3
1; (35.15)
whichisnotexactly right,butitdoesillustrate where thepreference forA!B
iscoming from.
1www.inference.phy.cam.ac.uk/it prnn/code/octave/luria0.m

<<<PAGE 463>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
36
Decision Theory
Decision theory istrivial, apart fromcomputational details (justlikeplaying
chess!).
Youhaveachoice ofvarious actions,a.Theworldmaybeinoneofmany
states x;whichoneoccurs maybeinuenced byyouraction. Theworld's
statehasaprobabilit ydistribution P(xja).Finally ,there isautilityfunction
U(x;a)whichspecies thepayoyoureceivewhen theworldisinstatexand
youchoseactiona.
Thetaskofdecision theory istoselect theaction thatmaximizes the
expected utility,
E[Uja]=Z
dKxU(x;a)P(xja): (36.1)
That's all.Thecomputational problem istomaximizeE[Uja]overa.[Pes-
simists mayprefer todene alossfunctionLinstead ofautilityfunctionU
andminimize theexpected loss.]
Isthere anything more tobesaidaboutdecision theory?
Well,inarealproblem, thechoice ofanappropriate utilityfunction may
bequite dicult. Furthermore, when asequence ofactions istobetaken,
witheachaction providing information aboutx,wehavetotakeintoaccoun t
theeect thatthisanticipated information mayhaveonoursubsequen tac-
tions. Theresulting mixture offorwardprobabilit yandinverseprobabilit y
computations inadecision problem isdistinctiv e.Inarealistic problem such
asplayingaboardgame, thetreeofpossible cogitations andactions thatmust
beconsidered becomes enormous, and`doing therightthing' isnotsimple be-
cause theexpected utilityofanaction cannot becomputed exactly (Russell
andWefald, 1991; Baum andSmith, 1993; Baum andSmith, 1997).
Let'sexplore anexample.
36.1 Rational prospecting
Supposeyouhavethetaskofchoosing thesiteforaTanzanite mine. Your
nalaction willbetoselect thesitefromalistofNsites. Thenthsitehas
anetvaluecalled thereturnxnwhichisinitially unkno wn,andwillbefound
outexactly onlyaftersitenhasbeenchosen. [xnequals therevenueearned
fromselling theTanzanite fromthatsite,minusthecostsofbuying thesite,
payingthesta,andsoforth.] Attheoutset, thereturnxnhasaprobabilit y
distribution P(xn),based ontheinformation already available.
Before youtakeyournalaction youhavetheopportunit ytodosome
prospecting. Prosp ecting atthenthsitehasacostcnandyields datadn
whichreduce theuncertain tyaboutxn.[We'llassume thatthereturns of
451

<<<PAGE 464>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
452 36|Decision Theory
theNsitesareunrelated toeachother, andthatprospecting atonesiteonly
yields information aboutthatsiteanddoesn'taect thereturn fromthatsite.]
Yourdecision problem is:
giventheinitial probabilit ydistributions P(x1),P(x2),...,P(xN),
rst,decide whether toprospect,andatwhichsites; thenchoose
whichsitetomine.
Forsimplicit y,let'smakeeverything intheproblem Gaussian andfocus Thenotation
P(y)=Normal( y;;2)indicates
thatyhasGaussian distribution with
mean andvariance 2.onthequestion ofwhether toprospectonceornot.We'llassume ourutility
function islinear inxn;wewishtomaximize ourexpected return. Theutility
function is
U=xna; (36.2)
ifnoprospecting isdone, wherenaisthechosen `action' site;andifprospecting
isdonetheutilityis
U= cnp+xna; (36.3)
wherenpisthesiteatwhichprospecting tookplace.
Thepriordistribution ofthereturn ofsitenis
P(xn)=Normal (xn;n;2
n): (36.4)
Ifyouprospectatsiten,thedatumdnisanoisy version ofxn:
P(dnjxn)=Normal(dn;xn;2): (36.5)
.Exercise 36.1.[2]Giventhese assumptions, showthatthepriorprobabilit ydis-
tribution ofdnis
P(dn)=Normal(dn;n;2+2
n) (36.6)
(mnemonic: when independen tvariables add,variances add), andthat
theposterior distribution ofxngivendnis
P(xnjdn)=Normal
xn;0
n;2
n0
(36.7)
where
0
n=dn=2+n=2
n
1=2+1=2nand1
2n0=1
2+1
2n(36.8)
(mnemonic: when Gaussians multiply ,precisions add).
Tostartwithlet'sevaluate theexpected utilityifwedonoprospecting (i.e.,
choosethesiteimmediately); thenwe'llevaluate theexpected utilityifwerst
prospectatonesiteandthenmakeourchoice. Fromthese tworesults wewill
beabletodecide whether toprospectonceorzerotimes, and,ifweprospect
once, atwhichsite.
So,rstweconsider theexpected utilitywithout anyprospecting.
Exercise 36.2.[2]Showthattheoptimal action, assuming noprospecting, isto
select thesitewithbiggest mean
na=argmax
nn; (36.9)
andtheexpected utilityofthisaction is
E[Ujoptimaln]=maxnn: (36.10)
[Ifyourintuition says`surely theoptimal decision should takeintoac-
countthedieren tuncertain tiesntoo?',theanswertothisquestion is
`reasonable {ifso,thentheutilityfunction should benonline arinx'.]

<<<PAGE 465>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
36.2: Further reading 453
Nowtheexciting bit.Should weprospect?Once wehaveprospected at
sitenp,wewillchoosethesiteusing thedescision rule(36.9) withthevalueof
meannpreplaced bytheupdated value0
ngivenby(36.8). What makesthe
problem exciting isthatwedon't yetknowthevalueofdn,sowedon't know
what ouractionnawillbe;indeed thewhole valueofdoing theprospecting
comes fromthefactthattheoutcomednmayaltertheaction fromtheone
thatwewouldhavetakenintheabsence oftheexperimen talinformation.
Fromtheexpression forthenewmean interms ofdn(36.8), andtheknown
variance ofdn(36.6) wecancompute theprobabilit ydistribution ofthekey
quantity,0
n,andcanworkouttheexpected utilitybyintegrating overall
possible outcomes andtheirassociated actions.
Exercise 36.3.[2]Showthattheprobabilit ydistribution ofthenewmean0
n
(36.8) isGaussian withmeannandvariance
s22
n2
n
2+2n: (36.11)
Consider prospecting atsiten.Letthebiggest mean oftheother sitesbe
1.When weobtain thenewvalueofthemean,0
n,wewillchoosesitenand
getanexpected return of0
nif0
n>1,andwewillchoosesite1andgetan
expected return of1if0
n<1.
Sotheexpected utilityofprospecting atsiten,thenpickingthebestsite,
is
E[Ujprospectatn]= cn+P(0
n<1)1+Z1
1d0
n0
nNormal (0
n;n;s2):
(36.12)
Thedierence inutilitybetweenprospecting andnotprospecting isthe
quantityofinterest, anditdependsonwhat wewouldhavedone without
prospecting; andthatdependsonwhether1isbigger thann.
E[Ujnoprospecting] =(
 1if1n
 nif1n:(36.13)
So
E[Ujprospectatn] E[Ujnoprospecting]
=8
>><
>>: cn+Z1
1d0
n(0
n 1)Normal (0
n;n;s2)if1n
 cn+Z1
 1d0
n(1 0
n)Normal (0
n;n;s2)if1n:(36.14)
Wecanplotthechange inexpected utilityduetoprospecting (omitting
cn)asafunction ofthedierence (n 1)(horizon talaxis)andtheinitial
standard deviationn(vertical axis). Inthegure thenoisevariance is2=1.-6-4-2024600.511.522.533.5
n
(n 1)
Figure 36.1.Thegaininexpected
utilityduetoprospecting. The
contours areequally spaced from
0.1to1.2insteps of0.1.To
decide whether itisworth
prospecting atsiten,ndthe
contourequal tocn(thecostof
prospecting); allpoints
[(n 1);n]abovethatcontour
areworthwhile.
36.2 Further reading
Iftheworldinwhichweactisalittlemorecomplicated thantheprospecting
problem {forexample, ifmultiple iterations ofprospecting arepossible, and
thecostofprospecting isuncertain {thennding theoptimal balance between
exploration andexploitation becomes amuchharder computational problem.
Reinforcemen tlearning addresses approximate metho dsforthisproblem (Sut-
tonandBarto, 1998).

<<<PAGE 466>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
454 36|Decision Theory
36.3 Further exercises
.Exercise 36.4.[2]Thefourdoorsproblem .
Anewgame showusesrulessimilar tothose ofthethree doors(exer-
cise3.8(p.57)),butthere arefourdoors,andthehostexplains: `First
youwillpointtooneofthedoors,andthenIwillopenoneoftheother
non-winners. Then youdecide whether tostickwithyouroriginal pick
orswitchtooneoftheremaining doors.Then Iwillopenanother (other
thanthecurren tpick)non-winner. Youwillthenmakeyournalde-
cision bystickingwiththedoorpickedontheprevious decision orby
switchingtotheonlyother remaining door.'
What istheoptimal strategy? Should youswitchontherstopportu-
nity?Should youswitchonthesecond opportunit y?
.Exercise 36.5.[3,p.715]Oneofthechallenges ofdecision theory isguring out
exactly what theutilityfunction is.Theutilityofmoney ,forexample,
isnotoriously nonlinear formostpeople.
Infact,thebehaviour ofmanypeople cannot becaptured byacoher-
entutilityfunction, asillustrated bytheAllias parado x,whichrunsas
follows.
Whichofthese choices doyoundmostattractiv e?
A.$1million guaran teed.
B.89%chance of$1million;
10%chance of$2.5million;
1%chance ofnothing.
Nowconsider these choices:
C.89%chance ofnothing;
11%chance of$1million.
D.90%chance ofnothing;
10%chance of$2.5million.
Manypeople prefer AtoBandDtoC.Provethatthese preferences are
inconsisten twithanyutilityfunctionU(x)formoney .
Exercise 36.6.[4]Optimal stopping .
Aqueue ofNpotentialpartners iswaiting atyourdoor,allasking to
marry you.They havearrivedinrandom order. Asyoumeet each
partner, youhavetodecide onthespot,based ontheinformation so
far,whether tomarry them orsayno.Eachpotentialpartner hasa
desirabilit ydn,whichyoundoutifandwhen youmeet them. You
mustmarry oneofthem, butyouarenotallowedtogobacktopeople
youhavesaidnoto.
There areseveralwaystodene theprecise problem.
(a)Assuming youraimistomaximize thedesirabilit ydn,i.e.,your
utilityfunction isd^n,where ^nisthepartner selected, whatstrategy
should youuse?
(b)Assuming youwishverymuchtomarry themostdesirableperson
(i.e.,yourutilityfunction is1ifyouachievethat,andzeroother-
wise); whatstrategy should youuse?

<<<PAGE 467>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
36.3: Further exercises 455
(c)Assuming youwishverymuchtomarry themostdesirable person,
andthatyourstrategy willbe`strategyM':
StrategyM{Meet therstMpartners andsaynotoall
ofthem. Memorize themaxim umdesirabilit ydmaxamong
them. Then meet theothers insequence, waiting untila
partner withdn>dmaxcomes along, andmarry them.
Ifnonemore desirable comes along, marry thenalNth
partner (andfeelmiserable).
{whatistheoptimal valueofM?
Exercise 36.7.[3]Regret asanobjective function?
Thepreceding exercise (parts bandc)involvedautilityfunction based
onregret. Ifonemarried thetenthmostdesirable candidate, theutility
function asserts thatonewouldfeelregret forhavingnotmade chosen
themostdesirable.
Manypeople working inlearning theory anddecision theory use`mini-
mizing themaximal possible regret' asanobjectivefunction, butdoes
thismakesense?Action
Buy Don't
Outcome buy
Nowin 1 0
Wins +9 0
Table36.2.Utilityinthelottery
ticketproblem.
Imagine thatFredhasboughtalottery ticket,andoers tosellittoyou
beforeit'sknownwhether theticketisawinner. Forsimplicit ysaythe
probabilit ythattheticketisawinner is1=100,andifitisawinner, it
isworth$10.Fredoers tosellyoutheticketfor$1.Doyoubuyit?
Thepossible actions are`buy' and`don't buy'. Theutilities ofthefour
possible action{outcome pairsareshownintable 36.2. Ihaveassumed
thattheutilityofsmall amoun tsofmoney foryouislinear. Ifyoudon't
buytheticketthentheutilityiszeroregardless ofwhether theticket
provestobeawinner. Ifyoudobuytheticketyoueither enduplosingAction
Buy Don't
Outcome buy
Nowin 1 0
Wins 0 9
Table36.3.Regret inthelottery
ticketproblem.
onepound (with probabilit y99=100)orgaining nine(with probabilit y
1=100). Intheminimax regret comm unity,actions arechosen tomini-
mizethemaxim umpossible regret. Thefourpossible regret outcomes
areshownintable 36.3. Ifyoubuytheticketanditdoesn'twin,you
havearegret of$1,because ifyouhadnotboughtityouwouldhave
been$1bettero.Ifyoudonotbuytheticketanditwins, youhave
aregret of$9,because ifyouhadboughtityouwouldhavebeen$9
bettero.Theaction thatminimizes themaxim umpossible regret is
thustobuytheticket.
Discuss whether thisuseofregret tochooseactions canbephilosophi-
callyjustied.
Theaboveproblem canbeturned intoaninvestmen tportfolio decision
problem byimagining thatyouhavebeengivenonepoundtoinvestin
twopossible funds foroneday:Fred'slottery fund, andthecashfund. If
youput$f1intoFred'slottery fund,Fredpromises toreturn $9f1toyou
ifthelottery ticketisawinner, andotherwise nothing. Theremaining
$f0(withf0=1 f1)iskeptascash. What isthebestinvestmen t?
Showthattheminimax regret comm unitywillinvestf1=9=10oftheir
money inthehighrisk,highreturn lottery fund, andonlyf0=1=10in
cash. Canthisinvestmen tmetho dbejustied?
Exercise 36.8.[3]Gambling oddities (from CoverandThomas (1991)). Ahorse
raceinvolvingIhorses occurs repeatedly ,andyouareobliged tobet
allyourmoney eachtime. Yourbetattimetcanberepresen tedby

<<<PAGE 468>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
456 36|Decision Theory
anormalized probabilit yvectorbmultiplied byyourmoneym(t).The
oddsoered bythebookiesaresuchthatifhorseiwinsthenyourreturn
ism(t+1)=bioim(t).Assuming thebookies' oddsare`fair', thatis,
X
i1
oi=1; (36.15)
andassuming thattheprobabilit ythathorseiwinsispi,workoutthe
optimal betting strategy ifyouraimisCover'saim,namely ,tomaximize
theexpectedvalueoflogm(T).Showthattheoptimal strategy setsb
equal top,independen tofthebookies' oddso.Showthatwhen this
strategy isused, themoney isexpected togrowexponentially as:
2nW(b;p)(36.16)
whereW=P
ipilogbioi.
Ifyouonlybetonce, istheoptimal strategy anydieren t?
Doyouthink thisoptimal strategy makessense? Doyouthink thatit's
`optimal', incommon language, toignore thebookies' odds?What can
youconclude about`Cover'saim'?
Exercise 36.9.[3]Twoordinary dicearethrownrepeatedly; theoutcome of
eachthrowisthesumofthetwonumbers.JoeShark, whosaysthat6
and8arehisluckynumbers,betsevenmoney thata6willbethrown
beforetherst7isthrown.Ifyouwereagambler,wouldyoutakethe
bet?What isyourprobabilit yofwinning? Joethenbetsevenmoney
thatan8willbethrownbeforetherst7isthrown.Wouldyoutake
thebet?
Havinggained yourcondence, Joesuggests combining thetwobetsinto
asingle bet:hebetsalarger sum,stillatevenodds,thatan8anda
6willbethrownbeforetwo7shavebeenthrown.Wouldyoutakethe
bet?What isyourprobabilit yofwinning?

<<<PAGE 469>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
37
Bayesian Inference andSampling Theory
There aretwoschoolsofstatistics. Sampling theorists concen trateonhaving
metho dsguaran teedtoworkmost ofthetime, givenminimal assumptions.
Bayesians trytomakeinferences thattakeintoaccoun tallavailable informa-
tionandanswerthequestion ofinterest giventheparticular dataset.Asyou
haveprobably gathered, Istrongly recommend theuseofBayesian metho ds.
Sampling theory isthewidely usedapproac htostatistics, andmost pa-
persinmostjournals reporttheirexperimen tsusing quantitieslikecondence
intervals,signicance levels,andp-values. Ap-value(e.g.p=0:05)istheprob-
ability,givenanullhypothesis fortheprobabilit ydistribution ofthedata, that
theoutcome wouldbeasextreme as,ormoreextreme than, theobserv edout-
come. Untrained readers {andperhaps, moreworryingly ,theauthors ofmany
papers{usually interpret suchap-valueasifitisaBayesian probabilit y(for
example, theposterior probabilit yofthenullhypothesis), aninterpretation
thatbothsampling theorists andBayesians wouldagree isincorrect.
Inthischapter westudy acouple ofsimple inference problems inorder to
compare these twoapproac hestostatistics.
While insomecases, theanswersfromaBayesianapproac handfromsam-
plingtheory areverysimilar, wecanalsondcaseswhere there aresignican t
dierences. Wehavealready seensuchanexample inexercise 3.15(p.59),
where asampling theorist gotap-valuesmaller than7%,andviewedthisas
strong evidence against thenullhypothesis, whereas thedataactually favour ed
thenullhypothesis overthesimplest alternativ e.Onp.64,another example
wasgivenwhere thep-valuewassmaller thanthemystical valueof5%,yetthe
dataagain favoured thenullhypothesis. Thusinsomecases, sampling theory
canbetrigger-happ y,declaring results tobe`sucien tlyimprobable thatthe
nullhypothesis should berejected', when those results actually weakly sup-
portthenullhypothesis. Aswewillnowsee,there arealsoinference problems
where sampling theory failstodetect `signican t'evidence where aBayesian
approac handeverydayintuition agree thattheevidence isstrong. Mosttelling
ofallaretheinference problems where the`signicance' assigned bysampling
theory changes depending onirrelev antfactors concerned withthedesign of
theexperimen t.
Thischapter isonlyprovided forthose readers whoarecurious aboutthe
sampling theory /Bayesian metho dsdebate. Ifyoundanyofthischapter
tough tounderstand, please skipit.There isnopointtrying tounderstand
thedebate. JustuseBayesian metho ds{theyaremucheasier tounderstand
thanthedebate itself!
457

<<<PAGE 470>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
458 37|Bayesian Inference andSampling Theory
37.1 Amedical example
Wearetrying toreduce theincidence ofanunpleasan tdisease
called microsoftus .Twovaccinations, AandB,aretested on
agroup ofvolunteers. Vaccination Bisacontroltreatmen t,a
placeb otreatmen twithnoactiveingredien ts.Ofthe40subjects,
30arerandomly assigned tohavetreatmen tAandtheother 10
aregiventhecontroltreatmen tB.Weobserv ethesubjectsforone
yearaftertheirvaccinations. Ofthe30ingroupA,onecontracts
microsoftus .Ofthe10ingroupB,three contractmicrosoftus .
Istreatmen tAbetterthantreatmen tB?
Sampling theoryhasago
Thestandard sampling theory approac htothequestion `isAbetterthanB?'
istoconstruct astatistical test.Thetestusually compares ahypothesis such
as
H1:`AandBhavedieren teectiv enesses'
withanullhypothesis suchas
H0:`AandBhaveexactly thesameeectiv enesses aseachother'.
Anovicemightobject`no,no,Iwanttocompare thehypothesis \Aisbetter
thanB"withthealternativ e\BisbetterthanA"!'butsuchobjections are
notwelcome insampling theory .
Oncethetwohypotheses havebeendened, thersthypothesis isscarcely
mentioned again {attentionfocusessolely onthenullhypothesis. Itmakesme
laugh towrite this,butit'strue! Thenullhypothesis isaccepted orrejected
purely onthebasisofhowunexp ected thedataweretoH0,notonhowmuch
betterH1predicted thedata. Onechoosesastatistic whichmeasures how
muchadatasetdeviates fromthenullhypothesis. Intheexample here,the
standard statistic tousewouldbeonecalled2(chi-squared). Tocompute
2,wetakethedierence betweeneachdatameasuremen tanditsexpected
valueassuming thenullhypothesis tobetrue,anddivide thesquare ofthat
dierence bythevarianc eofthemeasuremen t,assuming thenullhypothesis to
betrue.Inthepresen tproblem, thefourdatameasuremen tsaretheintegers
FA+,FA ,FB+,andFB ,thatis,thenumberofsubjectsgiventreatmen tA
whocontracted microsoftus (FA+),thenumberofsubjectsgiventreatmen tA
whodidn't (FA ),andsoforth. Thedenition of2is:
2=X
i(Fi hFii)2
hFii: (37.1)
Actually ,inmyelemen tarystatistics book(Spiegel, 1988) IndYates's cor-
rection isrecommended: IfyouwanttoknowaboutYates's
correction, read asampling theory
textbook.Thepointofthischapter is
nottoteachsampling theory; Imerely
mentionYates's correction because it
iswhat aprofessional sampling theo-
ristmightuse.2=X
i(jFi hFiij 0:5)2
hFii: (37.2)
Inthiscase,giventhenullhypothesis thattreatmen tsAandBareequally
eectiv e,andhaveratesf+andf forthetwooutcomes, theexpected counts
are:
hFA+i=f+NAhFA i=f NA
hFB+i=f+NBhFB i=f NB:(37.3)

<<<PAGE 471>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
37.1: Amedical example 459
Thetestaccepts orrejects thenullhypothesis onthebasisofhowbig2is.
Tomakethistestprecise, andgiveita`signicance level',wehavetowork
outwhatthesampling distribution of2is,taking intoaccoun tthefactthat Thesampling distribution ofastatis-
ticistheprobabilit ydistribution of
itsvalueunder repetitions oftheex-
perimen t,assuming thatthenullhy-
pothesis istrue.thefourdatapointsarenotindependen t(they satisfy thetwoconstrain ts
FA++FA =NAandFB++FB =NB)andthefactthattheparameters
farenotknown.These three constrain tsreduce thenumberofdegrees
offreedom inthedatafromfourtoone.[Ifyouwanttolearn more about
computing the`numberofdegrees offreedom', readasampling theory book;in
Bayesianmetho dswedon't needtoknowallthat,andquantitiesequivalentto
thenumberofdegrees offreedom popstraigh toutofaBayesiananalysis when
theyareappropriate.] These sampling distributions aretabulated bysampling
theory gnomes andcomeaccompanied bywarnings abouttheconditions under
whichtheyareaccurate. Forexample, standard tabulated distributions for2
areonlyaccurate iftheexpected numbersFiareabout5ormore.
Oncethedataarrive,sampling theorists estimate theunkno wnparameters
fofthenullhypothesis fromthedata:
^f+=FA++FB+
NA+NB;^f =FA +FB 
NA+NB; (37.4)
andevaluate2.Atthispoint,thesampling theory schooldivides itselfinto
twocamps. Onecamp usesthefollowingprotocol:rst,beforelooking atthe
data, pickthesignicance levelofthetest(e.g.5%),anddetermine thecritical
valueof2abovewhichthenullhypothesis willberejected. (Thesignicance
levelisthefraction oftimes thatthestatistic2wouldexceed thecritical
value,ifthenullhypothesis weretrue.) Then evaluate2,compare withthe
critical value,anddeclare theoutcome ofthetest,anditssignicance level
(whichwasxedbeforehand).
Thesecond camp looksatthedata, nds2,thenlooksinthetable of
2-distributions forthesignicance level,p,forwhichtheobserv edvalueof2
wouldbethecritical value. Theresult ofthetestisthenreported bygiving
thisvalueofp,whichisthefraction oftimes thataresult asextreme astheone
observ ed,ormore extreme, wouldbeexpected toariseifthenullhypothesis
weretrue.
Let's apply these twometho ds.First camp: let'spick5%asoursigni-
cance level.Thecritical valuefor2withonedegree offreedom is2
0:05=3:84.
Theestimated values offare
f+=1=10;f =9=10: (37.5)
Theexpected values ofthefourmeasuremen tsare
hFA+i=3 (37.6)
hFA i=27 (37.7)
hFB+i=1 (37.8)
hFB i=9 (37.9)
and2(asdened inequation (37.1)) is
2=5:93: (37.10)
Since thisvalueexceeds 3.84,wereject thenullhypothesis thatthetwotreat-
mentsareequivalentatthe0.05signicance level.However,ifweuseYates's
correction, wend2=3:33,andtherefore accept thenullhypothesis.

<<<PAGE 472>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
460 37|Bayesian Inference andSampling Theory
Camp tworunsanger across the2tablefound atthebackofanygood
sampling theory bookandnds2
:10=2:71.Interpolating between2
:10and
2
:05,camp tworeports`thep-valueisp=0:07'.
Notice thatthisanswerdoesnotsayhowmuchmoreeectiv eAisthanB,
itsimply saysthatAis`signican tly'dieren tfromB.Andhere,`signican t'
means only`statistically signican t',notpractically signican t.
Themaninthestreet, reading thestatemen tthat`thetreatmen twassig-
nican tlydieren tfromthecontrol(p=0:07)',mightcome totheconclusion
that`there isa93%chance thatthetreatmen tsdier ineectiv eness'. But
what`p=0:07'actually means is`ifyoudidthisexperimen tmanytimes, and
thetwotreatmen tshadequal eectiv eness, then7%ofthetimeyouwould
ndavalueof2more extreme thantheonethathappenedhere'. Thishas
almost nothing todowithwhat wewanttoknow,whichishowlikelyitis
thattreatmen tAisbetterthanB.
Letmethrough, I'maBayesian
OK,nowlet'sinfer what wereally wanttoknow.Wescrap thehypothesis
thatthetwotreatmen tshaveexactly equal eectiv enesses, since wedonot
believeit.There aretwounkno wnparameters, pA+andpB+,whicharethe
probabilities thatpeople giventreatmen tsAandB,respectively,contractthe
disease.
Giventhedata, wecaninferthese twoprobabilities, andwecananswer
questions ofinterest byexamining theposterior distribution.
Theposterior distribution is
P(pA+;pB+jfFig)=P(fFigjpA+;pB+)P(pA+;pB+)
P(fFig): (37.11)
Thelikelihoodfunction is
P(fFigjpA+;pB+)= 
NA
FA+!
pFA+
A+pFA 
A  
NB
FB+!
pFB+
B+pFB 
B (37.12)
= 
30
1!
p1
A+p29
A  
10
3!
p3
B+p7
B : (37.13)
What prior distribution should weuse? Theprior distribution givesusthe
opportunit ytoinclude knowledge fromother experimen ts,oraprior belief
thatthetwoparameters pA+andpB+,while dieren tfromeachother, are
expected tohavesimilar values.
Herewewillusethesimplest vanilla priordistribution, auniform distri-
bution overeachparameter.
P(pA+;pB+)=1: (37.14)
Wecannowplottheposterior distribution. Giventheassumption ofasepa-
rableprioronpA+andpB+,theposterior distribution isalsoseparable:
P(pA+;pB+jfFig)=P(pA+jFA+;FA )P(pB+jFB+;FB ): (37.15)
Thetwoposterior distributions areshowningure 37.1(except thegraphs
arenotnormalized) andthejointposterior probabilit yisshowningure 37.2.
Ifwewanttoknowtheanswertothequestion `howprobable isitthatpA+
issmaller thanpB+?',wecananswerexactly thatquestion bycomputing the
posterior probabilit y
P(pA+<pB+jData); (37.16)

<<<PAGE 473>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
37.1: Amedical example 461
00.2 0.4 0.6 0.8 1Figure 37.1.Posterior
probabilities ofthetwo
eectiv enesses. Treatmen tA{
solidline;B{dotted line.
pB+
00.20.40.60.8100.20.40.60.81
pA+00.20.40.60.8100.20.40.60.81Figure 37.2.Jointposterior
probabilit yofthetwo
eectiv enesses {contourplotand
surface plot.
whichistheintegral ofthejointposterior probabilit yP(pA+;pB+jData)
showningure 37.2overtheregion inwhichpA+<pB+,i.e.,theshaded
triangle ingure 37.3. Thevalueofthisintegral (obtained byastraigh tfor-pB+
pA+0 101
Figure 37.3.Theproposition
pA+<pB+istrueforallpointsin
theshaded triangle. Tondthe
probabilit yofthisproposition we
integrate thejointposterior
probabilit yP(pA+;pB+jData)
(gure 37.2)overthisregion.wardnumerical integration ofthelikelihoodfunction (37.13) overtherelevant
region) isP(pA+<pB+jData)=0:990.
Thusthere isa99%chance, giventhedataandourprior assumptions,
thattreatmen tAissuperiortotreatmen tB.Inconclusion, according toour
Bayesianmodel,thedata(1outof30contracted thedisease aftervaccination
A,and3outof10contracted thedisease aftervaccination B)giveverystrong
evidence {about99toone{thattreatmen tAissuperiortotreatmen tB.
IntheBayesian approac h,itisalsoeasytoanswerother relevantques-
tions. Forexample, ifwewanttoknow`howlikelyisitthattreatmen tAis
tentimes more eectiv ethantreatmen tB?',wecanintegrate thejointpos-
terior probabilit yP(pA+;pB+jData)overtheregion inwhichpA+<10pB+
(gure 37.4).pB+
pA+0 101
Figure 37.4.Theproposition
pA+<10pB+istrueforallpoints
intheshaded triangle.Modelcomparison
Ifthere wereasituation inwhichwereally didwanttocompare thetwo
hypothesesH0:pA+=pB+andH1:pA+6=pB+,wecanofcourse dothis
directly withBayesian metho dsalso.
Asanexample, consider thedataset:
D:Onesubject,giventreatmen tA,subsequen tlycontracted microsoftus .
Onesubject,giventreatmen tB,didnot.
Treatmen tAB
Gotdisease 10
Didnot 01
Totaltreated 11

<<<PAGE 474>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
462 37|Bayesian Inference andSampling Theory
Howstrongly doesthisdatasetfavourH1overH0?
Weanswerthisquestion bycomputing theevidence foreachhypothesis.
Let'sassume uniform priors overtheunkno wnparameters ofthemodels.The
rsthypothesisH0:pA+=pB+hasjustoneunkno wnparameter, let'scallit
p.
P(pjH0)=1p2(0;1): (37.17)
We'llusetheuniform prioroverthetwoparameters ofmodelH1thatweused
before:
P(pA+;pB+jH1)=1pA+2(0;1);pB+2(0;1): (37.18)
Now,theprobabilit yofthedataDunder modelH0isthenormalizing constan t
fromtheinference ofpgivenD:
P(DjH0)=Z
dpP(Djp)P(pjH0) (37.19)
=Z
dpp(1 p)1 (37.20)
=1=6: (37.21)
Theprobabilit yofthedataDunder modelH1isgivenbyasimple two-
dimensional integral:
P(DjH1)=ZZ
dpA+dpB+P(DjpA+;pB+)P(pA+;pB+jH1)(37.22)
=Z
dpA+pA+Z
dpB+(1 pB+) (37.23)
=1=21=2 (37.24)
=1=4: (37.25)
Thustheevidence ratio infavourofmodelH1,whichasserts thatthetwo
eectiv enesses areunequal, is
P(DjH1)
P(DjH0)=1=4
1=6=0:6
0:4: (37.26)
Soiftheprior probabilit yoverthetwohypotheses was50:50, theposterior
probabilit yis60:40 infavourofH1. 2
Isitnoteasytogetsensible answerstowell-posedquestions using Bayesian
metho ds?
[Thesampling theory answertothisquestion wouldinvolvetheidentical
signicance testthatwasusedinthepreceding problem; thattestwouldyield
a`notsignican t'result. Ithink itisgreatly preferable toacknowledge what
isobvious totheintuition, namely thatthedataDdogiveweakevidence in
favourofH1.Bayesian metho dsquantifyhowweaktheevidence is.]
37.2 Dependence ofp-values onirrelev antinformation
Inanexpensivelaboratory ,Dr.Bloggs tosses acoinlabelledaandbtwelve
times andtheoutcome isthestring
aaabaaaabaab;
whichcontainsthreebsandnineas.
What evidence dothese datagivethatthecoinisbiased infavourofa?

<<<PAGE 475>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
37.2: Dependence ofp-values onirrelev antinformation 463
Dr.Bloggs consults hissampling theory friend whosays`letrbethenum-
berofbsandn=12bethetotalnumberoftosses; Iviewrastherandom
variable andndtheprobabilit yofrtaking onthevaluer=3oramore
extreme value, assuming thenullhypothesispa=0:5tobetrue'. Hethus
computes
P(r3jn=12;H0)=3X
r=0 
n
r!
1/2n= 12
0+ 12
1+ 12
2+ 12
3
1/212
=0:07; (37.27)
andreports`atthesignicance levelof5%,there isnotsignican tevidence
ofbiasinfavourofa'.Or,ifthefriend prefers toreportp-values rather than
simply comparepwith5%,hewouldreport`thep-valueis7%,whichisnot
conventionally viewedassignican tlysmall'. Ifatwo-tailed testseemed more
appropriate, hemightcompute theprobabilit ytwo-tailed areatobetwice
theaboveprobabilit y,andreport`thep-valueis15%,whichisdenitely not
signican tlysmall'. Wewon'tfocusontheissueofthechoice betweenthe
one-tailed andtwo-tailed tests, aswehavebigger shtocatch.
Dr.Bloggs payscareful attentiontothecalculation (37.27), andresponds
`no,no,therandom variable intheexperimen twasnotr:Idecided before
running theexperimen tthatIwouldkeeptossing thecoinuntilIsawthree
bs;therandom variable isthusn'.
Suchexperimen taldesigns arenotunusual. Inmyexperimen tsonerror-correcting
codesIoften simulatethedecodingofacodeuntilachosen numberrofblockerrors
(bs)hasoccurred, sincetheerrorontheinferred valueoflogpbgoesroughly aspr,
independen tofn.
Exercise 37.1.[2]Find theBayesian inference aboutthebiaspaofthecoin
giventhedata, anddetermine whether aBayesian's inferences depend
onwhatstopping rulewasinforce.
According tosampling theory ,adieren tcalculation isrequired inorder
toassess the`signicance' oftheresultn=12.Theprobabilit ydistribution
ofngivenH0istheprobabilit ythattherstn 1tosses containexactlyr 1
bsandthenthenthtossisab.
P(njH0;r)= 
n 1
r 1!
1/2n: (37.28)
Thesampling theorist thuscomputes
P(n12jr=3;H0)=0:03: (37.29)
HereportsbacktoDr.Bloggs, `thep-valueis3%{thereissignican tevidence
ofbiasafterall!'
What doyouthink Dr.Bloggs should do?Should hepublish theresult,
withthismarvellousp-value,inoneofthejournals thatinsists thatallexper-
imentalresults havetheir`signicance' assessed using sampling theory? Or
should hebootthesampling theorist outofthedoorandseekacoheren t
metho dofassessing signicance, onethatdoesnotdependonthestopping
rule?
Atthispointtheaudience divides intwo.Halftheaudience intuitively
feelthatthestopping ruleisirrelev ant,anddon't needanyconvincing that
theanswertoexercise 37.1(p.463)is`theinferences aboutpadonotdepend
onthestopping rule'. Theother half,perhaps onaccoun tofathorough

<<<PAGE 476>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
464 37|Bayesian Inference andSampling Theory
training insampling theory ,intuitivelyfeelthatDr.Bloggs's stopping rule,
whichstoppedtossing themomen tthethirdbappeared, mayhavebiased the
experimen tsomeho w.Ifyouareinthesecond group, Iencourage youtoreect
onthesituation, andhopeyou'lleventually come round totheviewthatis
consisten twiththelikelihoodprinciple, whichisthatthestopping ruleisnot
relevanttowhat wehavelearned aboutpa.
Asathough texperimen t,consider some onlookerswho(inorder tosave
money) arespyingonDr.Bloggs's experimen ts:eachtimehetosses thecoin,
thespiesupdatethevaluesofrandn.Thespiesareeager tomakeinferences
fromthedataassoonaseachnewresult occurs. Should thespies' beliefs
aboutthebiasofthecoindependonDr.Bloggs's intentions regarding the
continuation oftheexperimen t?
Thefactthatthep-values ofsampling theory dodependonthestopping
rule(indeed, whole volumes ofthesampling theory literature areconcerned
withthetaskofassessing `signicance' when acomplicated stopping ruleis
required {`sequen tialprobabilit yratiotests', forexample) seems tomeacom-
pelling argumen tforhavingnothing todowithp-values atall.ABayesian
solution tothisinference problem wasgiveninsections 3.2and3.3andexer-
cise3.15(p.59).
Wouldithelpclarify thisissueifIadded onemore scene tothestory?
Thejanitor, who's beeneavesdropping onDr.Bloggs's conversation, comes in
andsays`Ihappenedtonotice thatjustafteryoustoppeddoing theexperi-
mentsonthecoin,theOcer forWhimsical Departmen talRules ordered the
immediate destruction ofallsuchcoins. Yourcoinwastherefore destro yedby
thedepartmen talsafetyocer. There isnowayyoucould havecontinuedthe
experimen tmuchbeyondn=12tosses. Seems tome,youneedtorecompute
yourp-value?'
37.3 Condence intervals
Inanexperimen tinwhichdataDareobtained fromasystem withanunkno wn
parameter,astandard concept insampling theory istheideaofacondence
intervalfor.Suchaninterval(min(D);max(D))hasassociated withita
condence levelsuchas95%whichisinformally interpreted as`theprobabilit y
thatliesinthecondence interval'.
Let's makeprecise what thecondence levelreally means, thengivean
example. Acondence intervalisafunction (min(D);max(D))ofthedata
setD.Thecondence levelofthecondence intervalisapropertythatwecan
compute beforethedataarrive.Weimagine generating manydatasetsfroma
particular truevalueof,andcalculating theinterval(min(D);max(D)),and
thencheckingwhether thetruevalueofliesinthatinterval.If,averaging
overallthese imagined repetitions oftheexperimen t,thetruevalueoflies
inthecondence intervalafractionfofthetime, andthispropertyholds for
alltruevalues of,thenthecondence levelofthecondence intervalisf.
Forexample, ifisthemean ofaGaussian distribution whichisknown
tohavestandard deviation 1,andDisasample fromthatGaussian, then
(min(D);max(D))=(D 2;D+2)isa95%condence intervalfor.
Letusnowlookatasimple example where themeaning ofthecondence
levelbecomes clearer. Lettheparameterbeaninteger, andletthedatabe
apairofpointsx1;x2,drawnindependen tlyfromthefollowingdistribution:
P(xj)=8
><
>:1/2x=
1/2x=+1
0forother values ofx.(37.30)

<<<PAGE 477>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
37.4: Some compromise positions 465
Forexample, ifwere39,thenwecould expectthefollowingdatasets:
D=(x1;x2)=(39;39)withprobabilit y1/4;
(x1;x2)=(39;40)withprobabilit y1/4;
(x1;x2)=(40;39)withprobabilit y1/4;
(x1;x2)=(40;40)withprobabilit y1/4.(37.31)
Wenowconsider thefollowingcondence interval:
[min(D);max(D)]=[min(x1;x2);min(x1;x2)]: (37.32)
Forexample, if(x1;x2)=(40;39),thenthecondence intervalforwouldbe
[min(D);max(D)]=[39;39].
Let's think aboutthiscondence interval.What isitscondence level?
Byconsidering thefourpossibilities shownin(37.31), wecanseethatthere
isa75%chance thatthecondence intervalwillcontainthetruevalue. The
condence intervaltherefore hasacondence levelof75%,bydenition.
Now,what ifthedataweacquire are(x1;x2)=(29;29)? Well,wecan
compute thecondence interval,anditis[29;29].Soshallwereportthis
interval,anditsassociated condence level,75%? Thiswouldbecorrect
bytherules ofsampling theory .Butdoesthismakesense? What dowe
actually knowinthiscase? Intuitively,orbyBayes'theorem, itisclearthat
could either be29or28,andbothpossibilities areequally likely(iftheprior
probabilities of28and29wereequal). Theposterior probabilit yofis50%
on29and50%on28.
What ifthedataare(x1;x2)=(29;30)? Inthiscase, thecondence
intervalisstill[29;29],anditsassociated condence levelis75%. Butinthis
case,byBayes'theorem, orcommon sense, weare100% surethatis29.
Inneither caseistheprobabilit ythatliesinthe`75%condence interval'
equal to75%!
Thus
1.thewayinwhichmanypeopleinterpret thecondence levelsofsampling
theory isincorrect;
2.givensomedata, whatpeople usually wanttoknow(whether theyknow
itornot)isaBayesian posterior probabilit ydistribution.
Areallthese examples contrived?AmImaking afussaboutnothing?
Ifyouaresceptical aboutthedogmatic views Ihaveexpressed, Iencourage
youtolookatacasestudy: lookindepth atexercise 35.4(p.446)andthe
reference (Kepler andOprea, 2001), inwhichsampling theory estimates and
condence intervalsforamutation rateareconstructed. Trybothmetho ds
onsimulated data{theBayesian approac hbased onsimply computing the
likelihoodfunction, andthecondence intervalfromsampling theory; andlet
meknowifyoudon't ndthattheBayesian answerisalwaysbetterthanthe
sampling theory answer;andoften much,muchbetter. Thissuboptimalit y
ofsampling theory ,achievedwithgreat eort, iswhyIampassionate about
Bayesianmetho ds.Bayesianmetho dsarestraigh tforward,andtheyoptimally
usealltheinformation inthedata.
37.4 Some compromise positions
Let's endonaconciliatory note. Manysampling theorists arepragmatic {
theyarehappytochoosefromaselection ofstatistical metho ds,choosing

<<<PAGE 478>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
466 37|Bayesian Inference andSampling Theory
whicheverhasthe`best'long-run properties. Incontrast, Ihavenoproblem
withtheideathatthere isonlyoneanswertoawell-posedproblem; butit's
notessentialtoconvertsampling theorists tothisviewp oint:instead, wecan
oerthem Bayesianestimators andBayesiancondence intervals,andrequest
thatthesampling theoretical properties ofthese metho dsbeevaluated. We
don't needtomentionthatthemetho dsarederivedfromaBayesian per-
spective.Ifthesampling properties aregoodthenthepragmatic sampling
theorist willchoosetousetheBayesian metho ds.Itisindeed thecasethat
manyBayesian metho dshavegoodsampling-theoretical properties. Perhaps
it'snotsurprising thatametho dthatgivestheoptimal answerforeachindi-
vidual caseshould alsobegoodinthelongrun!
Another piece ofcommon ground canbeconceded: while Ibelievethat
mostwell-posedinference problems haveaunique correct answer,whichcan
befound byBayesian metho ds,notallproblems arewell-posed. Acommon
question arising indatamodelling is`amIusing anappropriate model?'Model
criticism, thatis,huntingfordefects inacurren tmodel,isataskthatmay
beaided bysampling theory tests, inwhichthenullhypothesis (`thecurren t
modeliscorrect') iswelldened, butthealternativ emodelisnotspecied.
Onecould usesampling theory measures suchasp-valuestoguide one'ssearch
fortheaspectsofthemodelmostinneedofscrutin y.
Further reading
Myfavourite reading onthistopicincludes (Jaynes, 1983; Gull,1988; Loredo,
1990; Berger, 1985; Jaynes, 2003). Treatises onBayesian statistics fromthe
statistics comm unityinclude (BoxandTiao, 1973; O'Hagan, 1994).
37.5 Further exercises
.Exercise 37.2.[3C]Atrac surveyrecords trac ontwosuccessiv edays.On
Fridaymorning, there are12vehicles inonehour. OnSaturda ymorn-
ing,there are9vehicles inhalfanhour. Assuming thatthevehicles are
Poisson distributed withratesFandS(invehicles perhour) respec-
tively,
(a)isSgreater thanF?
(b)bywhatfactor isSbigger orsmaller thanF?
.Exercise 37.3.[3C]Writeaprogram tocompare treatmen tsAandBgiven
dataFA+,FA ,FB+,FB asdescrib edinsection 37.1. Theoutputs
oftheprogram should be(a)theprobabilit ythattreatmen tAismore
eectiv ethantreatmen tB;(b)theprobabilit ythatpA+<10pB+;(c)
theprobabilit ythatpB+<10pA+.

<<<PAGE 479>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
PartV
Neural networks

<<<PAGE 480>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
38
Introduction toNeural Networks
Intheeldofneural networks,westudy theproperties ofnetworksofidealized
`neurons'.
Three motivations underlie workinthisbroad andinterdisciplinary eld.
Biology .Thetaskofunderstanding howthebrain worksisoneoftheout-
standing unsolv edproblems inscience. Some neural networkmodelsare
intended toshedlightonthewayinwhichcomputation andmemory
areperformed bybrains.
Engineering .Manyresearc herswouldliketocreate machinesthatcan`learn',
perform `pattern recognition' or`discoverpatterns indata'.
Complex systems .Athird motivation forbeinginterested inneural net-
worksisthattheyarecomplex adaptiv esystems whose properties are
interesting intheirownright.
Ishould emphasize severalpointsattheoutset.
Thisbookgivesonlyataste ofthiseld. There aremanyinteresting
neural networkmodelswhichwewillnothavetimetotouchon.
Themodelsthatwediscuss arenotintended tobefaithful modelsof
biological systems. Iftheyareatallrelevanttobiology ,theirrelevance
isonanabstract level.
Iwilldescrib esome neural networkmetho dsthatarewidely usedin
nonlinear datamodelling, butIwillnotbeabletogiveafulldescription
ofthestate oftheart.Ifyouwishtosolverealproblems withneural
networks,please readtherelevantpapers.
38.1 Memories
Inthenextfewchapters wewillmeet severalneural networkmodelswhich
come withsimple learning algorithms whichmakethem function asmemories .
Perhaps weshould dwellforamomen tontheconventional ideaofmemory
indigital computation. Amemory (astring of5000bitsdescribing thename
ofaperson andanimage oftheirface,say)isstored inadigital computer
atanaddress .Toretriev ethememory youneedtoknowtheaddress. The
address hasnothing todowiththememory itself. Notice theproperties that
thisscheme doesnothave:
1.Address-based memory isnotassociative.Imagine youknowhalfofa
memory ,saysomeone's face,andyouwouldliketorecall therestofthe
468

<<<PAGE 481>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
38.1: Memories 469
memory {theirname. Ifyourmemory isaddress-based thenyoucan't
getatamemory without knowingtheaddress. [Computer scientistshave
devotedeort towrapping traditional address-based memories inside
cunning softwaretoproducecontent-addressable memories, butcontent-
addressabilit ydoesnotcome naturally .Ithastobeadded on.]
2.Address-based memory isnotrobust orfault-toleran t.Ifaone-bit mis-
takeismade inspecifying theaddressthenacompletely dieren tmem-
orywillberetriev ed.Ifonebitofamemory isippedthenwhenev er
thatmemory isretriev edtheerrorwillbepresen t.Ofcourse, inallmod-
erncomputers, error-correcting codesareusedinthememory ,sothat
small numbersoferrors canbedetected andcorrected. Butthiserror-
tolerance isnotanintrinsic propertyofthememory system. Ifminor
damage occurs tocertain hardw arethatimplemen tsmemory retriev al,
itislikelythatallfunctionalit ywillbecatastrophically lost.
3.Address-based memory isnotdistributed. Inaserial computer that
isaccessing aparticular memory ,onlyatinyfraction ofthedevices
participate inthememory recall: theCPU andthecircuits thatare
storing therequired byte.Alltheother millions ofdevices inthemachine
aresitting idle.
Arethere modelsoftruly parallel computation, inwhichmultiple de-
vicesparticipate inallcomputations? [Presen t-dayparallel computers
scarcely dier fromserial computers fromthispointofview. Memory
retriev alworksinjustthesame way,andcontrolofthecomputation
processresides inCPUs. There aresimply afewmore CPUs. Most of
thedevices sitidlemostofthetime.]
Biological memory systems arecompletely dieren t.
1.Biological memory isassociative.Memory recall iscontent-addr essable .
Givenaperson's name, wecanoften recall theirface;andviceversa.
Memories areapparen tlyrecalled spontaneously ,notjustattherequest
ofsome CPU.
2.Biological memory recall iserror-toleran tandrobust.
Errors inthecuesformemory recall canbecorrected. Anexample
asksyoutorecall `AnAmerican politician whowasveryintelligen t
andwhose politician father didnotlikebroccoli'. Manypeople
think ofpresiden tBush {eventhough oneofthecuescontainsan
error.
Hardw arefaults canalsobetolerated. Ourbrains arenoisy lumps
ofmeat thatareinacontinualstate ofchange, withcellsbeing
damaged bynatural processes, alcohol, andboxing. While thecells
inourbrains andtheproteins inourcellsarecontinuallychanging,
manyofourmemories persist unaected.
3.Biological memory isparallel anddistributed {notcompletely distributed
throughout thewhole brain: there doesappeartobesome functional
specialization {butintheparts ofthebrain where memories arestored,
itseems thatmanyneurons participate inthestorage ofmultiple mem-
ories.
These properties ofbiological memory systems motivatethestudy of`arti-
cialneural networks'{parallel distributed computational systems consisting

<<<PAGE 482>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
470 38|Introduction toNeural Networks
ofmanyinteracting simple elemen ts.Thehopeisthatthese modelsystems
mightgivesome hintsastohowneural computation isachievedinrealbio-
logical neural networks.
38.2 Terminology
Eachtimewedescrib eaneural networkalgorithm wewilltypically specify
three things. [Ifanyofthisterminology ishardtounderstand, it'sprobably
besttodivestraigh tintothenextchapter.]
Architecture .Thearchitecture species what variables areinvolvedinthe
networkandtheirtopological relationships {forexample, thevariables
involvedinaneural netmightbetheweightsoftheconnections between
theneurons, along withtheactivities oftheneurons.
Activit yrule.Most neural networkmodelshaveshort time-scale dynamics:
localrulesdene howtheactivities oftheneurons change inresponse
toeachother. Typically theactivit yruledependsontheweights(the
parameters) inthenetwork.
Learning rule.Thelearning rulespecies thewayinwhichtheneural net-
work's weightschange withtime. Thislearning isusually viewedas
taking place onalonger timescalethanthetimescaleofthedynamics
under theactivit yrule. Usually thelearning rulewilldependonthe
activities oftheneurons. Itmayalsodependonthevalues oftarget
values supplied byateacherandonthecurren tvalueoftheweights.
Where dothese rulescome from? Often, activit yrulesandlearning rulesare
inventedbyimaginativ eresearc hers. Alternativ ely,activit yrulesandlearning
rulesmaybederivedfromcarefully chosen objectivefunctions .
Neural networkalgorithms canberoughly divided intotwoclasses.
Supervised neural networks aregivendataintheformofinputs andtar-
gets,thetargets beingateacher'sspecication ofwhat theneural net-
work'sresponsetotheinput should be.
Unsup ervised neural networks aregivendatainanundivided form{sim-
plyasetofexamplesfxg.Some learning algorithms areintended simply
tomemorize these datainsuchawaythattheexamples canberecalled
inthefuture. Other algorithms areintended to`generalize', todiscover
`patterns' inthedata, orextract theunderlying `features' fromthem.
Some unsup ervised algorithms areabletomakepredictions {forexam-
ple,somealgorithms can`llin'missing variables inanexample x{and
socanalsobeviewedassupervised networks.

<<<PAGE 483>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
39
TheSingle Neuron asaClassier
39.1 Thesingle neuron
Wewillstudy asingle neuron fortworeasons. First, manyneural network
modelsarebuiltoutofsingle neurons, soitisgoodtounderstand them in
detail. Andsecond, asingle neuron isitself capable of`learning' {indeed,
various standard statistical metho dscanbeviewedinterms ofsingle neurons
{sothismodelwillserveasarstexample ofasupervised neural network.
Denition ofasingle neuron
Wewillstart bydening thearchitecture andtheactivit yruleofasingle
neuron, andwewillthenderivealearning rule.
EEEEE

AAAAA
bbbbbw06
y
x1xIw1wI
:::
Figure 39.1.Asingle neuron
Architecture .Asingle neuron hasanumberIofinputsxiandoneoutput
whichwewillherecally.(Seegure 39.1.) Associated witheachinput
isaweightwi(i=1;:::;I).There maybeanadditional parameter
w0oftheneuron called abiaswhichwemayviewasbeingtheweight
associated withaninputx0whichispermanen tlysetto1.Thesingle
neuron isafeedforw arddevice {theconnections aredirected fromthe
inputs totheoutput oftheneuron.
Activit yrule.Theactivit yrulehastwosteps.
1.First, inresponsetotheimposedinputs x,wecompute theactiva-
tionoftheneuron,
a=X
iwixi; (39.1)
where thesumisoveri=0;:::;Iifthere isabiasandi=1;:::;I
otherwise.
2.Second, theoutputyissetasafunctionf(a)oftheactivation.
Theoutput isalsocalled theactivit yoftheneuron, nottobe
confused withtheactivationa.There areseveralpossible activationactivation activit y
a!y(a)functions ;herearethemostpopular.
(a)Deterministic activation functions:
i.Linear.
y(a)=a: (39.2)
ii.Sigmoid (logistic function).
01
-505 y(a)=1
1+e a(y2(0;1)): (39.3)
471

<<<PAGE 484>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
472 39|TheSingle Neuron asaClassier
iii.Sigmoid (tanh).
-101
-505y(a)=tanh(a)(y2( 1;1)): (39.4)
iv.Threshold function.
-101
-505 y(a)=(a)(
1a>0
 1a0:(39.5)
(b)Stochastic activationfunctions:yisstochastically selected from
1.
i.Heatbath.
y(a)=8
<
:1withprobabilit y1
1+e a
 1otherwise.(39.6)
ii.TheMetrop olisruleproduces theoutput inawaythat
dependsontheprevious output statey:
Compute =ay
If0;ipytotheother state
Elseipytotheother statewithprobabilit ye .
39.2 Basic neural networkconcepts
Aneural networkimplemen tsafunctiony(x;w);the`output' ofthenetwork,
y,isanonlinear function ofthe`inputs' x;thisfunction isparameterized by
`weights'w.
Wewillstudy asingle neuron whichproduces anoutput between0and1
asthefollowingfunction ofx:
y(x;w)=1
1+e wx: (39.7)
Exercise 39.1.[1]Inwhatcontextshaveweencoun teredthefunctiony(x;w)=
1=(1+e wx)already?
Motivations forthelinearlogisticfunction
Insection 11.2westudied `thebestdetection ofpulses', assuming thatone
oftwosignals x0andx1hadbeentransmitted overaGaussian channel with
variance{co variance matrix A 1.Wefound thattheprobabilit ythatthesource
signal wass=1rather thans=0,giventhereceivedsignaly,was
P(s=1jy)=1
1+exp( a(y)); (39.8)
wherea(y)wasalinear function ofthereceivedvector,
a(y)=wTy+; (39.9)
withwA(x1 x0).
Thelinear logistic function canbemotivatedinseveralother ways{see
theexercises.

<<<PAGE 485>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
39.2: Basic neural networkconcepts 473
-10-50510-10-505100.51
x1x2
w=(0;2)Figure 39.2.Output ofasimple
neural networkasafunction ofits
input.
Inputspaceandweight space
Forconvenience letusstudy thecasewhere theinput vectorxandtheparam-
etervectorwarebothtwo-dimensional: x=(x1;x2),w=(w1;w2).Then we
canspelloutthefunction performed bytheneuron thus:
y(x;w)=1
1+e (w1x1+w2x2): (39.10)
Figure 39.2showstheoutput oftheneuron asafunction oftheinput vector,
forw=(0;2).Thetwohorizon talaxesofthisgure aretheinputsx1andx2,
withtheoutputyonthevertical axis.Notice thatonanylineperpendicular
tow,theoutput isconstan t;andalong alineinthedirection ofw,theoutput
isasigmoid function.
Wenowintroducetheideaofweightspace,thatis,theparameter space of
thenetwork.Inthiscase,there aretwoparameters w1andw2,sotheweight
space istwodimensional. Thisweightspace isshowningure 39.3. Fora
selection ofvalues oftheparameter vectorw,smaller insetgures showthe
function ofxperformed bythenetworkwhenwissettothose values. Eachof
these smaller gures isequivalenttogure 39.2. Thuseachpointinwspace
corresp ondstoafunction ofx.Notice thatthegainofthesigmoid function
(thegradien toftheramp) increases asthemagnitude ofwincreases.
Now,thecentralideaofsupervised neural networksisthis.Givenexamples
ofarelationship betweenaninput vectorx,andatargett,wehopetomake
theneural network`learn' amodeloftherelationship betweenxandt.A
successfully trained networkwill,foranygivenx,giveanoutputythatis
close (insome sense) tothetarget valuet.Training thenetworkinvolves
searchingintheweightspace ofthenetworkforavalueofwthatproduces a
function thattstheprovided training datawell.
Typically anobjectivefunction orerrorfunction isdened, asafunction
ofw,tomeasure howwellthenetworkwithweightssettowsolvesthetask.
Theobjectivefunction isasumofterms, oneforeachinput/target pairfx;tg,
measuring howclosetheoutputy(x;w)istothetargett.Thetraining process
isanexercise infunction minimization {i.e.,adjusting winsuchawayasto
ndawthatminimizes theobjectivefunction. Manyfunction-minimization
algorithms makeusenotonlyoftheobjectivefunction, butalsoitsgradient
withrespecttotheparameters w.Forgeneral feedforw ardneural networks
thebackpropagation algorithm ecien tlyevaluates thegradien toftheoutput
ywithrespecttotheparameters w,andthence thegradien toftheobjective
function withrespecttow.

<<<PAGE 486>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
474 39|TheSingle Neuron asaClassier
-6
 3 2 1 0 1 2 3 4 5 6 2 1012345 w2
w1-10-50510-10-50510
00.51
x1x2
w=( 2;3)
-10-50510-10-505100.51
x1x2
w=(0;2)-10-50510-10-50510
00.51
x1x2
w=(1;4)-10-50510-10-505100.51
x1x2
w=(5;4)
-10-50510-10-50510
00.51
x1x2
w=(2;2)
-10-50510-10-50510
00.51
x1x2
w=(2; 2)-10-50510-10-505100.51
x1x2
w=(5;1)
-10-50510-10-505100.51
x1x2
w=( 2; 1)-10-50510-10-505100.51
x1x2
w=(1;0)-10-50510-10-505100.51
x1x2
w=(3;0)
Figure 39.3.Weightspace.

<<<PAGE 487>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
39.3: Training thesingle neuron asabinary classier 475
39.3 Training thesingle neuron asabinary classier
Weassume wehaveadatasetofinputsfx(n)gN
n=1withbinary labelsft(n)gN
n=1,
andaneuron whose outputy(x;w)isbounded between0and1.Wecanthen
write downthefollowingerrorfunction :
G(w)= X
nh
t(n)lny(x(n);w)+(1 t(n))ln(1 y(x(n);w))i
:(39.11)
Eachterminthisobjectivefunction mayberecognized astheinformation
content ofoneoutcome. Itmayalsobedescrib edastherelativ eentropybe-
tweentheempirical probabilit ydistribution (t(n);1 t(n))andtheprobabilit y
distribution implied bytheoutput oftheneuron (y;1 y).Theobjectivefunc-
tionisbounded belowbyzeroandonlyattains thisvalueify(x(n);w)=t(n)
foralln.
Wenowdieren tiatethisobjectivefunction withrespecttow.
Exercise 39.2.[2]Thebackpropagation algorithm. Showthatthederivativeg=
@G=@wisgivenby:
gj=@G
@wj=NX
n=1 (t(n) y(n))x(n)
j: (39.12)
Notice thatthequantitye(n)t(n) y(n)istheerror onexamplen{the
dierence betweenthetarget andtheoutput. Thesimplest thing todowith
agradien tofanerrorfunction istodescendit(eventhough thisisoften di-
mensionally incorrect, sinceagradien thasdimensions [1/parameter], whereas
achange inaparameter hasdimensions [parameter]). Since thederivative
@G=@wisasumoftermsg(n)dened by
g(n)
j (t(n) y(n))x(n)
j (39.13)
forn=1;:::;N,wecanobtain asimple on-line algorithm byputting each
input through thenetworkoneatatime,andadjusting walittleinadirection
oppositetog(n).
Wesummarize thewhole learning algorithm.
Theon-line gradient-desc entlearning algorithm
Architecture .Asingle neuron hasanumberIofinputsxiandoneoutput
y.Associated witheachinput isaweightwi(i=1;:::;I).
Activit yrule.1.First, inresponsetothereceivedinputs x(whichmaybe
arbitrary realnumbers),wecompute theactivation oftheneuron,
a=X
iwixi; (39.14)
where thesumisoveri=0;:::;Iifthere isabiasandi=1;:::;I
otherwise.
2.Second, theoutputyissetasasigmoid function oftheactivation.
y(a)=1
1+e a: (39.15)
Thisoutput mightbeviewedasstating theprobabilit y,according tothe
neuron, thatthegiveninput isinclass1rather thanclass0.

<<<PAGE 488>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
476 39|TheSingle Neuron asaClassier
Learning rule.Theteachersupplies atarget valuet2f0;1gwhichsays
what thecorrect answerisforthegiveninput. Wecompute theerror
signal
e=t y (39.16)
thenadjust theweightswinadirection thatwouldreduce themagnitude
ofthiserror:
wi=exi; (39.17)
whereisthe`learning rate'. Commonly issetbytrialanderrortoa
constan tvalueortoadecreasing function ofsimulation timesuchas
0=.
Theactivit yruleandlearning rulearerepeated foreachinput/target pair
(x;t)thatispresen ted.Ifthere isaxed datasetofsizeN,wecancycle
through thedatamultiple times.
Batch learning versus on-line learning
Herewehavedescrib edtheon-line learning algorithm, inwhichachange in
theweightsismade aftereveryexample ispresen ted.Analternativ eparadigm
istogothrough abatchofexamples, computing theoutputs anderrors and
accum ulating thechanges specied inequation (39.17) whicharethenmade
attheendofthebatch.
Batch learning forthesingle neuronclassier
Foreachinput/target pair(x(n);t(n))(n=1;:::;N),compute
y(n)=y(x(n);w),where
y(x;w)=1
1+exp( P
iwixi); (39.18)
denee(n)=t(n) y(n),andcompute foreachweightwi
g(n)
i= e(n)x(n)
i: (39.19)
Then let
wi= X
ng(n)
i: (39.20)
Thisbatchlearning algorithm isagradien tdescen talgorithm ,whereas the
on-line algorithm isastochastic gradien tdescen talgorithm. Source code
implemen tingbatchlearning isgiveninalgorithm 39.5. Thisalgorithm is
demonstrated ingure 39.4foraneuron withtwoinputs withweightsw1and
w2andabiasw0,performing thefunction
y(x;w)=1
1+e (w0+w1x1+w2x2): (39.21)
Thebiasw0isincluded, incontrasttogure 39.3,where itwasomitted. The
neuron istrained onadatasetoftenlabelledexamples.

<<<PAGE 489>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
39.3: Training thesingle neuron asabinary classier 477
Figure 39.4.Asingle neuron learning toclassify bygradien tdescen t.Theneuron hastwoweightsw1
andw2andabiasw0.Thelearning ratewassetto=0:01andbatch-modegradien t
descen twasperformed using thecodedispla yedinalgorithm 39.5. (a)Thetraining data.
(b)Evolution ofweightsw0,w1andw2asafunction ofnumberofiterations (onlogscale).
(c)Evolution ofweightsw1andw2inweightspace. (d)TheobjectivefunctionG(w)asa
function ofnumberofiterations. (e)Themagnitude oftheweightsEW(w)asafunction of
time. (f{k)Thefunction performed bytheneuron (shownbythreeofitscontours) after30,
80,500,3000, 10000and40000iterations. Thecontoursshownarethose corresp onding to
a=0;1,namelyy=0:5;0:27and0:73.Alsoshownisavectorproportional to(w1;w2).
Thelarger theweightsare,thebigger thisvectorbecomes, andthecloser together arethe
contours.(a)
x1x2
0246810
0246810(c)w1w2
-0.500.511.522.53
-0.500.511.522.53
(f)0246810
0246810(g)0246810
0246810
(h)0246810
0246810(i)0246810
0246810
(j)0246810
0246810(k)0246810
0246810(b)
-12-10-8-6-4-202
110100100010000100000w0
w1
w2
(d)
01234567
110100100010000100000G(w)
(e)
050100150200250300350400
110100100010000100000E_W(w)

<<<PAGE 490>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
478 39|TheSingle Neuron asaClassier
Algorithm 39.5.Octave source
codeforagradien tdescen t
optimizer ofasingle neuron,
batchlearning, withoptional
weightdecay(ratealpha ).
Octave notation: theinstruction
a=x*wcauses the(NI)
matrixxconsisting ofallthe
input vectors tobemultiplied by
theweightvectorw,giving the
vectoralisting theactivations for
allNinput vectors;x'means
x-transp ose;thesingle command
y=sigmoid(a) computes the
sigmoid function ofallelemen tsof
thevectora.globalx; #xisanN*Imatrixcontaining alltheinputvectors
globalt; #tisavectoroflengthNcontaining allthetargets
forl=1:L #loopLtimes
a=x*w; #compute allactivations
y=sigmoid(a) ; #compute outputs
e=t-y; #compute errors
g=-x'*e; #compute thegradient vector
w=w-eta*(g+alpha*w);#makestep,usinglearning rateeta
#andweightdecayalpha
endfor
function f=sigmoid (v)
f=1.0./(1.0.+exp(-v));
endfunction
=0:01 =0:1 =1
(a)
-12-10-8-6-4-202
110100100010000100000w0
w1
w2
-5-4-3-2-1012
110100100010000100000w0
w1
w2
-5-4-3-2-1012
110100100010000100000w0
w1
w2
(b)
-0.500.511.522.53
-0.500.511.522.53-0.500.511.522.53
-0.500.511.522.53-0.500.511.522.53
-0.500.511.522.53
(c)
01234567
110100100010000100000G(w)
M(w)
01234567
110100100010000100000G(w)
M(w)
01234567
110100100010000100000G(w)
M(w)
(d)
0246810
02468100246810
02468100246810
0246810Figure 39.6.Theinuence of
weightdecayonasingle neuron's
learning. Theobjectivefunction is
M(w)=G(w)+EW(w).The
learning metho dwasasin
gure 39.4.(a)Evolution of
weightsw0,w1andw2.(b)
Evolution ofweightsw1andw2in
weightspace shownbypoints,
contrasted withthetrajectory
followedinthecaseofzeroweight
decay,shownbyathinline(from
gure 39.4). Notice thatforthis
problem weightdecayhasan
eect verysimilar to`early
stopping'. (c)Theobjective
functionM(w)andtheerror
functionG(w)asafunction of
numberofiterations. (d)The
function performed bytheneuron
after40000iterations.

<<<PAGE 491>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
39.4: Beyonddescen tontheerrorfunction: regularization 479
39.4 Beyonddescen tontheerror function: regularization
Iftheparameterissettoanappropriate value,thisalgorithm works: the
algorithm ndsasetting ofwwhichcorrectly classies asmanyoftheexamples
aspossible.
Iftheexamples areinfactlinearly separable thentheneuron ndsthislin-
earseparation anditsweightsdivergetoever-larger values asthesimulation
continues.Thiscanbeseenhappening ingure 39.4(f{k). Thisisanexam-
pleofovertting ,where amodeltsthedatasowellthatitsgeneralization
performance islikelytobeadversely aected.
Thisbehaviour maybeviewedasundesirable. Howcanitberectied?
Anadhocsolution toovertting istouseearly stopping ,thatis,use
analgorithm originally intended tominimize theerror functionG(w),then
preventitfromdoing sobyhalting thealgorithm atsome point.
Amoreprincipled solution toovertting makesuseofregularization .Reg-
ularization involvesmodifying theobjectivefunction insuchawayastoin-
corporate abiasagainst thesortsofsolution wwhichwedislike.Intheabove
example, what wedislikeisthedevelopmen tofaverysharp decision bound-
aryingure 39.4k; thissharp boundary isassociated withlargeweightvalues,
soweusearegularizer thatpenalizes large weightvalues. Wemodifythe
objectivefunction to:
M(w)=G(w)+EW(w) (39.22)
where thesimplest choice ofregularizer istheweightdecayregularizer
EW(w)=1
2X
iw2
i: (39.23)
Theregularization constan tiscalled theweightdecayrate.Thisadditional
termfavourssmall valuesofwanddecreases thetendency ofamodeltoovert
nedetails ofthetraining data. Thequantityisknownasahyperparameter .
Hyperparameters playaroleinthelearning algorithm butplaynoroleinthe
activit yruleofthenetwork.
Exercise 39.3.[1]Compute thederivativeofM(w)withrespecttowi.Whyis
theaboveregularizer knownasthe`weightdecay'regularizer?
Thegradien tdescen tsource codeofalgorithm 39.5implemen tsweightdecay.
Thisgradien tdescen talgorithm isdemonstrated ingure 39.6using weightde-
cayrates=0:01;0:1and1.Astheweightdecayrateisincreased thesolution
becomes biased towardsbroader sigmoid functions withdecision boundaries
thatarecloser totheorigin.
Note
Gradien tdescen twithastepsizeisingeneral notthemostecien twayto
minimize afunction. Amodication ofgradien tdescen tknownasmomen tum,
while impro vingconvergence, isalsonotrecommended. Most neural network
expertsusemore advanced optimizers suchasconjugate gradien talgorithms.
[Please donotconfuse momen tum,whichissometimes giventhesymbol,
withweightdecay.]

<<<PAGE 492>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
480 39|TheSingle Neuron asaClassier
39.5 Further exercises
Moremotivations forthelinearneuron
.Exercise 39.4.[2]Consider thetaskofrecognizing whichoftwoGaussian distri-
butions avectorzcomes from. Unlikethecasestudied insection 11.2,
where thedistributions haddieren tmeans butacommon variance{
covariance matrix, wewillassume thatthetwodistributions haveex-
actly thesame mean butdieren tvariances. Lettheprobabilit yofz
givens(s2f0;1g)be
P(zjs)=IY
i=1Normal (zi;0;2
si); (39.24)
where2
siisthevariance ofziwhen thesource symboliss.Showthat
P(s=1jz)canbewritten intheform
P(s=1jz)=1
1+exp( wTx+); (39.25)
wherexiisanappropriate function ofzi,xi=g(zi).
Exercise 39.5.[2]Thenoisy LED.
41
72
563c(2)= c(3)= c(8)=
Consider anLEDdispla ywith7elemen tsnumberedasshownabove.The
stateofthedispla yisavectorx.When thecontroller wantsthedispla y
toshowcharacter numbers,e.g.s=2,eachelemen txj(j=1;:::;7)
either adopts itsintended statecj(s),withprobabilit y1 f,orisipped,
withprobabilit yf.Let'scallthetwostates ofx`+1'and` 1'.
(a)Assuming thattheintended charactersisactually a2ora3,what
istheprobabilit yofs,giventhestatex?ShowthatP(s=2jx)
canbewritten intheform
P(s=2jx)=1
1+exp( wTx+); (39.26)
andcompute thevalues oftheweightswinthecasef=0:1.
(b)Assuming thatsisoneoff0;1;2;:::;9g,withprior probabilities
ps,whatistheprobabilit yofs,giventhestatex?Putyouranswer
intheform
P(sjx)=eas
X
s0eas0; (39.27)
wherefasgarefunctions offcj(s)gandx.
Could youmakeabetteralphab etof10characters foranoisy LED, i.e.,
analphab etlesssusceptible toconfusion?0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Table39.7.Analternativ e
15-character alphab etforthe
7-elemen tLEDdispla y.
.Exercise 39.6.[2]A(3;1)error-correcting codeconsists ofthetwocodewords
x(1)=(1;0;0)andx(2)=(0;0;1).Asource bits2f1;2ghavingproba-
bilitydistributionfp1;p2gisusedtoselect oneofthetwocodewordsfor
transmission overabinary symmetric channel withnoise levelf.The

<<<PAGE 493>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
39.5: Further exercises 481
receivedvectorisr.Showthattheposterior probabilit yofsgivenrcan
bewritten intheform
P(s=1jr)=1
1+exp
 w0 P3
n=1wnrn;
andgiveexpressions forthecoecien tsfwng3
n=1andthebias,w0.
Describ e,withadiagram, howthisoptimal decodercanbeexpressed in
terms ofa`neuron'.

<<<PAGE 494>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
Problems tolookatbeforeChapter 40
.Exercise 40.1.[2]What isPN
K=0 N
K?
[Thesymbol N
Kmeans thecombinationN!
K!(N K)!.]
.Exercise 40.2.[2]IfthetoprowofPascal's triangle (whichcontainsthesingle
number`1')isdenoted rowzero,what isthesumofallthenumbersin
thetriangle aboverowN?
.Exercise 40.3.[2]3pointsareselected atrandom onthesurface ofasphere.
What istheprobabilit ythatallofthem lieonasingle hemisphere?
Thischapter's material isoriginally duetoPolya(1954) andCover(1965) andthe
exposition thatfollowsisYaserAbu-Mostafa's.
482

<<<PAGE 495>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
40
Capacit yofaSingle Neuron
ftngN
n=1
fxngN
n=1-
6Learning
algorithm-w-w
fxngN
n=16-f^tngN
n=1Figure 40.1.Neural network
learning viewedas
comm unication.
40.1 Neural networklearning ascomm unication
Manyneural networkmodelsinvolvetheadaptation ofasetofweightswin
responsetoasetofdatapoints,forexample asetofNtarget valuesDN=
ftngN
n=1atgivenlocationsfxngN
n=1.Theadapted weightsarethenusedto
processsubsequen tinput data. Thisprocesscanbeviewedasacomm unication
process,inwhichthesender examines thedataDNandcreates amessage w
thatdependsonthose data. Thereceiverthenusesw;forexample, the
receivermightusetheweightstotrytoreconstruct what thedataDNwas.
[Inneural networkparlance, thisisusing theneuron for`memory' rather than
for`generalization'; `generalizing' means extrap olating fromtheobserv eddata
tothevalueoftN+1atsome newlocation xN+1.]Justasadiskdriveisa
comm unication channel, theadapted networkweightswtherefore playthe
roleofacomm unication channel, conveying information aboutthetraining
datatoafuture userofthatneural net.Thequestion wenowaddress is,
`what isthecapacit yofthischannel?' {thatis,`howmuchinformation can
bestored bytraining aneural network?'
Ifwehadalearning algorithm thateither produces anetworkwhose re-
sponsetoallinputs is+1oranetworkwhose responsetoallinputs is0,
depending onthetraining data, thentheweightsallowustodistinguish be-
tweenjusttwosortsofdataset.Themaxim uminformation suchalearning
algorithm could conveyaboutthedataistherefore 1bit,thisinformation con-
tentbeingachievedifthetwosortsofdatasetareequiprobable. Howmuch
more information canbeconveyedifwemakefulluseofaneural network's
abilitytorepresen tother functions?
40.2 Thecapacit yofasingle neuron
Wewilllookatthesimplest case,thatofasingle binary threshold neuron. We
willndthatthecapacit yofsuchaneuron istwobitsperweight .Aneuron
withKinputs canstore2Kbitsofinformation.
Toobtain thisinteresting result welaydownsome rulestoexclude less
interesting answers,suchas:`thecapacit yofaneuron isinnite, because each
483

<<<PAGE 496>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
484 40|Capacit yofaSingle Neuron
ofitsweightsisarealnumberandsocanconveyaninnite numberofbits'.
Weexclude thisanswerbysayingthatthereceiverisnotabletoexamine the
weightsdirectly ,noristhereceiverallowedtoprobetheweightsbyobserving
theoutput oftheneuron forarbitrarily chosen inputs. Weconstrain the
receivertoobserv etheoutput oftheneuron atthesamexedsetofNpoints
fxngthatwereinthetraining set.What matters nowishowmanydieren t
distinguishable functions ourneuron canproduce, giventhatwecanonly
observ ethefunction attheseNpoints.Howmanydieren tbinary labellings
ofNpointscanalinear threshold function produce? Andhowdoesthis
numbercompare withthemaxim umpossible numberofbinary labellings,
2N?Ifnearly allofthe2Nlabellings canberealized byourneuron, thenitis
acomm unication channel thatcanconveyallNbits(thetarget valuesftng)
withsmall probabilit yoferror. Wewillidentifythecapacit yoftheneuron as
themaxim umvaluethatNcanhavesuchthattheprobabilit yoferrorisvery
small. [Wearedeparting alittlefromthedenition ofcapacit yinChapter 9.]
Wethusexamine thefollowing scenario. Thesender isgivenaneuron
withKinputs andadatasetDNwhichisalabelling ofNpoints.The
sender usesanadaptiv ealgorithm totrytondawthatcanreproducethis
labelling exactly .Wewillassume thealgorithm ndssuchawifitexists. The
receiverthenevaluates thethreshold function ontheNinput values. What
istheprobabilit ythatallNbitsarecorrectly reproduced? HowlargecanN
become, foragivenK,without thisprobabilit ybecoming substan tially less
thanone?
Generalposition
Onetechnical detail needs tobepinned down:whatsetofinputsfxngarewe
considering? Ouranswermightdependonthischoice. Wewillassume that
thepointsareingeneralposition (p.484),whichmeans inK=3dimensions,
forexample, thatnothree pointsarecolinear andnofourpointsarecoplanar.
Denition 40.1AsetofpointsfxnginK-dimensional spaceareingeneral
position ifanysubset ofsizeKislinearlyindependent.
InK=3dimensions, forexample, asetofpointsareingeneral position ifno
three pointsarecolinear andnofourpointsarecoplanar. Theintuitiveideais
thatpointsingeneral position arelikerandom pointsinthespace, interms of
thelinear dependences betweenpoints.Youdon't expectthree random points
inthree dimensions tolieonastraigh tline.
Thelinearthreshold function
Theneuron wewillconsider performs thefunction
y=f KX
k=1wkxk!
(40.1)
where
f(a)=(
1a>0
0a0:(40.2)
Wewillnothaveabiasw0;thecapacit yforaneuron withabiascanbe
obtained byreplacingKbyK+1inthenalresult below,i.e.,considering
oneoftheinputs tobexedto1.(These input pointswouldnotthenbein
general position; thederivation stillworks.)

<<<PAGE 497>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
40.3: Countingthreshold functions 485
(a)x1x2
x(1)
(b)w1w2
(1)(0)Figure 40.2.Onedatapointina
two-dimensional input space, and
thetworegions ofweightspace
thatgivethetwoalternativ e
labellings ofthatpoint.
40.3 Countingthreshold functions
Letusdenote byT(N;K)thenumberofdistinct threshold functions onN
pointsingeneral position inKdimensions. Wewillderiveaformulafor
T(N;K).
Tostartwith, letusworkoutafewcases byhand.
InK=1dimension, foranyN
TheNpointslieonaline.Bychanging thesignoftheoneweightw1wecan
labelallpointsontherightsideoftheorigin 1andtheothers 0,orviceversa.
Thusthere aretwodistinct threshold functions.T(N;1)=2.
WithN=1point,foranyK
Ifthere isjustonepointx(1)thenwecanrealize bothpossible labellings by
setting w=x(1).ThusT(1;K)=2.
InK=2dimensions
Intwodimensions withNpoints,wearefreetospintheseparating linearound
theorigin. Eachtimethelinepasses overapointweobtain anewfunction.
Once wehavespunthelinethrough 360degrees wereproducethefunction
westarted from. Because thepointsareingeneral position, theseparating
plane (line) crosses onlyonepointatatime. Inonerevolution, everypoint
ispassed overtwice. There aretherefore 2Ndistinct threshold functions.
T(N;2)=2N.
Comparing withthetotalnumberofbinary functions, 2N,wemaynote
thatforN3,notallbinary functions canberealized byalinear threshold
function. Onefamous example ofanunrealizable function withN=4and
K=2istheexclusiv e-orfunction onthepointsx=(1;1).[These points
arenotingeneral position, butyoumayconrm thatthefunction remains
unrealizable evenifthepointsareperturb edintogeneral position.]
InK=2dimensions, fromthepointofviewofweight space
There isanother wayofvisualizing thisproblem. Instead ofvisualizing a
plane separating pointsinthetwo-dimensional input space, wecanconsider
thetwo-dimensional weight space,colouring regions inweightspace dieren t
colours iftheylabelthegivendatap ointsdieren tly.Wecanthencountthe
numberofthreshold functions bycountinghowmanydistinguishable regions
there areinweightspace. Consider rstthesetofweightvectors inweight

<<<PAGE 498>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
486 40|Capacit yofaSingle Neuron
(a)x1x2
x(1)x(2)
(b)w1w2
(1,1)(0,0)
(1,0)(0,1)Figure 40.3.Twodatapointsina
two-dimensional input space, and
thefourregions ofweightspace
thatgivethefouralternativ e
labellings.
(a)x1x2
x(1)x(2)x(3)
(b)                                                                                                                       
w1w2
(1,1,0)(1,0,0)(0,1,1)(0,0,1)
(1,0,1)
(0,1,0)Figure 40.4.Three datapointsin
atwo-dimensional input space,
andthesixregions ofweight
space thatgivealternativ e
labellings ofthose points.Inthis
case,thelabellings (0;0;0)and
(1;1;1)cannot berealized. For
anythree pointsingeneral
position there arealwaystwo
labellings thatcannot berealized.
space thatclassify aparticular example x(n)asa1.Forexample, gure 40.2a
showsasingle pointinourtwo-dimensional x-space, andgure 40.2b shows
thetwocorresp onding setsofpointsinw-space. Onesetofweightvectors
occupythehalfspace
x(n)w>0; (40.3)
andtheothers occupyx(n)w<0.Ingure 40.3a wehaveadded asecond
pointintheinput space. There arenow4possible labellings: (1;1),(1;0),
(0;1),and(0;0).Figure 40.3b showsthetwohyperplanes x(1)w=0and
x(2)w=0whichseparate thesetsofweightvectors thatproduceeachof
these labellings. WhenN=3(gure 40.4), weightspace isdivided bythree
hyperplanes intosixregions. Notalloftheeightconceiv ablelabellings canbe
realized. ThusT(3;2)=6.
InK=3dimensions
Wenowusethisweightspace visualization tostudy thethree dimensional
case.
Letusimagine adding onepointatatimeandcountthenumberofthresh-
oldfunctions aswedoso.WhenN=2,weightspace isdivided bytwohy-
perplanes x(1)w=0andx(2)w=0intofourregions; inanyoneregion all
vectorswproducethesamefunction onthe2input vectors. ThusT(2;3)=4.
Adding athirdpointingeneral position produces athirdplane inwspace,
sothatthere are8distinguishable regions.T(3;3)=8.Thethree bisecting
planes areshowningure 40.5a.
Atthispointmatters become slightlymore tricky.Asgure 40.5b illus-
trates, thefourth plane inthethree-dimensional wspace cannot transect all
eightofthesetscreated bytherstthree planes. Sixoftheexisiting regions
arecutintwoandtheremaining twoareunaected. SoT(4;3)=14.Two

<<<PAGE 499>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
40.3: Countingthreshold functions 487
(a)                




(b)

																																																	




























































Figure 40.5.Weightspace
illustrations forT(3;3)and
T(4;3).(a)T(3;3)=8.Three
hyperplanes (corresp onding to
three pointsingeneral position)
divide 3-space into8regions,
shownherebycolouring the
relevantpartofthesurface ofa
hollow,semi-transparen tcube
centredontheorigin. (b)
T(4;3)=14.Fourhyperplanes
divide 3-space into14regions, of
whichthisgure shows13(the
14thregion isoutofviewonthe
right-hand face.Compare with
gure 40.5a: alloftheregions
thatarenotcoloured white have
beencutintotwo.
K
N 12345678
1 22222222
2 244
3 268
4 2814
5 210
6 212Table40.6.ValuesofT(N;K)
deduced byhand.
(a)




(b)
                                                                                                            (c)!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""Figure 40.7.Illustration ofthe
cutting processgoing fromT(3;3)
toT(4;3).(a)Theeightregions
ofgure 40.5a withoneadded
hyperplane. Alloftheregions
thatarenotcoloured white have
beencutintotwo.(b)Here, the
hollowcubehasbeenmade solid,
sowecanseewhichregions are
cutbythefourth plane. Thefront
halfofthecubehasbeencut
away.(c)Thisgure showsthe
newtwodimensional hyperplane,
whichisdivided intosixregions
bythethree one-dimensional
hyperplanes (lines) whichcrossit.
Eachofthese regions corresp onds
tooneofthethree-dimensional
regions ingure 40.7a whichis
cutintotwobythisnew
hyperplane. Thisshowsthat
T(4;3) T(3;3)=6.Figure 40.7c
should becompared withgure
40.4b.

<<<PAGE 500>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
488 40|Capacit yofaSingle Neuron
ofthebinary functions on4pointsin3dimensions cannot berealized bya
linear threshold function.
Wehavenowlledinthevalues ofT(N;K)shownintable 40.6. Canwe
obtain anyinsigh tsintoourderivation ofT(4;3)inorder tollintherestof
thetable forT(N;K)?WhywasT(4;3)greater thanT(3;3)bysix?
Sixisthenumberofregions thatthenewhyperplane bisected inw-space
(gure 40.7ab).Equiv alently,ifwelookintheK 1dimensional subspace
thatistheNthhyperplane, thatsubspace isdivided intosixregions bythe
N 1previous hyperplanes (gure 40.7c). Nowthisisaconcept wehavemet
before. Compare gure 40.7c withgure 40.4b. Howmanyregions arecreated
byN 1hyperplanes inaK 1dimensional space? Why,T(N 1;K 1),of
course! Inthepresen tcaseN=4,K=3,wecanlookupT(3;2)=6inthe
previous section. So
T(4;3)=T(3;3)+T(3;2): (40.4)
Recurrencerelation foranyN;K
Generalizing thispicture, weseethatwhen weaddanNthhyperplane inK
dimensions, itwillbisectT(N 1;K 1)oftheT(N 1;K)regions thatwere
created bythepreviousN 1hyperplanes. Therefore, thetotal numberof
regions obtained afteradding theNthhyperplane is2T(N 1;K 1)(since
T(N 1;K 1)outofT(N 1;K)regions aresplitintwo)plustheremaining
T(N 1;K) T(N 1;K 1)regions notsplitbytheNthhyperplane, which
givesthefollowingequation forT(N;K):
T(N;K)=T(N 1;K)+T(N 1;K 1): (40.5)
Nowallthatremains istosolvethisrecurrence relation giventheboundary
conditionsT(N;1)=2andT(1;K)=2.
Doestherecurrence relation (40.5) lookfamiliar? Maybeyouremem ber
building Pascal's triangle byadding together twoadjacen tnumbersinonerow
togetthenumberbelow.TheN;Kelemen tofPascal's triangle isequal to
C(N;K) 
N
K!
N!
(N K)!K!: (40.6)
K
N 01234567
0 1
1 11
2 121
3 1331
4 14641
5 15101051Table40.8.Pascal's triangle.
Combinations N
Ksatisfy theequation
C(N;K)=C(N 1;K 1)+C(N 1;K);forallN>0: (40.7)
[Here weareadopting theconventionthat N
K0ifK>NorK<0.]
So N
Ksatises therequired recurrence relation (40.5). Thisdoesn't mean
T(N;K)= N
K,since manyfunctions cansatisfy onerecurrence relation.

<<<PAGE 501>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
40.3: Countingthreshold functions 489
Figure 40.9.Thefraction offunctions onNpointsinKdimensions thatarelinear threshold functions,
T(N;K)=2N,shownfromvarious viewp oints.In(a)weseethedependence onK,which
isapproximately anerrorfunction passing through 0.5atK=N=2;thefraction reaches1
atK=N.In(b)weseethedependence onN,whichis1uptoN=Kanddrops sharply
atN=2K.Panel(c)showsthedependence onN=KforK=1000. There isasudden
dropinthefraction ofrealizable labellings whenN=2K.Panel(d)showsthevalues of
log2T(N;K)andlog22Nasafunction ofNforK=1000. These gures wereplotted
using theapproximation ofT=2Nbytheerrorfunction.(a)N=K
K=N/2
1020304050607010
20
30
40
50
60
7000.250.50.751
KN (b)N=K
N=2K
10203040506070
50 100 15000.250.50.751
K
N
(c)00.250.50.751
0 0.5 1 1.5 2 2.5 3
N/KN=K N=2K(d)1800190020002100220023002400
1800 1900 2000 2100 2200 2300 2400log T(N,K)
log 2^N
Butperhaps wecanexpressT(N;K)asalinear superposition ofcombination
functions oftheformC;(N;K) N+
K+.Bycomparing tables 40.8and
40.6wecanseehowtosatisfy theboundary conditions: wesimply needto
translate Pascal's triangle totherightby1,2,3,:::;superpose;add;multiply
bytwo,anddropthewhole table byoneline.Thus:
T(N;K)=2K 1X
k=0 
N 1
k!
: (40.8)
Using thefactthattheNthrowofPascal's triangle sums to2N,thatis,PN 1
k=0 N 1
k=2N 1,wecansimplify thecases whereK 1N 1.
T(N;K)=(
2NKN
2PK 1
k=0 N 1
kK<N:(40.9)
Interpr etation
Itisnatural tocompareT(N;K)withthetotalnumberofbinary functions on
Npoints,2N.TheratioT(N;K)=2Ntellsustheprobabilit ythatanarbitrary
labellingftngN
n=1canbememorized byourneuron. Thetwofunctions are
equal forallNK.ThelineN=Kisthusaspecialline,dening the
maxim umnumberofpointsonwhichanyarbitrary labelling canberealized.
Thisnumberofpointsisreferred toastheVapnik{Cherv onenkis dimension
(VCdimension) oftheclass offunctions. TheVCdimension ofabinary
threshold function onKdimensions isthusK.

<<<PAGE 502>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
490 40|Capacit yofaSingle Neuron
What isinteresting is(forlargeK)thenumberofpointsNsuchthat
almost anylabelling canberealized. TheratioT(N;K)=2Nis,forN<2K,
stillgreater than1/2,andforlargeKtheratioisverycloseto1.
Forourpurposesthesuminequation (40.9) iswellapproximated bythe
errorfunction,
KX
0 
N
k!
'2N 
K (N=2)p
N=2!
; (40.10)
where (z)Rz
 1exp( z2=2)=p
2.Figure 40.9showstherealizable fraction
T(N;K)=2Nasafunction ofNandK.Thetake-home message isshownin
gure 40.9c: although thefractionT(N;K)=2Nislessthan1forN>K,itis
onlynegligibly lessthan1uptoN=2K;there, there isacatastrophic drop
tozero,sothatforN>2K,onlyatinyfraction ofthebinary labellings can
berealized bythethreshold function.
Conclusion
Thecapacit yofalinear threshold neuron, forlargeK,is2bitsperweight.
Asingle neuron canalmost certainly memorize uptoN=2Krandom
binary labelsperfectly ,butwillalmost certainly failtomemorize more.
40.4 Further exercises
.Exercise 40.4.[2]Cananite setof2Ndistinct pointsinatwo-dimensional
space besplitinhalfbyastraigh tline
ifthepointsareingeneral position?
ifthepointsarenotingeneral position?
Can2NpointsinaKdimensional space besplitinhalfbyaK 1
dimensional hyperplane?
Exercise 40.5.[2,p.491]Fourpointsareselected atrandom onthesurface ofa
sphere. What istheprobabilit ythatallofthem lieonasingle hemi-
sphere? Howdoesthisquestion relate toT(N;K)?
Exercise 40.6.[2]Consider thebinary threshold neuron inK=3dimensions,
andthesetofpointsfxg=f(1;0;0);(0;1;0);(0;0;1);(1;1;1)g.Find
aparameter vectorwsuchthattheneuron memorizes thelabels:(a)
ftg=f1;1;1;1g;(b)ftg=f1;1;0;0g.
Findanunrealizable labellingftg.
.Exercise 40.7.[3]Inthischapter weconstrained allourhyperplanes togo
through theorigin. Inthisexercise, weremovethisconstrain t.
Figure 40.10.Three linesina
plane create sevenregions.Howmanyregions inaplane arecreated byNlinesingeneral position?
Exercise 40.8.[2]Estimate inbitsthetotalsensory experience thatyouhave
hadinyourlife{visual information, auditory information, etc.Estimate
howmuchinformation youhavememorized. Estimate theinformation
contentoftheworksofShakespeare.Compare these withthecapacit yof
yourbrain assuming youhave1011neurons eachmaking 1000synaptic
connections, andthatthecapacit yresult foroneneuron (twobitsper
connection) applies. Isyourbrain fullyet?

<<<PAGE 503>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
40.5: Solutions 491
.Exercise 40.9.[3]What isthecapacit yoftheaxonofaspiking neuron, viewed
asacomm unication channel, inbitspersecond? [SeeMacKa yand
McCullo ch(1952) foranearly publication onthistopic.] Multiply by
thenumberofaxons intheoptic nerve(about106)orcochlear nerve
(about50000perear)toestimate again therateofacquisition sensory
experience.
40.5 Solutions
Solution toexercise 40.5(p.490).Theprobabilit ythatallfourpointslieona
single hemisphere is
T(4;3)=24=14=16=7=8: (40.11)

<<<PAGE 504>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
41
Learning asInference
41.1 Neural networklearning asinference
InChapter 39wetrained asimple neural networkasaclassier byminimizing
anobjectivefunction
M(w)=G(w)+EW(w) (41.1)
made upofanerrorfunction
G(w)= X
nh
t(n)lny(x(n);w)+(1 t(n))ln(1 y(x(n);w))i
(41.2)
andaregularizer
EW(w)=1
2X
iw2
i: (41.3)
Thisneural networklearning processcanbegiventhefollowingprobabilistic
interpretation.
Weinterpret theoutputy(x;w)oftheneuron literally asdening (when
itsparameters warespecied) theprobabilit ythataninputxbelongs toclass
t=1,rather thanthealternativ et=0.Thusy(x;w)P(t=1jx;w).Then
eachvalueofwdenes adieren thypothesis abouttheprobabilit yofclass1
relativ etoclass0asafunction ofx.
Wedene theobserv eddataDtobethetargetsftg{theinputsfxgare
assumed tobegiven,andnottobemodelled. Toinferwgiventhedata, we
require alikelihoodfunction andaprior probabilit yoverw.Thelikelihood
function measures howwelltheparameters wpredict theobserv eddata; itis
theprobabilit yassigned totheobserv edtvaluesbythemodelwithparameters
settow.Nowthetwoequations
P(t=1jw;x)=y
P(t=0jw;x)=1 y(41.4)
canberewritten asthesingle equation
P(tjw;x)=yt(1 y)1 t=exp[tlny+(1 t)ln(1 y)]: (41.5)
SotheerrorfunctionGcanbeinterpreted asminustheloglikelihood:
P(Djw)=exp[ G(w)]: (41.6)
Similarly theregularizer canbeinterpreted interms ofalogpriorproba-
bilitydistribution overtheparameters:
P(wj)=1
ZW()exp( EW): (41.7)
492

<<<PAGE 505>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
41.2: Illustration foraneuron withtwoweights 493
IfEWisquadratic asdened above,thenthecorresp onding priordistribution
isaGaussian withvariance2
W=1=,and1=ZW()isequal to(=2)K=2,
whereKisthenumberofparameters inthevectorw.
TheobjectivefunctionM(w)thencorresp ondstotheinferenceofthe
parameters w,giventhedata:
P(wjD;)=P(Djw)P(wj)
P(Dj)(41.8)
=e G(w)e EW(w)=ZW()
P(Dj)(41.9)
=1
ZMexp( M(w)): (41.10)
Sothewfound by(locally)minimizing M(w)canbeinterpr etedasthe(locally)
mostprobableparameter vector,w.FromnowonwewillrefertowaswMP.
Whyisitnatural tointerpret theerrorfunctions aslogprobabilities? Error
functions areusually additiv e.Forexample,Gisasumofinformation con-
tents,andEWisasumofsquared weights.Probabilities, ontheother hand,
aremultiplicativ e:forindependen teventsXandY,thejointprobabilit yis
P(x;y)=P(x)P(y).Thelogarithmic mapping maintainsthiscorresp ondence.
Theinterpretation ofM(w)asalogprobabilit yhasnumerous benets,
some ofwhichwewilldiscuss inamomen t.
41.2 Illustration foraneuron withtwoweights
Inthecaseofaneuron withjusttwoinputs andnobias,
y(x;w)=1
1+e (w1x1+w2x2); (41.11)
wecanplottheposterior probabillit yofw,P(wjD;)/exp( M(w)).Imag-
inethatwereceivesomedataasshownintheleftcolumn ofgure 41.1. Each
datapointconsists ofatwodimensional input vectorxandatvalueindicated
by(t=1)or2(t=0).Thelikelihoodfunction exp( G(w))isshownasa
function ofwinthesecond column. Itisaproductoffunctions oftheform
(41.11).
Theproductoftraditional learning isapointinw-space, theestimator w,
whichmaximizes theposterior probabilit ydensit y.Incontrast, intheBayesian
view, theproductoflearning isanensemble ofplausible parameter values
(bottom rightofgure 41.1). Wedonotchooseoneparticular hypothesis w;
rather weevaluate theirposterior probabilities. Theposterior distribution is
obtained bymultiplying thelikelihoodbyaprior distribution overwspace
(shownasabroad Gaussian attheupperrightofgure 41.1). Theposterior
ensem ble(within amultiplicativ econstan t)isshowninthethird column of
gure 41.1,andasacontourplotinthefourth column. Astheamoun tofdata
increases (from toptobottom), theposterior ensem blebecomes increasingly
concen trated around themostprobable valuew.
41.3 Beyondoptimization: making predictions
Letusconsider thetaskofmaking predictions withtheneuron whichwe
trained asaclassier insection 39.3. Thiswasaneuron withtwoinputs and
abias.
y(x;w)=1
1+e (w0+w1x1+w2x2): (41.12)

<<<PAGE 506>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
494 41|Learning asInference
Figure 41.1.TheBayesian interpretation andgeneralization oftraditional neural networklearning.
Evolution oftheprobabilit ydistribution overparameters asdataarrive.Data set Likelihood Probabilit yofparameters
N=0 (constant)
-505-505
w1w2
-505-505
w1w2
N=2
-10-50510
-10 -5 0510x2
x1-505-505 0.5
w1w2
-505-505
w1w2
-505-505
w1w2
N=4
-10-50510
-10 -5 0510x2
x1-505-505 0.050.1
w1w2
-505-505
w1w2
-505-505
w1w2
N=6
-10-50510
-10 -5 0510x2
x1-505-5050.05
w1w2
-505-505
w1w2
-505-505
w1w2
Figure 41.2.Making predictions. (a)Thefunction performed byanoptimized neuron wMP(shownby
three ofitscontours) trained withweightdecay,=0:01(from gure 39.6). Thecontours
shownarethose corresp onding toa=0;1,namelyy=0:5;0:27and0:73.(b)Arethese
predictions more reasonable? (Contours shownarefory=0:5;0:27,0:73,0:12and0:88.)
(c)Theposterior probabilit yofw(schematic); theBayesianpredictions shownin(b)were
obtained byaveraging together thepredictions made byeachpossible valueoftheweights
w,witheachvalueofwreceiving avoteproportional toitsprobabilit yunder theposterior
ensem ble.Themetho dusedtocreate (b)isdescrib edinsection 41.4.(a)A
B
(b)A
B
0 5 100510
(c)wMP
2
w2w
1Samples from 
P(w|D,H)1

<<<PAGE 507>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
41.3: Beyondoptimization: making predictions 495
When welastplayedwithit,wetrained itbyminimizing theobjectivefunction
M(w)=G(w)+E(w): (41.13)
Theresulting optimized function forthecase=0:01isreproduced ing-
ure41.2a.
Wenowconsider thetaskofpredicting theclasst(N+1)corresp onding toa
newinputx(N+1).Itiscommon practice, when making predictions, simply to
useaneural networkwithitsweightsxedtotheiroptimized valuewMP,but
thisisnotoptimal, ascanbeseenintuitivelybyconsidering thepredictions
showningure 41.2a. Arethese reasonable predictions? Consider newdata
arriving atpointsAandB.Thebesttmodelassigns bothofthese examples
probabilit y0.2ofbeinginclass1,because theyhavethesamevalueofwMPx.
Ifwereally knew thatwwasequal towMP,thenthese predictions wouldbe
correct. Butwedonotknoww.Theparameters areuncertain .Intuitivelywe
mightbeinclined toassign alessconden tprobabilit y(closer to0.5)atBthan
atA,asshowningure 41.2b, sincepointBisfarfromthetraining data.The
besttparameters wMPoftengiveover-condent predictions. Anon-Ba yesian
approac htothisproblem istodownweightallpredictions uniformly ,byan
empirically determined factor (Copas, 1983). Thisisnotideal, sinceintuition
suggests thestrength ofthepredictions atBshould bedownweightedmore
thanthose atA.ABayesian viewp ointhelps ustounderstand thecause of
theproblem, andprovidesastraigh tforwardsolution. Inanutshell, weobtain
Bayesian predictions bytaking intoaccoun tthewhole posterior ensem ble,
shownschematically ingure 41.2c.
TheBayesianprediction ofanewdatum t(N+1)involvesmarginalizing over
theparameters (andoveranything elseaboutwhichweareuncertain). For
simplicit y,letusassume thattheweightswaretheonlyuncertain quantities
{theweightdecayrateandthemodelHitself areassumed tobexed.
Then bythesumrule,thepredictiv eprobabilit yofanewtarget t(N+1)ata
location x(N+1)is:
P(t(N+1)jx(N+1);D;)=Z
dKwP(t(N+1)jx(N+1);w;)P(wjD;);(41.14)
whereKisthedimensionalit yofw,three inthetoyproblem. Thusthe
predictions areobtained byweightingtheprediction foreachpossible w,
P(t(N+1)=1jx(N+1);w;)=y(x(N+1);w)
P(t(N+1)=0jx(N+1);w;)=1 y(x(N+1);w);(41.15)
withaweightgivenbytheposterior probabilit yofw,P(wjD;),whichwe
mostrecentlywrote downinequation (41.10). Thisposterior probabilit yis
P(wjD;)=1
ZMexp( M(w)); (41.16)
where
ZM=Z
dKwexp( M(w)): (41.17)
Insummary ,wecangettheBayesian predictions ifwecanndawayof
computing theintegral
P(t(N+1)=1jx(N+1);D;)=Z
dKwy(x(N+1);w)1
ZMexp( M(w));(41.18)
whichistheaverage oftheoutput oftheneuron atx(N+1)under theposterior
distribution ofw.

<<<PAGE 508>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
496 41|Learning asInference
(a)
DumbMetrop olisx(1)
Q(x;x(1))P(x)

(b)
Gradien tdescen t g
(c)
LangevinFigure 41.3.Onestepofthe
Langevin metho dintwo
dimensions (c),contrasted witha
traditional `dumb'Metrop olis
metho d(a)andwithgradien t
descen t(b).Theproposaldensit y
oftheLangevin metho disgiven
by`gradien tdescen twithnoise'.
Implementation
Howshallwecompute theintegral (41.18)? Forourtoyproblem, theweight
space isthree dimensional; forarealistic neural networkthedimensionalit y
Kmightbeinthethousands.
Bayesian inference forgeneral datamodelling problems maybeimple-
mentedbyexact metho ds(Chapter 25),byMonteCarlo sampling (Chapter
29),orbydeterministic approximate metho ds,forexample, metho dsthat
makeGaussian approximations toP(wjD;)using Laplace's metho d(Chap-
ter27)orvariational metho ds(Chapter 33).Forneural networkstherearefew
exact metho ds.Thetwomainapproac hestoimplemen tingBayesianinference
forneural networksaretheMonteCarlo metho dsdevelopedbyNeal(1996)
andtheGaussian approximation metho dsdevelopedbyMacKa y(1991).
41.4 MonteCarlo implemen tation ofasingle neuron
FirstwewilluseaMonteCarlo approac hinwhichthetaskofevaluating the
integral (41.18) issolvedbytreatingy(x(N+1);w)asafunctionfofwwhose
mean wecompute using
hf(w)i'1
RX
rf(w(r)) (41.19)
wherefw(r)garesamples fromtheposterior distribution1
ZMexp( M(w))(c.f.
equation (29.6)). Weobtain thesamples using aMetrop olismetho d(section
29.4). Asanaside, apossible disadv antageofthisMonteCarlo approac his
thatitisapoorwayofestimating theprobabilit yofanimprobable event,i.e.,
aP(tjD;H)thatisveryclosetozero,iftheimprobable eventismostlikely
tooccurinconjunction withimprobable parameter values.
Howtogenerate thesamplesfw(r)g?Radford Nealintroduced theHamil-
tonian MonteCarlo metho dtoneural networks. Wemetthissophisticated
Metrop olismetho d,whichmakesuseofgradien tinformation, inChapter 30.
Themetho dwenowdemonstrate isasimple version ofHamiltonian Monte
Carlo called theLangevin MonteCarlo metho d.
TheLangevin Monte Carlo method
TheLangevin metho d(algorithm 41.4) maybesummarized as`gradien tde-
scentwithadded noise', asshownpictorially ingure 41.3. Anoise vectorp
isgenerated fromaGaussian withunitvariance. Thegradien tgiscomputed,

<<<PAGE 509>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
41.4: MonteCarlo implemen tation ofasingle neuron 497
Algorithm 41.4.Octave source
codefortheLangevin Monte
Carlo metho d.Toobtain the
Hamiltonian MonteCarlo metho d,
werepeatthefourlinesmarked*
multiple times (algorithm 41.8).g=gradM(w); #setgradient usinginitial w
M=findM(w); #setobjective function too
forl=1:L #loopLtimes
p=randn(size(w) ); #initial momentum isNormal(0,1)
H=p'*p/2+M; #evaluate H(w,p)
*p=p-epsilon *g/2; #makehalf-step inp
*wnew=w+epsilon *p; #makestepinw
*gnew=gradM(wnew); #findnewgradient
*p=p-epsilon *gnew/2; #makehalf-step inp
Mnew=findM(wnew); #findnewobjective function
Hnew=p'*p/2+Mnew; #evaluate newvalueofH
dH=Hnew-H; #decidewhether toaccept
if(dH<0) accept=1;
elseif(rand()<exp(-dH) )accept=1;#compare withauniform
else accept=0;# variate
endif
if(accept)g=gnew;w=wnew;M=Mnew;endif
endfor
function gM=gradM(w) #gradient ofobjective function
a=x*w; #compute activations
y=sigmoid(a) ; #compute outputs
e=t-y; #compute errors
g=-x'*e; #compute thegradient ofG(w)
gM=alpha*w+g;
endfunction
function M=findM(w) #objective function
G=-(t'*log(y)+(1-t')*log(1-y));
EW=w'*w/2;
M=G+alpha*EW;
endfunction

<<<PAGE 510>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
498 41|Learning asInference
Figure 41.5.Asingle neuron learning under theLangevin MonteCarlo metho d.(a)Evolution of
weightsw0,w1andw2asafunction ofnumberofiterations. (b)Evolution ofweightsw1
andw2inweightspace. Alsoshownbyalineistheevolution oftheweightsusing the
optimizer ofgure 39.6.(c)TheerrorfunctionG(w)asafunction ofnumberofiterations.
Alsoshownistheerrorfunction during theoptimization ofgure 39.6. (d)Theobjective
functionM(x)asafunction ofnumberofiterations. Seealsogures 41.6and41.7.(a)-30-25-20-15-10-50510
010000 20000 30000 40000
(b)-3-2-1012345
-101234567
(c)02468101214
010000 20000 30000 40000G(w) - Langevin
G(w) - optimizer
(d)024681012
010000 20000 30000 40000M(w) - Langevin
M(w) - optimizer
andastepinwismade, givenby
w= 1
22g+p: (41.20)
Notice thatiftheptermwereomitted thiswouldsimply begradien tdescen t
withlearning rate=1
22.Thisstepinwisaccepted orrejected depending
onthechange inthevalueoftheobjectivefunctionM(w)andonthechange
ingradien t,withaprobabilit yofacceptance suchthatdetailed balance holds.
TheLangevin metho dhasonefreeparameter, ,whichcontrolsthetypical
stepsize.Ifissettotoolargeavalue,movesmayberejected. Ifitissetto
averysmall value,progress around thestatespace willbeslow.
Demonstr ationofLangevin method
TheLangevin metho disdemonstrated ingures 41.5,41.6and41.7.Here, the
objectivefunction isM(w)=G(w)+EW(w),with=0:01.These gures
include, forcomparison, theresults oftheprevious optimization metho dusing
gradien tdescen tonthesame objectivefunction (gure 39.6). Itcanbeseen
thatthemean evolution ofwissimilar totheevolution oftheparameters
under gradien tdescen t.TheMonteCarlo metho dappearstohaveconverged
totheposterior distribution afterabout10000iterations.
Theaverage acceptance rateduring thissimulation was93%; only7%of
theproposedmoveswererejected. Probably ,faster progress around thestate
space wouldhavebeenmade ifalarger stepsizehadbeenused, butthe
valuewaschosen sothatthe`descen trate'=1
22matchedthestepsizeof
theearlier simulations.
Making Bayesian predictions
Fromiteration 10,000 to40,000, theweightsweresampled every1000itera-
tions andthecorresp onding functions ofxareplotted ingure 41.6. There
isaconsiderable varietyofplausible functions. Weobtain aMonteCarlo ap-
proximation totheBayesianpredictions byaveraging these thirtyfunctions of
xtogether. Theresult isshowningure 41.7andcontrasted withthepredic-
tions givenbytheoptimized parameters. TheBayesian predictions become
satisfyingly moderate aswemoveawayfromtheregion ofhighest datadensit y.

<<<PAGE 511>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
41.4: MonteCarlo implemen tation ofasingle neuron 499
Figure 41.6.Samples obtained by
theLangevin MonteCarlo
metho d.Thelearning ratewasset
to=0:01andtheweightdecay
rateto=0:01.Thestepsizeis
givenby=p2.Thefunction
performed bytheneuron isshown
bythree ofitscontours every1000
iterations fromiteration 10000to
40000.Thecontours shownare
those corresp onding toa=0;1,
namelyy=0:5;0:27and0:73.
Alsoshownisavector
proportional to(w1;w2).
Figure 41.7.Bayesian predictions found bytheLangevin MonteCarlo metho dcompared withthe
predictions using theoptimized parameters. (a)Thepredictiv efunction obtained byav-
eraging thepredictions for30samples uniformly spaced betweeniterations 10000and
40000,showningure 41.6.Thecontoursshownarethose corresp onding toa=0;1;2,
namelyy=0:5;0:27,0:73,0:12and0:88.(b)Forcontrast, thepredictions givenbythe
`most probable' setting oftheneuron's parameters, asgivenbyoptimization ofM(w).(a)
0 5 100510
(b)
0 5 100510
0 5 100510

<<<PAGE 512>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
500 41|Learning asInference
Algorithm 41.8.Octave source
codefortheHamiltonian Monte
Carlo metho d.Thealgorithm is
identicaltotheLangevin metho d
inalgorithm 41.4,except forthe
replacemen tofthefourlines
marked*inthatalgorithm bythe
fragmen tshownhere.wnew=w;
gnew=g;
fortau=1:Tau
p=p-epsilon *gnew/2;#makehalf-step inp
wnew=wnew+epsilon *p;#makestepinw
gnew=gradM(wnew); #findnewgradient
p=p-epsilon *gnew/2;#makehalf-step inp
endfor
Langevin-30-25-20-15-10-50510
0 2000 4000 6000 8000 10000
HMC-40-35-30-25-20-15-10-505
0 2000 4000 6000 8000 10000Figure 41.9.Comparison of
sampling properties ofthe
Langevin MonteCarlo metho d
andtheHamiltonian MonteCarlo
(HMC) metho d.Thehorizon tal
axisisthenumberofgradien t
evalutations made. Eachgure
showstheweightsduring therst
10,000 iterations. Therejection
rateduring thisHamiltonian
MonteCarlo simulation was8%.
TheBayesianclassier isbetterabletoidentifythepointswhere theclassi-
cation isuncertain. Thispleasing behaviour results simply fromamechanical
application oftherulesofprobabilit y.
Optimization andtypicality
Analobserv ation concerns thebehaviour ofthefunctionsG(w)andM(w)
during theMonteCarlo sampling process,compared withthevaluesofGand
Mattheoptim umwMP(gure 41.5). ThefunctionG(w)uctuates around
thevalueofG(wMP),though notinasymmetrical way.ThefunctionM(w)
alsouctuates, butitdoesnotuctuate aroundM(wMP){obviously itcannot,
becauseMisminimized atwMP,soMcould notgoanysmaller {furthermore,
Monlyrarely drops closetoM(wMP).Inthelanguage ofinformation theory ,
thetypicalsetofwhasdierentproperties fromthemostprobablestatewMP.
Ageneral message therefore emerges {applicable toalldatamodels,not
justneural networks: oneshould becautious aboutmaking useofoptimize d
parameters, astheproperties ofoptimized parameters maybeunrepresen-
tativeoftheproperties oftypical, plausible parameters; andthepredictions
obtained using optimized parameters alone willoften beunreasonably over-
conden t.
Reducing random walkbehaviour usingHamiltonian Monte Carlo
Asanalstudy ofMonteCarlo metho ds,wenowcompare theLangevin Monte
Carlo metho dwithitsbigbrother, theHamiltonian MonteCarlo metho d.The
change toHamiltonian MonteCarlo issimple toimplemen t,asshowninalgo-
rithm 41.8. Eachsingle proposalmakesuseofmultiple gradien tevaluations

<<<PAGE 513>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
41.5: Implemen tinginference withGaussian approximations 501
along adynamical trajectory inw;pspace, whereparetheextra `momen tum'
variables oftheLangevin andHamiltonian MonteCarlo metho ds.Thenum-
berofsteps`Tau'wassetatrandom toanumberbetween100and200foreach
trajectory .Thestepsizewaskeptxedsoastoretain comparabilit ywith
thesimulations thathavegonebefore; itisrecommended thatonerandomize
thestepsizeinpractical applications, however.
Figure 41.9compares thesampling properties oftheLangevin andHamil-
tonian MonteCarlo metho ds.Theautocorrelation ofthestateoftheHamil-
tonian MonteCarlo simulation fallsmuchmore rapidly withsimulation time
thanthatoftheLangevin metho d.Forthistoyproblem, Hamiltonian Monte
Carlo isatleasttentimes more ecien tinitsuseofcomputer time.
41.5 Implemen tinginference withGaussian approximations
Physicists lovetotakenonlinearities andlocallylinearize them, andtheylove
toapproximate probabilit ydistributions byGaussians. Suchapproximations
oeranalternativ estrategy fordealing withtheintegral
P(t(N+1)=1jx(N+1);D;)=Z
dKwy(x(N+1);w)1
ZMexp( M(w));(41.21)
whichwejustevaluated using MonteCarlo metho ds.
Westartbymaking aGaussian approximation totheposterior probabilit y.
Wegototheminim umofM(w)(using agradien t-based optimizer) andTaylor-
expandMthere:
M(w)'M(wMP)+1
2(w wMP)TA(w wMP)+; (41.22)
whereAisthematrix ofsecond derivatives,alsoknownastheHessian ,dened
by
Aij@2
@wi@wjM(w)
w=wMP: (41.23)
Wethusdene ourGaussian approximation:
Q(w;wMP;A)=[det(A=2)]1=2exp
 1
2(w wMP)TA(w wMP)
:(41.24)
Wecanthink ofthematrix Aasdening errorbarsonw.Tobeprecise,Q
isanormal distribution whose variance{co variance matrix isA 1.
Exercise 41.1.[2]Showthatthesecond derivativeofM(w)withrespecttow
isgivenby
@2
@wi@wjM(w)=NX
n=1f0(a(n))x(n)
ix(n)
j+ij; (41.25)
wheref0(a)istherstderivativeoff(a)1=(1+e a),whichis
f0(a)=d
daf(a)=f(a)(1 f(a)); (41.26)
and
a(n)=X
jwjx(n)
j: (41.27)
Havingcomputed theHessian, ourtaskisthentoperform theintegral (41.21)
using ourGaussian approximation.

<<<PAGE 514>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
502 41|Learning asInference
(a) (a;s2)
-10
-5
0
5
10a246810
 s00.250.50.751
psi   
-10
-5
0
5
10a246810
 s00.250.50.751
psi   (b)
-10 -5 5 10a0.20.40.60.81 Figure 41.10.Themarginalized
probabilit y,andanapproximation
toit.(a)Thefunction (a;s2),
evaluated numerically .In(b)the
functions (a;s2)and(a;s2)
dened inthetextareshownasa
function ofafors2=4.From
MacKa y(1992b).
(a)
-3-2-1012345
0123456(b)
0 5 100510
A
B
0 5 100510 Figure 41.11.TheGaussian
approximation inweightspace
anditsapproximate predictions in
input space. (a)Aprojection of
theGaussian approximation onto
the(w1;w2)plane ofweight
space. Theone-and
two-standard-deviation contours
areshown.Alsoshownarethe
trajectory oftheoptimizer, and
theMonteCarlo metho d's
samples. (b)Thepredictiv e
function obtained fromthe
Gaussian approximation and
equation (41.30). (c.f.gure 41.2.)Calculating themarginalize dprobability
Theoutputy(x;w)onlydependsonwthrough thescalara(x;w),sowecan
reduce thedimensionalit yoftheintegral bynding theprobabilit ydensit yof
a.Weareassuming alocallyGaussian posterior probabilit ydistribution over
w=wMP+w,P(wjD;)'(1=ZQ)exp( 1
2wTAw).Foroursingle
neuron, theactivationa(x;w)isalinear function ofwwith@a=@w=x,so
foranyx,theactivationaisGaussian-distributed.
.Exercise 41.2.[2]Assuming wisGaussian-distributed withmeanwMPand
variance{co variance matrix A 1,showthattheprobabilit ydistribution
ofa(x)is
P(ajx;D;)=Normal (aMP;s2)=1p
2s2exp 
 (a aMP)2
2s2!
;
(41.28)
whereaMP=a(x;wMP)ands2=xTA 1x.
Thismeans thatthemarginalized output is:
P(t=1jx;D;)= (aMP;s2)Z
daf(a)Normal(aMP;s2): (41.29)
Thisistobecontrasted withy(x;wMP)=f(aMP),theoutput ofthemostprob-
ablenetwork.Theintegral ofasigmoid times aGaussian canbeapproximated
by:
 (aMP;s2)'(aMP;s2)f((s)aMP) (41.30)
with=1=p
1+s2=8(gure 41.10).

<<<PAGE 515>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
41.5: Implemen tinginference withGaussian approximations 503
Demonstr ation
Figure 41.11 showstheresult oftting aGaussian approximation attheop-
timumwMP,andtheresults ofusing thatGaussian approximation andequa-
tion(41.30) tomakepredictions. Comparing these predictions withthose of
theLangevin MonteCarlo metho d(gure 41.7)weobserv ethat,whilst quali-
tativelythesame, thetwoareclearly numerically dieren t.Soatleastoneof
thetwometho dsisnotcompletely accurate.
.Exercise 41.3.[2]IstheGaussian approximation toP(wjD;)tooheavy-tailed
ortoolight-tailed, orboth? ItmayhelptoconsiderP(wjD;)asa
function ofoneparameterwiandtothink ofthetwodistributions on
alogarithmic scale. Discuss theconditions under whichtheGaussian
approximation ismostaccurate.
Whymarginalize?
Iftheoutput isimmediately usedtomakea(0/1)decision andthecostsasso-
ciated witherroraresymmetrical, thentheuseofmarginalized outputs under
thisGaussian approximation willmakenodierence totheperformance ofthe
classier, compared withusing theoutputs givenbythemostprobable param-
eters, since bothfunctions passthrough 0:5ataMP=0.Butthese Bayesian
outputs willmakeadierence if,forexample, there isanoption ofsaying`I
don't know',inaddition tosaying`Iguess0'and`Iguess1'.Andevenif
there arejustthetwochoices `0'and`1',ifthecostsassociated witherrorare
unequal, thenthedecision boundary willbesome contourother thanthe0.5
contour,andtheboundary willbeaected bymarginalization.

<<<PAGE 516>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
Postscript onSupervised Neural
Networks
Oneofmystuden ts,Robert,asked:
MaybeI'mmissing something fundamen tal,butsupervised neural
networksseemequivalenttotting apre-dened function tosome
givendata, thenextrap olating {what's thedierence?
Iagree withRobert.Thesupervised neural networkswehavestudied sofar
aresimply parameterized nonlinear functions whichcanbetted todata.
Hopefully youwillagree withanother commen tthatRobertmade:
Unsup ervised networksseemmuchmoreinteresting thantheirsu-
pervised counterparts. I'mamazed thatitworks!
504

<<<PAGE 517>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
42
Hopeld Networks
Wehavenowspentthree chapters studying thesingle neuron. Thetimehas
come toconnect multiple neurons together, making theoutput ofoneneuron
betheinput toanother, soastomakeneural networks.
Neural networkscanbedivided intotwoclasses onthebasisoftheircon-
nectivit y.
(a) (b)Figure 42.1.(a)Afeedforw ard
network.(b)Afeedbac knetwork.
Feedforw ardnetworks.Inafeedforw ardnetwork,alltheconnections are
directed suchthatthenetworkforms adirected acyclic graph.
Feedbac knetworks.Anynetworkthatisnotafeedforw ardnetworkwillbe
called afeedbac knetwork.
Inthischapter wewilldiscuss afullyconnected feedbac knetworkcalled
theHopeld network.TheweightsintheHopeld networkareconstrained to
besymmetric ,i.e.,theweightfromneuronitoneuronjisequal totheweight
fromneuronjtoneuroni.
Hopeld networkshavetwoapplications. First, theycanactasassociative
memories .Second, theycanbeusedtosolveoptimization problems .Wewill
rstdiscuss theideaofassociativememory ,alsoknownascontent-addressable
memory.
42.1 Hebbian learning
InChapter 38,wediscussed thecontrastbetweentraditional digital memories
andbiological memories. Perhaps themoststriking dierence istheassociative
nature ofbiological memory .
Asimple modelduetoDonald Hebb (1949) captures theideaofassocia-
tivememory .Imagine thattheweightsbetweenneurons whose activities are
positively correlatedareincreased:
dwij
dtCorrelation (xi;xj): (42.1)
Nowimagine thatwhen stimulusmispresen t(forexample, thesmell ofa
banana), theactivit yofneuronmincreases; andthatneuronnisassociated
505

<<<PAGE 518>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
506 42|Hopeld Networks
withanother stimulus,n(forexample, thesightofayellowobject). Ifthese
twostimuli{ayellowsightandabanana smell {co-occurintheenvironmen t,
thentheHebbian learning rule(42.1) willincrease theweightswnmandwmn.
Thismeans thatwhen, onalateroccasion, stimulusnoccursinisolation, mak-
ingtheactivit yxnlarge, thepositiveweightfromntomwillcause neuronm
alsotobeactivated. Thustheresponsetothesightofayellowobjectisan
automatic association withthesmell ofabanana. Wecould callthis`pattern
completion'. Noteacherisrequired forthisassociativememory towork.No
signal isneeded toindicate thatacorrelation hasbeendetected orthatanas-
sociation should bemade. Theunsup ervised, locallearning algorithm andthe
unsup ervised, localactivit yrulespontaneously produceassociativememory .
Thisideaseems sosimple andsoeectiv ethatitmustberelevanttohow
memories workinthebrain.
42.2 Denition ofthebinary Hopeld network
Conventionforweights.Ourconventioningeneral willbethatwijdenotes
theconnection fromneuronjtoneuroni.
Architecture .AHopeld networkconsists ofIneurons. They arefully
connected through symmetric, bidirectional connections withweights
wij=wji.There arenoself-connections, sowii=0foralli.Biases
wi0maybeincluded (these maybeviewedasweightsfromaneuron `0'
whose activit yispermanen tlyx0=1).Wewilldenote theactivit yof
neuroni(itsoutput) byxi.
Activit yrule.Roughly ,aHopeld network's activit yruleisforeachneu-
rontoupdateitsstate asifitwereasingle neuron withthethreshold
activation function
x(a)=(a)(
1a0
 1a<0:(42.2)
Since there isfeedbackinaHopeld network(everyneuron's output is
aninput toalltheother neurons) wewillhavetospecifyanorder forthe
updates tooccur. Theupdates maybesynchronous orasynchronous.
Synchronous updates {allneurons compute theiractivations
ai=X
jwijxj (42.3)
thenupdatetheirstates simultaneously to
xi=(ai): (42.4)
Async hronous updates {oneneuron atatimecomputes itsactiva-
tionandupdates itsstate. Thesequence ofselected neurons may
beaxedsequence orarandom sequence.
Theproperties ofaHopeld networkmaybesensitiv etotheabove
choices.
Learning rule.Thelearning ruleisintended tomakeasetofdesired memo-
riesfx(n)gbestable states oftheHopeld network'sactivit yrule.Each
memory isabinary pattern, withxi2f 1;1g.

<<<PAGE 519>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
42.3: Denition ofthecontinuousHopeld network 507
(a)moscow------rus sia
lima----------p eru
london-----engl and
tokyo--------ja pan
edinburgh-scotl and
ottawa------can ada
oslo--------nor way
stockholm---swe den
paris-------fra nce(b)moscow---:::::::: :=)moscow------russ ia
::::::::::--canad a=)ottawa------cana da
(c)otowa-------cana da=)ottawa------can ada
egindurrh-sxotla nd=)edinburgh-scotl and
Figure 42.2.Associativememory
(schematic). (a)Alistofdesired
memories. (b)Therstpurposeof
anassociativememory ispattern
completion, givenapartial
pattern. (c)Thesecond purpose
ofamemory iserrorcorrection.Theweightsaresetusing thesumofouter products orHebb rule,
wij=X
nx(n)
ix(n)
j; (42.5)
whereisanunimp ortantconstan t.Topreventthelargest possible
weightfromgrowingwithNwemightchoosetoset=1=N.
Exercise 42.1.[1]Explain whythevalueofisnotimportantfortheHopeld
networkdened above.
42.3 Denition ofthecontinuousHopeld network
Using theidenticalarchitecture andlearning rulewecandene aHopeld
networkwhose activities arerealnumbersbetween 1and1.
Activit yrule.AHopeld network'sactivit yruleisforeachneuron toup-
dateitsstate asifitwereasingle neuron withasigmoid activation
function. Theupdates maybesynchronous orasynchronous, andin-
volvetheequations
ai=X
jwijxj (42.6)
and
xi=tanh(ai): (42.7)
Thelearning ruleisthesame asinthebinary Hopeld network,butthe
valueofbecomes relevant.Alternativ ely,wemayxandintroduceagain
2(0;1)intotheactivation function:
xi=tanh(ai): (42.8)
Exercise 42.2.[1]Where haveweencoun tered equations 42.6, 42.7, and42.8
before?
42.4 Convergence oftheHopeld network
ThehopeisthattheHopeld networkswehavedened willperform associa-
tivememory recall, asshownschematically ingure 42.2. Wehopethatthe
activit yruleofaHopeld networkwilltakeapartial memory oracorrupted
memory ,andperform pattern completion orerror correction torestore the
original memory .
Butwhyshould weexpectanypattern tobestable under theactivit yrule,
letalone thedesired memories?
Weaddress thecontinuousHopeld network,sincethebinary networkis
aspecialcaseofit.Wehavealready encoun tered theactivit yrule(42.6, 42.8)

<<<PAGE 520>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
508 42|Hopeld Networks
when wediscussed variational metho ds(section 33.2): when weapproximated
thespinsystem whose energy function was
E(x;J)= 1
2X
m;nJmnxmxn X
nhnxn (42.9)
withaseparable distribution
Q(x;a)=1
ZQexp X
nanxn!
(42.10)
andoptimized thelatter soastominimize thevariational freeenergy
~F(a)=X
xQ(x;a)E(x;J) X
xQ(x;a)ln1
Q(x;a); (42.11)
wefound thatthepairofiterativ eequations
am= X
nJmnxn+hm!
(42.12)
and
xn=tanh(an) (42.13)
wereguaran teedtodecrease thevariational freeenergy
~F(a)= 
 1
2X
m;nJmnxmxn X
nhnxn!
 X
nH(e)
2(qn): (42.14)
Ifwesimply replaceJbyw,xbyx,andhnbywi0,weseethatthe
equations oftheHopeld networkareidenticaltoasetofmean eldequations
thatminimize
~F(x)= 1
2xTWx X
iH(e)
2[(1+xi)=2]: (42.15)
There isageneral name forafunction thatdecreases under thedynamical
evolution ofasystem andthatisbounded below:suchafunction isaLyapuno v
function forthesystem. Itisuseful tobeabletoprovetheexistence of
Lyapuno vfunctions: ifasystem hasaLyapuno vfunction thenitsdynamics
arebound tosettle downtoaxedpoint,whichisalocalminim umofthe
Lyapuno vfunction, oralimitcycle,along whichtheLyapuno vfunction isa
constan t.Chaotic behaviour isnotpossible forasystem withaLyapuno v
function. Ifasystem hasaLyapuno vfunction thenitsstate space canbe
divided intobasins ofattraction ,onebasin associated witheachattractor.
So,thecontinuousHopeld network'sactivit yrules(ifimplemen tedasyn-
chronously) haveaLyapuno vfunction. ThisLyapuno vfunction isaconvex
function ofeachparameteraisoaHopeld network's dynamics willalways
convergetoastable xedpoint.
Thisconvergence proofdependscrucially onthefactthattheHopeld
network'sconnections aresymmetric .Italsodependsontheupdates being
made asynchronously .
Exercise 42.3.[2,p.520]Showbyconstructing anexample thatifafeedbac k
networkdoesnothavesymmetric connections thenitsdynamics may
failtoconvergetoaxedpoint.
Exercise 42.4.[2,p.521]Showbyconstructing anexample thatifaHopeld
networkisupdated synchronously that,fromsome initial conditions, it
mayfailtoconvergetoaxedpoint.

<<<PAGE 521>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
42.4: Convergence oftheHopeld network 509
(a)
.0000-22-222-2000200-202200-2-2
0.440-2-2-2-2-2-20-40-200-20-2-2442-2
04.40-2-2-2-2-2-20-40-200-20-2-2442-2
044.0-2-2-2-2-2-20-40-200-20-2-2442-2
0000.2-2-22-22-400-24-4-20-2200-22
-2-2-2-22.00004-22-202-20-200-2-204
2-2-2-2-20.004022-24-220-240-2-200
-2-2-2-2-200.0002220-224200-2-200
2-2-2-22000.00-22202-20204-2-2-40
2-2-2-2-20400.022-24-220-240-2-200
-2-2-2-2240000.-22-202-20-200-2-204
0000-4-222-22-2.002-44202-2002-2
0-4-4-402222220.02002022-4-4-22
00000-2-222-2-200.-20024-2200-2-2
2-2-2-2-204004022-2.-220-240-2-200
000042-2-22-22-400-2.-4-20-2200-22
0000-4-222-22-24002-4.202-2002-2
-2-2-2-2-20040002220-22.200-2-200
00000-2-222-2-2004-2002.-2200-2-2
2-2-2-2-204004022-24-220-2.0-2-200
2-2-2-22000400-22202-2020.-2-2-40
04440-2-2-2-2-2-20-40-200-20-2-2.42-2
04440-2-2-2-2-2-20-40-200-20-2-24.2-2
-2222-2000-4002-2-20-220-20-422.0
-2-2-2-22400004-22-202-20-200-2-20.(b)!
(c)!
(d)!!
(e)!
(f)!
(g)!
(h)!
(i)! (j)! (k)!!
(l)!! (m)!!Figure 42.3.Binary Hopeld
networkstoring fourmemories.
(a)Thefourmemories, andthe
weightmatrix. (b{h) Initial states
thatdier byone,two,three, four,
orevenvebitsfromadesired
memory arerestored tothat
memory inoneortwoiterations.
(i{m) Some initial conditions that
arefarfromthememories leadto
stable states other thanthefour
memories; in(i),thestable state
lookslikeamixture oftwo
memories, `D'and`J';stable state
(j)islikeamixture of`J'and`C';
in(k),wendacorrupted version
ofthe`M'memory (twobits
distan t);in(l)acorrupted version
of`J'(fourbitsdistan t)andin
(m),astatewhichlooksspurious
untilwerecognize thatitisthe
inverseofthestable state(l).

<<<PAGE 522>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
510 42|Hopeld Networks
42.5 Theassociativememory inaction
Figure 42.3showsthedynamics ofa25-unit binary Hopeld networkthat
haslearntfourpatterns byHebbian learning. Thefourpatterns aredispla yed
asvebyvebinary images ingure 42.3a. Fortwelveinitial conditions,
panels (b{m) showthestate ofthenetwork,iteration byiteration, all25
unitsbeingupdated asynchronously ineachiteration. Foraninitial condition
randomly perturb edfromamemory ,itoften onlytakesoneiteration forall
theerrors tobecorrected. Thenetworkhasmore stable states inaddition
tothefourdesired memories: theinverseofanystable state isalsoastable
state; andthere areseveralstable states thatcanbeinterpreted asmixtures
ofthememories.
Braindamage
Thenetworkcanbeseverely damaged andstillworkneasanassociative
memory .Ifwetakethe300weightsofthenetworkshowningure 42.3and
randomly set50or100ofthem tozero,westillndthatthedesired memories
areattracting stable states. Imagine adigital computer thatstillworksne
evenwhen 20%ofitscomponentsaredestro yed!
.Exercise 42.5.[2]Implemen taHopeld networkandconrm thisamazing ro-
busterror-correcting capabilit y.
Morememories
Wecansquash more memories intothenetworktoo.Figure 42.4a showsa
setofvememories. When wetrainthenetworkwithHebbian learning, all
vememories arestable states, evenwhen 26oftheweightsarerandomly
deleted (asshownbythe`x'sinthegure). However,thebasins ofattraction
aresmaller thanbefore: gure 42.4(b{f )showsthedynamics resulting from
randomly chosen starting states closetoeachofthememories (3bitsipped).
Onlythree ofthememories arerecoveredcorrectly .
Ifwetrytostore toomanypatterns, theassociativememory failscatas-
trophically .When weaddasixth pattern, asshowningure 42.5,onlyone
ofthepatterns isstable; theothers allowintooneoftwospurious stable
states.
42.6 Thecontinuous-time continuousHopeld network
ThefactthattheHopeld network'sproperties arenotrobust totheminor
change fromasynchronous tosynchronous updates mightbeacause forcon-
cern; canthismodelbeauseful modelofbiological networks? Itturns out
thatoncewemovetoacontinuous-time version oftheHopeld networks,this
issuemelts away.
Weassume thateachneuron's activit yxiisacontinuousfunction oftime
xi(t)andthattheactivationsai(t)arecomputed instan taneously inaccordance
with
ai(t)=X
jwijxj(t): (42.16)
Theneuron's responsetoitsactivation isassumed tobemediated bythe
dieren tialequation:
d
dtxi(t)= 1
(xi(t) f(ai)); (42.17)

<<<PAGE 523>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
42.6: Thecontinuous-time continuousHopeld network 511
(a)
.-11-11xx-33xx-11-1x-11-3x13-11x-1
-1.35-1-1-3-1-3-1-31x1-31-1-1-1-1-3533-3
13.31-3-1x-1-3-1-1x-1-1-11-31-3-1351-1
-153.-1-1-3-1-3-1-31-51-31-1-1-1-1-35x3-3
1-11-1.1-1-3xx3-51-1-13x-31-33-11-33
x-1-3-11.-11-113-11-1-13-31x1x-1-313
x-3-1-3-1-1.-113113-35-33-1-1x1-3-1-11
-3-1x-1-31-1.-11-131x-1-11511-1x-31-1
3-3-1-3x-11-1.-11-33111-1-13-15-3-1x1
x-1-3-1x131-1.-131-13-1x1-35-1-1-31-1
x-3-1-3331-11-1.-33-311-1-1-1-11-3-1-15
-11-11-5-113-33-3.-111-33x-13-31-13-3
1xx-51131313-1.-13-111113-5-3-33
-11-11-1-1-3x1-1-31-1.x1-133-111-1-1-3
x-3-1-3-1-15-113113x.x3-1-131-3-1-11
-11-1133-3-11-11-3-11x.-5-1-1-111-1-11
1-11-1x-331-1x-131-13-5.111-1-111-1
-3-1-3-1-31-15-11-1x13-1-11.11-1-1-31-1
x-11-11x-113-3-1-113-1-111.-33-11-3-1
1-1-3-1-31x1-15-131-13-111-3.x-1-31-1
3-3-1-33x1-15-11-33111-1-13x.-3-1-51
-1535-1-1-3x-3-1-31-51-31-1-1-1-1-3.3x-3
135x1-3-1-3-1-3-1-1-3-1-1-11-31-3-13.1-1
x313-31-11x1-13-3-1-1-111-31-5x1.-1
-1-3-1-3331-11-15-33-311-1-1-1-11-3-1-1.(b)!!
(c)!!
(d)!
(e)!
(f)!!Figure 42.4.Hopeld network
storing vememories, and
suering deletion of26ofits300
weights.(a)Thevememories,
andtheweightsofthenetwork,
withdeleted weightsshownby`x'.
(b{f)Initial states thatdier by
three random bitsfroma
memory: somearerestored, but
someconvergetoother states.
Desired memories:
!! ! !!
!! !! !Figure 42.5.Anoverloaded
Hopeld networktrained onsix
memories, mostofwhicharenot
stable.

<<<PAGE 524>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
512 42|Hopeld Networks
Figure 42.6.Failure modesofa
Hopeld network(highly
schematic). Alistofdesired
memories, andtheresulting listof
attracting stable states. Notice
(1)somememories thatare
retained withasmall numberof
errors; (2)desired memories that
arecompletely lost(there isno
attracting stable stateatthe
desired memory ornearit);(3)
spurious stable states unrelated to
theoriginal list;(4)spurious
stable states thatare
confabulations ofdesired
memories.Desired memories
moscow------rus sia
lima----------p eru
london-----engl and
tokyo--------ja pan
edinburgh-scotl and
ottawa------can ada
oslo--------nor way
stockholm---swe den
paris-------fra nce!W!Attracting stable states
moscow------russi a
lima----------per u
londog-----englar d(1)
tonco--------japa n(1)
edinburgh-scotlan d
(2)
oslo--------norwa y
stockholm---swede n
paris-------franc e
wzkmhewn--xqwqwpo q(3)
paris-------swede n(4)
ecnarf-------sira p(4)
wheref(a)istheactivation function, forexamplef(a)=tanh(a).Fora
steady activationai,theactivit yxi(t)relaxes exponentially tof(ai)with
time-constan t.
Now,hereistheniceresult: aslongastheweightmatrix issymmetric,
thissystem hasthevariational freeenergy (42.15) asitsLyapuno vfunction.
.Exercise 42.6.[1]Bycomputingd
dt~F,provethatthevariational freeenergy
~F(x)isaLyapuno vfunction forthecontinuous-time Hopeld network.
Itisparticularly easytoprovethatafunctionLisaLyapuno vfunctions if
thesystem's dynamics perform steepestdescen tonL,withd
dtxi(t)/@
@xiL.In
thecaseofthecontinuous-time continuousHopeld network,itisnotquite
sosimple, buteverycomponentofd
dtxi(t)doeshavethesame signas@
@xi~F,
whichmeans thatwithanappropriately dened metric, theHopeld network
dynamics doperform steepestdescen tson~F(x).
42.7 Thecapacit yoftheHopeld network
Onewayinwhichweviewedlearning inthesingle neuron wasascomm unica-
tion{comm unication ofthelabelsofthetraining datasetfromonepointin
timetoalaterpointintime. Wefound thatthecapacit yofalinear threshold
neuron was2bitsperweight.
Similarly ,wemightviewtheHopeld associativememory asacomm u-
nication channel (gure 42.6). Alistofdesired memories isencodedintoa
setofweightsWusing theHebb ruleofequation (42.5), orperhaps some
other learning rule. Thereceiver,receiving theweightsWonly,nds the
stable states oftheHopeld network,whichheinterprets astheoriginal mem-
ories. Thiscomm unication system canfailinvarious ways,asillustrated in
thegure.
1.Individual bitsinsome memories mightbecorrupted, thatis,asta-
blestate oftheHopeld networkisdisplaced alittlefromthedesired
memory .
2.Entirememories mightbeabsentfromthelistofattractors ofthenet-
work;orastable statemightbepresen tbuthavesuchasmall basin of
attraction thatitisofnouseforpattern completion anderrorcorrection.
3.Spurious additional memories unrelated tothedesired memories might
bepresen t.
4.Spurious additional memories derivedfromthedesired memories byop-
erations suchasmixing andinversion mayalsobepresen t.

<<<PAGE 525>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
42.7: Thecapacit yoftheHopeld network 513
Ofthese failure modes,modes1and2areclearly undesirable, mode2espe-
cially so.Mode3mightnotmatter somuchaslongaseachofthedesired
memories hasalarge basin ofattraction. Thefourth failure modemightin
some contexts actually beviewedasbenecial. Forexample, ifanetworkis
required tomemorize examples ofvalidsentences suchas`John lovesMary'
and`John getscake',wemightbehappytondthat`John lovescake'wasalso
astable stateofthenetwork.Wemightcallthisbehaviour `generalization'.
Thecapacit yofaHopeld networkwithIneurons mightbedened tobe
thenumberofrandom patternsNthatcanbestored without failure-mo de2
havingsubstan tialprobabilit y.Ifwealsorequire failure-mo de1tohavetiny
probabilit ythentheresulting capacit yismuchsmaller. Wenowstudy these
alternativ edenitions ofthecapacit y.
ThecapacityoftheHopeld network {stringent denition
Wewillrstexplore theinformation storage capabilities ofabinary Hopeld
networkthatlearns using theHebb rulebyconsidering thestabilit yofjust
onebitofoneofthedesired patterns, assuming thatthestateofthenetwork
issettothatdesired pattern x(n).Wewillassume thatthepatterns tobe
stored arerandomly selected binary patterns.
Theactivation ofaparticular neuroniis
ai=X
jwijx(n)
j; (42.18)
where theweightsare,fori6=j,
wij=x(n)
ix(n)
j+X
m6=nx(m)
ix(m)
j: (42.19)
HerewehavesplitWintotwoterms, therstofwhichwillcontribute `signal',
reinforcing thedesired memory ,andthesecond `noise'. Substituting forwij,
theactivation is
ai=X
j6=ix(n)
ix(n)
jx(n)
j+X
j6=iX
m6=nx(m)
ix(m)
jx(n)
j (42.20)
=(I 1)x(n)
i+X
j6=iX
m6=nx(m)
ix(m)
jx(n)
j: (42.21)
Thersttermis(I 1)times thedesired statex(n)
i.Ifthisweretheonly
term, itwouldkeeptheneuron rmly clamp edinthedesired state. The
second termisasumof(I 1)(N 1)random quantitiesx(m)
ix(m)
jx(n)
j.A
momen t'sreection conrms thatthese quantities areindependen trandom
binary variables withmean 0andvariance 1.
Thus,considering thestatistics ofaiunder theensem bleofrandom pat-
terns, weconclude thataihasmean (I 1)x(n)
iandvariance (I 1)(N 1).
Forbrevit y,wewillnowassumeIandNarelarge enough thatwecan
neglect thedistinction betweenIandI 1,andbetweenNandN 1.Then
wecanrestate ourconclusion: aiisGaussian-distributed withmeanIx(n)
iand
varianceIN.p
IN
Iai
Figure 42.7.Theprobabilit y
densit yoftheactivationaiinthe
casex(n)
i=1;theprobabilit ythat
bitibecomes ippedisthearea
ofthetail.
What thenistheprobabilit ythattheselected bitisstable, ifweputthe
networkintothestatex(n)?Theprobabilit ythatbitiwillipontherst
iteration oftheHopeld network'sdynamics is
P(iunstable) =
 Ip
IN
= 
 1p
N=I!
; (42.22)

<<<PAGE 526>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
514 42|Hopeld Networks
00.20.40.60.81
00.020.040.060.080.10.120.140.160.950.960.970.980.991
0.090.10.110.120.130.140.15Figure 42.8.Overlapbetweena
desired memory andthestable
statenearest toitasafunction of
theloading fractionN=I.The
overlapisdened tobethescaled
inner productP
ixix(n)
i=I,which
is1when recall isperfect andzero
when thestable statehas50%of
thebitsipped.There isan
abrupt transition atN=I=0:138,
where theoverlapdrops from0.97
tozero.
where
(z)Zz
 1dz1p
2e z2=2: (42.23)
TheimportantquantityN=Iistheratioofthenumberofpatterns stored to
thenumberofneurons. If,forexample, wetrytostoreN'0:18Ipatterns
intheHopeld networkthenthere isachance of1%thataspecied bitina
specied pattern willbeunstable ontherstiteration.
Wearenowinaposition toderiveourrstcapacit yresult, forthecase
where nocorruption ofthedesired memories ispermitted.
.Exercise 42.7.[2]Assume thatwewishallthedesired patterns tobecompletely
stable {wedon't wantanyofthebitstoipwhen thenetworkisput
intoanydesired pattern state{andthetotalprobabilit yofanyerrorat
allisrequired tobelessthanasmall number.Using theapproximation
totheerrorfunction forlargez,
( z)'1p
2e z2=2
z; (42.24)
showthatthemaxim umnumberofpatterns thatcanbestored,Nmax,
is
Nmax'I
4lnI+2ln(1=): (42.25)
If,however,weallowasmall amoun tofcorruption ofmemories tooccur,the
numberofpatterns thatcanbestored increases.
Thestatistic alphysicists' capacity
Theanalysis thatledtoequation (42.22) tellsusthatifwetrytostoreN'
0:18Ipatterns intheHopeld networkthen, starting fromadesired memory ,
about1%ofthebitswillbeunstable ontherstiteration. Ouranalysis does
notshedlightonwhat isexpected tohappenonsubsequen titerations. The
ipping ofthese bitsmightmakesome oftheother bitsunstable too,causing
anincreasing numberofbitstobeipped.Thisprocessmightleadtoan
avalancheinwhichthenetwork'sstate endsupalongwayfromthedesired
memory .
Infact,whenN=Iislarge, suchavalanchesdohappen.WhenN=Iissmall,
theytendnotto{there isastable stateneartoeachdesired memory .Forthe
limitoflargeI,Amitetal.(1985) haveusedmetho dsfromstatistical physics
tondnumerically thetransition betweenthese twobehaviours. There isa
sharp discon tinuityat
Ncrit=0:138I: (42.26)

<<<PAGE 527>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
42.8: Impro vingonthecapacit yoftheHebb rule 515
Belowthiscritical value,there islikelytobeastable stateneareverydesired
memory ,inwhichasmall fraction ofthebitsareipped.WhenN=Iexceeds
0.138, thesystem hasonlyspurious stable states, knownasspinglassstates ,
noneofwhichiscorrelated withanyofthedesired memories. Justbelowthe
critical value,thefraction ofbitsthatareippedwhen adesired memory has
evolvedtoitsassociated stable state is1.6%. Figure 42.8showstheoverlap
betweenthedesired memory andthenearest stable stateasafunction ofN=I.
Some other transitions inproperties ofthemodeloccuratsomeadditional
values ofN=I,assummarized below.
ForallN=I,stable spinglass states exist, uncorrelated withthedesired
memories.
ForN=I>0:138,these spinglassstates aretheonlystable states.
ForN=I2(0;0:138),there arestable states closetothedesired memories.
ForN=I2(0;0:05),thestable states associated withthedesired memories
havelowerenergy thanthespurious spinglassstates.
ForN=I2(0:05;0:138),thespinglassstates dominate {there arespinglass
states thathavelowerenergy thanthestable states associated withthe
desired memories.
ForN=I2(0;0:03),there areadditional mixture states, whicharecombina-
tionsofseveraldesired memories. These stable states donothaveaslow
energy asthestable states associated withthedesired memories.
Inconclusion, thecapacit yoftheHopeld networkwithIneurons, ifwe
dene thecapacit yinterms oftheabrupt discon tinuitydiscussed above,is
0:138Irandom binary patterns, eachoflengthI,eachofwhichisreceived
with1.6%ofitsbitsipped.Inbits,thiscapacit yis
0:138I2(1 H2(0:016)) =0:122I2bits: (42.27)
Since there areI2=2weightsinthenetwork,wecanalsoexpress thecapacit y
as0.24bitsperweight.
42.8 Impro vingonthecapacit yoftheHebb rule
Thecapacities discussed intheprevious section arethecapacities oftheHop-
eldnetworkwhose weightsaresetusing theHebbian learning rule.Wecan
dobetterthantheHebb rulebydening anobjectivefunction thatmeasures
howwellthenetworkstores allthememories, andminimizing it.
Foranassociativememory tobeuseful, itmustbeabletocorrect at
leastoneippedbit.Let'smakeanobjectivefunction thatmeasures whether
ippedbitstendtoberestored correctly .Ourintentionisthat, forevery
neuroniinthenetwork,theweightstothatneuron should satisfy thisrule:
foreverypattern x(n),iftheneurons other thaniaresetcorrectly
toxj=x(n)
j,thentheactivation ofneuronishould besuchthat
itspreferred output isxi=x(n)
i.
Isthisruleafamiliar idea? Yes,itisprecisely what wewantedthesingle
neuron ofChapter 39todo.Eachpattern x(n)denes aninput, target pair
forthesingle neuroni.Anditdenes aninput, target pairforalltheother
neurons too.

<<<PAGE 528>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
516 42|Hopeld Networks
Algorithm 42.9.Octave source
codeforoptimizing theweightsof
aHopeld network,sothatit
worksasanassociativememory .
c.f.algorithm 39.5.Thedata
matrixxhasIcolumns andN
rows.Thematrixtisidenticalto
xexcept that 1sarereplaced by
0s.w=x'*x; #initialize theweights usingHebbrule
forl=1:L #loopLtimes
fori=1:I #
w(i,i)=0; #ensuretheself-weights arezero.
end #
a=x*w;#compute allactivations
y=sigmoid(a) ;#compute alloutputs
e=t-y;#compute allerrors
gw=x'*e;#compute thegradients
gw=gw+gw';#symmetrize gradients
w=w+eta*(gw-alpha*w);#makestep
endfor
So,justaswedened anobjectivefunction (39.11) forthetraining ofa
single neuron asaclassier, wecandene
G(W)= X
iX
nt(n)
ilny(n)
i+(1 t(n)
i)ln(1 y(n)
i) (42.28)
where
t(n)
i=(
1x(n)
i=1
0x(n)
i= 1(42.29)
and
y(n)
i=1
1+exp( a(n)
i);wherea(n)
i=Pwijx(n)
j: (42.30)
Wecanthenstealthealgorithm (algorithm 39.5,p.478)whichwewrote for
thesingle neuron, towrite analgorithm foroptimizing aHopeld network,
algorithm 42.9. TheconvenientsyntaxofOctave requires veryfewchanges;
theextra linesenforce theconstrain tsthattheself-weightswiishould allbe
zeroandthattheweightmatrix should besymmetrical (wij=wji).
Asexpected, thislearning algorithm doesabetterjobthantheone-shot
Hebbian learning rule.When thesixpatterns ofgure 42.5,whichcannot be
memorized bytheHebb rule,arelearned using algorithm 42.9,allsixpatterns
become stable states.
Exercise 42.8.[4C]Implemen tthislearning ruleandinvestigate empirically its
capacit yformemorizing random patterns; alsocompare itsavalanche
properties withthose oftheHebb rule.
42.9 Hopeld networksforoptimization problems
Since aHopeld network'sdynamics minimize anenergy function, itisnatural
toaskwhether wecanmapinteresting optimization problems ontoHopeld
networks. Biological dataprocessing problems often involveanelemen tof
constrain tsatisfaction {inscene interpretation, forexample, onemightwish
toinferthespatial location, orientation, brightnessandtexture ofeachvisible
elemen t,andwhichvisible elemen tsareconnected together inobjects. These
inferences areconstrained bythegivendataandbyprior knowledge about
continuityofobjects.

<<<PAGE 529>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
42.9: Hopeld networksforoptimization problems 517
B
C
D234 1
APlace intour
City
D
CAB
(a1)B
C
D234 1
APlace intour
City
D
CAB
(a2)(b)C
DA
B234 1
(c)C
DA
B234 1
 dBDFigure 42.10.Hopeld networkfor
solving atravelling salesman
problem withK=4cities. (a1,2)
Twosolution states ofthe
16-neuron network,withactivites
represen tedbyblack=1,white =
0;andthetours corresp onding to
these networkstates. (b)The
negativ eweightsbetweennodeB2
andother nodes;these weights
enforce validityofatour. (c)The
negativ eweightsthatembodythe
distance objectivefunction.
Hopeld andTank(1985) suggested thatonemighttakeaninteresting
constrain tsatisfaction problem anddesign theweightsofabinary orcontin-
uousHopeld networksuchthatthesettling processofthenetworkwould
minimize theobjectivefunction oftheproblem.
Thetravellingsalesman problem
Aclassic constrain tsatisfaction problem towhichHopeld networkshavebeen
applied isthetravelling salesman problem.
AsetofKcitiesisgiven,andamatrix oftheK(K 1)=2distances between
those cities. Thetaskistondaclosed tourofthecities, visiting eachcity
once, thathasthesmallest totaldistance. Thetravelling salesman problem is
equivalentindicult ytoanNP-complete problem.
Themetho dsuggested byHopeld andTankistorepresen tatentativeso-
lution totheproblem bythestateofanetworkwithI=K2neurons arranged
inasquare, witheachneuron represen tingthehypothesis thataparticular
citycomes ataparticular pointinthetour. Itwillbeconvenienttoconsider
thestates oftheneurons asbeingbetween0and1rather than 1and1.
Twosolution states forafour-cit ytravelling salesman problem areshownin
gure 42.10a.
TheweightsintheHopeld networkplaytworoles. First, theymustdene
anenergy function whichisminimized onlywhen thestate ofthenetwork
represen tsavalid tour. Avalidstate isonethatlookslikeapermutation
matrix, havingexactly one`1'ineveryrowandone`1'ineverycolumn. This
rulecanbeenforced byputting large negativ eweightsbetweenanypairof
neurons thatareinthesame roworthesame column, andsetting apositive
biasforallneurons toensure thatKneurons doturnon.Figure 42.10b shows
thenegativ eweightsthatareconnected tooneneuron, `B2',whichrepresen ts
thestatemen t`cityBcomes second inthetour'.
Second, theweightsmustencodetheobjectivefunction thatwewant
tominimize {thetotal distance. Thiscanbedone byputting negativ e
weightsproportional totheappropriate distances betweenthenodesinadja-
centcolumns. Forexample, betweentheBandDnodesinadjacen tcolumns,
theweightwouldbe dBD.Thenegativ eweightsthatareconnected toneu-
ronB2areshowningure 42.10c. Theresult isthatwhen thenetworkisin
avalidstate, itstotalenergy willbethetotaldistance ofthecorresp onding

<<<PAGE 530>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
518 42|Hopeld Networks
(a) (b)Figure 42.11.(a)Evolution ofthe
stateofacontinous Hopeld
networksolving atravelling
salesman problem using Aiyer's
(1991) graduated non-con vexity
metho d;thestateofthenetwork
isprojected intothe
two-dimensional space inwhich
thecities arelocated bynding
thecentreofmassforeachpoint
inthetour,using theneuron
activities asthemassfunction.
(b)Thetravelling scholar
problem. Theshortest tour
linking the27Cambridge
Colleges, theEngineering
Departmen t,theUniversity
Library ,andSreeAiyer'shouse.
FromAiyer(1991).
tour,plusaconstan tgivenbytheenergy associated withthebiases.
Now,since aHopeld networkminimizes itsenergy ,itishopedthatthe
binary orcontinuousHopeld network's dynamics willtakethestate toa
minim umthatisavalidtourandwhichmightbeanoptimal tour. Thishope
isnotfullled forlargetravelling salesman problems, however,without some
careful modications. Wehavenotspecied thesizeoftheweightsthatenforce
thetour's validity,relativ etothesizeofthedistance weights,andsetting this
scale factor posesdiculties. If`large' validity-enforcing weightsareused,
thenetwork'sdynamics willrattle intoavalidstatewithlittleregard forthe
distances. If`small' validity-enforcing weightsareused, itispossible thatthe
distance weightswillcause thenetworktoadopt aninvalid statethathaslower
energy thananyvalidstate. Ouroriginal formulation oftheenergy function
putstheobjectivefunction andthesolution's validityinpotentialconict
witheachother. Thisdicult yhasbeenresolv edbytheworkofSreeAiyer
(1991), whoshowedhowtomodifythedistance weightssothattheywouldnot
interfere withthesolution's validity,andhowtodene acontinuousHopeld
networkwhose dynamics areatalltimes conned toa`validsubspace'. Aiyer
usedagraduated non-con vexityordeterministic annealing approac htond
goodsolutions using these Hopeld networks. Thedeterministic annealing
approac hinvolvesgradually increasing thegainoftheneurons inthenetwork
from0to1,atwhichpointthestate ofthenetworkcorresp ondstoavalid
tour. Asequence oftrajectories generated byapplying thismetho dtoathirty-
citytravelling salesman problem isshowningure 42.11a.
Asolution tothe`travelling scholar problem' found byAiyerusing acon-
tinuousHopeld networkisshowningure 42.11b.

<<<PAGE 531>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
42.10: Further exercises 519
42.10 Further exercises
.Exercise 42.9.[3]Storingtwomemo ries.
Twobinary memories mandn(mi;ni2f 1;+1g)arestored byHeb-
bianlearning inaHopeld networkusing
wij=(
mimj+ninjfori6=j
0 fori=j.(42.31)
Thebiasesbiaresettozero.
Thenetworkisputinthestatex=m.Evaluate theactivationaiof
neuroniandshowthatincanbewritten intheform
ai=mi+ni: (42.32)
Bycomparing thesignal strength,,withthemagnitude ofthenoise
strength,jj,showthatx=misastable stateofthedynamics ofthe
network.
Thenetworkisputinastatexdiering inDplaces fromm,
x=m+2d; (42.33)
where theperturbation dsatisesdi2f 1;0;+1g.Disthenumber
ofcomponentsofdthatarenon-zero, andforeachdithatisnon-zero,
di= mi.Dening theoverlap betweenmandntobe
omn=IX
i=1mini; (42.34)
evaluate theactivationaiofneuroniagain andshowthatthedynamics
ofthenetworkwillrestore xtomifthenumberofippedbitssatises
D<1
4(I jomnj 2): (42.35)
Howdoesthisnumbercompare withthemaxim umnumberofipped
bitsthatcanbecorrected bytheoptimal decoder,assuming thevector
xiseither anoisy version ofmorofn?
Exercise 42.10.[3]Hopeld networkasacollection ofbinaryclassiers. Thisex-
ercise explores thelinkbetweenunsup ervised networksandsupervised
networks. IfaHopeld network's desired memories areallattracting
stable states, theneveryneuron inthenetworkhasweightsgoing toit
thatsolveaclassication problem personal tothatneuron. Taketheset
ofmemories andwrite them intheformx0(n);x(n)
i,where x0denotes all
thecomponentsxi0foralli06=i,andletw0denote thevectorofweights
wii0,fori06=i.
Using whatweknowaboutthecapacit yofthesingle neuron, showthat
itisalmost certainly impossible tostoremorethan2Irandom memories
inaHopeld networkofIneurons.

<<<PAGE 532>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
520 42|Hopeld Networks
Lyapunov functions
Exercise 42.11.[3]Erik's puzzle .Inastripp ed-downversion ofConway'sgame
oflife,cellsarearranged onasquare grid. Eachcelliseither aliveor
dead. Livecellsdonotdie.Dead cellsbecome aliveiftwoormore of
theirimmediate neighboursarealive.(Neigh bourstonorth, south, east
andwest.) What isthesmallest numberoflivecellsneeded inorder
thatthese rulesleadtoanentireNNsquare beingalive?
!!
Figure 42.12.Erik's dynamics.Inad-dimensional version ofthesame game, theruleisthatifdneigh-
boursarealivethenyoucome tolife.What isthesmallest numberof
livecellsneeded inorder thatanentireNNNhypercube
becomes alive?(And howshould those livecellsbearranged?)
Thesouthe astpuzzle
(a)u u
?-
-(b)u
?-u
-(c)u
?-u
u-(d)
uuu
u-:::-(z)
eeee
eee
eee
Figure 42.13.Thesoutheast
puzzle. Thesoutheast puzzle isplayedonasemi-innite chessboard, starting at
itsnorthwest(topleft)corner. There arethree rules:
1.Inthestarting position, onepieceisplaced inthenorthwest-most square
(gure 42.13a).
2.Itisnotpermitted formore thanonepiece tobeonanygivensquare.
3.Ateachstep,youremoveonepiece fromtheboard, andreplace itwith
twopieces, oneinthesquare immediately totheeast,andoneinthethe
square immediately tothesouth, asillustrated ingure 42.13b. Every
suchstepincreases thenumberofpieces ontheboardbyone.
After move(b)hasbeenmade, either piecemaybeselected forthenextmove.
Figure 42.13c showstheoutcome ofmovingthelowerpiece. Atthenextmove,
either thelowestpiece orthemiddle piece ofthethree maybeselected; the
uppermost piecemaynotbeselected, sincethatwouldviolate rule2.Atmove
(d)wehaveselected themiddle piece. Nowanyofthepieces maybemoved,
except fortheleftmost piece.
Now,hereisthepuzzle:
.Exercise 42.12.[4,p.521]Isitpossible toobtain aposition inwhichalltheten
squares closest tothenorthwestcorner, markedingure 42.13z, are
empty?
[Hint: thispuzzle hasaconnection todatacompression.]
42.11 Solutions
Solution toexercise 42.3(p.508).Takeabinary feedbac knetworkwith2neu-
ronsandletw12=1andw21= 1.Then whenev erneuron 1isupdated,
itwillmatchneuron 2,andwhenev erneuron 2isupdated, itwilliptothe
oppositestatefromneuron 1.There isnostable state.

<<<PAGE 533>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
42.11: Solutions 521
Solution toexercise 42.4(p.508).Takeabinary Hopeld networkwith2neu-
ronsandletw12=w21=1,andlettheinitial condition bex1=1,x2= 1.
Then ifthedynamics aresynchronous, oneveryiteration bothneurons will
iptheirstate. Thedynamics donotconvergetoaxedpoint.
Solution toexercise 42.12 (p.520).Thekeytothisproblem istonotice its
similarit ytotheconstruction ofabinary symbolcode.Starting fromthe
emptystring, wecanbuild abinary treebyrepeatedly splitting acodeword
intotwo.Everycodewordhasanimplicit probabilit y2 l,wherelisthe
depth ofthecodewordinthebinary tree. Whenev erwesplitacodewordin
twoandcreate twonewcodewordswhose length isincreased byone,thetwo
newcodewordseachhaveimplicit probabilit yequal tohalfthatoftheold
codeword.Foracomplete binary code,theKraft equalit yarms thatthe
sumofthese implicit probabilities is1.
Similarly ,insoutheast ,wecanassociatea`weight'witheachpieceonthe
board. Ifweassign aweightof1toanypiece sitting onthetopleftsquare;
aweightof1/2toanypiece onasquare whose distance fromthetopleftis
one;aweightof1/4toanypiecewhose distance fromthetopleftistwo;and
soforth, with`distance' beingthecity-blockdistance; theneverylegalmove
insoutheast leavesunchanged thetotal weightofallpieces ontheboard.
Lyapuno vfunctions come intwoavours: thefunction maybeafunction of
statewhose valueisknowntostayconstan t;oritmaybeafunction ofstate
thatisbounded below,andwhose valuealwaysdecreases orstaysconstan t.
ThetotalweightisaLyapuno vfunction ofthesecond type.
Thestarting weightis1,sonowwehaveapowerfultool:aconserv ed
function ofthestate. Isitpossible tondaposition inwhichthetenhighest-
weightsquares arevacant,andthetotalweightis1?What isthetotalweight
ifalltheother squares ontheboardareoccupied (gure 42.14)? Thetotaluuuuu
uu
u
uu
u
uu
uu
...::::::
......
Figure 42.14.Apossible position
forthesoutheast puzzle?
weightwouldbeP1
l=4(l+1)2 l.Whichisequal to3=4.Soitisimpossible to
emptyalltenofthose squares.

<<<PAGE 534>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
43
Boltzmann Machines
43.1 FromHopeld networkstoBoltzmann machines
Wehavenoticed thatthebinary Hopeld networkminimizes anenergy func-
tion
E(x)= 1
2xTWx (43.1)
andthatthecontinuous Hopeld networkwithactivation functionxn=
tanh(an)canbeviewedasapproximating theprobabilit ydistribution asso-
ciated withthatenergy function,
P(xjW)=1
Z(W)exp[ E(x)]=1
Z(W)exp1
2xTWx
: (43.2)
These observ ations motivatetheideaofworking withaneural networkmodel
thatactually implements theaboveprobabilit ydistribution.
Thestochastic Hopeld networkorBoltzmann machine(HintonandSe-
jnowski, 1986) hasthefollowingactivit yrule:
Activit yruleofBoltzmann machine: after computing theactiva-
tionan,
setxn=+1withprobabilit y1
1+e 2a
elsesetxn= 1:(43.3)
Thisruleimplemen tsGibbs sampling fortheprobabilit ydistribution (43.2).
Boltzmann machine learning
Givenasetofexamplesfx(n)gN
1fromtherealworld,wemightbeinterested
inadjusting theweightsWsuchthatthegenerativ emodel
P(xjW)=1
Z(W)exp1
2xTWx
(43.4)
iswellmatchedtothose examples. Wecanderivealearning algorithm by
writing downBayes'theorem toobtain theposterior probabilit yoftheweights
giventhedata:
P(Wjfx(n)gN
1g)="NY
n=1P(x(n)jW)#
P(W)
P(fx(n)gN
1g): (43.5)
Weconcen trateontherstterminthenumerator, thelikelihood,andderivea
maxim umlikelihoodalgorithm (though theremightbeadvantages inpursuing
522

<<<PAGE 535>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
43.1: FromHopeld networkstoBoltzmann machines 523
afullBayesian approac haswedidinthecaseofthesingle neuron). We
dieren tiatethelogarithm ofthelikelihood,
ln"NY
n=1P(x(n)jW)#
=NX
n=11
2x(n)TWx(n) lnZ(W)
; (43.6)
withrespecttowij,bearing inmind thatWisdened tobesymmetric with
wji=wij.
Exercise 43.1.[2]ShowthatthederivativeoflnZ(W)withrespecttowijis
@
@wijlnZ(W)=X
xxixjP(xjW)=hxixjiP(xjW): (43.7)
[This exercise issimilar toexercise 22.12 (p.307).]
Thederivativeoftheloglikelihoodistherefore:
@
@wijlnP(fx(n)gN
1gjW)=NX
n=1h
x(n)
ix(n)
j hxixjiP(xjW)i
(43.8)
=Nh
hxixjiData hxixjiP(xjW)i
:(43.9)
Thisgradien tisproportional tothedierence oftwoterms. Thersttermis
theempiric alcorrelation betweenxiandxj,
hxixjiData1
NNX
n=1h
x(n)
ix(n)
ji
; (43.10)
andthesecond termisthecorrelation betweenxiandxjunder thecurren t
model,
hxixjiP(xjW)X
xxixjP(xjW): (43.11)
TherstcorrelationhxixjiDataisreadily evaluated {itisjusttheempirical
correlation betweentheactivities intherealworld. Thesecond correlation,
hxixjiP(xjW),isnotsoeasytoevaluate, butitcanbeestimated byMonteCarlo
metho ds,thatis,byobserving theaverage valueofxixjwhile theactivit yrule
oftheBoltzmann machine,equation (43.3), isiterated.
InthespecialcaseW=0,wecanevaluate thegradien texactly because,
bysymmetry ,thecorrelationhxixjiP(xjW)mustbezero. Iftheweightsare
adjusted bygradien tdescen twithlearning rate,then, afteroneiteration,
theweightswillbe
wij=NX
n=1h
x(n)
ix(n)
ji
; (43.12)
precisely thevalueoftheweightsgivenbytheHebb rule,equation (16.5), with
whichwetrained theHopeld network.
Interpr etation ofBoltzmann machine learning
Onewayofviewing thetwoterms inthegradien t(43.9) isas`waking' and
`sleeping' rules. While thenetworkis`awake',itmeasures thecorrelation
betweenxiandxjintherealworld,andweightsareincreasedinproportion.
While thenetworkis`asleep', it`dreams' abouttheworldusing thegenerativ e
model(43.4), andmeasures thecorrelations betweenxiandxjinthemodel
world;these correlations determine aproportional decreaseintheweights.If
thesecond-order correlations inthedream worldmatchthecorrelations inthe
realworld,thenthetwoterms balance andtheweightsdonotchange.(a) (b)
Figure 43.1.The`shifter'
ensem bles.(a)Foursamples from
theplain shifter ensem ble.(b)
Fourcorresp onding samples from
thelabelledshifter ensem ble.

<<<PAGE 536>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
524 43|Boltzmann Machines
Criticism ofHopeld networks andsimple Boltzmann machines
Uptothispointwehavediscussed Hopeld networksandBoltzmann machines
inwhichalloftheneurons corresp ondtovisible variablesxi.Theresult
isaprobabilistic modelthat, when optimized, cancapture thesecond-order
statistics oftheenvironmen t.[Thesecond-order statistics ofanensem ble
P(x)aretheexpected valueshxixjiofallthepairwise productsxixj.]The
realworld,however,oftenhashigher-order correlations thatmustbeincluded
ifourdescription ofitistobeeectiv e.Often thesecond-order correlations
inthemselv esmaycarry littleornouseful information.
Consider, forexample, theensem bleofbinary images ofchairs. Wecan
imagine images ofchairs withvarious designs {four-legged chairs, comfy
chairs, chairswithvelegsandwheels, woodenchairs, cushioned chairs, chairs
withrockersinstead oflegs.Achildcaneasily learntodistinguish theseimages
fromimages ofcarrots andparrots. ButIexpectthesecond-order statistics of
therawdataareuseless fordescribing theensem ble.Second-order statistics
onlycapture whether twopixels arelikelytobeinthesame state aseach
other. Higher-order concepts areneeded tomakeagoodgenerativ emodelof
images ofchairs.
Asimpler ensem bleofimages inwhichhigh-order statistics areimportant
isthe`shifter ensem ble',whichcomes intwoavours. Figure 43.1a showsa
fewsamples fromthe`plain shifter ensem ble'.Ineachimage, thebottom eight
pixels areacopyofthetopeightpixels, either shifted onepixeltotheleft,
orunshifted, orshifted onepixeltotheright.(The topeightpixels areset
atrandom.) Thisensem bleisasimple modelofthevisual signals fromthe
twoeyesarriving atearly levelsofthebrain. Thesignals fromthetwoeyes
aresimilar toeachother butmaydier bysmall translations because ofthe
varying depth ofthevisual world.Thisensem bleissimple todescrib e,butits
second-order statistics conveynouseful information. Thecorrelation between
onepixelandanyofthethree pixels aboveitis1=3.Thecorrelation between
anyother twopixels iszero.
Figure 43.1b showsafewsamples from the`labelledshifter ensem ble'.
Here, theproblem hasbeenmade easier byincluding anextra three neu-
ronsthatlabelthevisual image asbeinganinstance ofeither the`shift left',
`noshift', or`shift right'sub-ensem ble.Butwiththisextra information, the
ensem bleisstillnotlearnable using second-order statistics alone. Thesecond-
order correlation betweenanylabelneuron andanyimage neuron iszero.We
needmodelsthatcancapture higher-order statistics ofanenvironmen t.
So,howcanwedevelopsuchmodels? Oneideamightbetocreate models
thatdirectly capture higher-order correlations, suchas:
P0(xjW;V;:::)=1
Z0exp0
@1
2X
ijwijxixj+1
6X
ijvijkxixjxk+1
A:(43.13)
Suchhigher-order Boltzmann machines areequally easytosimulate using
stochastic updates, andthelearning ruleforthehigher-order parameters vijk
isequivalenttothelearning ruleforwij.
.Exercise 43.2.[2]Derivethegradien toftheloglikelihoodwithrespecttovijk.
Itispossible thatthespines found onbiological neurons areresponsible for
detecting correlations betweensmall numbersofincoming signals. However,
tocapture statistics ofhighenough order todescrib etheensem bleofimages
ofchairs wellwouldrequire anunimaginable numberofterms. Tocapture

<<<PAGE 537>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
43.2: Boltzmann machinewithhidden units 525
merely thefourth-order statistics ina128128pixelimage, weneedmore
than107parameters.
Someasuring momen tsofimages isnotagoodwaytodescrib etheirun-
derlying structure. Perhaps what weneedinstead orinaddition arehidden
variables ,alsoknowntostatisticians aslatentvariables .Thisistheimportant
innovation introduced byHintonandSejnowski(1986). Theideaisthatthe
high-order correlations among thevisible variables aredescrib edbyinclud-
ingextra hidden variables andstickingtoamodelthathasonlysecond-order
interactions betweenitsvariables; thehidden variables induce higher-order
correlations betweenthevisible variables.
43.2 Boltzmann machinewithhidden units
Wenowaddhidden neurons toourstochastic model.These areneurons that
donotcorresp ondtoobserv edvariables; theyarefreetoplayanyroleinthe
probabilistic modeldened byequation (43.4). They mightactually takeon
interpretable roles, eectiv elyperforming `feature extraction'.
Learning inBoltzmann machines withhidden units
Theactivit yruleofaBoltzmann machinewithhidden unitsisidenticaltothat
oftheoriginal Boltzmann machine. Thelearning rulecanagain bederived
bymaxim umlikelihood,butnowweneedtotakeintoaccoun tthefactthat
thestates ofthehidden units areunkno wn.Wewilldenote thestates ofthe
visible units byx,thestates ofthehidden units byh,andthegeneric state
ofaneuron (either visible orhidden) byyi,withy(x;h).Thestateofthe
networkwhen thevisible neurons areclamp edinstatex(n)isy(n)(x(n);h).
ThelikelihoodofWgivenasingle dataexample x(n)is
P(x(n)jW)=X
hP(x(n);hjW)=X
h1
Z(W)exp1
2[y(n)]TWy(n)
;(43.14)
where
Z(W)=X
x;hexp1
2yTWy
: (43.15)
Equation (43.14) mayalsobewritten
P(x(n)jW)=Zx(n)(W)
Z(W)(43.16)
where
Zx(n)(W)=X
hexp1
2[y(n)]TWy(n)
: (43.17)
Dieren tiating thelikelihoodasbefore, wendthatthederivativewithre-
specttoanyweightwijisagain thedierence betweena`waking' termanda
`sleeping' term,
@
@wijlnP(fx(n)gN
1jW)=X
nn
hyiyjiP(hjx(n);W) hyiyjiP(x;hjW)o
:(43.18)
ThersttermhyiyjiP(hjx(n);W)isthecorrelation betweenyiandyjifthe
Boltzmann machineissimulated withthevisible variables clamp edtox(n)
andthehidden variables freely sampling fromtheirconditional distribution.
Thesecond termhyiyjiP(x;hjW)isthecorrelation betweenyiandyjwhen
theBoltzmann machinegenerates samples fromitsmodeldistribution.

<<<PAGE 538>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
526 43|Boltzmann Machines
HintonandSejnowskidemonstrated thatnon-trivial ensem blessuchas
thelabelledshifter ensem blecanbelearned using aBoltzmann machinewith
hidden units. Thehidden unitstakeontheroleoffeature detectors thatspot
patterns likelytobeassociated withoneofthethree shifts.
TheBoltzmann machineistime-consuming tosimulatebecause thecompu-
tation ofthegradien toftheloglikelihooddependsontaking thedierence of
twogradien ts,bothfound byMonteCarlo metho ds.SoBoltzmann machines
arenotinwidespread use.Itisanareaofactiveresearc htocreate models
thatembodythesame capabilities using more ecien tcomputations (Hinton
etal.,1995; Dayanetal.,1995; HintonandGhahramani, 1997; Hinton,2000;
HintonandTeh,2001).
43.3 Exercise
.Exercise 43.3.[3]Canthe`barsandstripes'ensem ble(gure 43.2) belearned
Figure 43.2.Foursamples from
the`barsandstripes'ensem ble.
Eachsample isgenerated byrst
pickinganorientation, horizon tal
orvertical; then, foreachrowof
spins inthatorientation (eachbar
orstriperespectively),switching
allspins onwithprobabilit y1/2.byaBoltzmann machinewithnohidden units? [Youmaybesurprised!]

<<<PAGE 539>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
44
Supervised Learning inMultila yer
Networks
44.1 Multila yerperceptrons
Nocourse onneural networkscould becomplete without adiscussion ofsu-
pervised multilayernetworks,alsoknownasbackpropagation networks.
Themultilayerperceptron isafeedforw ardnetwork.Ithasinput neurons,
hidden neurons andoutput neurons. Thehidden neurons maybearranged
inasequence oflayers.Themost common multilayerperceptrons havea
single hidden layer,andareknownas`two-layer'networks,thenumber`two'
countingthenumberoflayersofneurons notincluding theinputs.
Suchafeedforw ardnetworkdenes anonlinear parameterized mapping
fromaninputxtoanoutput y=y(x;w;A).Theoutput isacontinuous
function oftheinput andoftheparameters w;thearchitecture ofthenet,i.e.,
thefunctional formofthemapping, isdenoted byA.Feedforw ardnetworks
canbe`trained' toperform regression andclassication tasks.
Regression networksHiddens
InputsOutputs
Figure 44.1.Atypical two-layer
network,withsixinputs, seven
hidden units, andthree outputs.
Eachlinerepresen tsoneweight.
Inthecaseofaregression problem, themapping foranetworkwithonehidden
layermayhavetheform:
Hidden layer:a(1)
j=X
lw(1)
jlxl+(1)
j;hj=f(1)(a(1)
j) (44.1)
Output layer:a(2)
i=X
jw(2)
ijhj+(2)
i;yi=f(2)(a(2)
i) (44.2)
where, forexample,f(1)(a)=tanh(a),andf(2)(a)=a.Herelrunsover
theinputsx1;:::;xL,jrunsoverthehidden units, andirunsovertheout-
puts. The`weights'wand`biases'together makeuptheparameter vector
w.Thenonlinear sigmoid functionf(1)atthehidden layergivestheneu-
ralnetworkgreater computational exibilit ythanastandard linear regression
model.Graphically ,wecanrepresen ttheneural networkasasetoflayersof
connected neurons (gure 44.1).
What sortsoffunctions canthesenetworks implement?
Justasweexplored theweightspace ofthesingle neuron inChapter 39,
examining thefunctions itcould produce, letusexplore theweightspace of
amultilayernetwork.Ingure 44.2Itakeanetworkwithoneinput andone-1.4-1.2-1-0.8-0.6-0.4-0.200.20.4
-2 -1 0 1 2 3 4 5
Figure 44.3.Samples fromthe
prioroverfunctions ofaone-input
network.Foreachofasequence of
valuesofbias=8,6,4,3,2,1.6,
1.2,0.8,0.4,0.3,0.2,and
in=5w
bias,onerandom function
isshown.Theother
hyperparameters ofthenetwork
wereH=400,w
out=0:05.
output andalargenumberHofhidden units, setthebiases andweights(1)
j,
527

<<<PAGE 540>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
528 44|Supervised Learning inMultila yerNetworks
Hidden layer
InputOutput
ttttt t    @
@
@@ty
@
@
@@    
t
x
t
1-biasinout
-10-50510
-2-101234Output
Inputbias=inp
Hout 1=inFigure 44.2.Properties ofa
function produced byarandom
network.Thevertical scaleofa
typical function produced bythe
networkwithrandom weightsisof
orderp
Hout;thehorizon tal
range inwhichthefunction varies
signican tlyisoforderbias=in;
andtheshortest horizon tallength
scaleisoforder 1=in.The
function shownwasproduced by
making arandom networkwith
H=400hidden units, and
Gaussian weightswithbias=4,
in=8,andout=0:5.
w(1)
jl,(2)
iandw(2)
ijtorandom values, andplottheresulting functiony(x).I
setthehidden unitbiases(1)
jtorandom values fromaGaussian withzero
mean andstandard deviationbias;theinput tohidden weightsw(1)
jltorandom
values withstandard deviationin;andthebiasandoutput weights(2)
iand
w(2)
ijtorandom values withstandard deviationout.
Thesortoffunctions thatweobtain dependonthevalues ofbias,in
andout.Astheweightsandbiases aremade bigger weobtain morecomplex
functions withmore features andagreater sensitivit ytotheinput variable.
Thevertical scaleofatypical function produced bythenetworkwithrandom
weightsisoforderp
Hout;thehorizon talrange inwhichthefunction varies
signican tlyisoforderbias=in;andtheshortest horizon tallength scaleisof
order 1=in.
Radford Neal(1996) hasalsoshownthatinthelimit asH!1the
statistical properties ofthefunctions generated byrandomizing theweightsare
independen tofthenumberofhidden units; so,interestingly ,thecomplexit yof
thefunctions becomes independen tofthenumberofparameters inthemodel.
What determines thecomplexit yofthetypical functions isthecharacteristic
magnitude oftheweights.Thusweanticipate thatwhen wetthesemodelsto
realdata, animportantwayofcontrolling thecomplexit yofthetted function
willbetocontrolthecharacteristic magnitude oftheweights.
-1-0.500.51-1-0.500.51-2-101
Figure 44.4.Samples fromthe
priorofatwo-input network.A
typical function produced bya
two-input networkwith
fH;w
in;w
bias;w
outg=
f400;8:0;8:0;0:05g.
Figure 44.3showsonetypical function produced byanetworkwithtwo
inputs andoneoutput. Thisshould becontrasted withthefunction produced
byatraditional linear regression model,whichisaatplane. Neural networks
cancreate functions withmore complexit ythanalinear regression.
44.2 Howaregression networkistraditionally trained
Thisnetworkistrained using adatasetD=fx(n);t(n)gbyadjusting wsoas
tominimize anerrorfunction, e.g.,
ED(w)=1
2X
nX
i
t(n)
i yi(x(n);w)2: (44.3)
Thisobjectivefunction isasumofterms, oneforeachinput/target pairfx;tg,
measuring howclosetheoutput y(x;w)istothetarget t.
Thisminimization isbased onrepeated evaluation ofthegradien tofED.
Thisgradien tcanbeecien tlycomputed using thebackpropagation algorithm
(Rumelhart etal.,1986), whichusesthechainruletondthederivatives.

<<<PAGE 541>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
44.3: Neural networklearning asinference 529
Often, regularization (alsoknownasweightdecay)isincluded, modifying
theobjectivefunction to:
M(w)=ED+EW (44.4)
where, forexample,EW=1
2P
iw2
i.Thisadditional termfavourssmall values
ofwanddecreases thetendency ofamodeltoovertnoise inthetraining
data.
Rumelhart etal.(1986) showedthatmultilayerperceptrons canbetrained,
bygradien tdescen tonM(w),todiscoversolutions tonon-trivial problems
suchasdeciding whether animage issymmetric ornot.These networkshave
beensuccessfully applied toreal-w orldtasks asvariedaspronouncing English
textreading aloud (Sejno wskiandRosen berg,1987) andfocussing multiple-
mirror telescop es(Angel etal.,1990).
44.3 Neural networklearning asinference
Theneural networklearning processabovecanbegiventhefollowingproba-
bilistic interpretation. [Here werepeatandgeneralize thediscussion ofChap-
ter41.]
Theerror function isinterpreted asdening anoise model.EDisthe
negativ eloglikelihood:
P(Djw;;H)=1
ZD()exp( ED): (44.5)
Thus,theuseofthesum-squared errorED(44.3) corresp ondstoanassump-
tionofGaussian noise onthetarget variables, andtheparameterdenes a
noise level2
=1=.
Similarly theregularizer isinterpreted interms ofalogpriorprobabilit y
distribution overtheparameters:
P(wj;H)=1
ZW()exp( EW): (44.6)
IfEWisquadratic asdened above,thenthecorresp onding priordistribution
isaGaussian withvariance2
W=1=.Theprobabilistic modelHspecies
thearchitectureAofthenetwork,thelikelihood(44.5), andtheprior(44.6).
TheobjectivefunctionM(w)thencorresp ondstotheinference ofthe
parameters w,giventhedata:
P(wjD;;;H)=P(Djw;;H)P(wj;H)
P(Dj;;H)(44.7)
=1
ZMexp( M(w)): (44.8)
Thewfound by(locally) minimizing M(w)istheninterpreted asthe(locally)
mostprobable parameter vector,wMP.
Theinterpretation ofM(w)asalogprobabilit yaddslittle newatthis
stage. Butnewtoolswillemerge when weproceedtoother inferences. First,
though, letusestablish theprobabilistic interpretation ofclassication net-
works,towhichthesame toolsapply.

<<<PAGE 542>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
530 44|Supervised Learning inMultila yerNetworks
Binary classic ationnetworks
Ifthetargetstinadatasetarebinary classication labels(0;1),itisnatural
touseaneural networkwhose outputy(x;w;A)isbounded between0and1,
andisinterpreted asaprobabilit yP(t=1jx;w;A).Forexample, anetwork
withonehidden layercould bedescrib edbythefeedforw ardequations (44.1)
and(44.2), withf(2)(a)=1=(1+e a).TheerrorfunctionEDisreplaced by
thenegativ eloglikelihood:
G(w)= "X
nt(n)lny(x(n);w)+(1 t(n))ln(1 y(x(n);w))#
:(44.9)
Thetotalobjectivefunction isthenM=G+EW.Notethatthisincludes
noparameter(because there isnoGaussian noise).
Multi-class classic ationnetworks
Foramulti-class classication problem, wecanrepresen tthetargets bya
vector,t,inwhichasingle elemen tissetto1,indicating thecorrect class, and
allother elemen tsaresetto0.Inthiscaseitisappropriate tousea`softmax'
networkhavingcoupled outputs whichsumtooneandareinterpreted as
classprobabilities yi=P(ti=1jx;w;A).Thelastpartofequation (44.2) is
replaced by:
yi=eai
X
i0eai0: (44.10)
Thenegativ eloglikelihoodinthiscaseis
G(w)= X
nX
it(n)
ilnyi(x(n);w): (44.11)
Asinthecaseoftheregression network,theminimization oftheobjective
functionM(w)=G+EWcorresp ondstoaninference oftheform(44.8). A
varietyofuseful results canbebuiltonthisinterpretation.
44.4 Benets oftheBayesian approac htosupervised feedforw ard
neural networks
Fromthestatistical perspective,supervised neural networksarenothing more
thannonlinear curve-tting devices. Curvetting isnotatrivial taskhowever.
Theeectiv ecomplexit yofaninterpolating modelisofcrucial importance,
asillustrated ingure 44.5. Consider acontrolparameter thatinuences the
complexit yofamodel,forexample aregularization constan t(weightdecay
parameter). Asthecontrolparameter isvaried toincrease thecomplexit yof
themodel(descending fromgure 44.5a{c andgoing fromlefttorightacross
gure 44.5d), thebestttothetraining datathatthemodelcanachieve
becomes increasingly good.However,theempirical performance ofthemodel,
thetesterror,rstdecreases thenincreasesagain.Anover-complex model
overts thedataandgeneralizes poorly. Thisproblem mayalsocomplicate
thechoice ofarchitecture inamultilayerperceptron, theradius ofthebasis
functions inaradial basisfunction network,andthechoice oftheinput vari-
ables themselv esinanymultidimensional regression problem. Finding values
formodelcontrolparameters thatareappropriate forthedataistherefore an
importantandnon-trivial problem.
Theovertting problem canbesolvedbyusing aBayesian approac hto
controlmodelcomplexit y.

<<<PAGE 543>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
44.4: Benets oftheBayesian approac htosupervised feedforw ardneural networks 531
(a)
(b)
(c)(d)
Model Control ParametersTraining ErrorTest Error
(e)
Model Control ParametersLog Probability(Training Data | Control Parameters)Figure 44.5.Optimization of
modelcomplexit y.Panels(a{c)
showaradial basisfunction model
interpolating asimple dataset
withoneinput variable andone
output variable. Asthe
regularization constan tisvaried
toincrease thecomplexit yofthe
model(from (a)to(c)),the
interpolantisabletotthe
training dataincreasingly well,
butbeyondacertain pointthe
generalization ability(testerror)
ofthemodeldeteriorates.
Probabilit ytheory allowsusto
optimize thecontrolparameters
without needing atestset.
Ifwegiveaprobabilistic interpretation tothemodel,thenwecanevaluate
theevidence foralternativ evaluesofthecontrolparameters. Aswasexplained
inChapter 28,over-complex modelsturnouttobelessprobable, andthe
evidenceP(DatajControlParameters )canbeusedasanobjectivefunction
foroptimization ofmodelcontrolparameters (gure 44.5e). Thesetting of
thatmaximizes theevidence isdispla yedingure 44.5b.
Bayesianoptimization ofmodelcontrolparameters hasfourimportantad-
vantages. (1)No`testset'or`validation set'isinvolved,soallavailable training
datacanbedevotedtobothmodeltting andmodelcomparison. (2)Reg-
ularization constan tscanbeoptimized on-line, i.e.,simultaneously withthe
optimization ofordinary modelparameters. (3)TheBayesian objectivefunc-
tionisnotnoisy,incontrasttoacross-v alidation measure. (4)Thegradien tof
theevidence withrespecttothecontrolparameters canbeevaluated, making
itpossible tosimultaneously optimize alargenumberofcontrolparameters.
Probabilistic modelling alsohandles uncertain tyinanatural manner. It
oers aunique prescription, marginalization ,forincorp orating uncertain ty
aboutparameters intopredictions; thisprocedure yields betterpredictions, as
wesawinChapter 41.Figure 44.6showserror barsonthepredictions ofa
trained neural network.Figure 44.6.Error barsonthe
predictions ofatrained regression
network.Thesolidlinegivesthe
predictions ofthebestt
parameters ofamultilayer
perceptron trained onthedata
points.Theerrorbars(dotted
lines) arethose produced bythe
uncertain tyoftheparameters w.
Notice thattheerrorbarsbecome
larger where thedataaresparse.
Implementation ofBayesian inference
Aswasmentioned inChapter 41,Bayesian inference formultilayernetworks
maybeimplemen tedbyMonteCarlo sampling, orbydeterministic metho ds
emplo yingGaussian approximations (Neal, 1996; MacKa y,1992c).
Within theBayesian framew orkfordatamodelling, itiseasytoimpro ve
ourprobabilistic models.Forexample, ifwebelievethatsomeinput variables

<<<PAGE 544>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
532 44|Supervised Learning inMultila yerNetworks
inaproblem maybeirrelev anttothepredicted quantity,butwedon't know
which,wecandene anewmodelwithmultiple hyperparameters thatcaptures
theideaofuncertain input variable relevance(MacKa y,1994b; Neal, 1996;
MacKa y,1995b); these modelstheninferautomatically fromthedatawhich
aretherelevantinput variables foraproblem.
44.5 Exercises
Exercise 44.1.[4]Howtomeasure aclassier's quality.You'vejustwritten anew
classication algorithm andwanttomeasure howwellitperforms onatest
set,andcompare itwithother classiers. What performance measure should
youuse?There areseveralstandard answers.Let'sassume theclassier gives
anoutputy(x),wherexistheinput, whichwewon'tdiscuss further, andthat
thetruetarget valueist.Inthesimplest discussions ofclassiers, bothyand
tarebinary variables, butyoumightcaretoconsider cases whereyandtare
more general objectsalso.
Themost widely usedmeasure ofperformance onatestsetistheerror
rate{thefraction ofmisclassic ations made bytheclassier. Thismeasure
forces theclassier togivea0/1output andignores anyadditional information
thattheclassier mightbeabletooer{forexample, anindication ofthe
rmness ofaprediction. Unfortunately ,theerror ratedoesnotnecessarily
measure howinformative aclassier's output is.Consider frequency tables
showingthejointfrequency ofthe0/1output ofaclassier (horizon talaxis),
andthetrue0/1variable (vertical axis). Thenumbersthatwe'llshoware
percentages. Theerror rateeisthesumofthetwoo-diagonal numbers,
whichwecould callthefalsepositiveratee+andthefalsenegativ eratee .
Ofthefollowingthree classiers, AandBhavethesameerrorrateof10%
andChasagreater errorrateof12%.
Classier A Classier B Classier C
y01
t
0 900
1 100y01
t
0 8010
1 010y01
t
0 7812
1 010
Butclearly classier A,whichsimply guesses thattheoutcome is0forall
cases, isconveying noinformation atallaboutt;whereas classier Bhasan
informativ eoutput: ify=0thenwearesurethattreally iszero;andify=1
thenthere isa50%chance thatt=1,ascompared totheprior probabilit y
P(t=1)=0:1.Classier Cisslightlylessinformativ ethanB,butitisstill
muchmore useful thantheinformation-free classier A. Howcommon sense ranks the
classiers:
(best)B>C>A(worst).
Howerrorrateranks theclassiers:
(best)A=B>C(worst).Onewaytoimpro veontheerrorrateasaperformance measure istoreport
thepair(e+;e ),thefalsepositiveerrorrateandthefalsenegativ eerrorrate,
whichare(0;0:1)and(0:1;0)forclassiers AandB.Itisespecially important
todistinguish betweenthese twoerrorprobabilities inapplications where the
twosortsoferrorhavedieren tassociated costs. However,there areacouple
ofproblems withthe`error ratepair':
First, ifIsimply toldyouthatclassier Ahaserrorrates(0;0:1)andB
haserrorrates(0:1;0),itwouldnotbeimmediately eviden tthatclassier
Aisactually utterly worthless. Surely weshould haveaperformance
measure thatgivestheworstpossible score toA!
Second, ifweturntoamultiple-class classication problem suchasdigit
recognition, thenthenumberoftypesoferror increases fromtwoto

<<<PAGE 545>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
44.5: Exercises 533
109=90{oneforeachpossible confusion ofclasstwitht0.Itwould
benicetohavesome sensible wayofcollapsing these 90numbersintoa
single rankablenumberthatmakesmore sense thantheerrorrate.
Another reason fornotliking theerrorrateisthatitdoesn'tgiveaclassier
credit foraccurately specifying itsuncertain ty.Consider classiers thathave
three outputs available, `0',`1'andarejection class, `?',whichindicates that
theclassier isnotsure. Consider classiers DandEwiththefollowing
frequency tables, inpercentages:
Classier D Classier E
y0?1
t
0 74106
1 019y0?1
t
0 7866
1 055
Bothoftheseclassiers have(e+;e ;r)=(6%;0%;11%). Butaretheyequally
goodclassiers? Compare classier EwithC.Thetwoclassiers areequiva-
lent.EisjustCindisguise {wecould makeEbytaking theoutput ofCand
tossing acoinwhen Csays`1'inorder todecide whether togiveoutput `1'or
`?'.SoEisequal toCandthusinferior toB.Nowcompare DwithB.Can
youjustify thesuggestion thatDisamore informativ eclassier thanB,and
thusissuperiortoE?YetDandEhavethesame (e+;e ;r)scores.
Error rate
Rejection rate
Figure 44.7.Anerror-reject curve.
Some people usetheareaunder
thiscurveasameasure of
classier qualit y.People often ploterror-reject curves(also knownasROCcurves;ROC
stands for`receiv eroperating characteristic') whichshowthetotale=(e++
e )versusrasrisallowedtovaryfrom0to1,andusethese curvesto
compare classiers (gure 44.7). [Inthespecialcaseofbinary classication
problems,e+maybeplotted versuse instead.] Butaswehaveseen, error
ratescanbeundiscerning performance measures. Doesplotting oneerrorrate
asafunction ofanother makethisweakness oferrorratesgoaway?
Forthisexercise, either construct anexplicit example demonstrating that
theerror-reject curve,andtheareaunder it,arenotnecessarily goodwaysto
compare classiers; orprovethattheyare.
Asasuggested alternativ emetho dforcomparing classiers, consider the
mutual information betweentheoutput andthetarget,
I(T;Y)H(T) H(TjY)=X
y;tP(y)P(tjy)logP(t)
P(tjy); (44.12)
whichmeasures howmanybitstheclassier's output conveysaboutthetarget.
Evaluate themutual information forclassiers A{Eabove.
Investigate thisperformance measure anddiscuss whether itisauseful
one.Doesithavepractical drawbacks?

<<<PAGE 546>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
AboutChapter 45
Feedforw ardneural networkssuchasmultilayerperceptrons arepopular tools
fornonlinear regression andclassication problems. FromaBayesian per-
spective,achoice ofaneural networkmodelcanbeviewedasdening aprior
probabilit ydistribution overnonlinear functions, andtheneural network's
learning processcanbeinterpreted interms oftheposterior probabilit ydis-
tribution overtheunkno wnfunction. (Some learning algorithms searchforthe
function withmaxim umposterior probabilit yandother MonteCarlo metho ds
drawsamples fromthisposterior probabilit y.)
Inthelimit oflarge butotherwise standard networks, Neal(1996) has
shownthattheprior distribution overnonlinear functions implied bythe
Bayesian neural networkfallsinaclassofprobabilit ydistributions known
asGaussian processes. Thehyperparameters oftheneural networkmodel
determine thecharacteristic lengthscales oftheGaussian process.Neal's ob-
servationmotivatestheideaofdiscarding parameterized networksandworking
directly withGaussian processes. Computations inwhichtheparameters of
thenetworkareoptimized arethenreplaced bysimple matrix operations using
thecovariance matrix oftheGaussian process.
Inthischapter Iwillreview workonthisideabyWilliams andRasmussen
(1996), Neal(1997b), BarberandWilliams (1997) andGibbs andMacKa y
(2000), andwillassess whether, forsupervised regression andclassication
tasks, thefeedforw ardnetworkhasbeensuperceded.
.Exercise 45.1.[3]Iregret thatthischapter israther dry.There's nosimple
explanatory examples init,andfewpictures. Thisexercise asksyouto
create interesting pictures toexplain toyourself thischapter's ideas.
Source codeforcomputer demonstrations written inthefreelanguage
octave isavailable at:
http://www.inference.ph y.cam.ac.uk/mackay/itprnn/software.html.
Radford Neal's softwareforGaussian processes isavailable at:
http://www.cs.toronto.e du/~radford/.
534

<<<PAGE 547>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
45
Gaussian Processes
After thepublication ofRumelhart, HintonandWilliams's (1986) paperon
supervised learning inneural networksthere wasasurge ofinterest inthe
empirical modelling ofrelationships inhigh-dimensional datausing nonlinear
parametric modelssuchasmultilayerperceptrons andradial basisfunctions.
IntheBayesian interpretation ofthese modelling metho ds,anonlinear func-
tiony(x)parameterized byparameters wisassumed tounderlie thedata
fx(n);tngN
n=1,andtheadaptation ofthemodeltothedatacorresp ondstoan
inferenceofthefunction giventhedata. Wewilldenote thesetofinput vectors
byXNfx(n)gN
n=1andthesetofcorresp onding target values bythevector
tNftngN
n=1.Theinference ofy(x)isdescrib edbytheposterior probabilit y
distribution
P(y(x)jtN;XN)=P(tNjy(x);XN)P(y(x))
P(tNjXN): (45.1)
Ofthetwoterms ontheright-hand side,therst,P(tNjy(x);XN),isthe
probabilit yofthetarget values giventhefunctiony(x),whichinthecaseof
regression problems isoften assumed tobeaseparable Gaussian distribution;
andthesecond term,P(y(x)),isthepriordistribution onfunctions assumed
bythemodel.Thisprior isimplicit inthechoice ofparametric modeland
thechoice ofregularizers usedduring themodeltting. Theprior typically
species thatthefunctiony(x)isexpected tobecontinuous andsmooth,
andhaslesshighfrequency powerthanlowfrequency power,buttheprecise
meaning ofthepriorissomewhat obscured bytheuseoftheparametric model.
Now,fromthepointofviewofprediction offuture valuesoft,allthatmat-
tersistheassumed priorP(y(x))andtheassumed noisemodelP(tNjy(x);XN)
{theparameterization ofthefunctiony(x;w)isirrelev ant.
TheideaofGaussian processmodelling istoplace apriorP(y(x))directly
onthespace offunctions, without parameterizing y(x).Thesimplest typeof
prioroverfunctions iscalled aGaussian process.Itcanbethough tofasthe
generalization ofaGaussian distribution overanitevectorspace toafunction
space ofinnite dimension. JustasaGaussian distribution isfullyspecied
byitsmean andcovariance matrix, aGaussian processisspecied byamean
andacovariance function .Here, themean isafunction ofx(whichwewill
often taketobethezerofunction), andthecovariance isafunctionC(x;x0)
thatexpresses theexpected covariance betweenthevalues ofthefunctiony
atthepointsxandx0.Thefunctiony(x)inanyonedatamodelling problem
isassumed tobeasingle sample fromthisGaussian distribution. Gaussian
processes arealready wellestablished modelsforvarious spatial andtemporal
problems {forexample, Brownian motion, Langevin processes andWiener
processes areallexamples ofGaussian processes; Kalman lters, widely used
535

<<<PAGE 548>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
536 45|Gaussian Processes
tomodelspeechwaveforms, alsocorresp ondtoGaussian processmodels;the
metho dof`kriging' ingeostatistics isaGaussian processregression metho d.
Reservations aboutGaussian processes
Itmightbethough tthatitisnotpossible toreproducetheinteresting prop-
erties ofneural networkinterpolation metho dswithsomething sosimple asa
Gaussian distribution, butasweshallnowsee,manypopular nonlinear inter-
polation metho dsareequivalenttoparticular Gaussian processes. (Iusethe
term`interpolation' tocoverboththeproblem of`regression' {tting acurve
through noisy data{andthetaskoftting aninterpolantthatpasses exactly
through thegivendatapoints.)
Itmightalsobethough tthatthecomputational complexit yofinference
when weworkwithpriors overinnite-dimensional function spaces mightbe
innitely large. Butbyconcen trating onthejointprobabilit ydistribution of
theobserv eddataandthequantitieswewishtopredict, itispossible tomake
predictions withresources thatscaleaspolynomial functions ofN,thenumber
ofdatapoints.
45.1 Standard metho dsfornonlinear regression
Theproblem
WearegivenNdatapointsXN;tN=fx(n);tngN
n=1.Theinputs xarevec-
torsofsome xedinput dimensionI.Thetargetstareeither realnumbers,
inwhichcasethetaskwillbearegression orinterpolation task,ortheyare
categorical variables, forexamplet2f0;1g,inwhichcasethetaskisaclas-
sication task. Wewillconcen trate onthecaseofregression forthetime
being.
Assuming thatafunctiony(x)underlies theobserv eddata, thetaskisto
inferthefunction fromthegivendata, andpredict thefunction's value{or
thevalueoftheobserv ationtN+1{atanewpointx(N+1).
Parametric approaches totheproblem
Inaparametric approac htoregression weexpress theunkno wnfunctiony(x)
interms ofanonlinear functiony(x;w)parameterized byparameters w.
Example 45.2. Fixed basisfunctions. Using asetofbasisfunctionsfh(x)gH
h=1,
wecanwrite
y(x;w)=HX
h=1whh(x): (45.2)
Ifthebasis functions arenonlinear functions ofxsuchasradial basis
functions centredatxedpointsfchgH
h=1,
h(x)=exp"
 (x ch)2
2r2#
; (45.3)
theny(x;w)isanonlinear function ofx;however,sincethedependence
ofyontheparameters wislinear, wemightsometimes refertothisas
a`linear' model.Inneural networkterms, thismodelislikeamultilayer
networkwhose connections fromtheinput layertothenonlinear hidden
layerarexed; onlytheoutput weightswareadaptiv e.
Other possible setsofxedbasisfunctions include polynomials suchas
h(x)=xp
ixq
jwherepandqareinteger powersthatdependonh.

<<<PAGE 549>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
45.1: Standard metho dsfornonlinear regression 537
Example 45.3. Adaptive basisfunctions. Alternativ ely,wemightmakeafunc-
tiony(x)from basis functions thatdependonadditional parameters
included inthevectorw.Inatwo-layerfeedforw ardneural network
withnonlinear hidden units andalinear output, thefunction canbe
written
y(x;w)=HX
h=1w(2)
htanh IX
i=1w(1)
hixi+w(1)
h0!
+w(2)
0 (45.4)
whereIisthedimensionalit yoftheinput space andtheweightvector
wconsists oftheinput weightsfw(1)
hig,thehidden unitbiasesfw(1)
h0g,
theoutput weightsfw(2)
hgandtheoutput biasw(2)
0.Inthismodel,the
dependence ofyonwisnonlinear.
Havingchosen theparameterization, wetheninferthefunctiony(x;w)by
inferring theparameters w.Theposterior probabilit yoftheparameters is
P(wjtN;XN)=P(tNjw;XN)P(w)
P(tNjXN): (45.5)
ThefactorP(tNjw;XN)states theprobabilit yoftheobserv eddatapoints
when theparameters w(andhence, thefunctiony)areknown.Thisproba-
bilitydistribution isoften takentobeaseparable Gaussian, eachdatapoint
tndiering fromtheunderlying valuey(x(n);w)byadditiv enoise. Thefactor
P(w)species thepriorprobabilit ydistribution oftheparameters. Thistoo
isoften takentobeaseparable Gaussian distribution. Ifthedependence ofy
onwisnonlinear theposterior distribution P(wjtN;XN)isingeneral nota
Gaussian distribution.
Theinference canbeimplemen tedinvarious ways.IntheLaplace metho d,
weminimize anobjectivefunction
M(w)= ln[P(tNjw;XN)P(w)] (45.6)
withrespecttow,locating thelocallymostprobable parameters, thenusethe
curvature ofM,@2M(w)=@wi@wj,todene errorbarsonw.Alternativ elywe
canusemoregeneral MarkovchainMonteCarlo techniques tocreate samples
fromtheposterior distribution P(wjtN;XN).
Havingobtained oneofthese represen tations oftheinference ofwgiven
thedata, predictions arethenmade bymarginalizing overtheparameters:
P(tN+1jtN;XN+1)=Z
dHwP(tN+1jw;x(N+1))P(wjtN;XN):(45.7)
IfwehaveaGaussian represen tation oftheposterior distribution P(wjtN;XN),
thenthisintegral cantypically beevaluated directly .Inthealternativ eMonte
Carlo approac h,whichgeneratesRsamples w(r)thatareintended tobesam-
plesfromtheposterior distribution P(wjtN;XN),weapproximate thepre-
dictivedistribution by
P(tN+1jtN;XN+1)'1
RRX
r=1P(tN+1jw(r);x(N+1)): (45.8)
Nonparametric approaches.
Innonparametric metho ds,predictions areobtained without explicitly pa-
rameterizing theunkno wnfunctiony(x);y(x)livesintheinnite-dimensional

<<<PAGE 550>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
538 45|Gaussian Processes
space ofallcontinuousfunctions ofx.Onewellknownnonparametric ap-
proachtotheregression problem isthespline smoothing metho d(Kimeldorf
andWahba,1970). Aspline solution toaone-dimensional regression problem
canbedescrib edasfollows:wedene theestimator ofy(x)tobethefunction
^y(x)thatminimizes thefunctional
M(y(x))=1
2NX
n=1(y(x(n)) tn)2+1
2Z
dx[y(p)(x)]2; (45.9)
wherey(p)isthepthderivativeofyandpisapositivenumber.Ifpissetto
2thentheresulting function ^y(x)isacubic spline, thatis,apiecewise cubic
function thathas`knots' {discon tinuities initssecond derivative{atthedata
pointsfx(n)g.
Thisestimation metho dcanbeinterpreted asaBayesian metho dbyiden-
tifying thepriorforthefunctiony(x)as:
lnP(y(x)j)= 1
2Z
dx[y(p)(x)]2+const; (45.10)
andtheprobabilit yofthedatameasuremen tstN=ftngN
n=1assuming inde-
penden tGaussian noise as:
lnP(tNjy(x);)= 1
2NX
n=1(y(x(n)) tn)2+const: (45.11)
[Theconstan tsinequations (45.10) and(45.11) arefunctions ofandre-
spectively.Strictly theprior(45.10) isimprop ersinceaddition ofanarbitrary
polynomial ofdegree (p 1)toy(x)isnotconstrained. Thisimpropriet yis
easily rectied bytheaddition of(p 1)appropriate terms to(45.10).] Given
thisinterpretation ofthefunctions inequation (45.9),M(y(x))isequal tomi-
nusthelogoftheposterior probabilit yP(y(x)jtN;;),within anadditiv e
constan t,andthesplines estimation procedure canbeinterpreted asyielding
aBayesian MAP estimate. TheBayesian perspectiveallowsusadditionally
toputerror barsonthesplines estimate andtodrawtypical samples from
theposterior distribution, anditgivesanautomatic metho dforinferring the
hyperparameters and.
Comments
Splines priors areGaussian processes
Theprior distribution dened inequation (45.10) isourrstexample ofa
Gaussian process.Throwingmathematical precision tothewinds, aGaussian
processcanbedened asaprobabilit ydistribution onaspace offunctions
y(x)thatcanbewritten intheform
P(y(x)j(x);A)=1
Zexp
 1
2(y(x) (x))TA(y(x) (x))
;(45.12)
where(x)isthemean function andAisalinear operator, andwhere theinner
productoftwofunctionsy(x)Tz(x)isdened by,forexample,Rdxy(x)z(x).
Here, ifwedenote byDthelinear operator thatmapsy(x)tothederivative
ofy(x),wecanwrite equation (45.10) as
lnP(y(x)j)= 1
2Z
dx[Dpy(x)]2+const = 1
2y(x)TAy(x)+const;
(45.13)

<<<PAGE 551>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
45.2: Fromparametric modelstoGaussian processes 539
whichhasthesameformasequation (45.12) with(x)=0,andA[Dp]TDp.
Inorder forthepriorinequation (45.12) tobeaproperprior,Amustbea
positivedenite operator, i.e.,onesatisfyingy(x)TAy(x)>0forallfunctions
y(x)other thany(x)=0.
Splines canbewritten asparametric models
Splines maybewritten interms ofaninnite setofxedbasisfunctions, asin
equation (45.2), asfollows.Firstrescale thexaxissothattheinterval(0;2)
ismuchwider thantherange ofxvalues ofinterest. Letthebasisfunctions
beaFourier setfcoshx;sinhx,h=0;1;2;:::g,sothefunction is
y(x)=1X
h=0wh(cos)cos(hx)+1X
h=1wh(sin)sin(hx): (45.14)
Usetheregularizer
EW(w)=1X
h=01
2hp
2w2
h(cos)+1X
h=11
2hp
2w2
h(sin) (45.15)
todene aGaussian prioronw,
P(wj)=1
ZW()exp( EW): (45.16)
Ifp=2thenwehavethecubic splines regularizer EW(w)=Ry(2)(x)2dx,as
inequation (45.9); ifp=1wehavetheregularizer EW(w)=Ry(1)(x)2dx,
etc.(Tomaketheprior properwemustaddanextra regularizer onthe
termw0(cos).)Thusinterms ofthepriorP(y(x))there isnofundamen tal
dierence betweenthe`nonparametric' splines approac handother parametric
approac hes.
Representation isirrelevant forprediction
Fromthepointofviewofprediction atleast, there aretwoobjectsofinter-
est.Therstistheconditional distribution P(tN+1jtN;XN+1)dened in
equation (45.7). Theother objectofinterest, should wewishtocompare one
modelwithothers, isthejointprobabilit yofalltheobserv eddatagiventhe
model,theevidenceP(tNjXN),whichappeared asthenormalizing constan t
inequation (45.5). Neither ofthese quantitiesmakesanyreference totherep-
resentation oftheunkno wnfunctiony(x).Soattheendoftheday,ourchoice
ofrepresen tation isirrelev ant.
Thequestion wenowaddress is,inthecaseofpopular parametric models,
whatformdothese twoquantitiestake?Wewillseethatforstandard models
withxedbasisfunctions andGaussian distributions ontheunkno wnparame-
ters,thejointprobabilit yofalltheobserv eddatagiventhemodel,P(tNjXN),
isamultivariate Gaussian distribution withmean zeroandwithacovariance
matrix determined bythebasis functions; thisimplies thattheconditional
distribution P(tN+1jtN;XN+1)isalsoaGaussian distribution, whose mean
dependslinearly onthevaluesofthetargets tN.Standard parametric models
aresimple examples ofGaussian processes.
45.2 Fromparametric modelstoGaussian processes
Linearmodels
Letusconsider aregression problem usingHxedbasisfunctions, forexample
one-dimensional radial basisfunctions asdened inequation (45.3).

<<<PAGE 552>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
540 45|Gaussian Processes
Letusassume thatalistofNinput pointsfx(n)ghasbeenspecied and
dene theNHmatrix Rtobethematrix ofvalues ofthebasisfunctions
fh(x)gH
h=1atthepointsfxng,
Rnhh(x(n)): (45.17)
Wedene thevectoryNtobethevector ofvalues ofy(x)attheNpoints,
ynX
hRnhwh: (45.18)
Ifthepriordistribution ofwisGaussian withzeromean,
P(w)=Normal (w;0;2
wI); (45.19)
theny,beingalinear function ofw,isalsoGaussian distributed, withmean
zero. Thecovariance matrix ofyis
Q=hyyTi=hRwwTRTi=RhwwTiRT(45.20)
=2
wRRT: (45.21)
Sothepriordistribution ofyis:
P(y)=Normal (y;0;Q)=Normal (y;0;2
wRRT): (45.22)
Thisresult, thatthevector ofNfunction valuesyhasaGaussian distribu-
tion,istrueforanyselected pointsXN.Thisisthedening propertyofa
Gaussian process.Theprobability distribution ofafunctiony(x)isaGaus-
sianprocessifforanyniteselectionofpointsx(1);x(2);:::;x(N),thedensity
P(y(x(1));y(x(2));:::;y(x(N)))isaGaussian.
Now,ifthenumberofbasis functionsHissmaller thanthenumberof
datapointsN,thenthematrix Qwillnothavefullrank. Inthiscasethe
probabilit ydistribution ofymightbethough tofasaatelliptical pancak e
conned toanH-dimensional subspace intheN-dimensional space inwhich
ylives.
What aboutthetarget values? Ifeachtargettnisassumed todier by
additiv eGaussian noise ofvariance2
fromthecorresp onding function value
ynthentalsohasaGaussian priordistribution,
P(t)=Normal (t;0;Q+2
I): (45.23)
Wewilldenote thecovariance matrix oftbyC:
C=Q+2
I=2
wRRT+2
I: (45.24)
Whether ornotQhasfullrank, thecovariance matrix Chasfullranksince
2
Iisfullrank.
What doesthecovariance matrix Qlooklike?Ingeneral, the(n;n0)entry
ofQis
Qnn0=[2
wRRT]nn0=2
wX
hh(x(n))h(x(n0)) (45.25)
andthe(n;n0)entryofCis
Cnn0=2
wX
hh(x(n))h(x(n0))+nn02
; (45.26)
wherenn0=1ifn=n0and0otherwise.

<<<PAGE 553>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
45.2: Fromparametric modelstoGaussian processes 541
Example 45.4. Let's takeasanexample aone-dimensional case,withradial
basis functions. Theexpression forQnn0becomes simplest ifweassume we
haveuniformly spaced basisfunctions withthebasisfunction labelledhbeing
centredonthepointx=handtakethelimitH!1,sothatthesumover
hbecomes anintegral; toavoidhavingacovariance thatdiverges withH,
wehadbettermake2
wscaleasS=(H),where Histhenumberofbasis
functions perunitlength ofthex-axis, andSisaconstan t;then
Qnn0=SZhmax
hmindhh(x(n))h(x(n0)) (45.27)
=SZhmax
hmindhexp"
 (x(n) h)2
2r2#
exp"
 (x(n0) h)2
2r2#
:(45.28)
Ifweletthelimits ofintegration be1,wecansolvethisintegral:
Qnn0=p
r2Sexp"
 (x(n0) x(n))2
4r2#
: (45.29)
Wearearriving atanewperspectiveontheinterpolation problem. Instead of
specifying thepriordistribution onfunctions interms ofbasisfunctions and
priors onparameters, theprior canbesummarized simply byacovariance
function,
C(x(n);x(n0))1exp"
 (x(n0) x(n))2
4r2#
; (45.30)
where wehavegivenanewname,1,totheconstan toutfront.
Generalizing fromthisparticular case, avista ofinterpolation metho ds
opensup.Givenanyvalidcovariance functionC(x;x0){we'lldiscuss in
amomen twhat `valid'means {wecandene thecovariance matrix forN
function values atlocations XNtobethematrix Qgivenby
Qnn0=C(x(n);x(n0)) (45.31)
andthecovariance matrix forNcorresp onding target values, assuming Gaus-
siannoise, tobethematrix Cgivenby
Cnn0=C(x(n);x(n0))+2
nn0: (45.32)
Inconclusion, thepriorprobabilit yoftheNtarget valuestinthedatasetis:
P(t)=Normal (t;0;C)=1
Ze 1
2tTC 1t: (45.33)
Samples fromthisGaussian processandafewother simple Gaussian processes
aredispla yedingure 45.1.
Multilayer neuralnetworks andGaussian processes
Figures 44.2and44.3showsome random samples fromthepriordistribution
overfunctions dened byaselection ofstandard multilayerperceptrons with
largenumbersofhidden units. Those samples don't seemamillion miles away
fromtheGaussian processsamples ofgure 45.1. Andindeed Neal(1996)
showedthattheproperties ofaneural networkwithonehidden layer(as
inequation (45.4)) convergetothose ofaGaussian processasthenumberof
hidden neurons tends toinnit y,ifstandard `weightdecay'priors areassumed.
Thecovariance function ofthisGaussian processdependsonthedetails ofthe
priors assumed fortheweightsinthenetworkandtheactivation functions of
thehidden units.

<<<PAGE 554>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
542 45|Gaussian Processes
−3.0 −1.0 1.0 3.0 5.0
x−2.0−1.00.01.02.03.0t
−3.0 −1.0 1.0 3.0 5.0
x−4.0−2.00.02.04.0t
(a)2exp
 (x x0)2
2(1:5)2
(b)2exp
 (x x0)2
2(0:35)2
−3.0 −1.0 1.0 3.0 5.0
x−4.0−2.00.02.04.0t
−3.0 −1.0 1.0 3.0 5.0
x−4.0−2.00.02.04.06.0t
(c)2exp
 sin2((x x0)=3:0)
2(0:5)2
(d)2exp
 (x x0)2
2(1:5)2
+xx0Figure 45.1.Samples drawnfrom
Gaussian processpriors. Each
panel showstwofunctions drawn
fromaGaussian processprior.
Thefourcorresp onding covariance
functions aregivenbeloweach
plot.Thedecrease inlength scale
from(a)to(b)produces more
rapidly uctuating functions. The
periodicproperties ofthe
covariance function in(c)canbe
seen. Thecovariance function in
(d)containsthenon-stationary
termxx0corresp onding tothe
covariance ofastraigh tline,so
thattypical functions include
linear trends. FromGibbs (1997).
45.3 Using agivenGaussian processmodelinregression
Wehavespentsome timetalking aboutpriors. Wenowreturn toourdata
andtheproblem ofprediction. Howdowemakepredictions withaGaussian
process?
Havingformed thecovariance matrix Cdened inequation (45.32) ourtask
istoinfertN+1giventheobserv edvectortN.Thejointdensit yP(tN+1;tN)
isaGaussian; sotheconditional distribution
P(tN+1jtN)=P(tN+1;tN)
P(tN)(45.34)
isalsoaGaussian. Wenowdistinguish betweendieren tsizesofcovariance
matrix Cwithasubscript, suchthatCN+1isthe(N+1)(N+1)covariance
matrix forthevectortN+1(t1;:::;tN+1)T.Wedene submatrices ofCN+1
asfollows:
CN+12
666642
64CN3
752
64k3
75
h
kTih
i3
77775: (45.35)
Theposterior distribution (45.34) isgivenby
P(tN+1jtN)/exp"
 1
2h
tNtN+1i
C 1
N+1"
tN
tN+1##
: (45.36)
Wecanevaluate themean andstandard deviation oftheposterior distribution
oftN+1bybrute forceinversion ofCN+1.There isamore elegan texpression

<<<PAGE 555>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
45.4: Examples ofcovariance functions 543
forthepredictiv edistribution, however,whichisuseful whenev erpredictions
aretobemade atanumberofnewpointsonthebasisofthedatasetofsize
N.WecanwriteC 1
N+1interms ofCNandC 1
Nusing thepartitioned inverse
equations (Barnett, 1979)
C 1
N+1="
Mm
mTm#
(45.37)
where
m=
 kTC 1
Nk 1(45.38)
m= mC 1
Nk (45.39)
M=C 1
N+1
mmmT: (45.40)
When wesubstitute thismatrix intoequation (45.36) wend
P(tN+1jtN)=1
Zexp2
4 (tN+1 ^tN+1)2
22
^tN+13
5 (45.41)
where
^tN+1=kTC 1
NtN (45.42)
2
^tN+1= kTC 1
Nk: (45.43)
Thepredictiv emean atthenewpointisgivenby^tN+1and^tN+1denes the
errorbarsonthisprediction. Notice thatwedonotneedtoinvertCN+1in
order tomakepredictions atx(N+1).OnlyCNneeds tobeinverted. Thus
Gaussian processes allowonetoimplemen tamodelwithanumberofbasis
functionsHmuchlarger thanthenumberofdatapointsN,withthecom-
putational requiremen tbeingoforderN3,independen tofH.[We'lldiscuss
waysofreducing thiscostlater.]
Thepredictions produced byaGaussian processdependentirely onthe
covariance matrix C.Wenowdiscuss thesortsofcovariance functions one
mightchoosetodene C,andhowwecanautomate theselection ofthe
covariance function inresponsetodata.
45.4 Examples ofcovariance functions
Theonlyconstrain tonourchoice ofcovariance function isthatitmustgen-
erateanon-negativ e-denite covariance matrix foranysetofpointsfxngN
n=1.
Wewilldenote theparameters ofacovariance function by.Thecovariance
matrix ofthasentriesgivenby
Cmn=C(x(m);x(n);)+mnN(x(n);) (45.44)
whereCisthecovariance function andNisanoise modelwhichmightbe
stationary orspatially varying, forexample,
N(x;)=(3 forinput-indep enden tnoise
expPJ
j=1jj(x)
forinput-dep enden tnoise.(45.45)
Thecontinuityproperties ofCdetermine thecontinuityproperties oftypical
samples fromtheGaussian processprior. Anencyclopaedic paperonGaus-
sianprocesses giving manyvalidcovariance functions hasbeenwritten by
Abrahamsen (1997).

<<<PAGE 556>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
544 45|Gaussian Processes
Stationary covarianc efunctions
Astationary covariance function isonethatistranslation invariantinthatit
satises
C(x;x0;)=D(x x0;) (45.46)
forsome functionD,i.e.,thecovariance isafunction ofseparation only,also
knownastheautocovariance function. Ifadditionally Cdependsonlyonthe
magnitude ofthedistance betweenxandx0thenthecovariance function is
saidtobehomogenous .Stationary covariance functions mayalsobedescrib ed
interms oftheFourier transform ofthefunctionD,whichisknownasthe
powerspectrum oftheGaussian process.ThisFourier transform isnecessarily
apositivefunction offrequency .Onewayofconstructing avalidstationary
covariance function istoinventapositivefunction offrequency anddeneD
tobeitsinverseFourier transform.
Example 45.5. Letthepowerspectrum beaGaussian function offrequency .
Since theFourier transform ofaGaussian isaGaussian, theautoco-
variance function corresp onding tothispowerspectrum isaGaussian
function ofseparation. Thisargumen trederiv esthecovariance function
wederivedatequation (45.30).
Generalizing slightly,apopular form forCwithhyperparameters =
(1;2;frig)is
C(x;x0;)=1exp"
 1
2IX
i=1(xi x0
i)2
r2
i#
+2: (45.47)
xisanI-dimensional vector andriisalength scaleassociated withinputxi,
thelengthscale inthatdirection onwhichyisexpected tovarysignican tly.
Averylargelength scalemeans thatyisexpected tobeessentially aconstan t
function ofthatinput. Suchaninput could besaidtobeirrelev ant,asin
theautomatic relevancedetermination metho dforneural networks(MacKa y,
1994a; Neal, 1996). The1hyperparameter denes thevertical scaleofvaria-
tionsofatypical function. The2hyperparameter allowsthewhole function
tobeoset awayfromzerobysome unkno wnconstan t{tounderstand this
term, examine equation (45.25) andconsider thebasisfunction(x)=1.
Another stationary covariance function is
C(x;x0)=exp( jx x0j)0<2: (45.48)
For=2,thisisaspecialcaseoftheprevious covariance function. For
2(1;2),thetypical functions fromthisprior aresmoothbutnotanalytic
functions. For1typical functions arecontinuousbutnotsmooth.
Acovariance function thatmodelsafunction thatisperiodicwithknown
periodiintheithinput direction is
C(x;x0;)=1exp2
64 1
2X
i0
@sin
i(xi x0
i)
ri1
A23
75: (45.49)
Figure 45.1showssome random samples drawnfromGaussian processes
withavarietyofdieren tcovariance functions.

<<<PAGE 557>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
45.5: Adaptation ofGaussian processmodels 545
(a)
0.0 2.0 4.0 6.0−3.0−1.01.03.05.07.0
(b)
0.0 2.0 4.0 6.0−3.0−1.01.03.05.07.0
(c)0.511.522.533.540.511.522.533.54
3
r1Figure 45.2.Multimo dal
likelihoodfunctions forGaussian
processes. Adatasetofve
pointsismodelled withthesimple
covariance function (45.47), with
onehyperparameter 3controlling
thenoisevariance. Panelsaandb
showthemostprobable
interpolantandits1errorbars
when thehyperparameters are
settotwodieren tvaluesthat
(locally) maximize thelikelihood
P(tNjXN;):(a)r1=0:95,
3=0:0;(b)r1=3:5,3=3:0.
Panelcshowsacontourplotof
thelikelihoodasafunction ofr1
and3,withthetwomaxima
shownbycrosses. FromGibbs
(1997).
Nonstationary covarianc efunctions
Thesimplest nonstationary covariance function istheonecorresp onding toa
linear trend. Consider theplaney(x)=P
iwixi+c.Ifthefwigandchave
Gaussian distributions withzeromean andvariances2
wand2
crespectively
thentheplane hasacovariance function
Clin(x;x0;fw;cg)=IX
i=12
wxix0
i+2
c: (45.50)
Anexample ofrandom sample functions incorp orating thelinear termcanbe
seeningure 45.1d.
45.5 Adaptation ofGaussian processmodels
Letusassume thataformofcovariance function hasbeenchosen, butthatit
dependsonundetermined hyperparameters .Wewouldliketo`learn' these
hyperparameters fromthedata. Thislearning processisequivalenttothe
inference ofthehyperparameters ofaneural network,forexample, weight
decayhyperparameters. Itisacomplexit ycontrolproblem, onethatissolved
nicely bytheBayesian Occam's razor.
Ideally wewouldliketodene apriordistribution onthehyperparameters
andintegrate overthem inorder tomakeourpredictions, i.e.,wewouldlike
tond
P(tN+1jxN+1;D)=Z
P(tN+1jxN+1;;D)P(jD)d: (45.51)
Butthisintegral isusually intractable. There aretwoapproac heswecantake.

<<<PAGE 558>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
546 45|Gaussian Processes
1.Wecanapproximate theintegral byusing themost probable values of
hyperparameters.
P(tN+1jxN+1;D)'P(tN+1jxN+1;D;MP) (45.52)
2.Orwecanperform theintegration overnumerically using MonteCarlo
metho ds(Williams andRasmussen, 1996; Neal, 1997b).
Either ofthese approac hesisimplemen tedmostecien tlyifthegradien t
oftheposterior probabilit yofcanbeevaluated.
Gradient
Theposterior probabilit yofis
P(jD)/P(tNjXN;)P(): (45.53)
Thelogoftherstterm(theevidence forthehyperparameters) is
lnP(tNjXN;)= 1
2lndetCN 1
2tT
NC 1
NtN N
2ln2; (45.54)
anditsderivativewithrespecttoahyperparameter is
@
@lnP(tNjXN;)= 1
2Trace
C 1
N@CN
@
+1
2tT
NC 1
N@CN
@C 1
NtN:
(45.55)
Comments
Assuming thatnding thederivativesofthepriors isstraigh tforward,wecan
nowsearchforMP.Howevertherearetwoproblems thatweneedtobeaware
of.Firstly ,asillustrated ingure 45.2, theevidence maybemultimo dal.
Suitable priors andsensible optimization strategies often eliminate poorop-
tima. Secondly andperhaps most importantlytheevaluation ofthegradi-
entoftheloglikelihoodrequires theevaluation ofC 1
N.Anyexact inversion
metho d(suchasCholesky decomp osition, LUdecomp osition orGauss{Jordan
elimination) hasanassociated computational costthatisoforderN3andso
calculating gradien tsbecomes timeconsuming forlargetraining datasets.Ap-
proximate metho dsforimplemen tingthepredictions (equations (45.42) and
(45.43)) andgradien tcomputation (equation (45.55)) areanactiveresearc h
area. Oneapproac hbased ontheideas ofSkilling (1993) makesapproxima-
tionstoC 1tandTraceC 1using iterativ emetho dswithcostO(N2)(Gibbs
andMacKa y,1996; Gibbs, 1997). Further references onthistopic aregiven
attheendofthechapter.
45.6 Classication
Gaussian processes canbeintegrated intoclassication modelling oncewe
identifyavariable thatcansensibly begivenaGaussian processprior.
Inabinary classication problem, wecandene aquantityana(x(n))
suchthattheprobabilit ythattheclassis1rather than0is
P(tn=1jan)=1
1+e an: (45.56)
Large positivevalues ofacorresp ondtoprobabilities closetoone;largeneg-
ativevalues ofadene probabilities thatareclose tozero. Inaclassica-
tionproblem, wetypically intendthattheprobabilit yP(tn=1)should bea
smoothly varying function ofx.Wecanembodythispriorbeliefbydening
a(x)tohaveaGaussian processprior.

<<<PAGE 559>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
45.7: Discussion 547
Implementation
Itisnotsoeasytoperform inferences andadapt theGaussian processmodel
todatainaclassication modelasinregression problems because thelike-
lihoodfunction (45.56) isnotaGaussian function ofan.Sotheposterior
distribution ofagivensome observ ations tisnotGaussian andthenormal-
ization constan tP(tNjXN)cannot bewritten downanalytically .Barberand
Williams (1997) haveimplemen tedclassiers based onGaussian processpriors
using Laplace approximations (Chapter 27).Neal(1997b) hasimplemen teda
MonteCarlo approac htoimplemen tingaGaussian processclassier. Gibbs
andMacKa y(2000) haveimplemen tedanother cheapandcheerful approac h
based onthemetho dsofJaakk olaandJordan (section 33.8). Inthisvaria-
tional Gaussian processclassier ,weobtain tractable upperandlowerbounds
fortheunnormalized posterior densit yovera,P(tNja)P(a).These bounds
areparameterized byvariational parameters whichareadjusted inorder to
obtain thetightestpossible t.Using normalized versions oftheoptimized
bounds wethencompute approximations tothepredictiv edistributions.
Multi-class classication problems canalsobesolvedwithMonteCarlo
metho ds(Neal, 1997b) andvariational metho ds(Gibbs, 1997).
45.7 Discussion
Gaussian processes aremoderately simple toimplemen tanduse.Because very
fewparameters ofthemodelneedtobedetermined byhand (generally only
thepriors onthehyperparameters), Gaussian processes areuseful toolsfor
automated tasks where netuning foreachproblem isnotpossible. Wedo
notappeartosacrice anyperformance forthissimplicit y.
Itiseasytoconstruct Gaussian processes thathaveparticular desired
properties; forexample wecanmakeastraigh tforwardautomatic relevance
determination model.
Oneobvious problem withGaussian processes isthecomputational cost
associated withinverting anNNmatrix. Thecostofdirect metho dsof
inversion becomes prohibitiv ewhen thenumberofdatapointsNisgreater
thanabout1000.
Havewethrownthebabyoutwiththebathwater?
According tothehypeof1987, neural networksweremeanttobeintelligen t
modelsthatdiscoveredfeatures andpatterns indata. Gaussian processes in
contrast aresimply smoothing devices. HowcanGaussian processes possi-
blyreplace neural networks? Wereneural networksover-hyped,orhavewe
underestimated thepowerofsmoothing metho ds?
Ithink boththesepropositions aretrue.Thesuccess ofGaussian processes
showsthatmanyreal-w orlddatamodelling problems areperfectly wellsolved
bysensible smoothing metho ds.Themost interesting problems, thetaskof
feature discoveryforexample, arenotonesthatGaussian processes willsolve.
Butmaybemultilayerperceptrons can't solvethem either. Perhaps afresh
startisneeded, approac hingtheproblem ofmachinelearning fromaparadigm
dieren tfromthesupervised feedforw ardmapping.
Further reading
Thestudy ofGaussian processes forregression isfarfromnew. Time series
analysis wasbeingperformed bytheastronomer T.N.Thiele using Gaussian

<<<PAGE 560>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
548 45|Gaussian Processes
processes in1880(Lauritzen, 1981). Inthe1940s, Wiener{Kolmogoro vpre-
diction theory wasintroduced forprediction oftrajectories ofmilitary targets
(Wiener, 1948). Within thegeostatistics eld, Matheron (1963) proposeda
framew orkforregression using optimal linear estimators whichhecalled `krig-
ing'afterD.G. Krige, aSouth African mining engineer. Thisframew orkis
identicaltotheGaussian processapproac htoregression. Kriging hasbeen
developedconsiderably inthelastthirtyyears(seeCressie (1993) forare-
view) including severalBayesian treatmen ts(Omre, 1987; Kitanidis, 1986).
Howeverthegeostatistics approac htotheGaussian processmodelhascon-
centrated mainly onlow-dimensional problems andhaslargely ignored any
probabilistic interpretation ofthemodel.Kalman lters arewidely usedto
implemen tinferences forstationary one-dimensional Gaussian processes, and
arepopular modelsforspeechandmusicmodelling (Bar-Shalom andFort-
mann, 1988). Generalized radial basis functions (Poggio andGirosi, 1989),
ARMA models(Wahba,1990) andvariable metric kernel metho ds(Lowe,
1995) areallclosely related toGaussian processes. SeealsoO'Hagan (1978).
Theideaofreplacing supervised neural networksbyGaussian processes
wasrstexplored byWilliams andRasmussen (1996) andNeal(1997b). A
thorough comparision ofGaussian processes withother metho dssuchasneural
networksandMARS wasmade byRasmussen (1996). Metho dsforreducing
thecomplexit yofdatamodelling withGaussian processes remain anactive
researc harea(Poggio andGirosi, 1990; LuoandWahba,1997; Tresp,2000;
Williams andSeeger, 2001; Smola andBartlett, 2001; Rasmussen, 2002; Seeger
etal.,2003; OpperandWinther,2000).
Alonger review ofGaussian processes isin(MacKa y,1998b). Areview
paperonregression withcomplexit ycontrolusing hierarc hicalBayesianmodels
is(MacKa y,1992a).
Gaussian processes andsupportvectorlearning machines (Scholkopfetal.,
1995; Vapnik, 1995) havealotincommon. Botharekernel-based predictors,
thekernelbeinganother name forthecovariance function. ABayesianversion
ofsupportvectors, exploiting thisconnection, canbefound in(Chuetal.,
2001; Chuetal.,2002; Chuetal.,2003b; Chuetal.,2003a).

<<<PAGE 561>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
46
Decon volution
46.1 Traditional image reconstruction metho ds
Optimal linearlters
Inmanyimaging problems, thedatameasuremen tsfdngarelinearly related
totheunderlying image f:
dn=X
kRnkfk+nn: (46.1)
Thevectorndenotes theinevitable noise thatcorrupts realdata. Inthecase
ofacamera whichproduces ablurred picture, thevectorfdenotes thetrue
image, ddenotes theblurred andnoisy picture, andthelinear operator R
isaconvolution dened bythepointspread function ofthecamera. Inthis
specialcase, thetrueimage andthedatavector reside inthesame space;
butitisimportanttomaintainadistinction betweenthem. Wewillusethe
subscriptn=1;:::;Ntorunoverdatameasuremen ts,andthesubscripts
k;k0=1;:::;Ktorunoverimage pixels.
Onemightspeculate thatsincetheblurwascreated byalinear operation,
thenperhaps itmightbedeblurred byanother linear operation. Wecanderive
theoptimal linear lterintwoways.
Bayesian derivation
Weassume thatthelinear operator Risknown,andthatthenoisenis
Gaussian andindependen t,withaknownstandard deviation.
P(djf;;H)=1
(22)N=2exp 
 X
n(dn P
kRnkfk)2.
(22
)!
:(46.2)
Weassume thattheprior probabilit yoftheimage isalsoGaussian, witha
scaleparameterf.
P(fjf;H)=det 1
2C
(22
f)K=2exp0
@ X
k;k0fkCkk0f0
k(22
f)1
A: (46.3)
Ifweassume nocorrelations among thepixels thenthesymmetric, fullrank
matrix Cisequal totheidentitymatrix I.Themore sophisticated `intrinsic
correlation function' modelusesC=[GGT] 1,whereGisaconvolution that
takesusfromanimaginary `hidden' image, whichisuncorrelated, tothereal
correlated image. Theintrinsic correlation function should notbeconfused
withthepointspread function Rwhichdenes theimage-to-data mapping.
549

<<<PAGE 562>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
550 46|Decon volution
Azero-mean Gaussian priorisclearly apoorassumption ifitisknownthat
allelemen tsoftheimage farepositive,butletusproceed. Wecannowwrite
downtheposterior probabilit yofanimage fgiventhedatad.
P(fjd;;f;H)=P(djf;;H)P(fjf;H))
P(dj;f;H): (46.4)
Inwords,
Posterior =LikelihoodPrior
Evidence: (46.5)
The`evidence'P(dj;f;H)isthenormalizing constan tforthisposterior
distribution. Hereitisunimp ortant,butitisusedinamore sophisticated
analysis tocompare, forexample, dieren tvalues ofandf,ordieren t
pointspread functions R.
Since theposterior distribution istheproductoftwoGaussian functions of
f,itisalsoaGaussian, andcantherefore besummarized byitsmean, which
isalsothemostprobableimage ,fMP,anditscovariance matrix:
fjd[ rr logP(fjd;;f;H)] 1; (46.6)
whichdenes thejointerrorbarsonf.Inthisequation, thesymbolrdenotes
dieren tiation withrespecttotheimage parameters f.WecanndfMPby
dieren tiating thelogoftheposterior, andsolving forthederivativebeing
zero. Weobtain:
fMP="
RTR+2

2
fC# 1
RTd: (46.7)
Theoperator
RTR+2

2
fC 1
RTiscalled theoptimal linear lter. When
theterm2

2
fCcanbeneglected, theoptimal linear lteristhepseudoin verse
[RTR] 1RT.Theterm2

2
fCregularizes thisill-conditioned inverse.
Theoptimal linear ltercanalsobemanipulated intotheform:
Optimal linear lter=C 1RT"
RC 1RT+2

2
fI# 1
: (46.8)
Minimum squareerrorderivation
Thenon-Ba yesian derivation oftheoptimal linear lterstarts byassuming
thatwewill`estimate' thetrueimage fbyalinear function ofthedata:
^f=Wd: (46.9)
Thelinear operator Wisthen`optimized' byminimizing theexpected sum-
squared errorbetween^fandtheunkno wntrueimage .Inthefollowingequa-
tions, summations overrepeated indicesk,k0,nareimplicit. Theexpectation
hiisoverboththestatistics oftherandom variablesfnng,andtheensem ble
ofimages fwhichweexpecttobump into.Weassume thatthenoise iszero
mean anduncorrelated tosecond order withitselfandeverything else,with
hnnnn0i=2
nn0.
hEi=1
2D
(Wkndn fk)2E
(46.10)
=1
2D
(WknRnjfj fk)2E
+1
2WknWkn2
: (46.11)

<<<PAGE 563>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
46.1: Traditional image reconstruction metho ds 551
Dieren tiating withrespecttoW,andintroducing F
fj0fj(c.f.2
fC 1in
theBayesian derivation above),wendthattheoptimal linear lteris:
Wopt=FRTh
RFRT+2
Ii 1: (46.12)
IfweidentifyF=2
fC 1,weobtain theoptimal linear lter(46.8) ofthe
Bayesianderivation. Theadhocassumptions made inthisderivationwerethe
choice ofaquadratic errormeasure, andthedecision tousealinear estimator.
Itisinteresting thatwithout explicit assumptions ofGaussian distributions,
thisderivation hasreproduced thesame estimator astheBayesian posterior
mode,fMP.
TheadvantageofaBayesian approac histhatwecancriticize these as-
sumptions andmodifythem inorder tomakebetterreconstructions.
Other image models
ThebettermatchedourmodelofimagesP(fjH)istotherealworld,thebet-
terourimage reconstructions willbe,andthelessdatawewillneedtoanswer
anygivenquestion. TheGaussian modelswhichleadtotheoptimal linear
lterarespectacularly poorlymatchedtotherealworld. Forexample, the
Gaussian prior(46.3) failstospecifythatallpixelintensities inanimage are
positive.Thisomission leadstothemostpronounced artefacts where theim-
ageunder observ ationhashighcontrastorlargeblackpatches.Optimal linear
lters applied toastronomical datagivereconstructions withnegativ eareas in
them, corresp onding topatchesofskythatsuckenergy outoftelescop es!The
maxim umentropymodelforimage deconvolution (Gull andDaniell, 1978)
wasagreat success principally because thismodelforced thereconstructed
image tobepositive.Thespurious negativ eareas andcomplemen taryspu-
rious positiveareas areeliminated, andthequalityofthereconstruction is
greatly enhanced.
The`classic maxim umentropy'modelassigns anentropic prior
P(fj;m;HClassic)=exp(S(f;m))=Z; (46.13)
where
S(f;m)=X
i(filn(mi=fi)+fi mi) (46.14)
(Skilling, 1989). Thismodelenforces positivit y;theparameter denes a
characteristic dynamic range bywhichthepixelvalues areexpected todier
fromthedefault image m.
The`intrinsic-correlation-function maxim um-en tropy'model(Gull, 1989)
introduces anexpectation ofspatial correlations intotheprioronfbywriting
f=Gh,where Gisaconvolution withanintrinsic correlation function, and
putting aclassic maxen tpriorontheunderlying hidden image h.
Probabilistic movies
Havingfound notonlythemost probable image fMPbutalsoerror barson
it,fjd,onetaskistovisualize those error bars. Whether ornotweuse
MonteCarlo metho dstoinferf,acorrelated random walkaround theposterior
distribution canbeusedtovisualize theuncertain tiesandcorrelations. For
aGaussian posterior distribution, wecancreate acorrelated sequence ofunit
normal random vectors nusing
n(t+1)=cn(t)+sz; (46.15)

<<<PAGE 564>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
552 46|Decon volution
where zisaunitnormal random vector andc2+s2=1(ccontrolshow
persisten tthememory ofthesequence is).Wethenrender theimage sequence
dened by
f(t)=fMP+1=2
fjdn(t)(46.16)
where 1=2
fjdistheCholesky decomp osition offjd.
46.2 Supervised neural networksforimage deconvolution
Neural networkresearc hersoftenexploit thefollowingstrategy .Givenaprob-
lemcurren tlysolvedwithastandard algorithm: interpret thecomputations
performed bythealgorithm asaparameterized mapping fromaninput toan
output, andcallthismapping aneural network;thenadapt theparameters
todatasoastoproduceanother mapping thatsolvesthetaskbetter. By
construction, theneural networkcanreproducethestandard algorithm, so
thisdata-driv enadaptation canonlymaketheperformance better.
There areseveralreasons whystandard algorithms canbebettered inthis
way.
1.Algorithms areoften notdesigned tooptimize therealobjectivefunc-
tion. Forexample, inspeechrecognition, ahidden Markovmodelis
designed tomodelthespeechsignal, andistted soastotomaximize
thegenerativ eprobabilit ygiventheknownstring ofwordsinthetraining
data; buttherealobjectiveistodiscriminate betweendieren twords.
Ifaninadequate modelisbeingused, theneural-net-st yletraining of
themodelwillfocusthelimited resources ofthemodelontheaspects
relevanttothediscrimination task. Discriminativ etraining ofhidden
Markovmodelsforspeechrecognition doesimpro vetheirperformance.
2.Theneural networkcanbemoreexible thanthestandard model;some
oftheadaptiv eparameters mighthavebeenviewedasxedfeatures by
theoriginal designers. Aexible networkcanndproperties inthedata
thatwerenotincluded intheoriginal model.
46.3 Decon volution inhumans
Ahugefraction ofourbrain isdevotedtovision. Oneoftheneglected features
ofourvisual system isthattherawimage falling ontheretina isseverely
blurred: while most people canseewitharesolution ofabout1arcminute
(onesixtieth ofadegree) under anydaylightconditions, brightordim,the
image onourretinaisblurredthroughapointspreadfunction ofwidth as
largeas5arcminutes (WaldandGrin, 1947; HowarthandBradley ,1986).
Itisamazing thatweareabletoresolv epixels thataretwenty-vetimes
smaller inareathantheblobproduced onourretina byanypointsource.
Isaac Newton wasawareofthisconundrum. It'shardtomakealensthat
doesnothavechromatic aberration, andourcornea andlens,likealensmade
ofordinary glass, refract bluelightmorestrongly thanred.Typically oureyes
focuscorrectly forthemiddle ofthevisible spectrum (green), soifwelook
atasingle white dotmade ofred,green, andbluelight,theimage onour
retina consists ofasharply focussed green dotsurrounded byabroader red
blobsuperposedonanevenbroader blueblob. Thewidth oftheredandblue
blobs isproportional tothediameter ofthepupil, whichislargest under dim
lightingconditions. [Theblobs areroughly concen tric,though most people
haveaslightbias,suchthatinoneeyetheredblobiscentredatinydistance

<<<PAGE 565>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
46.3: Decon volution inhumans 553
totheleftandtheblueiscentredatinydistance totheright,andintheother
eyeit'stheother wayround. Thisslightbiasexplains whywhen welook
atblueandredwriting onadarkbackground mostpeople perceivetheblue
writing tobeataslightlygreater depth thanthered.Inaminorit yofpeople,
thissmall biasistheother wayround andthered/blue depth perception is
reversed. Butthiseect (whichmanypeople areawareof,havingnoticed it
incinemas, forexample) istinycompared withthechromatic aberration we
arediscussing.]
Youcanvividly demonstrate toyourself howenormous thechromatic aber-
ration inyoureyeiswiththehelpofasheet ofcardandacolour computer
screen.
Forthemost impressiv eresults {Iguaran teeyouwillbeamazed {use
adimroomwithnolightapart fromthecomputer screen; aprettystrong
eect willstillbeseeneveniftheroomhasdaylightcoming intoit,aslongas
itisnotbrightsunshine. Cutaslitabout1.5mmwideinthecard. Onthe
screen, displa yafewsmall coloured objectsonablackbackground. Iespecially
recommend thinvertical objectscoloured purered,pureblue,magen ta(i.e.,
redplusblue), andwhite (redplusblueplusgreen). Include alittleblack-and-
white textonthescreen too.Stand orsitsucien tlyfarawaythatyoucan
onlyjustreadthetext{perhaps adistance offourmetres orso,ifyouhave
normal vision. Now,holdtheslitvertically infrontofoneofyoureyes,and
closetheother eye.Holdtheslitneartoyoureye{brushing youreyelashes {
andlookthrough it.Waggle theslitslowlytotheleftandtotheright,sothat
theslitisalternately infrontoftheleftandrightsidesofyourpupil. What
doyousee?Iseetheredobjectswaggling toandfro,andtheblueobjects
waggling toandfro,through hugedistances andinoppositedirections, while
white objectsappeartostaystillandarenegligibly distorted. Thinmagen ta
objectscanbeseensplitting intotheirconstituen tredandblueparts. Measure
howlargethemotion oftheredandblueobjectsis{it'smorethan5minutes
ofarcforme,inadimroom.Then checkhowsharply youcanseeunder these
conditions {lookatthetextonthescreen, forexample: isitnotthecasethat
youcansee(through yourwhole pupil) features farsmaller thanthedistance
through whichtheredandbluecomponentswerewaggling? Yetwhen youare
using thewhole pupil, whatisfalling onyourretina mustbeanimage blurred
withablurring diameter equal tothewaggling amplitude.
Oneofthemainfunctions ofearlyvisual processing mustbetodeconvolve
thischromatic aberration. Neuroscien tistssometimes conjecture thattherea-
sonwhyretinal ganglion cellsandcellsinthelateral geniculate nucleus (the
main brain areatowhichretinal ganglion cellsproject)havecentre-surround
receptiv eelds withcolour opponency (long wavelength inthecentreand
medium wavelength inthesurround, forexample) isinorder toperform `fea-
tureextraction' or`edge detection', butIthink thisviewismistak en.The
reason wehavecentre-surround lters attherststage ofvisual processing
(inthefoveaatleast) isforthehugetaskofdeconvolution ofchromatic aber-
ration.
Ispeculate thattheMcCollough eect,anextremely long-lasting associ-
ation ofcolours withorientation (McCollough, 1965; MacKa yandMacKa y,
1974), isproduced bytheadaptation mechanism thattunes ourchromatic-
aberration-decon volution circuits. Ourdeconvolution circuits mustberapidly
tuneable, because thepointspread function ofoureyechanges withourpupil
diameter, whichcanchange within seconds; andindeed theMcCollough eect
canbeinduced within 30seconds. Atthesametime, theeect islong-lasting
when aneyeiscovered, because it'sinourinterests thatourdeconvolution

<<<PAGE 566>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
554 46|Decon volution
circuits should staywell-tuned while wesleep, sothatwecanseesharply the
instan twewakeup.
Ialsowonder whether themain reason thatweevolvedcolour vision was
not`inorder toseefruitbetter' but`soastobeabletoseeblackandwhite
sharper'{deconvolving chromatic aberration iseasier, eveninanentirely black
andwhite world,ifonehasaccess tochromatic information intheimage.
Andanalspeculation: whydooureyesmakemicro-saccades when we
lookatthings? These miniature eye-movementsareofanangular sizebigger
thanthespacing betweenthecones inthefovea(whicharespaced atroughly
1minuteofarc,theperceivedresolution oftheeye).Thetypical sizeofa
microsaccade is5{10minutesofarc(Ratli andL.A.,1950). Isitacoincidence
thatthisisthesameasthesizeofchromatic aberration? Surely micro-saccades
mustplayanessentialroleinthedeconvolution mechanism thatdeliversour
high-resolution vision.
46.4 Exercises
Exercise 46.1.[3]Bluranimage withacircular (tophat)pointspread function
andaddnoise. Thendeconvolvetheblurry noisyimage using theoptimal
linear lter. Finderrorbarsandvisualize thembymaking aprobabilistic
movie.

<<<PAGE 567>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
PartVI
Sparse Graph Codes

<<<PAGE 568>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
AboutPartVI
Thecentralproblem ofcomm unication theory istoconstruct anencoding
andadecodingsystem thatmakeitpossible tocomm unicate reliably over
anoisy channel. During the1990s, remark ableprogress wasmade towards
theShannon limit, using codesthataredened interms ofsparse random
graphs, andwhicharedecodedbyasimple probabilit y-based message-passing
algorithm.
Inasparse graph code,thenodesinthegraph represen tthetransmitted
bitsandtheconstrain tstheysatisfy .Foralinear codewithacodewordlength
NandrateR=K=N,thenumberofconstrain tsisoforderM=N K.
Anylinear codecanbedescrib edbyagraph, butwhatmakesasparse graph
codespecialisthateachconstrain tonlyinvolvesasmall numberofvariables
inthegraph: sothenumberofedges inthegraph scales roughly linearly with
N,rather thanquadratically .
Inthefollowingfourchapters wewilllookatfourfamilies ofsparse graph
codes:threefamilies thatareexcellen tforerror-correction: low-densit yparity-
checkcodes,turbocodes,andrepeat{accum ulate codes;andthefamily of
digital fountaincodes,whichareoutstanding forerasure-correction.
Allthesecodescanbedecodedbyalocalmessage-passing algorithm onthe
graph, thesum{pro ductalgorithm, and,while thisalgorithm isnotaperfect
maxim umlikelihooddecoder,theempirical results arerecord-breaking.
556

<<<PAGE 569>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
47
Low-Densit yParity-Chec kCodes
Alow-densit yparity-checkcode(orGallager code)isablockcodethathasa
parity-checkmatrix, H,everyrowandcolumn ofwhichis`sparse'.
Aregular Gallager codeisalow-densit yparity-checkcodeinwhichevery
column ofHhasthesameweightjandeveryrowhasthesameweightk;reg-
ularGallager codesareconstructed atrandom subjecttothese constrain ts.A
low-densit yparity-checkcodewithj=3andk=4isillustrated ingure 47.1.H=
Figure 47.1.Alow-densit y
parity-checkmatrix andthe
corresp onding graph ofarate-1/4
low-densit yparity-checkcode
withblocklengthN=16,and
M=12constrain ts.Eachwhite
circle represen tsatransmitted bit.
Eachbitparticipates inj=3
constrain ts,represen tedby
squares. Eachconstrain tforces
thesumofthek=4bitstowhich
itisconnected tobeeven.47.1 Theoretical properties
Low-densit yparity-checkcodeslendthemselv estotheoretical study.Thefol-
lowingresults areprovedinGallager (1963) andMacKa y(1999b).
Low-densit yparity-checkcodes,inspiteoftheirsimple construction, are
goodcodes,givenanoptimal decoder(goodcodesinthesenseofsection 11.4).
Furthermore, theyhavegooddistance (inthesenseofsection 13.2). These two
results holdforanycolumn weightj3.Furthermore, there aresequences of
low-densit yparity-checkcodesinwhichjincreases gradually withN,insuch
awaythattheratioj=Nstillgoestozero,thatareverygoodandthathave
verygooddistance.
However,wedon't haveanoptimal decoder,anddecodinglow-densit y
parity-checkcodesisanNP-complete problem. Sowhatcanwedoinpractice?
47.2 Practical decoding
Givenachannel output r,wewishtondthecodewordtwhose likelihood
P(rjt)isbiggest. Alltheeectiv edecodingstrategies forlow-densit yparity-
checkcodesaremessage-passing algorithms. Thebestalgorithm knownis
thesum{pro ductalgorithm, alsoknownasiterativ eprobabilistic decodingor
beliefpropagation.
We'llassume thatthechannel isamemoryless channel (though morecom-
plexchannels caneasily behandled byrunning thesum{pro ductalgorithm
onamorecomplex graph thatrepresen tstheexpected correlations among the
errors (Worthen andStark, 1998)). Foranymemoryless channel, there are
twoapproac hestothedecodingproblem bothofwhichleadtothegeneric
problem `ndthexthatmaximizes
P(x)=P(x)
 [Hx=z]'; (47.1)
whereP(x)isaseparable distribution onabinary vectorx,andzisanother
binary vector. Eachofthese twoapproac hesrepresen tsthedecodingproblem
interms ofafactor graph (Chapter 26).
557

<<<PAGE 570>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
558 47|Low-Densit yParity-Chec kCodes
tn(a)Thepriordistribution overcodewords
P(t)/ [Ht=0]:
Thevariable nodesarethetransmitted bitsftng.
Eachnoderepresen tsthefactor  [P
n2N(m)tn=0mod2].
tnP(rnjtn)(b)Theposterior distribution overcodewords,
P(tjr)/P(t)P(rjt):
Eachupperfunction noderepresen tsalikelihoodfactorP(rnjtn).
nnP(nn)
zm(c)Thejointprobabilit yofthenoisenandsyndrome z,
P(n;z)=P(n)  [z=Hn]:
Thetopvariable nodesarenowthenoisebitsfnng.
Theadded variable nodesatthebasearethesyndrome values
fzmg.
Eachdenitionzm=P
nHmnnnmod2isenforced byafactor.
Figure 47.2.Factor graphs
associated withalow-densit y
parity-checkcode.Thecodeworddecodingviewpoint
First, wenotethatthepriordistribution overcodewords,
P(t)/
 [Ht=0mod2]; (47.2)
canberepresen tedbyafactor graph (gure 47.2a), withthefactorization
being
P(t)/Y
m
 [X
n2N(m)tn=0mod2]: (47.3)
(We'llomitthe`mod2'sfromnowon.)Theposterior distribution overcode-
wordsisgivenbymultiplying thisprior bythelikelihood,whichintroduces
anotherNfactors, oneforeachreceivedbit.
P(tjr)/P(t)P(rjt)
/Y
m
 [X
n2N(m)tn=0]Y
nP(rnjtn) (47.4)
Thefactor graph corresp onding tothisfunction isshowningure 47.2b. It
isthesame asthegraph fortheprior, except fortheaddition oflikelihood
`dongles' tothetransmitted bits.
Inthisviewp oint,thereceivedsignalrncanliveinanyalphab et;allthat
matters arethevalues ofP(rnjtn).
Thesyndromedecodingviewpoint
Alternativ ely,wecanviewthechannel output interms ofabinary received
vectorrandanoise vectorn,withaprobabilit ydistribution P(n)thatcan
bederivedfromthechannel properties andwhatev eradditional information
isavailable atthechannel outputs.
Forexample, withabinary symmetric channel, wedene thenoise by
r=t+n,thesyndrome z=Hr,andnoise modelP(nn)=f.Forother
channels suchastheGaussian channel withoutput y,wemaydene areceived

<<<PAGE 571>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
47.3: Decodingwiththesum{pro ductalgorithm 559
binary vectorrhoweverwewishandobtain aneectiv ebinary noise model
P(n)fromy(exercises 9.18(p.155)and25.1(p.325)).
Thejointprobabilit yofthenoisenandsyndrome z=Hncanbefactored
as
P(n;z)=P(n)
 [z=Hn]
=Y
nP(nn)Y
m
 [zm=X
n2N(m)nn]: (47.5)
Thefactor graph ofthisfunction isshowningure 47.2c. Thevariables n
andzcanalsobedrawnina`beliefnetwork'(also knownasa`Bayesian
network',`causal network',or`inuence diagram') similar togure 47.2a, but
witharrowsontheedges fromtheuppercircular nodes(whichrepresen tthe
variables n)tothelowersquare nodes(whichnowrepresen tthevariables z).
Wecansaythateverybitxnistheparentofjcheckszm,andeachcheckzm
isthechildofkbits.
Both decodingviewp ointsinvolveessentially thesame graph. Either ver-
sionofthedecodingproblem canbeexpressed asthegeneric decodingproblem
`ndthexthatmaximizes
P(x)=P(x)
 [Hx=z]'; (47.6)
inthecodeworddecodingviewp oint,xisthecodewordt,andz=0;inthe
syndrome decodingviewp oint,xisthenoisen,andzisthesyndrome.
Itdoesn'tmatter whichviewp ointwetakewhen weapply thesum{pro duct
algorithm. Thetwodecodingalgorithms areisomorphic andwillgiveequiva-
lentoutcomes (unless numerical errors intervene).
Itendtousethesyndrome decodingviewp ointbecause ithasoneadvantage: one
doesnotneedtoimplemen tanencoderforacodeinorder tobeabletosimulate a
decodingproblem realistically .
We'llnowtalkinterms ofthegeneric decodingproblem.
47.3 Decodingwiththesum{pro ductalgorithm
Weaim,giventheobserv edchecks,tocompute themarginal posterior proba-
bilitiesP(xn=1jz;H)foreachn.Itishardtocompute these exactly because
thegraph contains manycycles. However,itisinteresting toimplemen tthe
decodingalgorithm thatwouldbeappropriate ifthere werenocycles, onthe
assumption thattheerrors introduced mightberelativ elysmall. Thisap-
proachofignoring cycles hasbeenusedinthearticial intelligence literature
butisnowfrowned uponbecause itproduces inaccurate probabilities. How-
ever,ifwearedecodingagooderror-correcting code,wedon't careabout
accurate marginal probabilities {wejustwantthecorrect codeword.Also,
theposterior probabilit y,inthecaseofagoodcodecomm unicating atan
achievablerate,isexpected typically tobehugely concen trated onthemost
probable decoding;sowearedealing withadistinctiv eprobabilit ydistribution
towhichexperience gained inother elds maynotapply.
Thesum{pro ductalgorithm waspresen tedinChapter 26.Wenowwrite
outexplicitly howitworksforsolving thedecodingproblem
Hx=z(mod2):
Forbrevit y,wereabsorb thedongles hanging othexandznodesing-
ure47.2c andmodifythesum{pro ductalgorithm accordingly .Thegraph in

<<<PAGE 572>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
560 47|Low-Densit yParity-Chec kCodes
whichxandzliveisthentheoriginal graph (gure 47.2a) whose edges are
dened bythe1sinH.Thegraph contains nodesoftwotypes,whichwe'll
callchecksandbits.Thegraph connecting thechecksandbitsisabipartite
graph: bitsconnect onlytochecks,andviceversa.Oneachiteration, aprob-
abilityratioispropagated along eachedgeinthegraph, andeachbitnodexn
updates itsprobabilit ythatitshould beinstate1.
Wedenote thesetofbitsnthatparticipate incheckmbyN(m)fn:
Hmn=1g.Similarly wedene thesetofchecksinwhichbitnparticipates,
M(n)fm:Hmn=1g.Wedenote asetN(m)withbitnexcluded by
N(m)nn.Thealgorithm hastwoalternating parts, inwhichquantitiesqmn
andrmnassociated witheachedgeinthegraph areiterativ elyupdated. The
quantityqx
mnismeanttobetheprobabilit ythatbitnofxhasthevaluex,
giventheinformation obtained viachecksother thancheckm.Thequantity
rx
mnismeanttobetheprobabilit yofcheckmbeingsatised ifbitnofxis
considered xedatxandtheother bitshaveaseparable distribution given
bytheprobabilitiesfqmn0:n02N(m)nng.Thealgorithm wouldproducethe
exact posterior probabilities ofallthebitsafteraxednumberofiterations
ifthebipartite graph dened bythematrix Hcontained nocycles.
Initialization .Letp0
n=P(xn=0)(thepriorprobabilit ythatbitxnis0),
andletp1
n=P(xn=1)=1 p0
n.Ifwearetaking thesyndrome decoding
viewp ointandthechannel isabinary symmetric channel thenp1
nwillequal
f.Ifthenoise levelvariesinaknownway(forexample ifthechannel isa
binary input Gaussian channel witharealoutput) thenp1
nisinitialized tothe
appropriate normalized likelihood.Forevery(n;m)suchthatHmn=1the
variablesq0
mnandq1
mnareinitialized tothevaluesp0
nandp1
nrespectively.
Horizon talstep.Inthehorizon talstepofthealgorithm (horizon talfrom
thepointofviewofthematrix H),werunthrough thechecksmandcompute
foreachn2N(m)twoprobabilities: rst,r0
mn,theprobabilit yoftheobserv ed
valueofzmarising whenxn=0,giventhattheother bitsfxn0:n06=nghave
aseparable distribution givenbytheprobabilitiesfq0
mn0;q1
mn0g,dened by:
r0
mn=X
fxn0:n02N(m)nngP zmjxn=0;xn0:n02N(m)nn	Y
n02N(m)nnqxn0
mn0
(47.7)
andsecond,r1
mn,theprobabilit yoftheobserv edvalueofzmarising when
xn=1,dened by:
r1
mn=X
fxn0:n02N(m)nngP zmjxn=1;xn0:n02N(m)nn	Y
n02N(m)nnqxn0
mn0:
(47.8)
Theconditional probabilities inthese summations areeither zeroorone,de-
pending onwhether theobserv edzmmatchesthehypothesized values forxn
andthefxn0g.
These probabilities canbecomputed invarious obvious waysbased on
equation (47.7) and(47.8). Thecomputations maybedonemostecien tly(if
jN(m)jislarge) byregardingzm+xnasthenalstateofaMarkovchainwith
states 0and1,thischainbeingstarted instate0,andundergoing transitions
corresp onding toadditions ofthevariousxn0,withtransition probabilities
givenbythecorresp ondingq0
mn0andq1
mn0.Theprobabilities forzmhavingits
observ edvaluegiveneitherxn=0orxn=1canthenbefound ecien tlyby
useoftheforward{bac kwardalgorithm (section 25.3).

<<<PAGE 573>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
47.3: Decodingwiththesum{pro ductalgorithm 561
Aparticularly convenientimplemen tation ofthismetho dusesforwardand
backwardpasses inwhichproducts ofthedierences qmnq0
mn q1
mnare
computed. Weobtainrmnr0
mn r1
mnfromtheidentity:
rmn=( 1)zmY
n02N(m)nnqmn0: (47.9)
Thisidentityisderivedbyiterating thefollowingobserv ation: if=x+
xmod2,andxandxhaveprobabilities q0
;q0
andq1
;q1
ofbeing0and1,
thenP(=1)=q1
q0
+q0
q1
andP(=0)=q0
q0
+q1
q1
.ThusP(=0) 
P(=1)=(q0
 q1
)(q0
 q1
).
Werecoverr0
mnandr1
mnusing
r0
mn=1/2(1+rmn);r1
mn=1/2(1 rmn): (47.10)
Thetransformations intodierencesqandbackfromrtofrgmaybeviewed
asaFourier transform andaninverseFourier transformation.
Vertical step.Thevertical steptakesthecomputed valuesofr0
mnandr1
mn
andupdates thevalues oftheprobabilities q0
mnandq1
mn.Foreachnwe
compute:
q0
mn=mnp0
nY
m02M(n)nmr0
m0n (47.11)
q1
mn=mnp1
nY
m02M(n)nmr1
m0n (47.12)
wheremnischosen suchthatq0
mn+q1
mn=1.These products canbeecien tly
computed inadownwardpassandanupwardpass.
Wecanalsocompute the`pseudop osterior probabilities' q0
nandq1
natthis
iteration, givenby:
q0
n=np0
nY
m2M(n)r0
mn; (47.13)
q1
n=np1
nY
m2M(n)r1
mn: (47.14)
These quantities areusedtocreate atentativedecoding^x,theconsistency
ofwhichisusedtodecide whether thedecodingalgorithm canhalt. (Halt if
H^x=z.)
Atthispoint,thealgorithm repeatsfromthehorizon talstep.
Thestop-when-it's-done decodingmetho d.Therecommended decod-
ingprocedure istoset^xnto1ifq1
n>0:5andseeifthechecksH^x=zmod2are
allsatised, halting when theyare,anddeclaring afailure ifsome maxim um
numberofiterations (e.g.200or1000) occurs without successful decoding. In
theeventofafailure, wemaystillreport^x,butweagthewhole blockasa
failure.
Wenoteinpassing thedierence betweenthisdecodingprocedure and
thewidespread practice intheturbocodecomm unity,where thedecoding
algorithm isrunforaxednumberofiterations (irresp ectiveofwhether the
decoderndsaconsisten tstateatsomeearlier time). Thispractice iswasteful
ofcomputer time,anditblursthedistinction betweenundetected anddetected
errors. Inourprocedure, `undetected' errors occurifthedecoderndsan^x
satisfying H^x=zmod2whichisnotequal tothetruex.`Detected' errors

<<<PAGE 574>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
562 47|Low-Densit yParity-Chec kCodes
Figure 47.3.Demonstration ofencodingwitharate1=2Gallager code.Theencoderisderivedfrom
averysparse 1000020000parity-checkmatrix withthree 1spercolumn. (a)Thecode
creates transmitted vectors consisting of10000source bitsand10000parity-checkbits.
(b)Here, thesource sequence hasbeenaltered bychanging therstbit.Notice thatmany
oftheparity-checkbitsarechanged. Eachparitybitdependsonabouthalfofthesource
bits. (c)Thetransmission forthecases=(1;0;0;:::;0).Thisvector isthedierence
(modulo2)betweentransmissions (a)and(b).[Dilbertimage Copyrightc1997United
Feature Syndicate, Inc.,usedwithpermission.](a)
 !
paritybits8
>>>>>>>><
>>>>>>>>:
(b)
 (c)
occurifthealgorithm runsforthemaxim umnumberofiterations without
nding avaliddecoding. Undetected errors areofscienticinterest because
theyrevealdistance properties ofacode.Andinengineering practice, itwould
seempreferable fortheblocksthatareknowntocontaindetected errors tobe
solabelledifpractically possible.
Cost.Inabrute force approac h,thetimetocreate thegenerator matrix
scales asN3,whereNistheblocksize.Theencodingtimescales asN2,but
encodinginvolvesonlybinary arithmetic, sofortheblocklengths studied here
ittakesconsiderably lesstimethanthesimulation oftheGaussian channel.
Decodinginvolvesapproximately 6Njoating pointmultiplies periteration,
sothetotalnumberofoperations perdecodedbit(assuming 20iterations) is
about120t=R,independen tofblocklength. Forthecodespresen tedinthe
nextsection, thisisabout800operations.
Theencodingcomplexit ycanbereduced bycleverencodingtricksinvented
byRichardson andUrbank e(2001b) orbyspecially constructing theparity-
checkmatrix (MacKa yetal.,1999).
Thedecodingcomplexit ycanbereduced, withonlyasmall lossinperfor-
mance, bypassing low-precision messages inplaceofrealnumbers(Richardson
andUrbank e,2001a).
47.4 Pictorial demonstration ofGallager codes
Figures 47.3{47.7 illustrate visually theconditions under whichlow-densit y
parity-checkcodescangivereliable comm unication overbinary symmetric
channels andGaussian channels. These demonstrations maybeviewedas
animations ontheworldwideweb(MacKa y,1997b).

<<<PAGE 575>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
47.4: Pictorial demonstration ofGallager codes 563
Figure 47.4.Alow-densit yparity-checkmatrix withN=20000columns ofweightj=3andM=
10000rowsofweightk=6.H=
Encoding
Figure 47.3illustrates theencodingoperation forthecaseofaGallager code
whose parity-checkmatrix isa1000020000matrix withthree 1spercol-
umn(gure 47.4). Thehighdensit yofthegeneratormatrix isillustrated in
gure 47.3b andcbyshowingthechange inthetransmitted vector when one
ofthe10000source bitsisaltered. Ofcourse, thesource images shownhere
arehighly redundan t,andsuchimages should really becompressed before
encoding. Redundan timages arechosen inthese demonstrations tomakeit
easier toseethecorrection processduring theiterativ edecoding. Thedecod-
ingalgorithm doesnottakeadvantageoftheredundancy ofthesource vector,
anditwouldworkinexactly thesamewayirrespectiveofthechoice ofsource
vector.
Iterativedecoding
Thetransmission issentoverachannel withnoise levelf=7:5%andthe
receivedvector isshownintheupperleftofgure 47.5. Thesubsequen t
pictures ingure 47.5showtheiterativ eprobabilistic decodingprocess.The
sequence ofgures showsthebestguess, bitbybit,givenbytheiterativ e
decoder,after0,1,2,3,10,11,12,13iterations. Thedecoderhalts after
the13thiteration when thebestguess violates noparitychecks.Thisnal
decodingiserrorfree.
Inthecaseofanunusually noisy transmission, thedecodingalgorithm fails
tondavaliddecoding. Forthiscodeandachannel withf=7:5%,such
failures happenaboutonceinevery100000transmissions. Figure 47.6shows
thiserrorratecompared withtheblockerrorratesofclassical error-correcting
codes.

<<<PAGE 576>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
564 47|Low-Densit yParity-Chec kCodes
Figure 47.5.Iterativ eprobabilistic decodingofalow{densit yparity{checkcodeforatransmission
receiv edoverachannel withnoise levelf=7:5%.Thesequence ofgures showsthebest
guess, bitbybit,givenbytheiterativ edecoder,after0,1,2,3,10,11,12,13iterations.
Thedecoderhaltsafterthe13thiteration when thebestguess violates noparitychecks.
Thisnaldecodingiserrorfree.received:
0
 1
 2
 3
10
 11
 12
 13
! decoded:
0.1
0.01
0.001
0.0001
1e-05
1e-06
00.20.40.60.81Probability of decoder error
RateGV
CShannon limitlow-densit y
parity-checkcodeFigure 47.6.Error probabilit yof
thelow{densit yparity{checkcode
(with errorbars) forbinary
symmetric channel withf=7:5%,
compared withalgebraic codes.
Squares: repetition codesand
Hamming (7;4)code;other
points:Reed{Muller andBCH
codes.

<<<PAGE 577>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
47.4: Pictorial demonstration ofGallager codes 565
(a1)
 (b1)
(a2)00.050.10.150.20.250.30.350.4
-4-2024P(y|`1') P(y|`0')
(b2)00.050.10.150.20.250.30.350.4
-4-2024P(y|`1') P(y|`0')Figure 47.7.Demonstration ofa
Gallager codeforaGaussian
channel. (a1)Thereceiv edvector
aftertransmission overaGaussian
channel withx==1:185
(Eb=N0=1:47dB).Thegreyscale
represen tsthevalueofthe
normalized likelihood.This
transmission canbeperfectly
decodedbythesum-pro duct
decoder.Theempirical
probabilit yofdecodingfailure is
about10 5.(a2)Theprobabilit y
distribution oftheoutputyofthe
channel withx==1:185foreach
ofthetwopossible inputs. (b1)
Thereceiv edtransmission overa
Gaussian channel withx==1:0,
whichcorresp ondstotheShannon
limit. (b2)Theprobabilit y
distribution oftheoutputyofthe
channel withx==1:0foreachof
thetwopossible inputs.
1e-061e-050.00010.0010.010.11
11.522.533.544.555.5(N=96)
N=204N=408
(N=204)N=816N=96
1e-050.00010.0010.010.11
11.5 22.5 33.5 4j=3j=4
j=5j=6
(a) (b)Figure 47.8.Performance of
rate-1/2Gallager codesonthe
Gaussian channel. Vertical axis:
blockerrorprobabilit y.Horizon tal
axis:signal tonoiseratioEb=N0.
(a)Dependence onblocklengthN
for(j;k)=(3;6)codes.Fromleft
toright:N=816,N=408,
N=204,N=96.Thedashed
linesshowthefrequency of
undetected errors, whichis
measurable onlywhen the
blocklength isassmall asN=96
orN=204.(b)Dependence on
column weightjforcodesof
blocklengthN=816.Gaussian channel
Ingure 47.7theleftpicture showsthereceivedvector after transmission
overaGaussian channel withx==1:185. Thegreyscale represen tsthe
valueofthenormalized likelihood,P(yjt=1)
P(yjt=1)+ P(yjt=0).Thissignal tonoiseratio
x==1:185isanoiselevelatwhichthisrate1/2Gallager codecomm unicates
reliably (theprobabilit yoferroris'10 5).Toshowhowclosewearetothe
Shannon limit, therightpanel showsthereceivedvector when thesignal to
noise ratioisreduced tox==1:0,whichcorresp ondstotheShannon limit
forcodesofrate1/2.
Variation ofperformanc ewithcodeparameters
Figure 47.8showshowtheparameters Nandjaect theperformance of
low{densit yparity{checkcodes.AsShannon wouldpredict, increasing the
blocklength leads toimpro vedperformance. Thedependence onjfollowsa
dieren tpattern. Givenanoptimal decoder,thebestperformance wouldbe
obtained forthecodesclosest torandom codes,thatis,thecodeswithlargest
j.However,thesum{pro ductdecodermakespoorprogress indense graphs,
sothebestperformance isobtained forasmall valueofj.Among thevalues

<<<PAGE 578>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
566 47|Low-Densit yParity-Chec kCodes
333
(a) (b)Figure 47.9.Schematic illustration
ofconstructions (a)ofa
completely regular Gallager code
withj=3,k=6andR=1=2;
(b)ofanearly-regular Gallager
codewithrate1=3.Notation: an
integerrepresen tsanumberof
permutation matrices superposed
onthesurrounding square. A
diagonal linerepresen tsan
identitymatrix.
Figure 47.10.MonteCarlo simulation ofdensit yevolution, followingthedecodingprocessforj=4;k=
8.Eachcurveshowstheaverage entropyofabitasafunction ofnumberofiterations,
asestimated byaMonteCarlo algorithm using 10000samples periteration. Thenoise
levelofthebinary symmetric channelfincreases bysteps of0:005frombottom graph
(f=0:010)totopgraph (f=0:100). There iseviden tlyathreshold ataboutf=0:075,
abovewhichthealgorithm cannot determine x.FromMacKa y(1999b).00.050.10.150.20.250.30.350.40.45
051015202530
ofjshowninthegure,j=3isthebest,forablocklength of816,downtoa
blockerrorprobabilit yof10 5.
Thisobserv ationmotivatesconstruction ofGallager codeswithsomecolumns
ofweight2.Aconstruction withM=2columns ofweight2isshowning-
ure47.9b. Toomanycolumns ofweight2andthecodebecomes amuchpoorer
code.
Aswe'lldiscuss later, wecandoevenbetterbymaking thecodeevenmore
irregular.
47.5 Densit yevolution
Onewaytostudy thedecodingalgorithm istoimagine itrunning onaninnite
tree-lik egraph withthesame localtopology astheGallager code'sgraph.
Figure 47.11.Localtopology of
thegraph ofaGallager codewith
column weightj=3androw
weightk=4.White nodes
represen tbits,xl;blacknodes
represen tchecks,zm;eachedge
corresp ondstoa1inH.Thelarger thematrix H,thecloser itsdecodingproperties should approac h
those oftheinnite graph.
Imagine aninnite beliefnetworkwithnoloops,inwhicheverybitxn
connects tojchecksandeverycheckzmconnects tokbits(gure 47.11).
Weconsider theiterativ eowofinformation inthisnetwork,andexamine
theaverage entropyofonebitasafunction ofnumberofiterations. Ateach
iteration, abithasaccum ulated information fromitslocalnetworkouttoa
radius equal tothenumberofiterations. Successful decodingwilloccuronly
iftheaverage entropyofabitdecreases tozeroasthenumberofiterations
increases.
Theiterations ofaninnite beliefnetworkcanbesimulated byMonte
Carlo metho ds{atechnique rstusedbyGallager (1963). Imagine anetwork
ofradiusI(thetotalnumberofiterations) centredononebit.Ouraimis
tocompute theconditional entropyofthecentralbitxgiventhestatezof
allchecksouttoradiusI.Toevaluate theprobabilit ythatthecentralbit
is1givenaparticular syndrome zinvolvesanI-step propagation fromthe
outside ofthenetworkintothecentre.Attheithiteration probabilities rat

<<<PAGE 579>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
47.6: Impro vingGallager codes 567
radiusI i+1aretransformed intoqsandthenintorsatradiusI iin
awaythatdependsonthestatesxoftheunkno wnbitsatradiusI i.In
theMonteCarlo metho d,rather thansimulating thisnetworkexactly ,which
wouldtakeatimethatgrowsexponentially withI,wecreate foreachiteration
arepresen tativesample (ofsize100,say)ofthevalues offr;xg.Inthecasex
rffffff
@@R@@R??  	  	
@
@@R 
  	f
?x
r
iteration
i 1
iteration
i
Figure 47.12.Atree-fragmen t
constructed during MonteCarlo
simulation ofdensit yevolution.
Thisfragmen tisappropriate fora
regularj=3,k=4Gallager code.ofaregular networkwithparameters j;k,eachnewpairfr;xginthelistat
theithiteration iscreated bydrawingthenewxfromitsdistribution and
drawingatrandom withreplacemen t(j 1)(k 1)pairsfr;xgfromthelistat
the(i 1)thiteration; these areassem bledintoatreefragmen t(gure 47.12)
andthesum-pro ductalgorithm isrunfromtoptobottom tondthenewr
valueassociated withthenewnode.
Asanexample, theresults ofrunswithj=4,k=8andnoise densitiesf
between0.01and0.10,using 10000samples ateachiteration, areshownin
gure 47.10. Runs withlowenough noiselevelshowacollapse tozeroentropy
afterasmall numberofiterations, andthose withhighnoise leveldecrease to
anon-zero entropycorresp onding toafailure todecode.
Theboundary betweenthese twobehaviours iscalled thethreshold ofthe
decodingalgorithm forthebinary symmetric channel. Figure 47.10 showsby
MonteCarlo simulation thatthethreshold forregular (j;k)=(4;8)codes
isabout0.075. Richardson andUrbank e(2001a) havederivedthresholds for
regular codesbyatourdeforce ofdirect analytic metho ds.Some ofthese
thresholds areshownintable 47.13.(j;k)fmax
(3,6) 0.084
(4,8) 0.076
(5,10) 0.068
Table47.13.Thresholds fmaxfor
regular low{densit yparity{check
codes,assuming sum{pro duct
decodingalgorithm, from
Richardson andUrbank e(2001a).
TheShannon limitforrate-1/2
codesisfmax=0:11.Approximate density evolution
Forpractical purposes,thecomputational costofdensit yevolution canbe
reduced bymaking Gaussian approximations totheprobabilit ydistributions
overthemessages indensit yevolution, andupdating onlytheparameters of
these approximations. Forfurther information aboutthese techniques, which
producediagrams knownasEXIT charts,see(tenBrink, 1999; Chungetal.,
2001; tenBrink etal.,2002).
47.6 Impro vingGallager codes
Since theredisco veryofGallager codes,twometho dshavebeenfound for
enhancing theirperformance.GF(4)$binary
0$00
1$01
A$10
B$11
Table47.14.Translation between
binary andGF(4)formessage
symbols.Clump bitsandcheckstogether
First, wecanmakeGallager codesinwhichthevariable nodesaregroup ed
together intometavariables consisting ofsay3binary variables, andthecheck
nodesaresimilarly group edtogether intometac hecks.Asbefore, asparse
graph canbeconstructed connecting metavariables tometac hecks,withalot
offreedom aboutthedetails ofhowthevariables andcheckswithin arewired
up.Onewaytosetthewiring istoworkinanite eldGF(q)suchasGF(4)
orGF(8),dene low-densit yparity-checkmatrices using elemen tsofGF(q),
andtranslate ourbinary messages intoGF(q)using amapping suchasthe
oneforGF(4)givenintable 47.14. Now,when messages arepassed during
decoding, those messages areprobabilities andlikelihoodsoverconjunctions
ofbinary variables. Forexample ifeachclump containsthree binary variables
thenthelikelihoodswilldescrib ethelikelihoodsoftheeightalternativ estates
ofthose bits.
With carefully optimized constructions, theresulting codesoverGF(4),GF(4)!binary
0!00
00
1!10
01
A!11
10
B!01
11
Table47.15.Translation between
binary andGF(4)formatrix
entries. AnMNparity-check
matrix overGF(4)canbeturned
intoa2M2Nbinary
parity-checkmatrix inthisway.

<<<PAGE 580>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
568 47|Low-Densit yParity-Chec kCodes
Algorithm 47.16.TheFourier
transform overGF(4).
TheFourier transformFofa
functionfoverGF(2)isgivenby
F0=f0+f1,F1=f0 f1.
Transforms overGF(2k)canbe
viewedasasequence ofbinary
transforms ineachofk
dimensions. Theinverse
transform isidenticaltothe
Fourier transform, except thatwe
alsodivide by2k.F0=[f0+f1]+[fA+fB]
F1=[f0 f1]+[fA fB]
FA=[f0+f1] [fA+fB]
FB=[f0 f1] [fA fB]
Figure 47.17.Comparison ofregular binary Gallager codeswithirregular codes,codesoverGF(q),
andother outstanding codesofrate1/4.Fromleft(bestperformance) toright:Irregular
low{densit yparity{checkcodeoverGF(8),blocklength 48000bits(Davey,1999); JPL
turbocode(JPL, 1996) blocklength 65536;Regular low{densit yparity{checkoverGF(16),
blocklength 24448bits(DaveyandMacKa y,1998); Irregular binary low{densit yparity{
checkcode,blocklength 16000bits(Davey,1999); Lubyetal.(1998) irregular binary
low{densit yparity{checkcode,blocklength 64000bits;JPLcodeforGalileo (in1992,
thiswasthebestknowncodeofrate1/4);Regular binary low{densit yparity{checkcode:
blocklength 40000bits(MacKa y,1999b). TheShannon limitisatabout 0:79dB.Asof
2003, evenbettersparse-graph codeshavebeenconstructed.1e-061e-050.00010.0010.010.1
-0.4 -0.2 0 0.2 0.4 0.6 0.8Empirical Bit-Error Probability
Signal to Noise ratio (dB)TurboIrreg GF(8) Reg GF(16)Luby
Irreg GF(2)Reg GF(2)
Gallileo
GF(8),andGF(16)perform nearly onedecibelbetterthancomparable binary
Gallager codes.
Thecomputational costfordecodinginGF(q)scales asqlogq,iftheap-
propriate Fourier transform isusedinthechecknodes:theupdaterulefor
thecheck-to-v ariable message,
ra
mn=X
x:xn=a
 2
4X
n02N(m)Hmn0xn0=zm3
5Y
j2N(m)nnqxj
mj; (47.15)
isaconvolution ofthequantitiesqa
mj,sothesummation canbereplaced by
aproductoftheFourier transforms ofqa
mjforj2N(m)nn,followedby
aninverseFourier transform. TheFourier transform forGF(4)isshownin
algorithm 47.16.
Makethegraphirregular
Thesecond wayofimpro vingGallager codes,introduced byLubyetal.(2001b),
istomaketheirgraphs irregular .Instead ofgiving allvariable nodesthesame
degreej,wecanhavesomevariable nodeswithdegree 2,some3,some4,and
afewwithdegree 20.Checknodescanalsobegivenunequal degrees {this
helps impro veperformance onerasure channels, butitturns outthatforthe
Gaussian channel, thebestgraphs haveregular checkconnectivit y.
Figure 47.17 illustrates thebenets oered bythese twometho dsforim-
provingGallager codes,focussing oncodesofrate1/4.Making thebinary code
irregular givesawinofabout0.4dB;switchingfromGF(2)toGF(16)gives

<<<PAGE 581>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
47.7: Fastencodingoflow-densit yparity-checkcodes 569
difference setcyclic codes
N72173273 1057 4161
M4102882244 730
K31145191 813 3431
d461018 34 66
k35917 33 65
0.00010.0010.010.11
1.522.533.54Gallager(273,82)
DSC(273,82)Figure 47.18.Analgebraically
constructed low-densit y
parity-checkcodesatisfying many
redundan tconstrain ts
outperforms anequivalentrandom
Gallager code.Thetableshows
theN,M,K,distanced,androw
weightkofsomedierence-set
cyclic codes,highligh tingthe
codesthathavelarged=N,small
k,andlargeN=M.Inthe
comparison theGallager codehad
(j;k)=(4;13),andrateidentical
totheN=273dierence-set
cyclic code.about0.6dB;andMatthew Davey'scodethatcombines boththese features {
it'sirregular overGF(8){givesawinofabout0.9dBovertheregular binary
Gallager code.
Metho dsforoptimizing theprole ofaGallager code(thatis,itsnumberof
rowsandcolumns ofeachdegree), havebeendevelopedbyRichardson etal.
(2001) andhaveledtolow{densit yparity{checkcodeswhose performance,
when decodedbythesum{pro ductalgorithm, iswithin ahair's breadth ofthe
Shannon limit.
Algebraicconstructions ofGallagercodes
Theperformance ofregular Gallager codescanbeenhanced inathird man-
ner:bydesigning thecodetohaveredundan tsparse constrain ts.There isa
dierence-set cyclic code,forexample, thathasN=273andK=191,but
thecodesatises notM=82butN,i.e.,273low-weightconstrain ts(gure
47.18). Itisimpossible tomakerandom Gallager codesthathaveanywhere
nearthismuchredundancy among theirchecks.Thedierence-set cyclic code
performs about0.7dBbetterthananequivalentrandom Gallager code.
Anopenproblem istodiscovercodessharing theremark ableproperties of
thedierence-set cyclic codesbutwithdieren tblocklengths andrates. Icall
thistasktheTanner challenge .
47.7 Fastencodingoflow-densit yparity-checkcodes
Wenowdiscuss metho dsforfastencodingoflow-densit yparity-checkcodes{
faster thanthestandard metho d,inwhichagenerator matrix Gisfound by
Gaussian elimination (atacostoforderM3)andtheneachblockisencoded
bymultiplying itbyG(atacostoforderM2).
Staircasecodes
Certain low-densit yparity-checkmatrices withMcolumns ofweight2orless
canbeencodedeasily inlinear time. Forexample, ifthematrix hasastaircase
structure asillustrated bytheright-hand sideof
H=2
666643
77775; (47.16)

<<<PAGE 582>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
570 47|Low-Densit yParity-Chec kCodes
andifthedatasareloaded intotherstKbits,thentheMparitybitsp
canbecomputed fromlefttorightinlinear time.
p1=PK
n=1H1nsn
p2=p1+PK
n=1H2nsn
p3=p2+PK
n=1H3nsn
...
pM=pM 1+PK
n=1HMnsn:(47.17)
Ifwecalltwoparts oftheHmatrix [HsjHp],wecandescrib etheencoding
operation intwosteps: rstcompute anintermediate parityvectorv=Hss;
thenpassvthrough anaccum ulator tocreate p.
Thecostofthisencodingmetho dislinear ifthesparsit yofHisexploited
when computing thesumsin(47.17).
Fastencodingofgenerallow-density parity-che ckcodes
Richardson andUrbank e(2001b) demonstrated anelegan tmetho dbywhich
theencodingcostofanylow-densit yparity-checkcodecanbereduced from
thestraigh tforwardmetho d'sM2toacostofN+g2,whereg,thegap,is
hopefully asmall constan t,andintheworstcasesscales asasmall fraction of
N.
DB
ET0
CA6
?M- M
6
?g
- N-g
@
@
@
@
@
@Figure 47.19.Theparity-check
matrix inapproximate
lower-triangular form.
Intherststep,theparity-checkmatrix isrearranged, byrow-interchange
andcolumn-in terchange, intotheapproximate lower-triangular formshownin
gure 47.19. Theoriginal matrix Hwasverysparse, sothesixmatrices A,
B,T,C,D;andEarealsoverysparse. Thematrix Tislowertriangular and
has1severywhere onthediagonal.
H="
ABT
CDE#
: (47.18)
Thesource vectorsoflengthK=N Misencodedintoatransmission
t=[s;p1;p2]asfollows.
1.Compute theuppersyndrome ofthesource vector,
zA=As: (47.19)
Thiscanbedoneinlinear time.
2.Findasetting ofthesecond paritybits,pA
2,suchthattheuppersyn-
drome iszero.
pA
2= T 1zA: (47.20)
Thisvector canbefound inlinear timebyback-substitution, i.e.,com-
puting therstbitofpA
2,thenthesecond, thenthethird, andsoforth.

<<<PAGE 583>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
47.8: Further reading 571
3.Compute thelowersyndrome ofthevector [s;0;pA
2]:
zB=Cs EpA
2: (47.21)
Thiscanbedoneinlinear time.
4.Nowwegettothecleverbit.Dene thematrix
F ET 1B+D; (47.22)
andnditsinverse,F 1.Thiscomputation needs tobedoneonceonly,
anditscostisoforderg3.ThisinverseF 1isadenseggmatrix. [IfF
isnotinvertible theneitherHisnotoffullrank, orelsefurther column
permutations ofHcanproduceanFthatisinvertible.]
Settherstparitybits,p1,to
p1= F 1zB: (47.23)
Thisoperation hasacostoforderg2.
Claim: Atthispoint,wehavefound thecorrect setting oftherstparity
bits,p1.
5.Discard thetentativeparitybitspA
2andndthenewuppersyndrome,
zC=zA+Bp1: (47.24)
Thiscanbedoneinlinear time.
6.Findasetting ofthesecond paritybits,p2,suchthattheuppersyndrome
iszero,
p2= T 1zC (47.25)
Thisvector canbefound inlinear timebyback-substitution.
47.8 Further reading
Low-densit yparity-checkcodescodeswererststudied in1962byGallager,
thenweregenerally forgotten bythecodingtheory comm unity.Tanner (1981)
generalized Gallager's workbyintroducing moregeneral constrain tnodes;the
codesthatarenowcalled turboproductcodesshould infactbecalled Tanner
productcodes,sinceTanner proposedthem, andhiscolleagues (Karplus and
Krit, 1991) implemen tedthem inhardw are.Publications onGallager codes
contributing totheir1990s rebirth include (Wibergetal.,1995; MacKa yand
Neal, 1995; MacKa yandNeal, 1996; Wiberg,1996; MacKa y,1999b; Spielman,
1996; Sipser andSpielman, 1996). Low-precision decodingalgorithms and
fastencodingalgorithms forGallager codesarediscussed in(Richardson and
Urbank e,2001a; Richardson andUrbank e,2001b). MacKa yandDavey(2000)
showedthatlow{densit yparity{checkcodescanoutperform Reed{Solomon
codes,evenontheReed{Solomon codes'home turf:highrateandshort block
lengths. Other importantpapersinclude (Lubyetal.,2001a; Lubyetal.,
2001b; Lubyetal.,1997; DaveyandMacKa y,1998; Richardson etal.,2001;
Chungetal.,2001). Useful toolsforthedesign ofirregular low{densit yparity{
checkcodesinclude (Chungetal.,1999a; Urbank e,2001).
See(Wiberg,1996; Frey,1998; McEliece etal.,1998) forfurther discussion
ofthesum-pro ductalgorithm.
Foraviewoflow{densit yparity{checkcodedecodinginterms ofgroup
theory andcodingtheory ,see(Forney,2001; Oer andSoljanin, 2000; Oer

<<<PAGE 584>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
572 47|Low-Densit yParity-Chec kCodes
andSoljanin, 2001); andforbackground reading onthistopic see(Hartmann
andRudolph, 1976; Terras, 1999). There isagrowingliterature ontheprac-
ticaldesign oflow-densit yparity-checkcodes(Mao andBanihashemi, 2000;
MaoandBanihashemi, 2001; tenBrink etal.,2002); theyarenowbeing
adopted forapplications fromharddrivestosatellite comm unications.
Forlow{densit yparity{checkcodesapplicable toquantumerror-correction,
seeMacKa yetal.(2003).
47.9 Exercises
Exercise 47.1.[2]The`hyperbolictangent' version ofthedecodingalgorithm.
Insection 47.3, thesum{pro ductdecodingalgorithm forlow{densit y
parity{checkcodeswaspresen tedrstinterms ofquantitiesq0=1
mnand
r0=1
mn,theninterms ofquantitiesqandr.There isathirddescription,
inwhichthefqgarereplaced bylogprobabilit y-ratios,
lmnlnq0
mn
q1mn: (47.26)
Showthat
qmnq0
mn q1
mn=tanh(lmn=2): (47.27)
Derivetheupdaterulesforfrgandflg.
Exercise 47.2.[2,p.572]Iamsometimes asked`whynotdecodeother linear
codes,forexample algebraic codes,bytransforming theirparity-check
matrices sothattheyarelow-densit y,andapplying thesum{pro duct
algorithm?' [Recall thatanylinear combination ofrowsofH,H0=PH,
isavalidparity-checkmatrix foracode,aslongasthematrix Pis
invertible; sothere aremanyparitycheckmatrices foranyonecode.]
Explain whyarandom linear codedoesnothavealow-densit yparity-
checkmatrix. [Here, low-densit ymeans `havingrow-weightatmostk',
wherekissome small constan tN.]
Exercise 47.3.[3]Showthatifalow-densit yparity-checkcodehasmore than
Mcolumns ofweight2{sayMcolumns, where>1{thenthecode
willhavewordswithweightoforder logM.
Exercise 47.4.[5]Insection 13.5wefound theexpected valueoftheweight
enumerator functionA(w),averaging overtheensem bleofallrandom
linear codes.Thiscalculation canalsobecarried outfortheensem bleof
low-densit yparity-checkcodes(Gallager, 1963; MacKa y,1999b; Litsyn
andShevelev,2002). Itisplausible, however,thatthemean valueof
A(w)isnotalwaysagoodindicator ofthetypicalvalueofA(w)inthe
ensem ble.Forexample, if,ataparticular valueofw,99%ofcodeshave
A(w)=0,and1%haveA(w)=100000,thenwhile wemightsaythe
typical valueofA(w)iszero,themean isfound tobe1000. Findthe
typicalweightenumerator function oflow-densit yparity-checkcodes.
47.10 Solutions
Solution toexercise 47.2(p.572).Consider codesofrateRandblocklength
N,havingK=RNsource bitsandM=(1 R)Nparity-checkbits.Letall

<<<PAGE 585>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
47.10: Solutions 573
thecodeshavetheirbitsordered sothattherstKbitsareindependen t,so
thatwecould ifwewishputthecodeinsystematic form,
G=[1KjPT];H=[Pj1M]: (47.28)
Thenumberofdistinct linear codesisthenumberofmatrices P,whichis
N1=2MK=2N2R(1 R).Canthese allbeexpressed asdistinct low{densit ylogN1'N2R(1 R)
parity{checkcodes?
Thenumberoflow-densit yparity-checkmatrices withrow-weightkis
 
N
k!M
(47.29)
andthenumberofdistinct codesthattheydene isatmost
N2= 
N
k!M,
M!; (47.30)
whichismuchsmaller thanN1,so,bythepigeon-hole principle, itisnot logN2<NklogN
possible foreveryrandom linear codetomapontoalow-densit yH.

<<<PAGE 586>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
48
Convolutional CodesandTurboCodes
Thischapter followstightlyonfromChapter 25.Itmakesuseoftheideas of
codesandtrellises andtheforward{bac kwardalgorithm.
48.1 Introduction toconvolutional codes
When westudied linear blockcodes,wedescrib edthem inthree ways:
1.Thegenerator matrix describ eshowtoturnastring ofKarbitrary
source bitsintoatransmission ofNbits.
2.Theparity-checkmatrix species theM=N Kparity-checkcon-
straintsthatavalidcodewordsatises.
3.Thetrellis ofthecodedescrib esitsvalidcodewordsinterms ofpaths
through atrellis withlabellededges.
Afourth wayofdescribing some blockcodes,thealgebraic approac h,isnot
coveredinthisbook(a)because ithasbeenwellcoveredbynumerous other
booksincodingtheory; (b)because, asthispartofthebookdiscusses, the
state oftheartinerror-correcting codesmakeslittleuseofalgebraic coding
theory; and(c)because Iamnotcompetenttoteachthissubject.
Wewillnowdescrib econvolutional codesintwoways:rst,interms of
mechanisms forgenerating transmissions tfromsource bitss;andsecond, in
terms oftrellises thatdescrib etheconstrain tssatised byvalidtransmissions.
48.2 Linear feedbac kshiftregisters
Wegenerate atransmission withaconvolutional codebyputting asource
stream through alinear lter. Thisltermakesuseofashiftregister, linear
output functions, and,possibly ,linear feedbac k.
Iwilldrawtheshiftregister inaright-to-left orientation: bitsrollfrom
righttoleftastimegoeson.
Figure 48.1showsthree linear feedbac kshiftregisters whichcould be
usedtodene convolutional codes.Therectangular boxsurrounding thebits
z1:::z7indicate thememory ofthelter, alsoknownasitsstate.Allthree
lters haveoneinput andtwooutputs. Oneachclockcycle, thesource sup-
pliesonebit,andthelteroutputs twobitst(a)andt(b).Byconcatenating
together thesebitswecanobtain fromoursource streams1s2s3:::atransmis-
sionstreamt(a)
1t(b)
1t(a)
2t(b)
2t(a)
3t(b)
3::::Because therearetwotransmitted bitsfor
everysource bit,thecodesshowningure 48.1haverate1/2.Because these
574

<<<PAGE 587>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
48.2: Linear feedbac kshiftregisters 575
Octal name
(a)z0hds`-t(a)
z1hdz2hdz3hdz4hdz5hdz6hdz7
?-t(b) ?-?- ?-?--(1;353)8
(b)z0
6-t(b)
hds z1
6-
hdz2
6-
hdz3hdz4hdz5
6-
hdz6hdz7-
?-t(a) ?- ?-?-?--(247;371)8
(c)z0
6-t(b)
hd
s`-t(a)z1
6-
hdz2
6-
hdz3hdz4hdz5
6-
hdz6hdz7-
6
?- ?-?-?--  
1;247
371
8Figure 48.1.Linear feedbac kshift
registers forgenerating
convolutional codeswithrate1=2.
Thesymbolhdindicates a
copyingwithadelayofoneclock
cycle. Thesymboldenotes
linear addition modulo2withno
delay.
lters requirek=7bitsofmemory ,thecodestheydene areknownasa
constraint-length 7codes.
Convolutional codescome inthree avours, corresp onding tothethree
typesoflteringure 48.1.
Systematic nonrecursive
Theltershowningure 48.1a hasnofeedbac k.Italsohasthepropertythat
oneoftheoutput bits,t(a),isidenticaltothesource bits.Thisencoderis
thuscalled systematic ,because thesource bitsarereproduced transparen tly
inthetransmitted stream, andnonrecursiv e,because ithasnofeedbac k.The
other transmitted bitt(b)isalinear function ofthestate ofthelter. One
wayofdescribing thatfunction isasadotproduct(modulo2)betweentwo
binary vectors oflengthk+1:abinary vectorg(b)=(1;1;1;0;1;0;1;1)and
thestate vectorz=(zk;zk 1;:::;z1;z0).Weinclude inthestate vector the
bitz0thatwillbeputintotherstbitofthememory onthenextcycle. The
vectorg(b)hasg(b)
=1foreverywhere there isatap(adownwardpointing
arrow)fromstatebitzintothetransmitted bitt(b).
Aconvenientwaytodescrib ethese binary tapvectors isinoctal.Thus,
thisltermakesuseofthetapvector3538.Ihavedrawnthedelaylinesfrom11101011
###
353
Table48.2.Howtapsinthedelay
lineareconverted tooctal.
righttolefttomakeiteasytorelate thediagrams tothese octalnumbers.
Nonsystematic nonrecursive
Theltershowningure 48.1b alsohasnofeedbac k,butitisnotsystematic.
Itmakesuseoftwotapvectorsg(a)andg(b)tocreate itstwotransmitted bits.
Thisencoderisthusnonsystematic andnonrecursive .Because oftheiradded
complexit y,nonsystematic codescanhaveerror-correcting abilities superiorto
those ofsystematic nonrecursiv ecodeswiththesame constrain tlength.

<<<PAGE 588>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
576 48|Convolutional CodesandTurboCodes
Systematic recursive
Theltershowningure 48.1c issimilar tothenonsystematic nonrecursiv e
ltershowningure 48.1b, butitusesthetapsthatformerly made upg(a)
tomakealinear signal thatisfedbackintotheshiftregister along withthe
source bit.Theoutputt(b)isalinear function ofthestate vector asbefore.
Theother output ist(a)=s,sothislterissystematic.
Arecursiv ecodeisconventionally identied byanoctalratio, e.g.,g-
ure48.1c's codeisdenoted by(247=371)8.
z0
6-t(b)
hds
pz1hdz2-
?-t(a) ?--
(a)(5;7)8
z0
6-t(b)
hd
s`-
pt(a)z1hdz2-
6
?--
(b)(5=7)8
Figure 48.3.Tworate1/2
convolutional codeswith
constrain tlengthk=2:
(a)non-recursiv e;(b)recursiv e.
Thetwocodesareequivalent.Equivalenc eofsystematic recursive andnonsystematic nonrecursive codes
Thetwolters ingure 48.1b,c arecode-equiv alentinthatthesetsofcode-
wordsthattheydene areidentical. Foreverycodewordofthenonsystematic
nonrecursiv ecodewecanchooseasource stream fortheother encodersuch
thatitsoutput isidentical(andviceversa).
Toprovethis,wedenote bypthequantityPk
=1g(a)
z,asshowning-
ure48.3a andb,whichshowsapairofsmaller butotherwise equivalentlters.
Ifthetwotransmissions aretobeequivalent{thatis,thet(a)sareequal in
bothgures andsoarethet(b)s{thenoneverycycle thesource bitinthe
systematic codemustbes=t(a).Sonowwemustsimply conrm thatfor
thischoice ofs,thesystematic code'sshiftregister willfollowthesame state
sequence asthatofthenonsystematic code,assuming thatthestates match
initially .Ingure 48.3a wehave
t(a)=pznonrecursiv e
0 (48.1)
whereas isgure 48.3b wehave
zrecursiv e
0 =t(a)p: (48.2)
Substituting fort(a),andusingpp=0weimmediately nd
zrecursiv e
0 =znonrecursiv e
0: (48.3)
Thus,anycodewordofanonsystematic nonrecursiv ecodeisacodewordof
asystematic recursiv ecodewiththesame taps{thesame tapsinthesense
thatthere arevertical arrowsinallthesame places ingure 48.3(a) and(b),
though oneofthearrowspointsupinstead ofdownin(b).
Now,while these twocodesareequivalent,thetwoencodersbehavedif-
ferently.Thenonrecursiv eencoderhasaniteimpulse response,thatis,if
oneputsinastring thatisallzeroesexcept forasingle one,theresulting
output stream containsanite numberofones. Once theonebithaspassed
through allthestates ofthememory ,thedelaylinereturns totheall-zero
state. Figure 48.4a showsthestatesequence resulting fromthesource string
s=(0,0,1,0,0,0,0,0).
Figure 48.4b showsthetrellis oftherecursiv ecodeofgure 48.3b andthe
responseofthisltertothesame source strings=(0,0,1,0,0,0,0,0).The
lterhasaninnite impulse response.Theresponsesettles intoaperiodic
statewithperiodequal tothree clockcycles.
.Exercise 48.1.[1]What istheinput totherecursiv eltersuchthatitsstate
sequence andthetransmission arethesameasthose ofthenonrecursiv e
lter? (Hint:seegure 48.5.)

<<<PAGE 589>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
48.2: Linear feedbac kshiftregisters 577
(a)00011011
0000111011000000 transmit
source0 0 1 0 0 0 0 0
(b)00011011
0000110101000101 transmit
source0 0 1 0 0 0 0 0Figure 48.4.Trellises oftherate
1/2convolutional codesof
gure 48.3.Itisassumed thatthe
initial stateofthelteris
(z2;z1)=(0;0).Time isonthe
horizon talaxisandthestateof
thelterateachtimestepisthe
vertical coordinate. Ontheline
segmen tsareshowntheemitted
symbolst(a)andt(b),withstars
for`1'andboxesfor`0'.The
paths takenthrough thetrellises
when thesource sequence is
00100000 arehighligh tedwitha
solidline.Thelightdotted lines
showthestatetrajectories that
arepossible forother source
sequences.
00011011
0000111011000000 transmit
source0 0 1 1 1 0 0 0Figure 48.5.Thesource sequence
forthesystematic recursiv ecode
(00111000 )produces thesame
paththrough thetrellis as
(00100000 )doesinthe
nonsystematic nonrecursiv ecase.

<<<PAGE 590>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
578 48|Convolutional CodesandTurboCodes
z0
6-t(b)
hd
s`-t(a)z1hdz2hdz3hdz4-
6
?-?-?--(21=37)8
0000000100100011010001010110011110001001101010111100110111101111
0110101000011110 receivedFigure 48.6.Thetrellis forak=4
codepaintedwiththelikelihood
function when thereceiv edvector
isequal toacodewordwithjust
onebitipped.There arethree
linestyles,depending onthevalue
ofthelikelihood:thicksolidlines
showtheedges inthetrellis that
matchthecorresp onding twobits
ofthereceiv edstring exactly;
thickdotted linesshowedges that
matchonebitbutmismatc hthe
other; andthindotted linesshow
theedges thatmismatc hboth
bits.
Ingeneral alinear feedbac kshiftregister withkbitsofmemory hasanimpulse
responsethatisperiodicwithaperiodthatisatmost2k 1,corresp onding
totheltervisiting everynon-zero stateinitsstatespace.
Inciden tally,cheappseudorandom numbergenerators andcheapcrypto-
graphic products makeuseofexactly these periodicsequences, though with
larger values ofkthan7;therandom numberseedorcryptographic keyse-
lectstheinitial stateofthememory .There isthusacloseconnection between
certain cryptanalysis problems andthedecodingofconvolutional codes.
48.3 Decodingconvolutional codes
Thereceiverreceivesabitstream, andwishes toinferthestate sequence
andthence thesource stream. Theposterior probabilit yofeachbitcanbe
found bythesum{pro ductalgorithm (alsoknownastheforward{bac kwardor
BCJR algorithm), whichwasintroduced insection 25.3. Themostprobable
state sequence canbefound using themin{sum algorithm ofsection 25.3
(alsoknownastheViterbi algorithm). Thenature ofthistaskisillustrated
ingure 48.6, whichshowsthecostassociated witheachedgeinthetrellis
forthecaseofasixteen-state code;thechannel isassumed tobeabinary
symmetric channel andthereceivedvectorisequal toacodewordexcept that
onebithasbeenipped.There arethree linestyles,depending onthevalue
ofthelikelihood:thicksolidlinesshowtheedges inthetrellis thatmatchthe
corresp onding twobitsofthereceivedstring exactly; thickdotted linesshow
edges thatmatchonebitbutmismatc htheother; andthindotted linesshow
theedges thatmismatc hbothbits. Themin{sum algorithm seeks thepath
through thetrellis thatusesasmanysolidlinesaspossible; more precisely ,it
minimizes thecostofthepath, where thecostiszeroforasolidline,onefor
athickdotted line,andtwoforathindotted line.
.Exercise 48.2.[1,p.581]Canyouspotthemostprobable pathandtheipped
bit?

<<<PAGE 591>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
48.4: Turbocodes 579
0000000100100011010001010110011110001001101010111100110111101111
1110101000011110 transmit
source1 1 1 1 0 0 1 1
0000000100100011010001010110011110001001101010111100110111101111
1110101000011101 transmit
source1 1 1 1 0 0 1 0Figure 48.7.Twopaths thatdier
intwotransmitted bitsonly.
0000000100100011010001010110011110001001101010111100110111101111Figure 48.8.Aterminated trellis.
Unequalprotection
Adefect oftheconvolutional codespresen tedthusfaristhattheyoerun-
equal protection tothesource bits.Figure 48.7showstwopaths through the
trellis thatdier inonlytwotransmitted bits.Thelastsource bitislesswell
protected thantheother source bits.Thisunequal protection ofbitsmotivates
thetermination ofthetrellis.
Aterminated trellis isshowningure 48.8. Termination slightlyreduces
thenumberofsource bitsusedpercodeword.Here, foursource bitsareturned
intoparitybitsbecause thek=4memory bitsmustbereturned tozero.
48.4 Turbocodes
An(N;K)turbocodeisdened byanumberofconstituen tconvolutional
encoders(often, two)andanequal numberofinterleaverswhichareKK
permutation matrices. Without lossofgeneralit y,wetaketherstinterleaver
tobetheidentitymatrix. Astring ofKsource bitsisencodedbyfeeding themC1
C2
--
---
Figure 48.10.Theencoderofa
turbocode.EachboxC1,C2,
containsaconvolutional code.
Thesource bitsarereordered
using apermutationbeforethey
arefedtoC2.Thetransmitted
codewordisobtained by
concatenating orinterleavingthe
outputs ofthetwoconvolutional
codes.
intoeachconstituen tencoderintheorder dened bytheassociated interleaver,
andtransmitting thebitsthatcome outofeachconstituen tencoder.Often
therstconstituen tencoderischosen tobeasystematic encoder,justlikethe
recursiv eltershowningure 48.6,andthesecond isanon-systematic oneof
rate1thatemits paritybitsonly.Thetransmitted codewordthenconsists of

<<<PAGE 592>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
580 48|Convolutional CodesandTurboCodes
Figure 48.9.Rate-1/3 (a)andrate-1/2 (b)turbocodesrepresen tedasfactor graphs. Thecircles
represen tthecodewordbits.Thetworectangles represen ttrellises ofrate1/2convolutional
codes,withthesystematic bitsoccupyingthelefthalfoftherectangle andtheparitybits
occupyingtherighthalf.Thepuncturing ofthese constituen tcodesintherate1/2turbo
codeisrepresen tedbythelackofconnections tohalfoftheparitybitsineachtrellis.(a) (b)
Ksource bitsfollowedbyM1paritybitsgenerated bytherstconvolutional
codeandM2paritybitsfromthesecond. Theresulting turbocodehasrate
1/3.
Theturbocodecanberepresen tedbyafactor graph inwhichthetwo
trellises arerepresen tedbytwolargerectangular nodes(gure 48.9a); theK
source bitsandtherstM1paritybitsparticipate inthersttrellis andtheK
source bitsandthelastM2paritybitsparticipate inthesecond trellis. Each
codewordbitparticipates ineither oneortwotrellises, depending onwhether
itisaparitybitorasource bit.Eachtrellis nodecontainsatrellis exactly like
theterminated trellis showningure 48.8,except onethousand times aslong.
[There areother factor graph represen tations forturbocodesthatmakeuse
ofmoreelemen tarynodes,butthefactor graph givenhereyields thestandard
version ofthesum{pro ductalgorithm usedforturbocodes.]
Ifaturbocodeofsmaller ratesuchas1/2isrequired, astandard modica-
tiontotherate-1/3codeistopuncture some oftheparitybits(gure 48.9b).
Turbocodesaredecodedusing thesum{pro ductalgorithm describ edin
Chapter 26.Ontherstiteration, eachtrellis receivesthechannel likelihoods,
andrunstheforward{bac kwardalgorithm tocompute, foreachbit,therelativ e
likelihoodofitsbeing1or0,giventheinformation abouttheother bits.
These likelihoodsarethenpassed across fromeachtrellis totheother, and
multiplied bythechannel likelihoodsontheway.Wearethenready forthe
second iteration: theforward{bac kwardalgorithm isrunagain ineachtrellis
using theupdated probabilities. After abouttenortwentysuchiterations, it's
hopedthatthecorrect decodingwillbefound. Itiscommon practice tostop
aftersome xednumberofiterations, butwecandobetter.
Asastopping criterion, thefollowingprocedure canbeusedateveryiter-
ation. Foreachtime-step ineachtrellis, weidentifythemostprobable edge,
according tothelocalmessages. Ifthese mostprobable edges joinupintotwo
validpaths, oneineachtrellis, andifthese twopaths areconsisten twitheach
other, itisreasonable tostop, assubsequen titerations areunlikelytotake
thedecoderawayfromthiscodeword.Ifamaxim umnumberofiterations is
reachedwithout thisstopping criterion beingsatised, adecodingerror can
bereported. Thisstopping procedure isrecommended forseveralreasons: it
allowsabigsavingindecodingtimewithnolossinerrorprobabilit y;itallows
decodingfailures thataredetected bythedecodertobesoidentied{knowing
thataparticular blockisdenitely corrupted issurely useful information for
thereceiver!Andwhen wedistinguish betweendetected andundetected er-
rors,theundetected errors givehelpful insigh tsintothelowweightcodewords

<<<PAGE 593>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
48.5: Parity-checkmatrices ofconvolutional codesandturbocodes 581
ofthecode,whichmayimpro vetheprocessofcodedesign.
Turbocodesasdescrib edherehaveexcellen tperformance downtodecoded
errorprobabilities ofabout10 5,butrandomly constructed turbocodestend
tohaveanerroroorstarting atthatlevel.Thiserrorooriscaused bylow-
weightcodewords. Toreduce theheightoftheerror oor,onecanattempt
tomodifytherandom construction toincrease theweightofthese low-weight
codewords. Thetweaking ofturbocodesisablackart,anditneversucceeds
intotalling eliminating low-weightcodewords;more precisely ,thelow-weight
codewordscanonlybeeliminated bysacricing theturbocode'sexcellen t
performance. Incontrast, low-densit yparity-checkcodesrarely haveerror
oors.
48.5 Parity-checkmatrices ofconvolutional codesandturbocodes(a)
(b)
Figure 48.11.Schematic pictures
oftheparity-checkmatrices of(a)
aconvolutional code,rate1/2,
and(b)aturbocode,rate1/3.
Notation: Adiagonal line
represen tsanidentitymatrix. A
band ofdiagonal linesrepresen ta
band ofdiagonal 1s.Acircle
inside asquare represen tsthe
random permutation ofallthe
columns inthatsquare. Anumber
inside asquare represen tsthe
numberofrandom permutation
matrices superposedinthat
square. Horizon talandvertical
linesindicate theboundaries of
theblockswithin thematrix.Weclosebydiscussing theparity-checkmatrix ofarate-1/2convolutional code
viewedasalinear blockcode.Weadopt theconventionthattheNbitsofone
blockaremade upoftheN=2bitst(a)followedbytheN=2bitst(b).
.Exercise 48.3.[2]Provethataconvolutional codehasalow-densit yparity-
checkmatrix asshownschematically ingure 48.11a.
Hint: It'seasiest togure outtheparityconstrain tssatised byaconvo-
lutional codebythinking aboutthenonsystematic nonrecursiv eencoder
(gure 48.1b). Consider putting through lteraastream that's been
through convolutional lterb,andviceversa;compare thetworesulting
streams. Ignore termination ofthetrellises.
Theparity-checkmatrix ofaturbocodecanbewritten downbylisting the
constrain tssatised bythetwoconstituen ttrellises (gure 48.11b). Soturbo
codesarealsospecialcases oflow-densit yparity-checkcodes.Ifaturbocode
ispunctured, itnolonger necessarily hasalow-densit yparity-checkmatrix,
butitalwayshasageneralized parity-checkmatrix thatissparse, asexplained
inthenextchapter.
Further reading
Forfurther reading aboutconvolutional codes,Johannesson andZigangiro v
(1999) ishighly recommended. Onetopic Iwouldhavelikedtoinclude is
sequen tialdecoding.Sequen tialdecodingexplores onlythemost promising
paths inthetrellis, andbacktrackswhen evidence accum ulates thatawrong
turning hasbeentaken.Sequen tialdecodingisusedwhen thetrellis istoo
bigforustobeabletoapply themaxim umlikelihoodalgorithm, themin{
sumalgorithm. Youcanreadaboutsequen tialdecodinginJohannesson and
Zigangiro v(1999).
Forfurther information abouttheuseofthesum{pro ductalgorithm in
turbocodes,andtherarely-used buthighly recommended stopping criteria for
halting theirdecodingalgorithm Frey(1998) ishighly recommended. (And
there's lotsmore goodstuinthesame book!)
48.6 Solutions
Solution toexercise 48.2(p.578).Therstbitwasipped.Themostprobable
pathistheupperoneingure 48.7.

<<<PAGE 594>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
49
Repeat{Accum ulateCodes
InChapter 1wediscussed averysimple andnotveryeectiv emetho dfor
comm unicating overanoisy channel: therepetition code.Wenowdiscuss a
codethatisalmost assimple, andwhose performance isoutstandingly good.
Repeat{accum ulate codeswerestudied byDivsalar etal.(1998) fortheo-
retical purposes,assimple turbo-likecodesthatmightbemore amenable to
analysis thanmessy turbocodes.Their practical performance turned outto
bejustasgoodasother sparse graph codes.
49.1 Theencoder
1.TakeKsource bits.
s1s2s3:::sK
2.Repeateachbitthree times, givingN=3Kbits.
s1s1s1s2s2s2s3s3s3:::sKsKsK
3.PermutetheseNbitsusing arandom permutation (axedrandom
permutation {thesameoneforeverycodeword). Callthepermuted
stringu.
u1u2u3u4u5u6u7u8u9:::uN
4.Transmit theaccum ulated sum.
t1=u1
t2=t1+u2(mod2)
:::tn=tn 1+un(mod2)::: (49.1)
tN=tN 1+uN(mod2):
5.That's it!
49.2 Graph
Figure 49.1a showsthegraph ofarepeat{accum ulate code,using fourtypes
ofnode:equalit yconstrain ts,intermediate binary variables (blackcircles),
parityconstrain ts,andthetransmitted bits(white circles).
Thesource setsthevaluesoftheblackbitsatthebottom, three atatime,
andtheaccum ulator computes thetransmitted bitsalong thetop.
582

<<<PAGE 595>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
49.3: Decoding 583
(a)
(b)11
00
11
00
11
00
11
00
11
00
1
0Figure 49.1.Factor graphs fora
repeat{accum ulatecodewithrate
1/3.(a)Using elemen tarynodes.
Eachwhite circle represen tsa
transmitted bit.Each
constrain tforces thesumofthe3
bitstowhichitisconnected tobe
even.Eachblackcircle represen ts
anintermediate binary variable.
Eachconstrain tforces thethree
variables towhichitisconnected
tobeequal.
(b)Factor graph normally used
fordecoding. Thetoprectangle
represen tsthetrellis ofthe
accum ulator, shownintheinset.
1e-050.00010.0010.010.11
1 2 3 4 5N=204
408
816
30009999N=30000total
undetectedFigure 49.2.Performance ofsix
rate-1/3repeat{accum ulatecodes
ontheGaussian channel. The
blocklengths range fromN=204
toN=30000.Vertical axis:
blockerrorprobabilit y;horizon tal
axis:Eb=N0.Thedotted lines
showthefrequency ofundetected
errors.
Thisgraph isafactor graph fortheprior probabilit yovercodewords,
withthecircles beingbinary variable nodes,andthesquares represen ting
twotypesoffactor nodes.Asusual, eachcontributes afactor oftheform [Px=0mod2];eachcontributes afactor oftheform
 [x1=x2=x3].
49.3 Decoding
Therepeat{accum ulatecodeisnormally decodedusing thesum{pro ductalgo-
rithm onthefactor graph depicted ingure 49.1b. Thetopboxrepresen tsthe
trellis oftheaccum ulator, including thechannel likelihoods.Inthersthalf
ofeachiteration, thetoptrellis receiveslikelihoodsforeverytransition inthe
trellis, andrunstheforward{bac kwardalgorithm soastoproducelikelihoods
foreachvariable node.Inthesecond halfoftheiteration, these likelihoods
aremultipled together atthe nodestoproducenewlikelihoodmessages to
sendbacktothetrellis.
AswithGallager codesandturbocodes,thestop-when-it's-done decoding
metho dcanbeapplied, soitispossible todistinguish betweenundetected
errors (whicharecaused bylow-weightcodewordsinthecode)anddetected
errors (where thedecodergetsstuckandknowsthatithasfailed tonda
validanswer).
Figure 49.3showstheperformance ofsixrandomly constructed repeat{
accum ulate codesontheGaussian channel. Ifonedoesnotmind theerror
oorwhichkicksinataboutablockerrorprobabilit yof10 4,theperformance
isstaggeringly goodforsuchasimple code(c.f.gure 47.17).

<<<PAGE 596>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
584 49|Repeat{Accum ulate Codes
1e-050.00010.0010.010.11
0.30.40.50.60.70.80.911.1total
detected
undetected
0200400600800100012001400160018002000
020406080100120140160180050010001500200025003000
020406080100120140160180
(a) (ii.b)Eb=N0=0:749dB (ii.c)Eb=N0=0:846dB
1101001000
10 20 304050607080901001101001000
10 20 30 405060708090100
(iii.b) (iii.c)
Figure 49.3.Histograms ofnumber
ofiterations tondavalid
decodingforarepeat{accum ulate
codewithsource blocklength
K=10000andtransmitted block
lengthN=30000.(a)Block
errorprobabilit yversussignal to
noiseratiofortheRAcode.(ii.b)
Histogram forx==0:89,
Eb=N0=0:749dB.(ii.c)
x==0:90,Eb=N0=0:846dB.
(iii.b, iii.c)Fitsofpowerlawsto
(ii.b) (1=6)and(ii.c)(1=9).49.4 Empirical distribution ofdecodingtimes
Itisinteresting tostudy thenumberofiterationsofthesum{pro ductalgo-
rithm required todecodeasparse graph code.Givenonecodeandasetof
channel conditions thedecodingtimevariesrandomly fromtrialtotrial. We
ndthatthehistogram ofdecodingtimes followsapowerlaw,P()/ p,
forlarge.Thepowerpdependsonthesignal tonoise ratioandbecomes
smaller (sothatthedistribution ismore heavy-tailed) asthesignal tonoise
ratio decreases. Wehaveobserv edpowerlawsinrepeat{accum ulate codes
andinirregular andregular Gallager codes.Figures 49.3(ii) and(iii)showthe
distribution ofdecodingtimes ofarepeat{accum ulate codeattwodieren t
signal-to-noise ratios. Thepowerlawsextend overseveralorders ofmagnitude.
Exercise 49.1.[5]Investigate these powerlaws.Doesdensit yevolution predict
them? Canthedesign ofacodebeusedtomanipulate thepowerlawin
auseful way?
49.5 Generalized parity-checkmatrices
Indthatitishelpful when relating sparse graph codestoeachother touse
acommon represen tation forthem all.Forney (2001) introduced theideaof
anormal graph inwhichtheonlynodesare and andallvariable nodes
havedegree oneortwo;variable nodeswithdegree twocanberepresen tedon
edges thatconnect anodetoanode.Thegeneralized parity-checkmatrix
isagraphical wayofrepresen tingnormal graphs. Inaparity-checkmatrix,
thecolumns aretransmitted bits,andtherowsarelinear constrain ts.Ina
generalized parity-checkmatrix, additional columns maybeincluded, which
represen tstatevariables thatarenottransmitted. Onewayofthinking ofthese
statevariables isthattheyarepunctured fromthecodebeforetransmission.
State variables areindicated byahorizon tallineabovethecorresp onding
columns. Theother pieces ofdiagramatic notation forgeneralized parity-check

<<<PAGE 597>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
49.5: Generalized parity-checkmatrices 585
GT= H=
fA;pg=Figure 49.4.Thegenerator
matrix, parity-checkmatrix, anda
generalized parity-checkmatrix of
arepetition codewithrate1/3.
matrices are,asin(MacKa y,1999b; MacKa yetal.,1998):
Adiagonal lineinasquare indicates thatthatpartofthematrix contains
anidentitymatrix.
Twoormoreparallel diagonal linesindicate aband-diagonal matrix with
acorresp onding numberof1sperrow.
Ahorizon talellipse withanarrowonitindicates thatthecorresp onding
columns inablockarerandomly permuted.
Avertical ellipse withanarrowonitindicates thatthecorresp onding
rowsinablockarerandomly permuted.
Anintegersurrounded byacircle represen tsthatnumberofsuperposed
random permutation matrices.
Denition. Ageneralized parity-checkmatrix isapairfA;pg,where Aisa
binary matrix andpisalistofthepunctured bits.Thematrix denes aset
ofvalidvectors x,satisfying
Ax=0; (49.2)
foreachvalidvector there isacodewordt(x)thatisobtained bypuncturing
fromxthebitsindicated byp.Foranyonecodethere aremanygeneralized
parity-checkmatrices.
Therateofacodewithgeneralized parity-checkmatrixfA;pgcanbe
estimated asfollows.IfAisLM0,andppuncturesSbitsandselectsN
bitsfortransmission (L=N+S),thentheeectiv enumberofconstrain tson
thecodeword,M,is
M=M0 S; (49.3)
thenumberofsource bitsis
K=N M=L M0; (49.4)
andtherateisgreater thanorequal to
R=1 M
N=1 M0 S
L S: (49.5)

<<<PAGE 598>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
586 49|Repeat{Accum ulate Codes
GT=3
3H=3
3Figure 49.5.Thegenerator matrix
andparity-checkmatrix ofa
systematic low-densit y
generator-matrix code.Thecode
hasrate1/3.
GT=3
3A;p=3
3Figure 49.6.Thegenerator matrix
andgeneralized parity-check
matrix ofanon-systematic
low-densit ygenerator-matrix
code.Thecodehasrate1/2.
Examples
Repetition code.Thegenerator matrix, parity-checkmatrix, andgeneralized
parity-checkmatrix ofasimple rate-1/3repetition codeareshowningure 49.4.
Systematic low-densit ygenerato r-matrix code.Inan(N;K)systematic low-
densit ygenerator-matrix code,there arenostate variables. Atransmitted
codewordtoflengthNisgivenby
t=GTs; (49.6)
where
GT="
IK
P#
; (49.7)
withIKdenoting theKKidentitymatrix, andPbeingaverysparseMK
matrix, whereM=N K.Theparity-checkmatrix ofthiscodeis
H=[PjIM]: (49.8)
Inthecaseofarate1/3code,thisparity-checkmatrix mightberepresen ted
asshowningure 49.5.
Non-systematic low-densit ygenerato r-matrix code.Inan(N;K)non-systematic
low-densit ygenerator-matrix code,atransmitted codewordtoflengthNis
givenby
t=GTs; (49.9)
whereGTisaverysparseNKmatrix. Thegeneralized parity-checkmatrix
ofthiscodeis
A=h
GTjINi
; (49.10)
andthecorresp onding generalized parity-checkequation is
Ax=0;where x="
s
t#
. (49.11)
Whereas theparity-checkmatrix ofthissimple codeistypically acom-
plex,dense matrix, thegeneralized parity-checkmatrix retains theunderlying
simplicit yofthecode.
Inthecaseofarate-1/2code,thisgeneralized parity-checkmatrix might
berepresen tedasshowningure 49.6.
Low-densit yparity-check codesandlinearMNcodes.Theparity-checkmatrix33
(a) (b)
Figure 49.7.Thegeneralized
parity-checkmatrices of(a)a
rate-1/3Gallager codewithM=2
columns ofweight2;(b)arate-1/2
linear MNcode.
ofarate-1/3 low-densit yparity-checkcodeisshowningure 49.7a.

<<<PAGE 599>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
49.5: Generalized parity-checkmatrices 587
Alinear MNcodeisanon-systematic low-densit yparity-checkcode.The
Kstate bitsofanMNcodearethesource bits. Figure 49.7b showsthe
generalized parity-checkmatrix ofarate-1=2linear MNcode.
Convolutional codes.Inanon-systematic, non-recursiv econvolutional code, (a)
(b)
Figure 49.8.Thegeneralized
parity-checkmatrices of(a)a
convolutional codewithrate1/2.
(b)arate-1/3turbocodebuiltby
parallel concatenation oftwo
convolutional codes.thesource bits,whichplaytheroleofstatebits,arefedintoadelay-line and
twolinear functions ofthedelay-line aretransmitted. Ingure 49.8a, these
twoparitystreams areshownastwosuccessiv evectors oflengthK.[Itis
common tointerleavethese twoparitystreams, abit-reordering thatisnot
relevanthere,andisnotillustrated.]
Concatenation. `Parallel concatenation' oftwocodesisrepresen tedinoneof
these diagrams byaligning thematrices oftwocodesinsuchawaythatthe
`source bits'lineup,andbyadding blocksofzero-en triestothematrix such
thatthestatebitsandparitybitsofthetwocodesoccupyseparate columns.
Anexample isgivenbytheturbocodebelow.In`serial concatenation', the
columns corresp onding tothetransmitte dbitsoftherstcodearealigned
withthecolumns corresp onding tothesourcebitsofthesecond code.
Turbocodes.Aturbocodeistheparallel concatenation oftwoconvolutional
codes.Thegeneralized parity-checkmatrix ofarate-1/3 turbocodeisshown
ingure 49.8b.
Repeat{accumulate codes.Thegeneralized parity-checkmatrices ofarate-1/3
repeat{accum ulatecodeisshowningure 49.9. Repeat-accum ulatecodesare
equivalenttostaircase codes(section 47.7,p.569).
Figure 49.9.Thegeneralized
parity-checkmatrix ofa
repeat{accum ulatecodewithrate
1/3.Intersection. Thegeneralized parity-checkmatrix oftheintersection oftwo
codesismade bystackingtheirgeneralized parity-checkmatrices ontopof
eachother insuchawaythatallthetransmitted bits'columns arecorrectly
aligned, andanypunctured bitsassociated withthetwocomponentcodes
occupyseparate columns.

<<<PAGE 600>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
AboutChapter 50
Thefollowingexercise providesahelpful background fordigital fountaincodes.
.Exercise 50.1.[3]Anauthor proofreads hisK=700-page bookbyinspecting
random pages. HemakesNpage-insp ections, anddoesnottakeany
precautions toavoidinspecting thesame pagetwice.
(a)AfterN=Kpage-insp ections, whatfraction ofpages doyouexpect
haveneverbeeninspected?
(b)AfterN>Kpage-insp ections, whatistheprobabilit ythatoneor
more pages haveneverbeeninspected?
(c)Showthatinorder fortheprobabilit ythatallKpages havebeen
inspected tobe1 ,werequireN'Kln(K=)page-insp ections.
[This problem iscommonly presen tedinterms ofthrowingNballsat
random intoKbins;what's theprobabilit ythateverybinhasatleast
oneball?]
588

<<<PAGE 601>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
50
Digital FountainCodes
Digital fountaincodesarerecord-breaking sparse-graph codesforchannels
witherasures.
Channels witherasures areofgreat importance. Forexample, lessent
overtheinternet arechoppedintopackets,andeachpacketiseither received
without errorornotreceived.Asimple channel modeldescribing thissituation
isaq-aryerasure channel, whichhas(forallinputs intheinput alphab et
f0;1;2;:::;q 1g)aprobabilit y1 foftransmitting theinput without error,
andprobabilit yfofdeliveringtheoutput `?'.Thealphab etsizeqis2l,where
listhenumberofbitsinapacket.
Common metho dsforcomm unicating oversuchchannels emplo yafeed-
backchannel fromreceivertosender thatisusedtocontroltheretransmission
oferased packets.Forexample, thereceivermightsendbackmessages that
identifythemissing packets,whicharethenretransmitted. Alternativ ely,the
receivermightsendbackmessages thatacknowledge eachreceivedpacket;the
sender keepstrackofwhichpacketshavebeenacknowledged andretransmits
theothers untilallpacketshavebeenacknowledged.
These simple retransmission protocolshavetheadvantagethattheywill
workregardless oftheerasure probabilit yf,butpurists whohavelearned their
Shannon theory willfeelthatthese retransmission protocolsarewasteful. If
theerasure probabilit yfislarge, thenumberoffeedbac kmessages sentby
therstprotocolwillbelarge. Under thesecond protocol,it'slikelythatthe
receiverwillendupreceiving multiple redundan tcopies ofsome packets,and
heavyuseismade ofthefeedbac kchannel. According toShannon, there isno
needforthefeedbac kchannel: thecapacit yoftheforwardchannel is(1 f)l
bits,whether ornotwehavefeedbac k.
Thewastefulness ofthesimple retransmission protocolsisespecially evi-
dentinthecaseofabroadcast channel witherasures {channels where one
sender broadcasts tomanyreceivers,andeachreceiverreceivesarandom
fraction (1 f)ofthepackets.Ifeverypacketthatismissed byoneormore
receivershastoberetransmitted, those retransmissions willbeterribly re-
dundan t.Everyreceiverwillhavealready receivedmostoftheretransmitted
packets.
So,wewouldliketomakeerasure-correcting codesthatrequire nofeed-
backoralmost nofeedbac k.Theclassic blockcodesforerasure correction are
called Reed{Solomon codes.An(N;K)Reed{Solomon code(overanalpha-
betofsizeq=2l)hastheidealpropertythatifanyKoftheNtransmitted
symbolsarereceivedthentheoriginalKsource symbolscanberecovered.
[SeeBerlek amp(1968) orLinandCostello (1983) forfurther information;
Reed{Solomon codesexistforN<q.]ButReed{Solomon codeshavethe
disadv antagethattheyarepractical onlyforsmallK,N,andq:standard im-
589

<<<PAGE 602>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
590 50|Digital FountainCodes
plemen tations ofencodinganddecodinghaveacostoforderK(N K)log2N
packetoperations. Furthermore, withaReed{Solomon code,aswithanyblock
code,onemustestimate theerasure probabilit yfandchoosethecoderate
R=K=Nbeforetransmission. Ifweareunluckyandfislarger thanexpected
andthereceiverreceivesfewerthanKsymbols,whatarewetodo?We'dlike
asimple waytoextend thecodeontheytocreate alower-rate (N0;K)code.
ForReed{Solomon codes,nosuchon-the-y metho dexists.
There isabetterway,pioneered byMichaelLuby(2002) athiscompan y
Digital Fountain,therstcompan ywhose business isbased onsparse graph
codes.
Thedigital fountaincodesIdescrib ehere, LTcodes,wereinventedby
Lubyin1998. Theideaofadigital fountaincodeisasfollows.Theencoderis LTstands for`Lubytransform'.
afountainthatproduces anendless supply ofwaterdrops (encodedpackets);
let'ssaytheoriginal source lehasasizeofKlbits,andeachdropcontains
lencodedbits. Now,anyonewhowishes toreceivetheencodedleholds a
bucketunder thefountainandcollects drops untilthenumberofdrops inthe
bucketisalittlelarger thanK.They canthenrecovertheoriginal le.
Digital fountaincodesarerateless inthesense thatthenumberofencoded
packetsthatcanbegenerated fromthesource message ispotentially limitless;
andthenumberofencodedpacketsgenerated canbedetermined onthey.
Regardless ofthestatistics oftheerasure eventsonthechannel, wecansend
asmanyencodedpacketsasareneeded inorder forthedecodertorecover
thesource data. Thesource datacanbedecodedfromanysetofK0encoded
packets,forK0slightlylarger thanK(inpractice, about5%larger).
Digital fountaincodesalsohavefantastically small encodinganddecod-
ingcomplexities. With probabilit y1 ,Kpacketscanbecomm unicated
withaverage encodinganddecodingcosts bothoforderKln(K=)packet
operations.
Lubycallsthese codesuniversal because theyaresimultaneously near-
optimal foreveryerasure channel, andtheyareveryecien tasthelelength
Kgrows.TheoverheadK0 Kisoforderp
K(ln(K=))2.
50.1 Adigital fountain's encoder
Eachencodedpackettnisproduced fromthesource les1s2s3:::sKas
follows:
1.Randomly choosethedegreednofthepacketfromadegree distri-
bution(d);theappropriate choice ofdependsonthesource le
sizeK,aswe'lldiscuss later.
2.Choose,uniformly atrandom,dndistinct input packets,andsettn
equal tothebitwisesum,modulo2ofthosednpackets.Thissum
canbedonebysuccessiv elyexclusiv e-or-ing thepacketstogether.
Thisencodingoperation denes agraph connecting encodedpacketsto
source packets.Ifthemean degree dissignican tlysmaller thanKthenthe
graph issparse. Wecanthink oftheresulting codeasanirregular low-densit y
generator-matrix code.
Thedecoderneeds toknowthedegree ofeachpacketthatisreceived,and
whichsource packetsitisconnected tointhegraph. Thisinformation can
becomm unicated tothedecoderinvarious ways.Forexample, ifthesender
andreceiverhavesynchronized clocks,theycould useidenticalpseudo-random

<<<PAGE 603>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
50.2: Thedecoder 591
numbergenerators, seeded bytheclock,tochooseeachrandom degree and
eachsetofconnections. Alternativ ely,thesender could pickarandom key,
n,givenwhichthedegree andtheconnections aredetermined byapseudo-
random process,andsendthatkeyintheheader ofthepacket.Aslongasthe
packetsizelismuchbigger thanthekeysize(whichneedonlybe32bitsor
so),thiskeyintroduces onlyasmall overhead cost.
50.2 Thedecoder
Decodingasparse graph codeisespecially easyinthecaseofanerasure
channel. Thedecoder's taskistorecoversfromt=Gs,where Gisthe
matrix associated withthegraph. Thesimple waytoattempt tosolvethis
problem isbymessage-passing. Wecanthink ofthedecodingalgorithm as
thesum{pro ductalgorithm ifwewish, butallmessages areeither completely
uncertain messages orcompletely certain messages. Uncertain messages assert
thatamessage packetskcould haveanyvalue,withequal probabilit y;certain
messages assert thatskhasaparticular value,withprobablit yone.s1
011
1011 11
1
10
111101
1c)
d)
e)
f)a)
b)
01
01s2s3
Figure 50.1.Example decodingfor
adigital fountaincodewith
K=3source bitsandN=4
encodedbits.Thissimplicit yofthemessages allowsasimple description ofthedecoding
process.We'llcalltheencodedpacketsftngchecknodes.
1.Findachecknodetnthatisconnected toonlyonesource packet
sk.(Ifthere isnosuchchecknode,thisdecodingalgorithm haltsat
thispoint,andfailstorecoverallthesource packets.)
(a)Setsk=tn.
(b)Addsktoallcheckstn0thatareconnected tosk:
tn0:=tn0+skforalln0suchthatGn0k=1. (50.1)
(c)Removealltheedges connected tothesource packetsk.
2.Repeat(1)untilallfskgaredetermined.
Thisdecodingprocessisillustrated ingure 50.1foratoycasewhere each
packetisjustonebit.There arethree source packets(shownbytheupper
circles) andfourreceivedpackets(shownbythelowerchecksymbols),which
havethevaluest1t2t3t4=1011atthestartofthealgorithm.
Attherstiteration, theonlychecknodethatisconnected toasolesource
bitistherstchecknode(panel a).Wesetthatsource bits1accordingly
(panel b),discard thechecknode,thenaddthevalueofs1(1)tothechecksto
whichitisconnected (panel c),disconnecting s1fromthegraph. Atthestart
ofthesecond iteration (panel c),thefourth checknodeisconnected toasole
source bit,s2.Wesets2tot4(0,inpanel d),andadds2tothetwochecks
itisconnected to(panel e).Finally ,wendthattwochecknodesareboth
connected tos3,andtheyagree aboutthevalueofs3(aswewouldhope!),
whichisrestored inpanel f.
50.3 Designing thedegree distribution
Theprobabilit ydistribution (d)ofthedegree isacritical partofthedesign:
occasional encodedpacketsmusthavehighdegree (i.e.,dsimilar toK)in
order toensure thatthere arenotsome source packetsthatareconnected to
no-one. Manypacketsmusthavelowdegree, sothatthedecodingprocess

<<<PAGE 604>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
592 50|Digital FountainCodes
cangetstarted, andkeepgoing, andsothatthetotal numberofaddition
operations involvedintheencodinganddecodingiskeptsmall. Foragiven
degree distribution (d),thestatistics ofthedecodingprocesscanbepredicted
byanappropriate version ofdensit yevolution.
00.10.20.30.40.5
01020304050rho
tau
Figure 50.2.Thedistributions
(d)and(d)forthecase
K=10000,c=0:2,=0:05,
whichgivesS=244,K=S=41,
andZ'1:3.Thedistribution is
largest atd=1andd=K=S.Ideally ,toavoidredundancy ,we'dlikethereceivedgraph tohavetheprop-
ertythatjustonechecknodehasdegree oneateachiteration. Ateachitera-
tion,when thischecknodeisprocessed, thedegrees inthegraph arereduced
insuchawaythatonenewdegree-one checknodeappears.Inexpectation ,
thisidealbehaviour isachievedbytheidealsoliton distribution ,
(1)=1=K
(d)=1
d(d 1)ford=2;3;:::;K.(50.2)
Theexpected degree under thisdistribution isroughly lnK.
.Exercise 50.2.[2]Derivetheideal soliton distribution. Attherstiteration
(t=0)letthenumberofpacketsofdegreedbeh0(d);showthat(for
d>1)theexpected numberofpacketsofdegreedthathavetheirdegree
reduced tod 1ish0(d)d=K;andatthetthiteration, whentofthe
Kpacketshavebeenrecoveredandthenumberofpacketsofdegreed
isht(d),theexpected numberofpacketsofdegreedthathavetheir
degree reduced tod 1isht(d)d=K t.Hence showthatinorder to
havetheexpected numberofpacketsofdegree 1satisfyht(1)=1forall
t2f0;:::K 1g,wemusttostartwithhaveh0(1)=1andh0(2)=K=2;
andmoregenerally ,ht(2)=(K t)=2;thenbyrecursion solveforh0(d)
ford=3upwards.
Thisdegree distribution workspoorlyinpractice, because uctuations
around theexpected behaviour makeitverylikelythatatsome pointinthe
decodingprocessthere willbenodegree-one checknodes;and,furthermore, a
fewsource nodeswillreceivenoconnections atall.Asmall modication xes
these problems.
Therobust soliton distribution hastwoextra parameters, cand;itis
designed toensure thattheexpected numberofdegree-one checksisabout
Scln(K=)p
K; (50.3)
rather than1,throughout thedecodingprocess.Theparameterisabound
ontheprobabilit ythatthedecodingfailstoruntocompletion afteracertain
numberK0ofpacketshavebeenreceived.Theparametercisaconstan tof
order 1,ifouraimistoproveLuby'smaintheorem aboutLTcodes;inpractice
howeveritcanbeviewedasafreeparameter, withavaluesomewhat smaller
than1giving goodresults. Wedene apositivefunction
(d)=8
>><
>>:S
K1
dford=1;2;:::(K=S) 1
S
Kln(S=)ford=K=S
0 ford>K=S(50.4)
(seegure 50.2andexercise 50.4(p.594))thenaddtheidealsoliton distribu-
tiontoandnormalize toobtain therobust soliton distribution, :
(d)=(d)+(d)
Z; (50.5)
whereZ=P
d(d)+(d):Thenumberofencodedpacketsrequired atthe
receiving endtoensure thatthedecodingcanruntocompletion, withproba-
bilityatleast1 ,isK0=KZ.020406080100120140
0.01 0.1delta=0.01
delta=0.1
delta=0.9
100001020010400106001080011000
0.01 0.1delta=0.01
delta=0.1
delta=0.9
c
Figure 50.3.Thenumberof
degree-one checksS(uppergure)
andthequantityK0(lowergure)
asafunction ofthetwo
parameters cand,for
K=10000.Luby'smaintheorem
provesthatthere exists avalueof
csuchthat,givenK0receiv ed
packets,thedecodingalgorithm
willrecovertheKsource packets
withprobabilit y1 .

<<<PAGE 605>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
50.4: Applications 593
Luby's(2002) analysis explains howthesmall-dendofhastheroleof
ensuring thatthedecodingprocessgetsstarted, andthespikeinatd=K=S
isincluded toensure thateverysource packetislikelytobeconnected toa
checkatleastonce. Luby'skeyresult isthat(foranappropriate valueofthe
10000 10500 11000 11500 1200010000 10500 11000 11500 1200010000 10500 11000 11500 12000
Figure 50.4.Histograms ofthe
actual numberofpacketsN
required inorder torecoverale
ofsizeK=10000packets.The
parameters wereasfollows:
tophistogram: c=0:01,=0:5
(S=10,K=S=1010, and
Z'1:01);
middle:c=0:03,=0:5(S=30,
K=S=337,andZ'1:03);
bottom:c=0:1,=0:5(S=99,
K=S=101,andZ'1:1).constan tc)receivingK0=K+2ln(S=)Schecksensures thatallpacketscan
berecoveredwithprobabilit yatleast1 .Intheillustrativ egures Ihave
settheallowabledecoderfailure probabilit yquite large, because theactual
failure probabilit yismuchsmaller thanissuggested byLuby'sconserv ative
analysis.
Inpractice, LTcanbetuned sothataleoforiginal sizeK'10000pack-
etsisrecoveredwithanoverhead ofabout5%.Figure 50.4showshistograms
oftheactual numberofpacketsrequired foracouple ofsettings oftheparam-
eters, achieving mean overheads smaller than5%and10%respectively.
50.4 Applications
Digital fountaincodesareanexcellen tsolution inawidevarietyofsituations.
Let'smentiontwo.
Storage
Youwishtomakeabackupofalargele,butyouareawarethatyourmagnetic
tapesandharddrivesareallunreliable inthesense thatcatastrophic failures,
inwhichsomestored packetsarepermanen tlylostwithin onedevice, occurat
arateofsomething like10 3perday.Howshould youstoreyourle?
Adigital fountaincanbeusedtosprayencodedpacketsallovertheplace,
oneverystorage device available. Then torecoverthebackuple,whose size
wasKpackets,onesimply needs tondK0'Kpacketsfromanywhere.
Corrupted packetsdonotmatter; wesimply skipoverthem andndmore
packetselsewhere.
Thismetho dofstorage alsohasadvantages interms ofspeedoflere-
covery.Inaharddrive,itisstandard practice tostore aleinsuccessiv e
sectors ofaharddrive,toallowrapid reading ofthele;butif,asoccasion-
allyhappens,apacketislost(owingtothereading headbeingotrackfor
amomen t,giving aburst oferrors thatcannot becorrected bythepacket's
error-correcting code),awhole revolution ofthedrivemustbeperformed to
bring backthepackettotheheadforasecond read. Thetimetakenforone
revolution produces anundesirable delayinthelesystem.
Ifleswereinstead stored using thedigital fountainprinciple, withthe
digital drops stored inoneormore consecutiv esectors onthedrive,thenone
wouldneverneedtoendure thedelayofre-reading apacket;packetlosswould
become lessimportant,andtheharddrivecould consequen tlybeoperated
faster, withhigher noise level,andwithfewerresources devotedtonoisy-
channel coding.
.Exercise 50.3.[2]Compare thedigital fountainmetho dofrobust storage on
multiple harddriveswithRAID (theredundan tarrayofindependen t
disks).
Broadcast
Imagine thattenthousand subscrib ersinanareawishtoreceiveadigital
moviefromabroadcaster. Thebroadcaster cansendthemovieinpackets

<<<PAGE 606>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
594 50|Digital FountainCodes
overabroadcast network{forexample, byawide-bandwidth phone line,or
bysatellite.
Imagine thatnotallpacketsarereceivedatallthehouses. Let's say
f=0:1%ofthem arelostateachhouse. Inastandard approac hinwhichthe
leistransmitted asaplainsequence ofpacketswithnoencoding,eachhouse
wouldhavetonotify thebroadcaster ofthefKmissing packets,andrequest
thattheyberetransmitted. Andwithtenthousand subscrib ersallrequesting
suchretransmissions, therewouldbearetransmission request foralmost every
packet.Thusthebroadcaster wouldhavetorepeattheentirebroadcast twice
inorder toensure thatmostsubscrib ershavereceivedthewhole movie,and
mostusers wouldhavetowaitroughly twiceaslongastheidealtimebefore
thedownload wascomplete.
Ifthebroadcaster usesadigital fountaintoencodethemovie,eachsub-
scribercanrecoverthemoviefromanyK0'Kpackets.Sothebroadcast
needs tolastforonly,say,1.1Kpackets,andeveryhouse isverylikelytohave
successfully recoveredthewhole le.
Another application isbroadcasting datatocars.Imagine thatwewantto
sendupdates toin-car navigation databases bysatellite. There arehundreds
ofthousands ofvehicles, andtheycanonlyreceivedatawhen theyareout
ontheopenroad; there arenofeedbac kchannels. Astandard metho dfor
sending thedataistoputitinacarousel ,broadcasting thepacketsinaxed
periodicsequence. `Yes,acarmaygothrough atunnel, andmissoutona
fewhundred packets,butitwillbeabletocollect those missed packetsan
hourlaterwhen thecarousel hasgonethrough afullrevolution (wehope);or
maybethefollowingday:::'
Ifinstead thesatellite usesadigital fountain,eachcarneeds toreceive
onlyanamoun tofdataequal totheoriginal lesize(plus5%).
Further reading
TheencodersanddecoderssoldbyDigital Fountainhaveevenhigher eciency
thantheLTcodesdescrib edhere, andtheyworkwellforallblocklengths,
notonlylargelengths suchasK>10000.Shokrollahi (2003) presen tsRaptor
codes,whichareanextension ofLTcodeswithlinear timeencodingand
decoding.
50.5 Further exercises
.Exercise 50.4.[2]Understanding therobust soliton distribution .
Repeattheanalysis ofexercise 50.2(p.592)butnowaimtohavethe
expected numberofpacketsofdegree 1beht(1)=1+Sforallt,instead
of1.Showthattheinitial required numberofpacketsis
h0(d)=K
d(d 1)+S
dford>1. (50.6)
Thereason fortruncating thesecond termbeyondd=K=Sandreplac-
ingitbythespikeatd=K=S(seeequation (50.4)) istoensure that
thedecodingcomplexit ydoesnotgrowlarger thanO(KlnK).
Estimate theexpected numberofpacketsP
dh0(d)andtheexpected
numberofedges inthesparse graphP
dh0(d)d(whichdetermines the
decodingcomplexit y)ifthehistogram ofpacketsisasgivenin(50.6).
Compare withtheexpected numbersofpacketsandedges when the
robust soliton distribution (50.4) isused.

<<<PAGE 607>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
50.5: Further exercises 595
Exercise 50.5.[4]Showthatthespikeatd=K=S(equation (50.4)) isanade-
quate replacemen tforthetailofhigh-w eightpacketsin(50.6).
Exercise 50.6.[3C]Investigate experimen tallyhownecessary thespikeatd=
K=S(equation (50.4)) isforsuccessful decoding. Investigate alsowhether
thetailof(d)beyondd=K=Sisnecessary .What happensifallhigh-
weightdegrees areremoved,boththespikeatd=K=Sandthetailof
(d)beyondd=K=S?
Exercise 50.7.[4]Fillinthedetails intheproofofLuby'smain theorem, that
receivingK0=K+2ln(S=)Schecksensures thatallthesource packets
canberecoveredwithprobabilit yatleast1 .
Exercise 50.8.[4C]Optimize thedegree distribution ofadigital fountaincode
foraleofK=10000packets.Pickasensible objectivefunction for
youroptimization, suchasminimizing themean ofN,thenumberof
packetsrequired forcomplete decoding, orthe95thpercentileofthe
histogram ofN(gure 50.4).
.Exercise 50.9.[3]Makeamodelofthesituation where adatastream isbroad-
casttocars,andquantifytheadvantagethatthedigital fountainhas
overthecarousel metho d.
Exercise 50.10.[2]Construct asimple example toillustrate thefactthatthe
digital fountaindecoderofsection 50.2issuboptimal {itsometimes
givesupeventhough theinformation available issucien ttodecode
thewhole le.Howdoesthecostoftheoptimal decodercompare?
.Exercise 50.11.[2]Ifeverytransmitted packetwerecreated byadding together
source packetsatrandom withprobabilit y1/2ofeachsource packet's
beingincluded, showthattheprobabilit ythatK0=Kreceivedpackets
suce fortheoptimal decodertobeabletorecovertheKsource packets
isjustalittlebelow1=2.[Toputitanother way,whatistheprobabilit y
thatarandomKKmatrix hasfullrank?]
ShowthatifK0=K+packetsarereceived,theprobabilit ythatthey
willnotsuce fortheoptimal decoderisroughly 2 .
.Exercise 50.12.[3C]Implemen tanoptimal digital fountaindecoderthatuses
themetho dofRichardson andUrbank e(2001b) derivedforfastencod-
ingofsparse graph codes(section 47.7) tohandle thematrix inversion
required foroptimal decoding. Nowthatyouhavechanged thedecoder,
youcanreoptimize thedegree distribution, using higher weightpackets.
Byhowmuchcanyoureduce theoverhead? Conrm theassertion that
thisapproac hmakesdigital fountaincodesviable aserasure-correcting
codesforallblocklengths, notjustthelargeblocklengths forwhichLT
codesareexcellen t.
.Exercise 50.13.[5]Digital fountaincodesareexcellen trateless codesforerasure
channels. Makearateless codeforachannel thathasbotherasures and
noise.

<<<PAGE 608>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
596 50|Digital FountainCodes
50.6 Summary ofsparse graph codes
Asimple metho dfordesigning error-correcting codesfornoisy channels, rst
pioneered byGallager (1962), hasrecentlybeenredisco veredandgeneralized,
andcomm unication theory hasbeentransformed. Thepractical performance
ofGallager's low-densit yparity-checkcodesandtheirmoderncousins isvastly
betterthantheperformance ofthecodeswithwhichtextbookshavebeenlled
intheintervening years.
Whichsparse graph codeis`best'foranoisy channel dependsonthe
chosen rateandblocklength, thepermitted encodinganddecodingcomplex-
ity,andthequestion ofwhether occasional undetected errors areacceptable.
Low{densit yparity{checkcodesarethemost versatile; it'seasytomakea
competitivelow{densit yparity{checkcodewithalmost anyrateandblock-
length, andlow{densit yparity{checkcodesvirtually nevermakeundetected
errors.
Forthespecialcaseoftheerasure channel, thesparse graph codesthatare
bestaredigital fountaincodes.
50.7 Conclusion
Thebestsolution tothecomm unication problem is:
Combineasimple, pseudo-random code
withamessage-passing decoder.

<<<PAGE 609>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
PartVII
Appendices

<<<PAGE 610>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
A
Notation
What doesP(AjB;C)mean?P(AjB;C)ispronounced `theprobabilit y
thatAistruegiventhatBistrueandCistrue'. Or,morebriey ,`the
probabilit yofAgivenBandC'.
What dologandlnmean? Inthisbook,logxmeans thebase-t wologa-
rithm, log2x;lnxmeans thenatural logarithm, logex.
What does^smean? Usually ,a`hat'overavariable denotes aguess ores-
timator. So^sisaguess atthevalueofs.
Integrals .There isnodierence betweenRf(u)duandRduf(u).Theinte-
grand isf(u)inbothcases.
What doesNY
n=1mean? ThisislikethesummationPN
n=1butitdenotes a
product. It'spronounced `productovernfrom1toN'.So,forexample,
NY
n=1n=123N=N!=exp"NX
n=1lnn#
: (A.1)
Iliketochoosethename ofthefreevariable inasumoraproduct{
here,n{tobethelowercaseversion oftherange ofthesum. Son
usually runsfrom1toN,andmusually runsfrom1toM.Thisisa
habit IlearntfromYaserAbu-Mostafa, andIthink itmakesformulae
easier tounderstand.
What does 
N
n!
mean? Thisispronounced `Nchoosen',anditisthe
numberofwaysofselecting anunordered setofnobjectsfromasetof
sizeN.  
N
n!
=N!
(N n)!n!: (A.2)
Thisfunction isknownasthecombination function.
What is (x)?Thegamma function isdened by (x)R1
0duux 1e u,
forx>0.Thegamma function isanextension ofthefactorial function
torealnumberargumen ts.Ingeneral,  (x+1)=x (x),andforinteger
argumen ts, (x+1)=x!.Thedigamma function isdened by	(x)
d
dxlog (x).
Forlargex(forpractical purposes,0:1x1),
log (x)'
x 1
2
log(x) x+1
2log2+O(1=x); (A.3)
598

<<<PAGE 611>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
A|Notation 599
andforsmallx(forpractical purposes,0x0:5):
log (x)'log1
x ex+O(x2) (A.4)
whereeisEuler's constan t.
What doesH 1
2(1 R=C)mean? Justassin 1(s)denotes theinversefunc-
tiontos=sin(x),soH 1
2(h)istheinversefunction toh=H2(x).
There ispotentialconfusion when people usesin2xtodenote (sinx)2,
since thenwemightexpectsin 1stodenote 1=sin(s);Itherefore like
toavoidusing thenotation sin2x.
What doesf0(x)mean? Theanswerdependsonthecontext. Often, a
`prime' isusedtodenote dieren tiation:
f0(x)d
dxf(x); (A.5)
similarly ,adotdenotes dieren tiation withrespecttotime,t:
_xd
dtx: (A.6)
However,theprime isalsoauseful indicator for`another variable', for
example `anewvalueforavariable'. So,forexample,x0mightdenote
`thenewvalueofx'.Also, ifthere aretwointegers thatbothrange from
1toN,Iwilloften name those integersnandn0.
Somyruleis:ifaprime occurs inanexpression thatcould beafunc-
tion,suchasf0(x)orh0(y),thenitdenotes dieren tiation; otherwise it
indicates `another variable'.
What istheerror function? Denitions ofthisfunction vary.Idene it
tobethecumulativeprobabilit yofastandard (variance =1)normal
distribution,
(z)Zz
 1exp( z2=2)=p
2dz: (A.7)
What doesE(r)mean?E[r]ispronounced `theexpected valueofr'or`the
expectation ofr',anditisthemean valueofr.Another symbolfor
`expected value'isthepairofangle-brac kets,hri:
What doesjxjmean? Thevertical bars`jj'havetwomeanings. IfAisa
set,thenjAjdenotes thenumberofelemen tsintheset;ifxisanumber,
thenjxjistheabsolute valueofx.
What does[AjP]mean? Here,AandParematrices withthesame num-
berofrows.[AjP]denotes thedouble-width matrix obtained byputting
Aalongside P.Thevertical barisusedtoavoidconfusion withthe
productAP.
What doesxTmean? Thesuperscript Tispronounced `transp ose'.Trans-
posing arow-vector turns itintoacolumn vector:
(1;2;3)T=0
B@1
2
31
CA; (A.8)
andviceversa.[Normally myvectors, indicated byboldfacetype(x),
arecolumn vectors.]
Similarly ,matrices canbetransp osed. IfMijistheentryinrowiand
columnjofmatrix M,andN=MT,thenNji=Mij.

<<<PAGE 612>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
600 A|Notation
What areTraceManddetM?Thetrace ofamatrix isthesumofitsdi-
agonal elemen ts,
TraceM=X
iMii: (A.9)
Thedeterminan tofMisdenoted detM.
What doesmnmean? Thematrix istheidentitymatrix.
mn=(
1ifm=n
0ifm6=n.
Another name fortheidentitymatrix isIor1.Sometimes Iinclude a
subscript onthissymbol{1K{whichindicates thesizeofthematrix
(KK).
What does(x)mean? Thedelta function hastheproperty
Z
dxf(x)(x)=f(0): (A.10)
Another possible meaning for(S)isthetruth function, whichis1ifthepropo-
sition SistruebutIhaveadopted another notation forthat. After all,the
symbolisquite busyalready ,withthetworolesmentioned aboveinaddition
toitsroleasasmall realnumberandanincremen toperator (asinx)!
What does
 [S]mean?
 [S]isthetruth function, whichis1ifthepropo-
sitionSistrueand0otherwise. Forexample, thenumberofpositive
numbersinthesetT=f 2;1;3gcanbewritten
X
x2T
 [x>0]: (A.11)
What isthedierence between`:='and`='?Inanalgorithm, x:=y
means thatthevariablexisupdated byassigning itthevalueofy.
Incontrast,x=yisaproposition, astatemen tthatxisequal toy.
SeeChapters 23and29forfurther denitions andnotation relating to
probabilit ydistributions.

<<<PAGE 613>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
B
Some Physics
B.1Aboutphase transitions
Asystem withstatesxincontactwithaheatbathattemperatureT=1=
hasprobabilit ydistribution
P(xj)=1
Z()exp( E(x)): (B.1)
Thepartition function is
Z()=X
xexp( E(x)): (B.2)
Theinversetemperaturecanbeinterpreted asdening anexchange rate
betweenentropyandenergy. (1=)istheamoun tofenergy thatmustbe
giventoaheatbathtoincrease itsentropybyonenat.
Often, thesystem willbeaected bysome other parameters suchasthe
volume oftheboxitisin,V,inwhichcaseZisafunction ofVtoo,Z(;V).
Foranysystem withanite numberofstates, thefunctionZ()isevi-
dentlyacontinuousfunction of,since itissimply asumofexponentials.
Moreo ver,allthederivativesofZ()withrespecttoarecontinuoustoo.
What phase transitions areallabout,however,isthis:phase transitions
corresp ondtovaluesofandV(called critical points)atwhichthederivatives
ofZhavediscon tinuities ordivergences.
Immediately wecandeduce:
Only systems withaninnite numberofstates canshowphase
transitions.
Often, weinclude aparameterNdescribing thesizeofthesystem. Phase
transitions mayappearinthelimitN!1.Realsystems mayhaveavalue
ofNlike1023.
Ifwemakethesystem largebysimply grouping togetherNindependen t
systems whose partition function isZ(1)(),thennothing interesting happens.
Thepartition function forNindependen tidenticalsystems issimply
Z(N)()=[Z(1)()]N: (B.3)
Now,while thisfunctionZ(N)()maybeaveryrapidly varying function of,
thatdoesn'tmean itisshowingphase transitions. Thenatural waytolookat
thepartition function isinthelogarithm
lnZ(N)()=NlnZ(1)(): (B.4)
601

<<<PAGE 614>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
602 B|Some Physics
Duplicating theoriginal systemNtimes simply scales upallproperties like
theenergy andheatcapacit yofthesystem byafactor ofN.Soiftheoriginal
system showednophase transitions thenthescaled upsystem won'thaveany
either.
Onlysystems withlong-range correlations showphase transitions.
Long-range correlations donotrequire long-range energetic couplings; for
example, amagnet hasonlyshort-range couplings (betweenadjacen tspins)
butthese aresucien ttocreate long-range order.
Whyarepointsatwhich derivatives divergeinteresting?
ThederivativesoflnZdescrib eproperties liketheheatcapacit yofthesys-
tem(that's thesecond derivative)oritsuctuations inenergy .Ifthesecond
derivativeoflnZdivergesatatemperature 1=,thentheheatcapacit yofthe
system diverges there, whichmeans itcanabsorb orrelease energy without
changing temperature (think oficemelting inicewater); when thesystem is
atequilibrium atthattemperature, itsenergy uctuates alot,incontrastto
thenormal law-of-large-n umbersbehaviour, where theenergy onlyvariesby
onepartinp
N.
Atoysystem thatshows aphase transition
Imagine acollection ofNcoupled spins thathavethefollowingenergy asa
function oftheirstatex2f0;1gN.
E(x)=(
 Nx=(0;0;0;:::;0)
0 otherwise.(B.5)
Thisenergy function describ esaground stateinwhichallthespinsarealigned
inthezerodirection; theenergy perspininthisstate is .ifanyspin
changes state thentheenergy iszero. Thismodelislikeanextreme version
ofamagnetic interaction, whichencourages pairsofspins tobealigned.
Wecancontrastitwithanordinary system ofNindependent spinswhose
energy is:
E0(x)=X
n(2xn 1): (B.6)
Liketherstsystem, thesystem ofindependen tspins hasasingle ground
state(0;0;0;:::;0)withenergy N,andithasroughly 2Nstates withenergy
verycloseto0,sothelow-temp erature andhigh-temp erature properties ofthe
independen t-spin system andthecoupled-spin system arevirtually identical.
Thepartition function ofthecoupled-spin system isgivenby
Z()=eN+2N 1: (B.7)
Thefunction
lnZ()=ln
eN+2N 1
(B.8)
issketchedingure B.1aalong withitslowtemperature behaviour,
lnZ()'N;!1; (B.9)
anditshightemperature behaviour,
lnZ()'Nln2;!0: (B.10)

<<<PAGE 615>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
B.1:Aboutphase transitions 603
(a)betalog Z
N beta epsilon
N log (2)
(c)betavar(E) N=24
var(E)  N=8
(b)betalog Z
N beta epsilon
N log (2)Figure B.1.(a)Partition function
oftoysystem whichshowsaphase
transition forlargeN.Thearrow
marks thepointc=log2=.(b)
Thesame, forlargerN.
(c)Thevariance oftheenergy of
thesystem asafunction offor
twosystem sizes. AsNincreases
thevariance hasanincreasingly
sharp peakatthecritical pointc.
Contrastwithgure B.2.
(a)betalog Z
N beta epsilon
N log (2)
(b)betavar(E) N=24
var(E)  N=8Figure B.2.Thepartition function
(a)andenergy-v ariance (b)ofa
system consisting ofN
independen tspins. Thepartition
function changes gradually from
oneasymptote totheother,
regardless ofhowlargeNis;the
variance oftheenergy doesnot
haveapeak.Theuctuations are
largest athightemperature (small
)andscalelinearly withsystem
sizeN.
Thearrowmarks thepoint
=ln2
(B.11)
atwhichthese twoasymptotes intersect. InthelimitN!1,thegraph of
lnZ()becomes more andmore sharply bentatthispoint(gure B.1b).
Thesecond derivativeoflnZ,whichdescrib esthevariance oftheenergy
ofthesystem, hasapeakvalue,at=ln2=,roughly equal to
N22
4; (B.12)
whichcorresp ondstothesystem spending halfofitstimeintheground state
andhalfitstimeintheother states.
Atthiscritical point,theheatcapacit yofthissystem isthusproportional
toN2;theheatcapacit yperspinisproportional toN,which,forinniteN,is
innite, incontrasttothebehaviour ofsystems awayfromphase transitions,
whose capacit yperatom isanite number.
Forcomparison, gure B.2showsthepartition function andenergy-v ariance
oftheordinary independen t-spin system.
Moregenerally
Phase transitions canbecategorized into`rst-order' and`continuous' transi-
tions. Inarst-order phase transition, there isadiscon tinuouschange ofone

<<<PAGE 616>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
604 B|Some Physics
ormore order-parameters; inacontinuous transition, allorder-parameters
change continuously .[What's anorder-parameter? {ascalar function ofthe
stateofthesystem; or,tobeprecise, theexpectation ofsuchafunction.]
Inthevicinit yofacritical point,theconcept of`typicalit y'dened in
Chapter 4doesnothold. Forexample, ourtoysystem, atitscritical point,
hasa50%chance ofbeinginastatewithenergy N,androughly a1=2N+1
chance ofbeingineachoftheother states thathaveenergy zero.Itisthusnot
thecasethatln1=P(x)isverylikelytobeclosetotheentropyofthesystem
atthispoint,unlikeasystem withNi.i.d.components.
Remem berthatinformation content(ln1=P(x))andenergy areveryclosely
related. Iftypicalit yholds, thenthesystem's energy hasnegligible uctua-
tions, andviceversa.

<<<PAGE 617>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
C
Some Mathematics
C.1 Finite eldtheory
Mostlinearcodesareexpressedinthelanguage ofGalois theory
WhyareGalois elds anappropriate language forlinear codes?First, ade-
nition andsome examples.
AeldFisasetF=f0;F0gsuchthat
1.Fforms anAbeliangroup under anaddition operation `+',with
0beingtheidentity;[Abelian means allelemen tscomm ute,i.e.,
satisfya+b=b+a.]
2.F0forms anAbelian group under amultiplication operation `';
multiplication ofanyelemen tby0yields 0;
3.these operations satisfy thedistributiv erule(a+b)c=ac+bc.+01
001
11001
000
101
TableC.1.Addition and
multiplication tables forGF(2).
Forexample, therealnumbersformaeld, with`+'and`'denoting
ordinary addition andmultiplication.
AGalois eldGF(q)isaeldwithanite numberofelemen tsq.
Aunique Galois eldexists foranyq=pm,wherepisaprime number
andmisapositiveinteger; there arenoother nite elds.
GF(2):Theaddition andmultiplication tables forGF(2)areshowninta-
bleC.1.These aretherulesofaddition andmultiplication modulo2.
GF(p):Foranyprime numberp,theaddition andmultiplication rules are
those forordinary addition andmultiplication, modulop.
GF(4):TherulesforGF(pm),withm>1,arenotthose ofordinary addition
andmultiplication. Forexample thetables forGF(4)(table C.2)arenot+01AB
001AB
110BA
AAB01
BBA10
01AB
00000
101AB
A0AB1
B0B1A
TableC.2.Addition and
multiplication tables forGF(4).therulesofaddition andmultiplication modulo4.Notice that1+1=0,
forexample. SohowcanGF(4)bedescrib ed?Itturns outthatthe
elemen tscanberelated topolynomials .Consider polynomial functions
ofxofdegree 1andwithcoecien tsthatareelemen tsofGF(2).The
polynomials shownintable C.3obeytheaddition andmultiplication
rulesofGF(4)ifaddition andmultiplication aremodulothepolynomial
x2+x+1,andthecoecien tsofthepolynomials arefromGF(2).For
example,BB=x2+(1+1)x+1=x=A.Eachelemen tmayalsobe
represen tedasabitpattern asshownintable C.3,withaddition being
bitwisemodulo2,andmultiplication dened withanappropriate carry
operation.Elemen tPolynomial Bitpattern
0 0 00
1 1 01
A x 10
B x+1 11
TableC.3.Represen tations ofthe
elemen tsofGF(4).
605

<<<PAGE 618>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
606 C|Some Mathematics
GF(8).Wecandenote theelemen tsofGF(8)byf0;1;A;B;C;D;E;Fg.Each
elemen tcanbemappedontoapolynomial overGF(2).Themultiplica-
tionandaddition operations aregivenbymultiplication andaddition of
thepolynomials, modulox3+x+1.Themultiplication table isgiven
below.
elemen tpolynomial binary represen tation
0 0 000
1 1 001
A x 010
Bx+1 011
C x2100
Dx2+1 101
Ex2+x 110
Fx2+x+1 11101ABCDEF
000000000
101ABCDEF
A0ACEB1FD
B0BEDFC1A
C0CBFEAD1
D0D1CAFBE
E0EF1DBAC
F0FDA1ECB
WhyareGalois elds relevanttolinear codes?Imagine generalizing abinary
generator matrix Gandbinary vectorstoamatrix andvectorwithelemen ts
fromalarger set,andgeneralizing theaddition andmultiplication operations
thatdene theproductGs.Inorder toproduceanappropriate input for
asymmetric channel, itwouldbeconvenientif,forrandom s,theproduct
Gsproduced allelemen tsintheenlarged setwithequal probabilit y.This
uniform distribution iseasiest toguaran teeifthese elemen tsformagroup
under bothaddition andmultiplication, because thenthese operations donot
break thesymmetry among theelemen ts.When tworandom elemen tsofa
multiplicativ egroup aremultiplied together, allelemen tsareproduced with
equal probabilit y.Thisisnottrueofother setssuchastheintegers, forwhich
themultiplication operation ismore likelytogiverisetosome elemen ts(the
compositenumbers)thanothers. Galois elds, bytheirdenition, avoidsuch
symmetry-breaking eects.
C.2 Eigenvectors andeigenvalues
Aright-eigen vector ofasquare matrix Aisanon-zero vectoreRthatsatises
AeR=eR; (C.1)
whereistheeigenvalueassociated withthateigenvector. Theeigenvalue
maybearealnumberorcomplex numberanditmaybezero. Eigenvectors
mayberealorcomplex.
Aleft-eigen vector ofamatrix AisavectoreLthatsatises
eT
LA=eT
L: (C.2)
Thefollowingstatemen tsforright-eigen vectors alsoapply toleft-eigen vectors.
Ifamatrix hastwoormorelinearly independen tright-eigen vectors with
thesameeigenvaluethenthateigenvalueiscalled adegenerate eigenvalue
ofthematrix, orarepeated eigenvalue. Anylinear combination ofthose
eigenvectors isanother right-eigen vector withthesame eigenvalue.
Theprincipal right-eigen vector ofamatrix is,bydenition, theright-
eigenvector withthelargest associated eigenvalue.
Ifarealmatrix hasaright-eigen vector withcomplex eigenvalue=
x+yithenitalsohasaright-eigen vector withtheconjugate eigenvalue
=x yi.

<<<PAGE 619>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
C.2:Eigenvectors andeigenvalues 607
Symmetric matric es
IfAisarealsymmetricNNmatrix then
1.alltheeigenvalues andeigenvectors ofAarereal;
2.everyleft-eigen vectorofAisalsoaright-eigen vectorofAwiththesame
eigenvalue,andviceversa;
3.asetofNeigenvectors andeigenvaluesfe(a);agN
a=1canbefound that
areorthonormal, thatis,
e(a)e(b)=ab; (C.3)
thematrix canbeexpressed asaweightedsumofouter products ofthe
eigenvectors:
A=NX
a=1a[e(a)][e(a)]T: (C.4)
(Whereas Ioften useiandnasindices forsetsofsizeIandN,Iwillusethe
indicesaandbtorunovereigenvectors, evenifthere areNofthem. Thisis
toavoidconfusion withthecomponentsoftheeigenvectors, whichareindexed
byn,e.g.e(a)
n.)
Generalsquarematric es
AnNNmatrix canhaveuptoNdistinct eigenvalues. Generically ,there
areNeigenvalues, alldistinct, andeachhasoneleft-eigen vectorandoneright-
eigenvector. Incaseswhere twoormoreeigenvaluescoincide, foreachdistinct
eigenvaluethatisnon-zero there isatleastoneleft-eigen vector andoneright-
eigenvector.
Left-andright-eigen vectors thathavedieren teigenvalueareorthogonal,
thatis,
ifa6=bthene(a)
Le(b)
R=0: (C.5)
Non-ne gative matric es
Denition. Ifalltheelemen tsofanon-zero matrix CsatisfyCmn0thenC
isanon-negativ ematrix. Similarly ,ifalltheelemen tsofanon-zero vectorc
satisfyci0thencisanon-negativ evector.
Properties. Anon-negativ ematrix hasaprincipal eigenvector thatisnon-
negativ e.Itmayalsohaveother eigenvectors withthesame eigenvaluethat
arenotnon-negativ e.Butiftheprincipal eigenvalueofanon-negativ ematrix
isnotdegenerate, thenthematrix hasonlyoneprincipal eigenvectore(1),and
itisnon-negativ e.
Generically ,alltheother eigenvalues aresmaller inabsolute magnitude.
[There canbeseveraleigenvalues ofidenticalmagnitude inspecialcases.]
Transition probability matric es
Animportantexample ofanon-negativ ematrix isatransition probabilit y
matrix Q.
Denition. Atransition probabilit ymatrix Qhascolumns thatareprobabilit y
vectors, thatis,itsatises Q0and
X
iQij=1forallj: (C.6)

<<<PAGE 620>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
608 C|Some Mathematics
Matrix Eigenvaluesandeigenvectors eL;eR
2
4120
110
0013
52:41 1 0:412
4:58
:82
03
52
4:82
:58
03
52
40
0
13
52
40
0
13
52
4 :58
:82
03
52
4 :82
:58
03
5
01
11 1:62 0:62:53
:85:53
:85:85
 :53:85
 :53
2
6641100
0001
1000
00113
7751:62 0:5+0:9i 0:5 0:9i 0:622
664:60
:37
:37
:603
7752
664:60
:37
:37
:603
7752
664:1 :5i
 :3 :4i
:3+:4i
 :1+:5i3
7752
664:1 :5i
:3+:4i
 :3 :4i
 :1+:5i3
7752
664:1+:5i
 :3+:4i
:3 :4i
 :1 :5i3
7752
664:1+:5i
:3 :4i
 :3+:4i
 :1 :5i3
7752
664:37
 :60
 :60
:373
7752
664:37
 :60
 :60
:373
775
TableC.4.Some matrices andtheireigenvectors.
TableC.5.Transition probabilit ymatrices forgenerating random paths through trellises.Matrix EigenvaluesandeigenvectorseL;eR
0:38
1:62 1 0:38:71
:71:36
:93 :93
:36 :71
:71
2
40:350
00:46
1:65:543
51 0:2 0:3i 0:2+0:3i2
4:58
:58
:583
52
4:14
:41
:903
52
4 :8+:1i
 :2 :5i
:2+:2i3
52
4:2 :5i
 :6+:2i
:4+:3i3
52
4 :8 :1i
 :2+:5i
:2 :2i3
52
4:2+:5i
 :6 :2i
:4 :3i3
5
Thispropertycanberewritten interms oftheall-ones vectorn=(1;1;:::;1)T:
nTQ=nT: (C.7)
Sonistheprincipal left-eigen vector ofQwitheigenvalue1=1.
e(1)
L=n: (C.8)
Because itisanon-negativ ematrix, Qhasaprincipal right-eigen vector that
isnon-negativ e,e(1)
R.Generically ,forMarkovprocesses thatareergodic,this
eigenvector istheonlyright-eigen vector witheigenvalueofmagnitude 1(see
tableC.6forillustrativ eexceptions). Thisvector, ifwenormalize itsuchthat
e(1)
Rn=1,iscalled theinvariantdistribution ofthetransition probabilit y
matrix. Itistheprobabilit ydensit ythatisleftunchanged underQ.Unlike
theprincipal left-eigen vector, whichweexplicitly identied above,wecan't
usually identifytheprincipal right-eigen vector without computation.
Thematrix mayhaveuptoN 1other right-eigen vectors allofwhichare
orthogonal totheleft-eigen vectorn,thatis,theyarezero-sum vectors.
C.3 Perturbation theory
Perturbation theory isnotusedinthisbook,butitisuseful inthisbook's
elds. Inthissection wederiverstorder perturbation theory fortheeigen-
vectors andeigenvalues ofsquare, notnecessarily symmetric ,matrices. Most
presen tations ofperturbation theory focusonsymmetric matrices, butnon-
symmetric matrices (suchastransition matrices) alsodeserv etobeperturb ed!

<<<PAGE 621>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
C.3:Perturbation theory 609
TableC.6.Illustrativ etransition probabilit ymatrices andtheireigenvectors showingthetwowaysof
beingnon-ergo dic.(a)More thanoneprincipal eigenvectorwitheigenvalue1because the
statespace fallsintotwounconnected pieces. (a')Asmall perturbation breaks thedegen-
eracy oftheprincipal eigenvectors. (b)Under thischain,thedensit ymayoscillate between
twoparts ofthestate space. Inaddition totheinvariantdistribution, there isanother
right-eigen vector witheigenvalue 1.Ingeneral suchcirculating densities corresp ondto
complex eigenvalueswithmagnitude 1.Matrix EigenvaluesandeigenvectorseL;eR
(a)2
664:90:2000
:10:8000
00:90:20
00:10:803
7751 1 0:70 0:702
6640
0
:71
:713
7752
6640
0
:89
:453
7752
664:71
:71
0
03
7752
664:89
:45
0
03
7752
664:45
 :89
0
03
7752
664:71
 :71
0
03
7752
6640
0
 :45
:893
7752
6640
0
 :71
:713
775
(a')2
664:90:2000
:10:79:020
0:01:88:20
00:10:803
7751 0:98 0:70 0:692
664:50
:50
:50
:503
7752
664:87
:43
:22
:113
7752
664 :18
 :15
:66
:723
7752
664 :66
 :28
:61
:333
7752
664:20
 :40
 :40
:803
7752
664:63
 :63
 :32
:323
7752
664 :19
:41
 :44
:773
7752
664 :61
:65
 :35
:303
775
(b)2
66400:90:20
00:10:80
:90:2000
:10:80003
7751 0:70 0:70 12
664:50
:50
:50
:503
7752
664:63
:32
:63
:323
7752
664 :32
:63
 :32
:633
7752
664:50
 :50
:50
 :503
7752
664:32
 :63
 :32
:633
7752
664 :50
:50
:50
 :503
7752
664:50
:50
 :50
 :503
7752
664:63
:32
 :63
 :323
775
Weassume thatwehaveanNNmatrix Hthatisafunction H()of
arealparameter,with=0beingourstarting point.Weassume thata
Taylorexpansion ofH()isappropriate:
H()=H(0)+V+ (C.9)
where
V@H
@: (C.10)
Weassume thatforallofinterest, H()hasacomplete setofNright-
eigenvectors andleft-eigen vectors, andthatthese eigenvectors andtheireigen-
valuesarecontinuousfunctions of.Thislastassumption isnotnecessarily a
goodone:ifH(0)hasdegenerate eigenvalues thenitispossible fortheeigen-
vectors tobediscon tinuousin;insuchcases, degenerate perturbation theory
isneeded. That's afuntopic, butlet'sstickwiththenon-degenerate case
here.
Wewrite theeigenvectors andeigenvalues asfollows:
H()e(a)
R()=(a)()e(a)
R(); (C.11)
andweTaylorexpand
(a)()=(a)(0)+(a)+ (C.12)
with
(a)@(a)()
@(C.13)
and
e(a)
R()=e(a)
R(0)+f(a)
R+ (C.14)

<<<PAGE 622>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
610 C|Some Mathematics
with
f(a)
R@e(a)
R
@; (C.15)
andsimilar denitions fore(a)
Landf(a)
L.Wedene these left-vectors toberow
vectors, sothatthe`transp ose'operation isnotneeded andcanbebanished.
Wearefreetoconstrain themagnitudes oftheeigenvectors inwhatev erway
weplease. Eachleft-eigen vector andeachright-eigen vector hasanarbitrary
magnitude. Thenatural constrain tstouseareasfollows.First, weconstrain
theinner products with:
e(a)
L()e(a)
R()=1;foralla: (C.16)
Expanding theeigenvectors in,equation (C.19) implies
(e(a)
L(0)+f(a)
L+)(e(a)
R(0)+f(a)
R+)=1; (C.17)
fromwhichwecanextract theterms in,whichsay:
e(a)
L(0)f(a)
R+f(a)
Le(a)
R(0)=0 (C.18)
Wearenowfreetochoosethetwoconstrain ts:
e(a)
L(0)f(a)
R=0;f(a)
Le(a)
R(0)=0; (C.19)
whichinthespecialcaseofasymmetric matrix corresp ondtoconstraining
theeigenvectors tobeofconstan tlength, asdened bytheEuclidean norm.
OK,nowthatwehavedened ourcastofcharacters, whatdothedening
equations (C.11) and(C.9) tellusaboutourTaylorexpansions (C.13) and
(C.15)? Weexpand equation (C.11) in.
(H(0)+V+)(e(a)
R(0)+f(a)
R+)=((a)(0)+(a)+)(e(a)
R(0)+f(a)
R+):
(C.20)
Identifying theterms oforder,wehave:
H(0)f(a)
R+Ve(a)
R(0)=(a)(0)f(a)
R+(a)e(a)
R(0): (C.21)
Wecanextract interesting results fromthisequation byhitting itwithe(b)
L(0):
e(b)
L(0)H(0)f(a)
R+e(b)
L(0)Ve(a)
R(0)=e(b)
L(0)(a)(0)f(a)
R+(a)e(b)
L(0)e(a)
R(0):
)(b)e(b)
L(0)f(a)
R+e(b)
L(0)Ve(a)
R(0)=(a)(0)e(b)
L(0)f(a)
R+(a)ab:(C.22)
Settingb=aweobtain
e(a)
L(0)Ve(a)
R(0)=(a): (C.23)
Alternativ ely,choosingb6=a,weobtain:
e(b)
L(0)Ve(a)
R(0)=h
(a)(0) (b)(0)i
e(b)
L(0)f(a)
R(C.24)
)e(b)
L(0)f(a)
R=1
(a)(0) (b)(0)e(b)
L(0)Ve(a)
R(0): (C.25)
Now,assuming thattheright-eigen vectorsfe(b)
R(0)gN
b=1formacomplete basis,
wemustbeabletowrite
f(a)
R=X
bwbe(b)
R(0); (C.26)

<<<PAGE 623>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
C.3:Perturbation theory 611
where
wb=e(b)
L(0)f(a)
R; (C.27)
so,comparing (C.25) and(C.27), wehave:
f(a)
R=X
b6=ae(b)
L(0)Ve(a)
R(0)
(a)(0) (b)(0)e(b)
R(0): (C.28)
Equations (C.23) and(C.28) arethesolution totherstorder perturbation
theory problem, giving respectivelytherstderivativeoftheeigenvalueand
theeigenvectors.
Second-or derperturbationtheory
Ifweexpand theeigenvectorequation (C.11) tosecond order in,andassume
thattheequation
H()=H(0)+V (C.29)
isexact, thatis,Hisapurely linear function of,thenwehave:
(H(0)+V)(e(a)
R(0)+f(a)
R+1
22g(a)
R+)
=((a)(0)+(a)+1
22(a)+)(e(a)
R(0)+f(a)
R+1
22g(a)
R+)(C.30)
whereg(a)
Rand(a)arethesecond derivativesoftheeigenvectorandeigenvalue.
Equating thesecond order terms ininequation (C.30),
Vf(a)
R+1
2H(0)g(a)
R=1
2(a)(0)g(a)
R+1
2(a)e(a)
R(0)+(a)f(a)
R(C.31)
Hitting thisequation ontheleftwithe(a)
L(0),weobtain:
e(a)
L(0)Vf(a)
R+1
2(a)e(a)
L(0)g(a)
R
=1
2(a)(0)e(a)
L(0)g(a)
R+1
2(a)e(a)
L(0)e(a)
R(0)+(a)e(a)
L(0)f(a)
R:(C.32)
Theterme(a)
L(0)f(a)
Risequal tozerobecause ofourconstrain ts(C.19), so
e(a)
L(0)Vf(a)
R=1
2(a); (C.33)
sothesecond derivativeoftheeigenvaluewithrespecttoisgivenby
1
2(a)=e(a)
L(0)VX
b6=ae(b)
L(0)Ve(a)
R(0)
(a)(0) (b)(0)e(b)
R(0) (C.34)
=X
b6=a[e(b)
L(0)Ve(a)
R(0)][e(a)
L(0)Ve(b)
R(0)]
(a)(0) (b)(0): (C.35)
Thisisasfaraswewilltaketheperturbation expansion.
Summary
Ifweintroducetheabbreviation Vbafore(b)
L(0)Ve(a)
R(0),wecanwrite the
eigenvectors ofH()=H(0)+Vtorstorder as
e(a)
R()=e(a)
R(0)+X
b6=aVba
(a)(0) (b)(0)e(b)
R(0)+ (C.36)
andtheeigenvalues tosecond order as
(a)()=(a)(0)+Vaa+2X
b6=aVbaVab
(a)(0) (b)(0)+: (C.37)

<<<PAGE 624>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
612 C|Some Mathematics
C.4 Some numbers
28192102466Numberofdistinct 1-kilob yteles
2102410308Numberofstates ofa2DIsingmodelwith3232spins
2100010301Numberofbinary strings oflength 1000
2500310150
246910141Numberofbinary strings oflength 1000having1001sand9000s
22661080Numberofelectrons inuniverse
22001:61060
21901057Numberofelectrons insolarsystem
217131051Numberofelectrons intheearth
21001030
29831029Ageofuniverse/picoseconds
25831017Ageofuniverse/seconds
2501015
2401012
1011Numberofneurons inhuman brain
1011Numberofbitsstored onaDVD
31010Numberofbitsinthewheat genome
6109Numberofbitsinthehuman genome
2326109Population ofearth
230109
2:5108Numberofbres inthecorpus callosum
2108NumberofbitsinC.Elegans(aworm)genome
2108NumberofbitsinArabidopsis thaliana (aoweringplantrelated tobroccoli) genome
2253107Oneyear/seconds
2107Numberofbitsinthecompressed PostScript lethatisthisbook
2107Numberofbitsinunixkernel
107NumberofbitsintheE.Coligenome, orinaoppydisk
4106Numberofyearssincehuman/c himpanzee divergence
2201061048576
2105Numberofgenerations sincehuman/c himpanzee divergence
3104Numberofgenes inhuman genome
3104Numberofgenes inArabidopsis thaliana genome
1:5103Numberofbasepairsinagene
210e7103210=1024;e7=1096
201001
2 22:510 1Lifetime probabilit yofdying fromsmoking onepackofcigarettes perday.
10 2Lifetime probabilit yofdying inamotor vehicle acciden t
2 1010 3
10 5Lifetime probabilit yofdeveloping cancer because ofdrinking 2litres perdayof
watercontaining 12p.p.b. benzene
2 2010 6
310 8Probabilit yoferrorintransmission ofcodingDNA, pernucleotide, pergeneration
2 3010 9
2 6010 18Probabilit yofundetected errorinaharddiskdrive,aftererrorcorrection

<<<PAGE 625>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
Bibliograph y
Abrahamsen, P.(1997) Areview ofGaussian random elds and
correlation functions. Technical Report917,Norwegian Com-
puting Center,Box114,Blindern, N-0314 Oslo, Norway.2nd
edition.
Abramson, N.(1963) Information TheoryandCoding.McGra w-
Hill.
Adler, S.L.(1981) Over-relaxation metho dfortheMonte-Carlo
evaluation ofthepartition function formultiquadratic actions.
Physic alReview D{Particles andFields 23(12):2901{2904.
Aiyer, S.V.B.(1991) Solving Combinatorial Optimization Prob-
lemsUsing NeuralNetworks .Cambridge Univ. Engineering
Departmen tPhDdissertation. CUED/F-INFENG/TR 89.
Aji,S.,Jin,H.,Khandekar, A.,McEliece, R.J.,and
MacKay,D.J.C.(2000) BSCthresholds forcodeensem bles
based on`typical pairs' decoding.InCodes,Systems andGraph-
icalModels,ed.byB.Marcus andJ.Rosen thal,volume 123of
IMAVolumes inMathematics anditsApplic ations .Springer-
Verlag.
Amari, S.,Cichocki, A.,andYang,H.H.(1996) Anewlearn-
ingalgorithm forblindsignal separation. InAdvancesinNeural
Information Processing Systems ,ed.byD.S.Touretzky ,M.C.
Mozer, andM.E.Hasselmo, volume 8,pp.757{763. MITPress.
Amit, D.J.,Gutfreund, H.,andSompolinsky, H.(1985)
Storing innite numbersofpatterns inaspinglass modelof
neural networks.Phys. Rev.Lett.55:1530.
Angel, J.R.P.,Wizino wich, P.,Lloyd-Har t,M.,andSan-
dler, D.(1990) Adaptiv eoptics forarraytelescop esusing
neural-net worktechniques. Nature348:221{224.
Bahl, L.R.,Cocke, J.,Jelinek, F.,andRaviv,J.(1974)
Optimal decodingoflinear codesforminimizing symbolerror
rate.IEEE Trans.Info.TheoryIT-20 :284{287.
Baldwin,J.(1896) Anewfactor inevolution. AmericanNatu-
ralist30:441{451.
Bar-Shalom, Y.,andFortmann, T.(1988) Tracking andData
Association .Academic Press.
Barber, D.,andWilliams, C.K.I.(1997) Gaussian processes
forBayesian classication viahybrid MonteCarlo. InNeural
Information Processing Systems 9,ed.byM.C.Mozer, M.I.
Jordan, andT.Petsche,pp.340{346. MITPress.
Barnett, S.(1979) Matrix MethodsforEngine ersandScientists .
McGra w-Hill.
Battail,G.(1993) Wecanthink ofgoodcodes,andevende-
codethem. InEurocode'92.Udine, Italy, 26-30 October,ed.
byP.Camion, P.Charpin, andS.Harari, number339inCISM
Courses andLectures, pp.353{368. Springer.
Baum,E.,Boneh, D.,andGarrett, C.(1995) Ongenetic
algorithms. InProc.Eighth Annual Conf. onComputational
Learning Theory,pp.230{239. ACM.
Baum,E.B.,andSmith, W.D.(1993) Bestplayforimperfect
playersandgame treesearch.Technical report,NEC, Prince-
ton,NJ.
Baum,E.B.,andSmith, W.D.(1997) ABayesianapproac hto
relevanceingame playing.Articial Intelligence97(1-2): 195{
242.Baum,L.E.,andPetrie, T.(1966) Statistical inference
forprobabilistic functions ofnite-state Markovchains. Ann.
Math. Stat.37:1559{1563.
Beal, M.J.,Ghahramani, Z.,andRasmussen, C.E.(2002)
Theinnite hidden Markovmodel.InAdvancesinNeuralIn-
formation Processing Systems 14.MITPress.
Bell, A.J.,andSejnowski, T.J.(1995) Aninformation maxi-
mization approac htoblindseparation andblinddeconvolution.
NeuralComputation 7(6):1129{1159.
Bentley, J.(2000) Programming Pearls.Addison-W esley,sec-
ondedition.
Berger,J.(1985) Statistic alDecision theoryandBayesian Anal-
ysis.Springer.
Berlekamp, E.R.(1968) AlgebraicCodingTheory.McGra w-
Hill.
Berlekamp, E.R.(1980) Thetechnology oferror-correcting
codes.IEEE Trans.Info.Theory68:564{593.
Berlekamp, E.R.,McEliece, R.J.,andvanTilbor g,H.
C.A.(1978) Ontheintractabilit yofcertain codingproblems.
IEEE Trans.Info.Theory24(3):384{386.
Berrou,C.,andGlavieux, A.(1996) Nearoptim umerrorcor-
recting codinganddecoding: Turbo-codes.IEEE Trans.on
Communic ations 44:1261{1271.
Berrou,C.,Glavieux, A.,andThitimajshima, P.(1993) Near
Shannon limit error-correcting codinganddecoding: Turbo-
codes.InProc.1993IEEE International Conf. onCommuni-
cations, Geneva, Switzerland ,pp.1064{1070.
Berzuini, C.,Best, N.G.,Gilks, W.R.,andLarizza, C.
(1997) Dynamic conditional independence modelsandMarkov
chainMonteCarlo metho ds.J.AmericanStatistic alAssocia-
tion92(440): 1403{1412.
Berzuini, C.,andGilks, W.R.(2001) Followingamovingtar-
get{MonteCarlo inference fordynamic Bayesian models.J.
RoyalStatistic alSocietySeries B{Statistic alMethodology63
(1):127{146.
Bhattacharyya,A.(1943) Onameasure ofdivergence between
twostatistical populations dened bytheirprobabilit ydistri-
butions. Bull.Calcutta Math. Soc.35:99{110.
Bishop, C.M.(1992) Exact calculation oftheHessian matrix for
themultilayerperceptron. NeuralComputation 4(4):494{501.
Bishop, C.M.(1995) NeuralNetworks forPattern Recognition .
Oxford Univ. Press.
Bishop, C.M.,andWinn, J.M.(2000) Non-linear Bayesian
image modelling. InProc.SixthEuropeanConf. onComputer
Vision ,volume 1,pp.3{17. Springer-V erlag.
Bishop, C.M.,andWinn, J.M.(2003) Structured variational
distributions inVIBES. InAdvancesinNeuralInformation
Processing Systems .
Bishop, C.M.,Winn, J.M.,andSpiegelhal ter,D.(2002)
VIBES: Avariational inference engine forBayesian networks.
InAdvancesinNeuralInformation Processing Systems .
Blahut, R.E.(1987) Principles andPracticeofInformation
Theory.Addison-W esley.
613

<<<PAGE 626>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
614 Bibliograph y
Bottou, L.,Howard,P.G.,andBengio, Y.(1998) TheZ-
coderadaptiv ebinary coder.InProc.DataCompr ession Conf.
Snowbir d,Utah, March1998,pp.13{22.
Box,G.E.P.,andTiao,G.C.(1973) Bayesian Inferencein
Statistic alAnalysis .Addison{W esley.
Braunstein, A.,Mezard, M.,andZecchina, R.,(2003) Survey
propagation: analgorithm forsatisabilit y.cs.CC/0212002.
Bretthorst, G.(1988) Bayesian Spectrum Analysis andParam-
eterEstimation .Springer. Alsoavailable atbayes.wustl.edu .
Bridle, J.S.(1989) Probabilistic interpretation offeedforw ard
classication networkoutputs, withrelationships tostatisti-
calpattern recognition. InNeuro-computing: Algorithms, Ar-
chitecturesandApplic ations ,ed.byF.Fougelman-Soulie and
J.Herault. Springer{V erlag.
Bulmer, M.(1985) TheMathematic alTheoryofQuantitative
Genetics .Oxford Univ. Press.
Burrows,M.,andWheeler, D.J.(1994) Ablock-sorting loss-
lessdatacompression algorithm. Technical Report124,Digital
SRC.
Byers, J.,Luby, M.,Mitzenma cher, M.,andRege, A.(1998)
Adigital fountainapproac htoreliable distribution ofbulkdata.
InProc.ofACMSIGCOMM '98,Septemb er2{4,1998.
Cairns-Smith, A.G.(1985) Seven Clues totheOrigin ofLife.
Cambridge Univ. Press.
Calderbank, A.R.,andShor, P.W.(1996) Goodquantum
error-correcting codesexist. Phys. Rev.A54:1098.quant-ph/
9512032 .
Carroll,L.(1998) Alice'sAdventur esinWonderland; and,
Through theLooking-glass: andwhat AliceFound There.
Macmillan Children's Books.
Childs, A.M.,Patterson, R.B.,andMacKay,D.J.C.
(2001) Exact sampling fromnon-attractiv edistributions using
summary states. Physic alReview E.
Chu, W.,Keerthi,S.S.,andOng, C.J.(2001) Aunied
lossfunction inBayesian framew orkforsupportvector regres-
sion. InProc.18thInternational Conf. onMachine Learning ,
pp.51{58.
Chu,W.,Keerthi,S.S.,andOng,C.J.(2002) AnewBayesian
design metho dforsupportvectorclassication. InSpecialSec-
tiononSupportVectorMachines ofthe9thInternational Conf.
onNeuralInformation Processing .
Chu,W.,Keerthi,S.S.,andOng, C.J.(2003a) Bayesian
supportvector regression using aunied lossfunction. IEEE
Trans.onNeuralNetworks .Submitted.
Chu,W.,Keerthi,S.S.,andOng, C.J.(2003b) Bayesian
trigonometric supportvector classier. NeuralComputation .
Chung, S.-Y.,Richardson, T.J.,andUrbanke, R.L.(2001)
Analysis ofsum-pro ductdecodingoflow-densit yparity-check
codesusing aGaussian approximation. IEEE Trans.Info.The-
ory47(2):657{670.
Chung, S.-Y.,Urbanke, R.L.,andRichardson, T.J.,
(1999a) LDPC codedesign applet.lids.mit.edu/~sychung/
gath.html .
Chung, S.-Y.,Urbanke, R.L.,andRichardson, T.J.,
(1999b) LDPC codedesign applet.lids.mit.edu/~sychung/
gaopt.html .
Comon, P.,Jutten, C.,andHerault,J.(1991) Blind sepa-
ration ofsources. 2.Problems statemen t.Signal Processing 24
(1):11{20.
Copas,J.B.(1983) Regression, prediction andshrink age(with
discussion). J.R.Statist.So cB45(3):311{354.
Cover,T.M.(1965) Geometrical andstatistical properties of
systems oflinear inequalities withapplications inpattern recog-
nition. IEEE Trans.onElectronicComputers 14:326{334.
Cover,T.M.,andThomas, J.A.(1991) Elements ofInforma-
tionTheory.Wiley.Cowles, M.K.,andCarlin, B.P.(1996) Markov-chainMonte-
Carlo convergence diagnostics {acomparativ ereview. J.Amer-
icanStatistic alAssociation 91(434): 883{904.
Cox,R.(1946) Probabilit y,frequency ,andreasonable expecta-
tion.Am.J.Physics 14:1{13.
Cressie, N.(1993) Statistics forSpatialData.Wiley.
Davey,M.C.(1999) Error-correctionusingLow-Density Parity-
CheckCodes.Univ. ofCambridge PhDdissertation.
Davey,M.C.,andMacKay,D.J.C.(1998) Lowdensit ypar-
itycheckcodesoverGF(q).IEEE Communic ations Letters 2
(6):165{167.
Davey,M.C.,andMacKay,D.J.C.(2000) Watermark codes:
Reliable comm unication overinsertion/deletion channels. In
Proc.2000IEEE International Symposium onInfo.Theory,
p.477.
Davey,M.C.,andMacKay,D.J.C.(2001) Reliable comm u-
nication overchannels withinsertions, deletions andsubstitu-
tions. IEEE Trans.Info.Theory47(2):687{698.
Dawid,A.,Stone, M.,andZidek, J.(1996) Critique of
E.TJaynes's `parado xesofprobabilit ytheory'. Technical Re-
port172,Departmen tofStatistical Science, Univ. College Lon-
don.
Dayan,P.,Hinton, G.E.,Neal, R.M.,andZemel, R.S.
(1995) TheHelmholtz machine. NeuralComputation 7
(5):889{904.
Divsalar, D.,Jin,H.,andMcEliece, R.J.(1998) Codingthe-
orems for`turbo-like'codes.InProc.36thAllerton Conf. on
Communic ation, Control,andComputing, Sept.1998,pp.201{
210.Allerton House.
Doucet, A.,deFreitas,J.,andGordon, N.eds.(2001) Se-
quential Monte Carlo MethodsinPractice.Springer-V erlag.
Durbin, R.,Eddy,S.R.,Krogh,A.,andMitchison, G.(1998)
BiologicalSequenceAnalysis. Probabilistic ModelsofProteins
andNucleic Acids.Cambridge Univ. Press.
Dyson, F.J.(1985) Origins ofLife.Cambridge Univ. Press.
Elias, P.(1975) Universalcodewordsetsandrepresen tations of
theintegers. IEEE Trans.Info.Theory21(2):194{203.
Eyre-W alker, A.,andKeightley, P.(1999) High genomic
deleterious mutation ratesinhominids. Nature397:344{347.
Felsenstein, J.(1985) Recom bination andsex:isMaynard
Smith necessary? InEvolution. Essays inHonour ofJohn
Maynar dSmith ,ed.byP.J.Green wood,P.H.Harvey,and
M.Slatkin, pp.209{220. Cambridge Univ. Press.
Ferreira, H.,Clarke, W.,Helber g,A.,Abdel-Ghaff ar,
K.S.,andVinck, A.H.(1997) Insertion/deletion correction
withspectral nulls.IEEE Trans.Info.Theory43(2):722{732.
Feynman, R.P.(1972) Statistic alMechanics .Addison{W esley.
Forney, Jr.,G.D.(1966) Concatenate dCodes.MITPress.
Forney, Jr.,G.D.(2001) Codesongraphs: Normal realiza-
tions. IEEE Trans.Info.Theory47(2):520{548.
Frey, B.J.(1998) GraphicalModelsforMachine Learning and
Digital Communic ation.MITPress.
Galla ger,R.G.(1962) Lowdensit yparitycheckcodes.IRE
Trans.Info.TheoryIT-8:21{28.
Galla ger,R.G.(1963) LowDensity Parity CheckCodes.Num-
ber21inMITResearc hmonograph series. MITPress. Avail-
ablefromwww.inference.phy.cam.ac.uk/ mackay/gallager/
papers/ .
Galla ger,R.G.(1968) Information TheoryandReliable Com-
munication.Wiley.
Galla ger,R.G.(1978) Variations onatheme byHuman.
IEEE Trans.Info.TheoryIT-24 (6):668{674.
Gibbs, M.N.(1997) Bayesian Gaussian ProcessesforRegres-
sionandClassic ation.Cambridge Univ. PhDdissertation.
www.inference.phy.cam.ac.uk/ mng10/.

<<<PAGE 627>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
Bibliograph y 615
Gibbs, M.N.,andMacKay,D.J.C.,(1996) Ef-
cientimplemen tation ofGaussian processes forinterpo-
lation. www.inference.phy.cam.ac.uk/ mackay/abstracts/
gpros.html .
Gibbs, M.N.,andMacKay,D.J.C.(2000) Variational Gaus-
sianprocessclassiers. IEEE Trans.onNeuralNetworks 11
(6):1458{1464.
Gilks, W.,Roberts,G.,andGeorge,E.(1994) Adaptiv e
direction sampling. Statistician 43:179{189.
Gilks, W.,andWild, P.(1992) Adaptiv erejection sampling for
Gibbs sampling. Applie dStatistics 41:337{348.
Gilks, W.R.,Richardson, S.,andSpiegelhal ter,D.J.
(1996) Markov Chain Monte Carlo inPractice.Chapman and
Hall.
Goldie, C.M.,andPinch, R.G.E.(1991) Communic ation
theory.Cambridge Univ. Press.
Golomb, S.W.,Peile, R.E.,andSchol tz,R.A.(1994) Basic
ConceptsinInformation TheoryandCoding:TheAdventur es
ofSecretAgent00111 .PlenumPress.
Good, I.J.(1979) Studies inthehistory ofprobabilit yandstatis-
tics.XXXVI I.A.M. Turing's statistical workinWorldWarII.
Biometrika 66(2):393{396.
Graham, R.L.(1966) Onpartitions ofanite set.Journal of
Combinatorial Theory1:215{223.
Graham, R.L.,andKnowlton,K.C.,(1968) Metho dofiden-
tifying conductors inacable byestablishing conductor connec-
tiongroupings atbothendsofthecable. U.S.Patent3,369,177
(13Feb1968).
Green, P.J.(1995) Reversible jump MarkovchainMonteCarlo
computation andBayesian modeldetermination. Biometrika
82:711{732.
Gregor y,P.C.,andLoredo, T.J.(1992) Anewmetho dfor
thedetection ofaperiodicsignal ofunkno wnshapeandperiod.
InMaximum EntropyandBayesian Methods,,ed.byG.Erick-
sonandC.Smith. Kluwer.AlsoinAstrophysic alJournal ,398,
p.146-168, Oct10,1992.
Gull, S.F.(1988) Bayesian inductiv einference andmaxim um
entropy.InMaximum EntropyandBayesian MethodsinSci-
enceandEngine ering, vol.1:Foundations ,ed.byG.Erickson
andC.Smith, pp.53{74. Kluwer.
Gull, S.F.(1989) Developmen tsinmaxim umentropydataanal-
ysis.InMaximum EntropyandBayesian Methods,Cambridge
1988,ed.byJ.Skilling, pp.53{71. Kluwer.
Gull, S.F.,andDaniell, G.(1978) Image reconstruction from
incomplete andnoisy data.Nature272:686{690.
Hanson, R.,Stutz, J.,andCheeseman, P.(1991a) Bayesian
classication theory .Technical ReportFIA{90-12-7-01, NASA
Ames.
Hanson, R.,Stutz, J.,andCheeseman, P.(1991b) Bayesian
classication withcorrelation andinheritance. InProc.12thIn-
tern.JointConf. onArticial Intelligence,Sydney, Australia,
volume 2,pp.692{698. Morgan Kaufmann.
Hartmann, C.R.P.,andRudolph, L.D.(1976) Anoptim um
symbolbysymboldecodingruleforlinear codes.IEEE Trans.
Info.TheoryIT-22 :514{517.
Harvey,M.,andNeal, R.M.(2000) Inference forbeliefnet-
worksusing coupling fromthepast.InUncertainty inArticial
Intelligence:Proc.SixteenthConf.
/,pp.256{263.
Hebb, D.O.(1949) TheOrganization ofBehavior .Wiley.
Hendin, O.,Horn, D.,andHopfield, J.J.(1994) Decom-
position ofamixture ofsignals inamodeloftheolfactory
bulb.Proc.National Academy ofScienc esoftheUnitedStates
ofAmerica91(13):5942{5946.
Hertz,J.,Krogh,A.,andPalmer, R.G.(1991) Introduction
totheTheoryofNeuralComputation .Addison-W esley.Hinton, G.,(2000) Training products ofexpertsbyminimizing
contrastiv edivergence. Technical ReportGCNU TR2000-004,
Gatsb yComputational Neuroscience Unit, Univ. College Lon-
don.
Hinton, G.,andNowlan, S.(1987) Howlearning canguide
evolution. Complex Systems 1:495{502.
Hinton, G.E.,Dayan,P.,Frey, B.J.,andNeal, R.M.
(1995) Thewake-sleep algorithm forunsup ervised neural net-
works.Scienc e268(5214): 1158{1161.
Hinton, G.E.,andGhahramani, Z.(1997) Generativ emodels
fordiscoveringsparse distributed represen tations. Philosophic al
Trans.RoyalSocietyB.
Hinton, G.E.,andSejnowski, T.J.(1986) Learning and
relearning inBoltzmann machines. InParallelDistribute dPro-
cessing ,ed.byD.E.Rumelhart andJ.E.McClelland, pp.282{
317.MITPress.
Hinton, G.E.,andTeh, Y.W.(2001) Discovering multi-
pleconstrain tsthatarefrequen tlyapproximately satised. In
Proc.International Conf. onUncertainty inArticial Intelli-
gence,volume 17.
Hinton, G.E.,andvanCamp, D.(1993) Keeping neural
networkssimple byminimizing thedescription length ofthe
weights.InProc.6thAnnu.Workshop onComput. Learning
Theory,pp.5{13. ACMPress, NewYork,NY.
Hinton, G.E.,Welling, M.,Teh,Y.W.,andOsinder o,S.
(2001) AnewviewofICA. InProc.International Conf. on
Independent Component Analysis andBlindSignal Separation,
volume 3.
Hinton, G.E.,andZemel, R.S.(1994) Autoencoders, min-
imumdescription length andHelmholtz freeenergy .InAd-
vancesinNeuralInformation Processing Systems 6,ed.byJ.D.
Cowan,G.Tesauro, andJ.Alspector. Morgan Kaufmann.
Hodges, A.(1983) AlanTuring: TheEnigma .Simon andSchus-
ter.
Hojen-Sorensen, P.A.,Winther, O.,andHansen, L.K.
(2002) Mean eldapproac hestoindependen tcomponentanal-
ysis.NeuralComputation 14:889{918.
Holmes, C.,andMallick, B.(1998) Perfect simulation for
orthogonal modelmixing. Technical report,ImperialCollege,
London.
Hopfield, J.,andBrody,C.(2000) What isamomen t?\Corti-
cal"sensory integration overabriefinterval.PNAS 97:13919{
13924.
Hopfield, J.,andBrody,C.(2001) What isamomen t?Tran-
sientsynchronyasacollectiv emechanism forspatiotemp oral
integration. PNAS 98:1282{1287.
Hopfield, J.J.(1974) Kinetic proofreading: Anewmecha-
nismforreducing errors inbiosyn thetic processes requiring high
specicit y.Proc.National Academy ofScienc es71(10):4135{
4139.
Hopfield, J.J.(1978) Origin ofthegenetic code:Atestable
hypothesis based ontrnastructure, sequence, andkinetic proof-
reading. Proc.National Academy ofScienc esoftheUnited
States ofAmerica75(9):4334{4338.
Hopfield, J.J.(1980) Theenergy relay:Aproofreading scheme
based ondynamic cooperativit yandlackingallcharacteristic
symptoms ofkinetic proofreading indnareplication andprotein
synthesis. Proc.National Academy ofScienc esoftheUnited
States ofAmerica77(9):5248{5252.
Hopfield, J.J.(1982) Neural networksandphysical systems
withemergen tcollectiv ecomputational abilities. Proc.Natl.
Acad.Sci.USA79:2554{8.
Hopfield, J.J.(1984) Neurons withgraded responseproperties
havecollectiv ecomputational properties likethose oftwo-state
neurons. Proc.Natl.Acad.Sci.USA81:3088{92.

<<<PAGE 628>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
616 Bibliograph y
Hopfield, J.J.(1987) Learning algorithms andprobabilit ydis-
tributions infeed-forw ardandfeed-bac knetworks.Proc.Natl.
Acad.Sci.USA84:8429{33.
Hopfield, J.J.,andTank,D.W.(1985) Neural computation
ofdecisions inoptimization problems. BiologicalCybernetics
52:1{25.
Howarth,P.,andBradley, A.(1986) Thelongitudinal aberra-
tionofthehuman eyeanditscorrection. Vision Res.26:361{
366.
Ichika wa,K.,Bhadeshia, H.K.D.H.,andMacKay,D.J.C.
(1996) Modelforhotcrackinginlow-allo ysteelweldmetals.
Scienc eandTechnolo gyofWelding andJoining 1:43{50.
Isard, M.,andBlake, A.(1996) Visual trackingbystochastic
propagation ofconditional densit y.InProc.Fourth European
Conf. Computer Vision ,pp.343{356.
Isard, M.,andBlake, A.(1998) Condensation {conditional
densit ypropagation forvisual tracking. International Journal
ofComputer Vision 29(1):5{28.
Jaakk ola,T.S.,andJordan,M.I.(1996) Computing upper
andlowerbounds onlikelihoodsinintractable networks. In
Proc.Twelfth Conf. onUncertainty inAI.Morgan Kaufman.
Jaakk ola,T.S.,andJordan,M.I.(2000a) Bayesian logistic
regression: avariational approac h.Statistics andComputing
10:25{37.
Jaakk ola,T.S.,andJordan,M.I.(2000b) Bayesianparame-
terestimation viavariational metho ds.Statistics andComput-
ing10(1):25{37.
Jaynes, E.(2003) Probability Theory:TheLogicofScienc e.
Cambridge Univ. Press. Edited byG.Larry Bretthorst.
Jaynes, E.T.(1983) Bayesian intervalsversus condence in-
tervals.InE.T.Jaynes. PapersonProbability, Statistics and
Statistic alPhysics ,ed.byR.D.Rosenkran tz,p.151.Kluwer.
Jensen, F.V.(1996) AnIntroduction toBayesian Networks .
UCLpress.
Johannesson, R.,andZigangir ov,K.S.(1999) Fundamentals
ofConvolutional Coding.IEEE Press.
Jordan,M.I.ed.(1998) Learning inGraphicalModels.NATO
Science Series. KluwerAcademic Publishers.
JPL,(1996) Turbocodesperformance. Available from
www331.jpl.nasa.gov/public/Turb oPerf.html.
Jutten, C.,andHerault,J.(1991) Blind separation ofsources.
1.Anadaptiv ealgorithm based onneuromimetic architecture.
Signal Processing 24(1):1{10.
Karplus, K.,andKrit, H.(1991) Asemi-systolic decoderfor
thePDSC{73 error-correcting code.DiscreteApplie dMathe-
matics 33:109{128.
Kepler, T.,andOprea, M.(2001) Impro vedinference ofmuta-
tionrates: I.Anintegral represen tation oftheLuria-Delbr uck
distribution. TheoreticalPopulation Biology59:41{48.
Kimeldorf, G.S.,andWahba, G.(1970) Acorresp ondence be-
tweenBayesianestimation ofstochastic processes andsmooth-
ingbysplines. Annals ofMathematic alStatistics 41(2):495{
502.
Kitanidis, P.K.(1986) Parameter uncertain tyinestimation of
spatial functions: Bayesiananalysis. Water ResourcesResearch
22:499{507.
Knuth, D.E.(1968) TheArtofComputer Programming. Volume
1:Fundamental Algorithms .Addison Wesley.
Kondrasho v,A.S.(1988) Deleterious mutations andtheevo-
lution ofsexual reproduction. Nature336(6198): 435{440.
Kschischang, F.R.,Frey, B.J.,andLoeliger, H.-A. (2001)
Factor graphs andthesum-pro ductalgorithm. IEEE Trans.
Info.Theory47(2):498{519.
Kschischang, F.R.,andSorokine, V.(1995) Onthetrel-
lisstructure ofblockcodes.IEEE Trans.Info.Theory41
(6):1924{1937.Lampor t,L.(1986) LATEX:ADocument Preparation System .
Addison-W esley.
Lauritzen, S.L.(1981) Time series analysis in1880, adiscussion
ofcontributions made byT.N.Thiele. ISIReview49:319{333.
Lauritzen, S.L.(1996) GraphicalModels.Number17inOxford
Statistical Science Series. Clarendon Press.
Lauritzen, S.L.,andSpiegelhal ter,D.J.(1988) Localcom-
putations withprobabilities ongraphical structures andtheir
application toexpertsystems. J.RoyalStatistic alSocietyB
50:157{224.
Levenshtein, V.I.(1966) Binary codescapable ofcorrecting
deletions, insertions, andreversals. Soviet Physics {Doklady
10(8):707{710.
Lin,S.,andCostello, Jr.,D.J.(1983) ErrorControlCoding:
Fundamentals andApplic ations .Prentice-Hall.
Litsyn, S.,andShevelev, V.(2002) Onensem blesoflow-
densit yparity-checkcodes:asymptotic distance distributions.
IEEE Trans.Info.Theory48(4):887{908.
Loredo, T.J.(1990) FromLaplace tosupernovaSN1987A:
Bayesian inference inastroph ysics. InMaximum Entropy
andBayesian Methods,Dartmouth, U.S.A., 1989,ed.by
P.Fougere, pp.81{142. Kluwer.
Lowe,D.G.(1995) Similarit ymetric learning foravariable
kernelclassier. NeuralComputation 7:72{85.
Luby, M.(2002) LTcodes.InProc.ofThe43rdAnnual IEEE
Symposium onFoundations ofComputer Scienc e,Novemb er
16{19 2002,pp.271{282.
Luby, M.G.,Mitzenma cher, M.,Shokr ollahi, M.A.,and
Spielman, D.A.(1998) Impro vedlow-densit yparity-check
codesusing irregular graphs andbeliefpropagation. InProc.
IEEE International Symposium onInfo.Theory,p.117.
Luby, M.G.,Mitzenma cher, M.,Shokr ollahi, M.A.,and
Spielman, D.A.(2001a) Ecien terasure correcting codes.
IEEE Trans.Info.Theory47(2):569{584.
Luby, M.G.,Mitzenma cher, M.,Shokr ollahi, M.A.,and
Spielman, D.A.(2001b) Impro vedlow-densit yparity-check
codesusing irregular graphs andbeliefpropagation. IEEE
Trans.Info.Theory47(2):585{584.
Luby, M.G.,Mitzenma cher, M.,Shokr ollahi, M.A.,Spiel-
man,D.A.,andStemann, V.(1997) Practical loss-resilien t
codes. InProc.Twenty-Ninth Annual ACMSymposium on
TheoryofComputing (STOC) .
Luo,Z.,andWahba, G.(1997) Hybrid Adaptiv eSplines. J.
Amer.Statist. Assoc.92:107{116.
Luria, S.E.,andDelbr uck,M.(1943) Mutations ofbacteria
from virus sensitivit ytovirus resistance. Genetics 28:491{
511. Reprin tedinMicrobiology:ACentenary Perspective,
Wolfgang K.Joklik, ed.,1999, ASM Press, andavailable from
www.esp.org/ .
Luttrell, S.P.(1989) Hierarc hicalvector quantisation. Proc.
IEEPartI136:405{413.
Luttrell, S.P.(1990) Derivation ofaclassoftraining algo-
rithms. IEEE Trans.onNeuralNetworks 1(2):229{232.
MacKay,D.J.,Mitchison, G.,andMcFadden, P.L.,(2003)
Sparse graph codesforquantumerror-correction. quant-
ph/0304161. Submitted toIEEE Trans.Info.TheoryMay8,
2003.
MacKay,D.J.C.(1991) Bayesian MethodsforAdaptive Models.
California Institute ofTechnology PhDdissertation.
MacKay,D.J.C.(1992a) Bayesianinterpolation. NeuralCom-
putation 4(3):415{447.
MacKay,D.J.C.(1992b) Theevidence framew orkapplied to
classication networks.NeuralComputation 4(5):698{714.
MacKay,D.J.C.(1992c) Apractical Bayesian framew orkfor
backpropagation networks.NeuralComputation 4(3):448{472.

<<<PAGE 629>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
Bibliograph y 617
MacKay,D.J.C.(1994a) Bayesian metho dsforbackprop-
agation networks. InModelsofNeuralNetworks III,ed.by
E.Doman y,J.L.vanHemmen, andK.Schulten, chapter 6,
pp.211{254. Springer-V erlag.
MacKay,D.J.C.(1994b) Bayesian non-linear modelling for
theprediction competition. InASHRAE Trans.V.100, Pt.2,
pp.1053{1062. American SocietyofHeating, Refrigeration, and
Air-conditioning Engineers.
MacKay,D.J.C.(1995a) Freeenergy minimization algorithm
fordecodingandcryptanalysis. ElectronicsLetters31(6):446{
447.
MacKay,D.J.C.(1995b) Probable networksandplausible
predictions {areview ofpractical Bayesian metho dsforsu-
pervised neural networks. Network: Computation inNeural
Systems 6:469{505.
MacKay,D.J.C.,(1997a) Ensem blelearning forhidden Markov
models.www.inference.phy.cam.ac.uk/ mackay/abstracts/
ensemblePaper.html .
MacKay,D.J.C.,(1997b) Iterativ eprobabilistic decodingoflow
densit yparitycheckcodes.Animations available onworldwide
web.www.inference.phy.cam.ac.uk/m ackay/codes/gifs/.
MacKay,D.J.C.(1998a) Choice ofbasisforLaplace approxi-
mation. Machine Learning 33(1):77{86.
MacKay,D.J.C.(1998b) Introduction toGaussian processes. In
NeuralNetworks andMachine Learning ,ed.byC.M.Bishop,
NATOASISeries, pp.133{166. Kluwer.
MacKay,D.J.C.(1999a) Comparison ofapproximate meth-
odsforhandling hyperparameters. NeuralComputation 11
(5):1035{1068.
MacKay,D.J.C.(1999b) Gooderrorcorrecting codesbased on
verysparse matrices. IEEE Trans.Info.Theory45(2):399{
431.
MacKay,D.J.C.,(2000) Analternativ etorunlength-limiting
codes:Turntiming errors intosubstitution errors. Available
fromwww.inference.phy.cam.ac.uk/m ackay/.
MacKay,D.J.C.,(2001) Aproblem withvariational free
energy minimization. www.inference.phy.cam.ac.uk /mackay/
abstracts/minima.html .
MacKay,D.J.C.,andDavey,M.C.(2000) Evaluation ofGal-
lager codesforshort blocklength andhighrateapplications.
InCodes,Systems andGraphicalModels,ed.byB.Marcus and
J.Rosen thal,volume 123ofIMAVolumes inMathematics and
itsApplic ations ,pp.113{130. Springer-V erlag.
MacKay,D.J.C.,andNeal, R.M.(1995) Goodcodesbased
onverysparse matrices. InCrypto graphyandCoding.5thIMA
Conf., LNCS 1025,ed.byC.Boyd,pp.100{111. Springer.
MacKay,D.J.C.,andNeal, R.M.(1996) Near Shannon
limitperformance oflowdensit yparitycheckcodes.Electron-
icsLetters 32(18):1645{1646. Reprin tedElectronicsLetters,
33(6):457{458, March1997.
MacKay,D.J.C.,andPeto, L.(1995) Ahierarc hicalDirichlet
language model.NaturalLanguage Engine ering1(3):1{19.
MacKay,D.J.C.,Wilson, S.T.,andDavey,M.C.(1998)
Comparison ofconstructions ofirregular Gallager codes. In
Proc.36thAllerton Conf. onCommunic ation, Control,and
Computing, Sept.1998,pp.220{229. Allerton House.
MacKay,D.J.C.,Wilson, S.T.,andDavey,M.C.(1999)
Comparison ofconstructions ofirregular Gallager codes.IEEE
Trans.onCommunic ations 47(10):1449{1454.
MacKay,D.M.,andMacKay,V.(1974) Thetimecourse
oftheMcCollough eect anditsphysiological implications. J.
Physiol. 237:38{39.
MacKay,D.M.,andMcCulloch, W.S.(1952) Thelimiting
information capacit yofaneuronal link.Bull.Math. Biophys.
14:127{135.MacWilliams, F.J.,andSloane,N.J.A.(1977) TheTheory
ofError-correctingCodes.North-Holland.
Mandelbr ot,B.(1982) TheFractalGeometry ofNature.W.H.
Freeman andCo.
Mao,Y.,andBanihashemi, A.(2000) Design ofgoodLDPC
codesusing girth distribution. InIEEE International Sympo-
siumonInfo.Theory,Italy,June, 2000.
Mao,Y.,andBanihashemi, A.(2001) Aheuristic searchfor
goodLDPC codesatshort blocklengths. InIEEE Interna-
tional Conf. onCommunic ations .
Marinari, E.,andParisi, G.(1992) Simulated tempering {a
newMonte-Carlo scheme. Europhysics Letters19(6):451{458.
Matheron,G.(1963) Principles ofgeostatistics. Economic Ge-
ology58:1246{1266.
Maynard Smith, J.(1968) `Haldane's dilemma' andtherateof
evolution. Nature219(5159): 1114{1116.
Maynard Smith, J.(1978) TheEvolution ofSex.Cambridge
Univ. Press.
Maynard Smith, J.(1988) Games, SexandEvolution .
Harvester{Wheatsheaf.
Maynard Smith, J.,andSzathmar y,E.(1995) TheMajor
Transitions inEvolution .Freeman.
Maynard Smith, J.,andSzathmar y,E.(1999) TheOrigins of
Life.Oxford Univ. Press.
McCollough, C.(1965) Color adaptation ofedge-detectors in
thehuman visual system. Scienc e149:1115{1116.
McEliece, R.J.(2002) TheTheoryofInformation andCoding.
Cambridge Univ. Press, second edition.
McEliece, R.J.,MacKay,D.J.C.,andCheng, J.-F. (1998)
Turbodecodingasaninstance ofPearl's `beliefpropagation' al-
gorithm. IEEE Journal onSelectedAreasinCommunic ations
16(2):140{152.
McMillan, B.(1956) Twoinequalities implied byunique deci-
pherabilit y.IRETrans.Inform. Theory2:115{116.
Minka, T.(2001) Afamily ofalgorithms forapproximate
Bayesian inference.MITPhDdissertation.
Miskin, J.W.(2001) Ensemble Learning forIndependent Com-
ponent Analysis .Departmen tofPhysics, Univ. ofCambridge
PhDdissertation.
Miskin, J.W.,andMacKay,D.J.C.(2000) Ensem ble
learning forblind image separation anddeconvolution. InAd-
vancesinIndependent Component Analysis ,ed.byM.Giro-
lami. Springer-V erlag.
Miskin, J.W.,andMacKay,D.J.C.(2001) Ensem blelearning
forblind source separation. InICA:Principles andPractice,
ed.byS.RobertsandR.Everson. Cambridge Univ. Press.
Mosteller, F.,andWallace,D.L.(1984) Applie dBayesian
andClassic alInference.ThecaseofTheFederalist papers.
Springer.
Neal, R.M.(1991) Bayesianmixture modelling byMonteCarlo
simulation. Technical ReportCRG{TR{91{2, Computer Sci-
ence, Univ. ofToronto.
Neal, R.M.(1993a) Bayesian learning viastochastic dynamics.
InAdvancesinNeuralInformation Processing Systems 5,ed.
byC.L.Giles, S.J.Hanson, andJ.D.Cowan,pp.475{482.
Morgan Kaufmann.
Neal, R.M.(1993b) Probabilistic inference using Markovchain
MonteCarlo metho ds.Technical ReportCRG{TR{93{1, Dept.
ofComputer Science, Univ. ofToronto.
Neal, R.M.(1995) Suppressing random walksinMarkovchain
MonteCarlo using ordered overrelaxation. Technical Report
9508, Dept. ofStatistics, Univ. ofToronto.
Neal, R.M.(1996) Bayesian Learning forNeuralNetworks .
Springer.
Neal, R.M.(1997a) MarkovchainMonteCarlo metho dsbased
on`slicing' thedensit yfunction. Technical Report9722, Dept.
ofStatistics, Univ. ofToronto.

<<<PAGE 630>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
618 Bibliograph y
Neal, R.M.(1997b) MonteCarlo implemen tation ofGaussian
processmodelsforBayesianregression andclassication. Tech-
nicalReportCRG{TR{97{2, Dept. ofComputer Science, Univ.
ofToronto.
Neal, R.M.(1998) Annealed importance sampling. Technical
Report9805, Dept. ofStatistics, Univ. ofToronto.
Neal, R.M.(2001) Dening priors fordistributions using Dirich-
letdiusion trees. Technical Report0104, Dept. ofStatistics,
Univ. ofToronto.
Neal, R.M.(2003) Slicesampling. AnnalsofStatistics .
Neal, R.M.,andHinton, G.E.(1998) AnewviewoftheEM
algorithm thatjusties incremen tal,sparse, andother variants.
InLearning inGraphicalModels,ed.byM.I.Jordan, NATO
Science Series, pp.355{368. Kluwer.
Nielsen, M.,andChuang, I.(2000) Quantum Computation and
Quantum Information .Cambridge Univ. Press.
Offer, E.,andSoljanin, E.(2000) Analgebraic description
ofiterativ edecodingschemes. InCodes,Systems andGraphi-
calModels,ed.byB.Marcus andJ.Rosen thal,volume 123of
IMAVolumes inMathematics anditsApplic ations ,pp.283{
298.Springer-V erlag.
Offer, E.,andSoljanin, E.(2001) LDPC codes:agroup al-
gebra formulation. InProc.Internat. Workshop onCodingand
Crypto graphyWCC 2001, 8-12Jan.2001, Paris.
O'Hagan,A.(1978) Oncurvetting andoptimal design for
regression. J.RoyalStatistic alSociety, B40:1{42.
O'Hagan,A.(1987) MonteCarlo isfundamen tallyunsound. The
Statistician 36:247{249.
O'Hagan,A.(1994) Bayesian Inference,volume 2BofKendal l's
AdvancedTheoryofStatistics .EdwardArnold.
Omre, H.(1987) Bayesian kriging {merging observ ations and
qualied guesses inkriging. Mathematic alGeology19:25{39.
Opper, M.,andWinther, O.(2000) Gaussian processes for
classication: Mean-eld algorithms. NeuralComputation 12
(11):2655{2684.
Patrick, J.D.,andWallace,C.S.(1982) Stone circle geome-
tries: aninformation theory approac h.InArchaeoastronomy in
theOldWorld ,ed.byD.C.Heggie, pp.231{264. Cambridge
Univ. Press.
Pearl, J.(1988) Probabilistic Reasoning inIntelligent Systems:
Networks ofPlausible Inference.Morgan Kaufmann.
Pearl, J.(2000) Causality .Cambridge Univ. Press.
Pearlmutter, B.A.,andParra, L.C.(1996) Acontext-
sensitiv egeneralization ofICA.InInternational Conf. onNeu-
ralInformation Processing, Hong Kong, Septemb er24-27,
1996.
Pearlmutter, B.A.,andParra, L.C.(1997) Maxim um
likelihoodblind source separation: Acontext-sensitiv egeneral-
ization ofICA. InAdvancesinNeuralInformation Processing
Systems ,ed.byM.C.Mozer, M.I.Jordan, andT.Petsche,
volume 9,p.613.TheMITPress.
Pinto, R.L.,andNeal, R.M.(2001) Impro vingMarkovchain
MonteCarlo estimators bycoupling toanapproximating chain.
Technical Report0101, Dept. ofStatistics, Univ. ofToronto.
Poggio, T.,andGirosi,F.(1989) Atheory ofnetworksfor
approximation andlearning. Technical ReportA.I.1140, MIT.
Poggio, T.,andGirosi,F.(1990) Networksforapproximation
andlearning. Proc.ofIEEE78:1481{1497.
Polya,G.(1954) Induction andAnalogyinMathematics .Prince-
tonUniv. Press.
Propp,J.G.,andWilson, D.B.(1996) Exact sampling with
coupled Markovchains andapplications tostatistical mechan-
ics.Random Structur esandAlgorithms 9(1-2): 223{252.
Rabiner, L.R.,andJuang, B.H.(1986) Anintroduction to
hidden Markovmodels.IEEE ASSP Magazine pp.4{16.Rasmussen, C.E.(1996) Evaluation ofGaussian Processesand
Other MethodsforNon-Line arRegression .Univ. ofToronto
PhDdissertation.
Rasmussen, C.E.(2000) Theinnite Gaussian mixture model.
InAdvancesinNeuralInformation Processing Systems 12,ed.
byT.L.S.A.SollaandK.-R. Muller, pp.554{560. MITPress.
Rasmussen, C.E.,(2002) Reduced rankGaussian processlearn-
ing.Unpublished manuscript.
Rasmussen, C.E.,andGhahramani, Z.(2002) Innite mixtures
ofGaussian processexperts. InAdvancesinNeuralInforma-
tionProcessing Systems 14,ed.byT.G.Diettric h,S.Becker,
andZ.Ghahramani. MITPress.
Rasmussen, C.E.,andGhahramani, Z.(2003) BayesianMonte
Carlo. InAdvancesinNeuralInformation Processing Systems
XV,ed.byS.T.Suzanna BeckerandK.Obermayer.
Ratliff, F.,andL.A.,R.(1950) Involuntarymotions oftheeye
during monocular xation. J.Exptl. Psychol. 40:687{701.
Ratzer, E.A.,andMacKay,D.J.(2003) Sparse low-densit y
parity-checkcodesforchannels withcross-talk. InProc.of2003
IEEE Info.TheoryWorkshop, Paris.
Reif,F.(1965) Fundamentals ofStatistic alandThermal Physics .
McGra w{Hill.
Richardson, T.,Shokr ollahi, M.A.,andUrbanke, R.
(2001) Design ofcapacit y-approac hingirregular low-densit y
paritycheckcodes.IEEE Trans.Info.Theory47(2):619{637.
Richardson, T.,andUrbanke, R.(2001a) Thecapacit yof
low-densit yparitycheckcodesunder message-passing decod-
ing.IEEE Trans.Info.Theory47(2):599{618.
Richardson, T.,andUrbanke, R.(2001b) Ecien tencoding
oflow-densit yparity-checkcodes.IEEE Trans.Info.Theory
47(2):638{656.
Ripley, B.D.(1991) Statistic alInferenceforSpatialProcesses.
Cambridge Univ. Press.
Ripley, B.D.(1996) Pattern Recognition andNeuralNetworks .
Cambridge Univ. Press.
Rumelhar t,D.E.,Hinton, G.E.,andWilliams, R.J.(1986)
Learning represen tations byback-propagating errors. Nature
323:533{536.
Russell, S.,andWefald,E.(1991) DotheRightThing: Studies
inLimite dRationality .MITPress.
Schneier, B.(1996) Applie dCrypto graphy.Wiley.
Scholk opf,B.,Burges,C.,andVapnik, V.(1995) Extract-
ingsupportdataforagiventask. InProc.FirstInternational
Conf. onKnowledgeDiscovery andDataMining ,ed.byU.M.
FayyadandR.Uthurusam y.AAAI Press.
Schol tz,R.A.(1982) Theorigins ofspread-sp ectrum comm u-
nications. IEEE Trans.onCommunic ations 30(5):822{854.
Seeger, M.,Williams, C.K.I.,andLawrence, N.(2003)
Fastforwardselection tospeedupsparse Gaussian processre-
gression. InProc.Ninth International Workshop onArticial
IntelligenceandStatistics ,ed.byC.Bishop andB.J.Frey.
SocietyforArticial Intelligence andStatistics.
Sejnowski, T.J.(1986) Higher order Boltzmann machines. In
Neuralnetworks forcomputing ,ed.byJ.Denker,pp.398{403.
American Institute ofPhysics.
Sejnowski, T.J.,andRosenber g,C.R.(1987) Parallel net-
worksthatlearntopronounce English text.Journal ofComplex
Systems 1(1):145{168.
Shannon, C.E.(1948) Amathematical theory ofcomm unica-
tion.BellSys.Tech.J.27:379{423, 623{656.
Shannon, C.E.(1993) Thebestdetection ofpulses. InCollected
PapersofClaude Shannon ,ed.byN.J.A.Sloane andA.D.
Wyner, pp.148{150. IEEE Press.
Shannon, C.E.,andWeaver,W.(1949) TheMathematic al
TheoryofCommunic ation.Univ. ofIllinois Press.
Shokr ollahi, A.,(2003) Raptor codes.Preprin t.
Sipser, M.,andSpielman, D.A.(1996) Expander codes.IEEE
Trans.Info.Theory42(6.1): 1710{1722.

<<<PAGE 631>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
Bibliograph y 619
Skilling, J.(1989) Classic maxim umentropy.InMaxi-
mumEntropyandBayesian Methods,Cambridge 1988,ed.by
J.Skilling. Kluwer.
Skilling, J.(1993) Bayesian numerical analysis. InPhysics and
Probability ,ed.byW.T.Grandy ,Jr.andP.Milonni. Cam-
bridge Univ. Press.
Skilling, J.,andMacKay,D.J.C.(2003) Slicesampling {
abinary implemen tation. Annals ofStatistics .Discussion of
SliceSampling byRadford M.Neal.
Smola, A.J.,andBartlett, P.(2001) Sparse Greedy Gaus-
sianProcessRegression. InAdvancesinNeuralInformation
Processing Systems 13,ed.byT.K.Leen, T.G.Diettric h,and
V.Tresp,pp.619{625. MITPress.
Spiegel, M.R.(1988) Statistics .Schaum's outline series.
McGra w-Hill, 2ndedition.
Spielman, D.A.(1996) Linear-time encodable anddecod-
ableerror-correcting codes. IEEE Trans.Info.Theory42
(6.1): 1723{1731.
Sutton, R.S.,andBarto,A.G.(1998) Reinforcement Learn-
ing:AnIntroduction .MITPress.
Swanson, L.(1988) AnewcodeforGalileo. InProc.1988IEEE
International Symposium Info.Theory,pp.94{95.
Tanner, M.A.(1996) ToolsforStatistic alInference:Methods
fortheExplor ation ofPosterior Distributions andLikeliho od
Functions .Springer Series inStatistics. Springer Verlag, 3rd
edition.
Tanner, R.M.(1981) Arecursiv eapproac htolowcomplexit y
codes.IEEE Trans.Info.Theory27(5):533{547.
Teahan, W.J.(1995) Probabilit yestimation forPPM. In
Proc.NZCSRSC'95 .Available fromciteseer.nj.nec.com/
teahan95probability.html .
tenBrink, S.(1999) Convergence ofiterativ edecoding. Elec-
tronicsLetters35(10):806{808.
tenBrink, S.,Kramer, G.,andAshikhmin, A.,(2002) Design
oflow-densit yparity-checkcodesformulti-an tenna modulation
anddetection. Submitted toIEEE Trans.onComm unications.
Terras, A.(1999) Fourier Analysis onFinite GroupsandAp-
plications .Cambridge Univ. Press.
Thomas, A.,Spiegelhal ter,D.J.,andGilks, W.R.(1992)
BUGS: Aprogram toperform Bayesian inference using Gibbs
sampling. InBayesian Statistics 4,ed.byJ.M.Bernardo,
J.O.Berger, A.P.Dawid,andA.F.M.Smith, pp.837{842.
Clarendon Press.
Tresp, V.(2000) ABayesian committee machine.NeuralCom-
putation 12(11):2719{2741.
Urbanke, R.,(2001) LdpcOpt{afastandaccurate degree dis-
tribution optimizer forldpccodeensem bles.lthcwww.epfl.ch/
research/ldpcopt/ .
Vapnik, V.(1995) TheNatureofStatistic alLearning Theory.
Springer Verlag.
Viterbi, A.J.(1967) Error bounds forconvolutional codesand
anasymptotically optim umdecodingalgorithm. IEEE Trans.
Info.TheoryIT-13 :260{269.
Wahba, G.(1990) Spline ModelsforObservational Data.Society
forIndustrial andApplied Mathematics. CBMS-NSF Regional
Conf. series inapplied mathematics.
Wainwright, M.J.,Jaakk ola,T.,andWillsky, A.S.(2002)
Tree-based reparameterization framew orkforanalysis ofsum-
productandrelated algorithms. Technical ReportP-2510, Lab-
oratory forInformation andDecision Systems, MIT. Accepted
forpublication inIEEE Trans.Info.Theory .
Wald,G.,andGriffin, D.(1947) Thechange inrefractiv e
poweroftheeyeinbrightanddimlight.J.Opt.Soc.Am.
37:321{336.
Wallace,C.,andBoulton,D.(1968) Aninformation measure
forclassication. Comput. J.11(2):185{194.
Wallace,C.S.,andFreeman, P.R.(1987) Estimation and
inference bycompact coding.J.R.Statist. Soc.B49(3):240{
265.Ward,D.J.,Blackwell, A.F.,andMacKay,D.J.C.(2000)
Dasher {Adataentryinterface using continuousgestures and
language models.InProc.ofUserInterfac eSoftwar eandTech-
nology2000,pp.129{137.
Ward,D.J.,andMacKay,D.J.C.(2002) Fasthands-free
writing bygazedirection. Nature418(6900): 838.
Welling, M.,andTeh,Y.W.(2001) Belief optimization for
binary networks: Astable alternativ etoloopybeliefpropaga-
tion.InUncertainty inArticial Intelligence:Proc.Sevente enth
Conf. (UAI-2001) ,pp.554{561. Morgan Kaufmann Publishers.
Wiber g,N.(1996) CodesandDecodingonGeneralGraphs.
Dept. ofElectrical Engineering, Linkoping, SwedenPhDdis-
sertation. Linkoping Studies inScience andTechnology No.
440.
Wiber g,N.,Loeliger, H.-A. ,andKotter, R.(1995) Codes
anditerativ edecodingongeneral graphs. EuropeanTrans.on
Telecommunic ations 6:513{525.
Wiener, N.(1948) Cybernetics .Wiley.
Williams, C.K.I.,andRasmussen, C.E.(1996) Gaussian
processes forregression. InAdvancesinNeuralInformation
Processing Systems 8,ed.byD.S.Touretzky ,M.C.Mozer,
andM.E.Hasselmo. MITPress.
Williams, C.K.I.,andSeeger, M.(2001) Using theNystrom
Metho dtoSpeedUpKernel Machines. InAdvancesinNeural
Information Processing Systems 13,ed.byT.K.Leen, T.G.
Diettric h,andV.Tresp,pp.682{688. MITPress.
Witten, I.H.,Neal, R.M.,andClear y,J.G.(1987) Arith-
metic codingfordatacompression. Communic ations ofthe
ACM30(6):520{540.
Wolf,J.K.,andSiegel, P.(1998) Ontwo-dimensional arrays
andcrossw ordpuzzles. InProc.36thAllerton Conf. onCom-
munication, Control,andComputing, Sept.1998,pp.366{371.
Allerton House.
Worthen, A.,andStark,W.(1998) Low-densit yparitycheck
codesforfading channels withmemory .InProc.36thAller-
tonConf. onCommunic ation, Control,andComputing, Sept.
1998,pp.117{125.
Yedidia, J.S.(2000) Anidiosyncratic journey beyondmean eld
theory .Technical report,Mitsubishi Electric Researc hLabora-
tories. MERL TR-2000-27.
Yedidia, J.S.,Freeman, W.T.,andWeiss, Y.(2000a) Bethe
freeenergy ,Kikuc hiapproximations andbeliefpropagation al-
gorithms. Technical report,Mitsubishi Electric Researc hLab-
oratories. MERL TR-2001-16.
Yedidia, J.S.,Freeman, W.T.,andWeiss, Y.(2000b) Char-
acterization ofbeliefpropagation anditsgeneralizations. Tech-
nicalreport,Mitsubishi Electric Researc hLaboratories. MERL
TR-2001-15.
Yedidia, J.S.,Freeman, W.T.,andWeiss, Y.(2000c) Gener-
alized beliefpropagation. Technical report,Mitsubishi Electric
Researc hLaboratories. MERL TR-2000-26.
Yedidia, J.S.,Freeman, W.T.,andWeiss, Y.(2002) Con-
structing freeenergy approximations andgeneralized belief
propagation algorithms. Technical report,Mitsubishi Electric
Researc hLaboratories. MERL TR-2002-35.
Yeung, R.W.(1991) AnewoutlookonShannon-information
measures. IEEE Trans.Info.Theory37(3.1): 466{474.
Yuille, A.L.(2001) Adouble-lo opalgorithm tominimize
theBethe andKikuc hifreeenergies. InEnergyMinimization
MethodsinComputer Vision andPattern Recognition. Third
International Workshop, EMMCVPR 2001, Sophia Antipolis
France,Septemb er3-5,2001, Proc._,ed.byM.Figueiredo,
J.Zerubia, andA.Jain, number2134 inLNCS, pp.3{18.
Springer.
Zipf,G.K.(1949) Human Behavior andthePrinciple ofLeast
Eort .Addison-W esley.

<<<PAGE 632>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
Index
 ,598
(z),514
2,323,458
,119
NandN 1,320
:=,600
?,419
acceptance rate, 365,367,369,380,
383,394
acceptance ratiometho d,379
accum ulator, 254,570,582
activation, 471
activation function, 471
activit y,471
activit yrule,470,471
adaptiv edirection sampling, 394
adaptiv emodels,101
adaptiv erejection sampling, 370
address, 201,468
Aiyer,Sree,518
Alberto,56
alchemists, 74
algorithm
covariant,442
EM,432
exact sampling, 413
expectation{maximization, 432
function minimization, 473
genetic, 395,396
Hamiltonian MonteCarlo, 387,
496
independen tcomponentanalysis,
443
Langevin MonteCarlo, 496
leapfrog, 389
max{pro duct, 339
perfect simulation, 413
sum{pro duct, 334
Viterbi, 340
Alice, 199
Allias parado x,454
alphab etical ordering, 194
America, 354
American, 238,260
amino acid,201,204,279,362
anagram, 200
Angel, J.R.P.,529
annealed importance sampling, 379
annealing
deterministic, 518
antiferromagnetic, 400
ape,269
approximationbyGaussian, 2,301,341,350,496
Laplace, 341,547
ofcomplex distribution, 185,282,
364,422,433
ofdensit yevolution, 567
saddle-p oint,341
Stirling, 1
variational, 422
arabic, 126
architecture, 470,529
arithmetic coding, 101,110,111
decoder,118
software,121
uses beyondcompression, 118,
250,255
arithmetic progression, 344
armsrace,278
associativememory ,505
assumptions, 26
astronom y,551
asymptotic equipartition, 80,384
whyitisamisleading term, 83
Atlantic,173
AutoClass, 306
automatic relevance determination,
544
automobile datareception, 594
average, 26,seeexpectation
AWGN,177
background rate,307
backpropagation, 473,475,528,535
backwardpass,244
bad,seeerror-correcting code,bad
Balakrishnan, Sree,518
balance, 66
Baldwin eect, 279
ban(unit), 264
Banburism us,265
Banbury,265
bandwidth, 178
bar-co de,262,399
basetransitions, 373
base-pairing, 280
basisdependence, 306,342
bat,213,214
battleships, 71
Bayes'theorem, 6,24,25,27,28,48{
50,53,148,324,344,347,
446,493,522
Bayes,Rev.Thomas, 51
Bayesian, 26
Bayesian beliefnetworks,293
Bayesian inference, 457BCH codes,13
BCJR, 330,578
Belarusian, 238
belief,57
beliefpropagation, 330,557,seesum{
productalgorithm
Benford's law,446
bentcoin,51
Berlek amp, Elwyn, 172,213
Bernoulli distribution, 117
Berrou, C.,186
bet,200,209,455
betadistribution, 316
Bethe freeenergy ,434
Bhattac haryyaparameter, 215
bias,345,506
inneural net,471
instatistics, 306,307,321
biased, 321
biexponential,88
biexponentialdistribution, 313,448
bifurcation, 89,291
binary entropyfunction, 2,15
binary erasure channel, 148,151
binary images, 399
binary represen tations, 132
binary symmetric channel, 4,148,148,
149,151,211,215
binding DNA, 201
binomial distribution, 1,311
bipartite graph, 19
birthda y,156,157,160,198,200
bit(unit), 264
bitsback,104,108,353
bivariate Gaussian, 388
black,354
BletchleyPark,265
Blind Watchmaker,269,396
blockcode,9,seesource codeorerror-
correcting code
block-sorting, 121
blowup,306
blur,549
Bob,199
Boltzmann entropy,85
bombes,265
Bottou, Leon, 121
bound, 85
bounded-distance decoder,207,212
box,343,350
boyishmatters, 58
brain, 468
Braunstein, A.,340
Bridge, 126
620

<<<PAGE 633>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
Index 621
British, 260
broadcast, 594
broadcast channel, 237,239
Brody,Carlos, 246
Brownian motion, 280,316,535
BSC,seechannel, binary symmetric
budget, 94,96
Buon's needle, 38
burglar alarm andearthquak e,293
Burro ws{Wheeler transform, 121
burst errors, 185,186
bus-stop fallacy ,39,46,107
calculator, 320
camera, 549
canonical, 88
capacit y,14,146,150,151,183,484
Hopeld network,514
neural network,483
neuron, 483
symmetry argumen t,151
cardatareception, 594
card, 233
casting outnines, 198
Cauchydistribution, 85,88,313,362
caution, seesermon
equipartition, 83
Gaussian distribution, 312
importance sampling, 362,382
sampling theory ,64
cave,214
caveat,seecaution
cellphone, seemobile phone
cellular automaton, 130
centreofgravity,35
chainrule,528
channel
AWGN,177
binary erasure, 148,151
binary symmetric, 146,148,148,
149,206,211,215
broadcast, 237,239,594
bursty,185,557
capacit y,14,146,150
complex, 184,557
constrained, 248,255,256
continuous, 178
discrete memoryless, 147
erasure, 219,589
extended, 153
fading, 186
Gaussian, 155,177,186
input ensem ble,150
multiple access, 237
multiterminal, 239
noiseless, 248
noisy,146
noisy typewriter, 148,152
symmetric, 171
two-dimensional, 262
unkno wnnoise level,238
variable symboldurations, 256
withcorrelated sources, 236
withmemory ,557
Zchannel, 148,149,150,172
channel capacit yconnection withphysics, 257
cheat, 200
Chebyshev inequalit y,81,85
checkerboard, 404
Cherno bound, 85
chessboard, 520
chi-squared, 27,40,323,458
Cholesky decomp osition, 552
chromatic aberration, 552
cinema, 187
circle, 316
classical statistics, 64
criticisms, 32,50,457
classier, 532
Claude Shannon, 3
Clockville, 39
clustering, 284,284,303
coalescence, 413
cockedhat,307
code,seeerror-correcting code,source
code(fordatacompression),
symbolcode,arithmetic cod-
ing, linear code,random
codeorhashcode
dual, seeerror-correcting code,
dual
forconstrained channel, 249
variable-length, 249,255
code-equiv alent,576
codebreak ers,265
codeword, seesource code,sym-
bolcode,orerror-correcting
code
codingtheory ,4,215
coin,38,63
coincidence, 267,343,350,351
collision, 200
coloured noise, 179
combination, 2,490,598
competitivelearning, 285
complexit y,531,548
complexit ycontrol,346,347,349
complexit y-control,289
compress, 119
compression, seesource code
algorithms, 119,121
Burro ws{Wheeler transform, 121
forcomplex sources, 353
lossless, 74
lossy,74,284,285
ofalready-compressed les,74
ofanyle,74
computer, 370
concatenation, 185,214,220
error-correcting codes, 16,21,
184,185
incompression, 92
inMarkovchains, 373
concave_,35
conditional entropy,138,146
cones, 554
condence interval,457,464
condence level,464
confused gamesho whost,57
conjugate gradien t,479conjugate prior, 319
conjuror, 233
connection
error correcting codeandlatent
variable model,437
connection between
channel capacit yandphysics, 257
pattern recognition anderror-
correction, 481
supervised and unsup ervised
learning, 515
vector quantization anderror-
correction, 285
connection matrix, 253
constrained channel, 248,257,260,399
constrain tsatisfaction, 516
content-addressable memory ,192,193,
469,505
continuouschannel, 178
controltreatmen t,458
conventions, seenotation
errorfunction, 156
logarithms, 2
matrices, 147
vectors, 147
convexhull,102
convex^,35
convexity,370
convolution, 568
convolutional code,184,186
Conway,JohnH.,86,520
Copernicus, 346
costfunction, 180
costofmales, 277
countingargumen t,20
coupling from thepast, 413,414,see
MonteCarlo metho ds,exact
sampling
covariance, 441
covariance function, 535
covariance matrix, 176
covariantalgorithm, 442
Cover,Thomas, 456
Coxaxioms, 26
crib,265,268
critical path, 246
cross-v alidation, 353
crosso ver,396
crossw ord,260
cryptanalysis, 578
cryptograph y,200
digital signatures, 199
tamperdetection, 199
cumulativ eprobabilit yfunction, 156
cycles ingraphs, 242
cyclic, 19
Dasher, 118
data, 529
datacompression, 73,seesource code
dataentry,118
datamodelling, 285
dataset
little'n'large, 288
Davey,Matthew C.,569
death penalty,354

<<<PAGE 634>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
622 Index
deciban (unit), 264
decibel,186
decision theory ,346,451
decoder,4,146,152
bounded-distance, 207
decoder,bitwise,221,324
decoder,codeword,220,324
decoder,probabilit yoferror, 221
degree, 568
degree sequence, seeprole
degrees ofbelief,26
degrees offreedom, 322,459
dejavu,121
delayline,575
Delbruck,Max, 446
deletions, 187
delta function, 600
densit yevolution, 566,567,592
densit ymodelling, 284,303
depth oflake,359
design theory ,209
detailed balance, 391
detection
offorgery ,199
deterministic annealing, 518
dictionary ,72,119
dierence-set cyclic code,569
dieren tiator, 254
diusion, 316
digital cinema, 187
digital fountain,590
digital signature, 199,200
digital video broadcast, 593
dimer, 204
directory ,193
Dirichletdistribution, 316
Dirichletmodel,117
discriminan tfunction, 179
discriminativ etraining, 552
disease, 25,458
diskdrive,3,188,215,248,255
distance, 205
DKL,34
bad,207,214
distance distribution, 206
entropydistance, 140
Gilbert{Varshamo v,212,221
good,207
Hamming, 206
isn'teverything, 215
ofcode,206,214,220
good/bad, 207
ofcode,anderrorprobabilit y,221
ofconcatenated code,214
ofproductcode,214
verybad,207
distribution
beta,316
biexponential,313
binomial, 311
Cauchy,312
Dirichlet,316
exponential,311,313
gamma, 313
Gaussian, 312sample from, 312
inverse-cosh, 313
log-normal, 315
Luria{Delbr uck,446
normal, 312
Poisson, 175,311,315
Studen t-t,312
useful, 311
VonMises, 315
distributions
Cauchy,88
overperiodicvariables, 315
divergence, 34
DjVu,121
DNA, 3,55,201,204,257,421
replication, 280
dotherightthing, 451
dodecahedron code,20,206,207
dongle, 558
doors,ongame show,57
drawstraws,233
DSC,seedierence-set cyclic code
dual, 216
dumbMetrop olis,394,496
Eb=N0,177,178,223
earthquak eandburglar alarm, 293
earthquak e,during game show,57
Ebert,Todd,222
edge, 251
eigenvalue,409
Elias, 111,135
EMalgorithm, 283,432
email, 201
emptystring, 119
encoder,4
energy ,291,401,601
English, 72,110
Enigma, 265,268
ensem ble,67
extended, 76
ensem blelearning, 429
entropic distribution, 318
entropic prior, 551
entropy,67,601
Boltzmann, 85
conditional, 138
Gibbs, 85
joint,138
marginal, 139
mutual information, 139
relativ e,34
entropydistance, 140
epicycles, 346
equipartition, 80
erasure channel, 219,589
erasure-correction, 188,190,220
erf,156
ergodic,120,373
errorbars,301,501
errorcorrection, 203
inprotein synthesis, 280
errordetection, 198,199,203
erroroor,581
errorfunction, 156,473,490,514,529,
599errorprobabilit y
block,152
incompression, 74
error-correcting code,188
bad,183,207
blockcode,151,183
concatenated, 184{186, 214
convolutional, 184
cyclic, 19
decoding, 184
densit yevolution, 566
dierence-set cyclic, 569
distance, seedistance ofcode
dodecahedron, 20,206,207
dual, 216,218
erasure channel, 589
Gallager, seeerror-correcting
codes, low-densit yparity-
check
Golay,209
good,183,184,207,214,218
Hamming, 19,214
interleaving,186
linear, 9,171,183,184,229
low-densit y generator-matrix,
218,590
low-densit yparity-check,20,187,
218,557,596
fastencoding, 569
LTcode,590
maxim umdistance separable, 220
nonlinear, 187
P3,218
parity-checkcode,220
pentagonful, 221
perfect, 208,211,212
practical, 183,187
productcode,184,214
quantum,572
random, 184
random linear, 211,212
rate,152,229
rateless, 590
rectangular, 184
Reed{Solomon code,571,589
repetition, 183
simple parity,218
sparse graph, 556
densit yevolution, 566
syndrome decoding, 371
variable rate,238
variable-rate, 590
verybad,207
verygood,183
weightenumerator, 206
withvarying levelofprotection,
239
error-correction
inDNA replication, 280
error-reject curves,533
errors, seechannel
estimate, 459
estimator, 48,307,320,446
eugenics, 273
euro, 63

<<<PAGE 635>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
Index 623
evidence, 29,53,298,322,347,531
typical behaviour of,54,60
evolution, 269,279
aslearning, 278
Baldwin eect, 279
colour vision, 554
ofthegenetic code,279
evolutionary computing, 394,395
exact sampling, 413,seeMonteCarlo
metho ds
exchange rate,601
exchangeabilit y,263
exclusiv eor,590
EXIT chart,567
expectation, 27,35,37
expectation propagation, 340
expectation{maximization algorithm,
432,seeEMalgorithm
experimen taldesign, 463
experimen talskill,309
explaining away,293,295
exploit, 453
explore, 453
exponentialdistribution, 45,313
exponentialdistribution onintegers,
311
exponential-family ,307,308
expurgation, 167,171
extended channel, 153,159
extended code,92
extended ensem ble,76
extra bit,98,101
extreme value,446
eyemovements,554
factor analysis, 437,444
factor graph, 334{336, 434
factorial, 2
fading channel, 186
feedbac k,506
female, 277
ferromagnetic, 400
Feynman, Richard, 422
Fibonacci, 253
eld,605
le
storage, 188
nger, 118
nite eldtheory ,seeGalois eld
Finland, 209
tness, 269,279
xedpoint,508
Florida, 355
uctuation analysis, 446
uctuations, 401,404
focus,529
football pools,209
forensic, 421
forgery ,199,200
forwardpass,244
forwardprobabilit y,27
forward{bac kwardalgorithm, 326,330
Fotherington{Thomas, 241
Fourier transform, 88,219,339,544,
568
fovea,554freeenergy ,257,407,409,410
minimization, 423
variational, 423
frequency ,26
frequen tist,320,seesampling theory
Frey,Brendan J.,353
Frobenius{P erron theorem, 410
frustration, 406
fullprobabilistic model,156
function minimization, 473
functions, 246
gain,507
Galileo code,186
Gallager code,557,seeerror-correcting
codes, low-densit yparity-
check
Gallager, Robert,170,172,187
Galois eld,605
Galois elds, 185
game, seepuzzle
Bridge, 126
guess thattune, 204
guessing, 110
life,520
sixty-three ,70
submarine ,71
three doors,57,60,454
twentyquestions, 70
game show,57,454
game-pla ying,451
gamma distribution, 313,319
gamma function, 598
ganglion cells,491
Gaussian, 398,501
Gaussian channel, 155,177
Gaussian distribution, 2,176,312,
321,549
N{dimensional, 124
parameters, 319
sample from, 312
Gaussian processes, 535
variational Gaussian processclas-
sier, 547
generalization, 483
generalized parity-checkmatrix, 581
generating function, 88
generativ emodel,27,156
generator matrix, 9,183
genes, 201
genetic algorithm, 395,396
genetic code,279
genome, 201,280
geometric progression, 258
George, E.I.,394
geostatistics, 536,548
GF(q),568
Gibbs entropy,85
Gibbs sampling, 370,391,418,see
MonteCarlo metho ds
Gibbs' inequalit y,34,37,44
Gilbert{Varshamo vconjecture, 212
Gilbert{Varshamo vdistance, 212,221
Gilbert{Varshamo vrate,212
Gilks, W.R., 394
girlie stu, 58Glaub erdynamics, 370,seeMonte
Carlo metho ds,Gibbs sam-
pling
Glavieux, A.,186
Golaycode,209
golden ratio, 253
good,seeerror-correcting code,good
Good,Jack,265
gradien tdescen t,476,479,498,529
natural, 443
graduated non-con vexity,518
Graham, Ronald L.,175
graph, 251
factor graph, 334
ofcode,20
graphs andcycles, 242
graphs anderror-correcting codes,19
guessing game, 110,111,115
gzip,119
Haldane, 278
Hamilton, 278
Hamiltonian MonteCarlo, 387,397,
496,496,497
Hamming code,9,12,13,17,19,183,
184,190,208,209,214,219
graph, 19
Hamming distance, 206
harddrive,593
hashcode,193,231
hashfunction, 195,200,228
linear, 231
one-w ay,200
hatpuzzle, 222
heatbath, 601,seeMonteCarlo meth-
ods,Gibbs sampling
heatcapacit y,401,404
Hebb, Donald, 505
Hebbian learning, 505
Hertz, 178
Hessian, 501
hidden Markovmodels,437
hidden neurons, 525
hierarc hicalclustering, 284
hierarc hicalmodel,379,548
highdimensions, lifein,37
hintforcomputing mutual informa-
tion,149
Hinton,Georey E.,353,429,432,522
hitchhiker,280
homogenous, 544
Hooke,Robert,200
Hopeld network,283,505,506,517
capacit y,514
Hopeld, JohnJ.,246,280,517
hot-sp ot,275
Human code,91,99,103
`optimalit y',99,101
disadv antages, 100,115
general alphab et,104,107
human, 269
human{mac hineinterfaces, 118,126
hybrid MonteCarlo, 387,seeHamilto-
nianMonteCarlo
hydrogen bond,280
hyperparameter, 64,318,319,379,479

<<<PAGE 636>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
624 Index
hypersphere, 42
hypothesis testing, seemodelcompar-
ison,sampling theory
i.i.d., 80
ICA,seeindependen tcomponentanal-
ysis
ICF(intrinsic correlation function),
551
identicaltwin,111
identitymatrix, 600
ignorance, 446
image, 549
image analysis, 343,350
image compression, 74,284
image models,399
image processing, 246
image reconstruction, 551
implicit assumptions, 186
implicit probabilities, 97,98,102
importance sampling, 361,362,379,
seeMonteCarlo metho ds
weakness of,382
improp er,314,316,319,320,342
improp erprior, 353
in-car navigation, 594
independence, 138
independen tcomponentanalysis, 313,
437,443
indicator function, 600
inequalit y,35,81
inference, 27,529
andlearning, 493
inference problems
bentcoin,51
information, 66
information content,72,73,91,349
howtomeasure, 67
Shannon, 67
information maximization, 443
information retriev al,193
information theory ,4
inner code,184
Inquisition, 346
insertions, 187
instan taneous, 92
integral image, 246
interleavers,579
interleaving,184,186
internet, 188,589
intersection, 66,222
intrinsic correlation function, 549,551
invariance, 445
invariantdistribution, 372
inverseprobabilit y,27
inverse-arithmetic-co der,118
inverse-cosh distribution, 313
inverse-gamma distribution, 314
inversion ofhashfunction, 199
investmen tportfolio, 455
irregular, 568
ISBN, 235
Isingmodel,130,283,399,400
iterativ eprobabilistic decoding, 557
Jaakk ola,Tommi S.,433,547Jereys prior, 316
Jensen's inequalit y,35,44
JetPropulsion Laboratory ,186
jointensem ble,138
jointentropy,138
jointtypicalit y,162
jointtypicalit ytheorem, 163
jointlytypical, 162
Jordan, MichaelI.,433,547
journal
publication policy,463
judge, 55
juggling, 15
junction treealgorithm, 340
jury,26,55
K-means algorithm, 303
K-means clustering, 285,303
soft,289
kaboom,306,433
Kalman lter, 535
kernel, 548
keypoints
howmuchdataneeded, 53
howtosolveprobabilit yproblems,
61
keyboard, 118
Kikuc hifreeenergy ,434
KLdistance, 34
Knowlton{Graham partitions, 175
Kolmogoro v,Andrei Nikolaevic h,548
Kraft inequalit y,521
Kraft, L.G., 95
Kraft inequalit y,94
kriging, 536
Kullbac k{Leibler divergence, 34,see
relativ eentropy
Lagrange multiplier, 174
lake,359
Langevin metho d,498
Langevin process,535
language model,118
Laplace approximation, seeLaplace's
metho d
Laplace model,117
Laplace prior, 316
Laplace's metho d,341,354,496,501,
537,547
Laplace's rule,52
latentvariable, 437
latentvariable model
compression, 353
latent-variable modelling, 283
lawoflargenumbers,36
lawyer,55,58,61
LeCun,Yann,121
leaf,336
leapfrog algorithm, 389
learning, 471
ascomm unication, 483
asinference, 492,493
Hebbian, 505
inevolution, 278
learning algorithms, 468
backpropagation, 528Boltzmann machine,522
classication, 475
competitivelearning, 285
Hopeld network,505
K-means clustering, 286,289,
303
multilayerperceptron, 528
single neuron, 475
learning rule,470
Lempel{Ziv coding, 110,119{122
criticisms, 128
life,520
lifeinhighdimensions, 37,124
likelihood,6,28,49,152,324,529,558
contrasted withprobabilit y,28
subjectivit y,30
likelihoodequivalence, 447
likelihoodprinciple, 32,61,464
limitcycle, 508
linear blockcode,9
linear feedbac kshiftregister, 184
linear regression, 342,527
Litsyn, Simon, 572
little'n'largedataset,288
log-normal, 315
logarithms, 2
logit, 316
longthinstrip, 409
loopy,340,556
loopybeliefpropagation, 434
loopymessage-passing, 338
lossycompression, 168,284,285
low-densit ygenerator-matrix code,
207,590
low-densit yparity-checkcode,556,
557
staircase, 569
LTcode,590
Luby,MichaelG.,568,590
Luria, Salvador, 446
Lyapuno vfunction, 287,291,508,520,
521
Mezard, Marc, 340
macho,319
MacKa y,David,187,496
magic, 233
magician, 233
magnetic recording, 593
majorityvote,5
male, 277
Mandelbrot, Benoit, 262
MAP,6,325,538,seemaxim umapos-
teriori
MAP decoding, 325
mapping, 92
marginal entropy,139
marginal likelihood,29,298,322
marginalization, 29,295,319
Markovchain, 141,168
MarkovchainMonteCarlo, seeMonte
Carlo metho ds
Markovmodel,111,seeMarkovchain
andhidden Markovmodel
matrix, 409
matrix identities, 438

<<<PAGE 637>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
Index 625
max{pro duct, 339
maxen t,308,seemaxim umentropy
maxim umdistance separable, 219,see
error-correcting code,maxi-
mumdistance separable
maxim umentropy,308,551
maxim umlikelihood,6,300,347
maxim umlikelihooddecoder,152
maxim umaposteriori ,307
MCMC (Mark ovchainMonteCarlo),
seeMonteCarlo metho ds
McMillan, B.,95
MD5, 200
MDL, seeminim umdescription length
MDS, 220,seeerror-correcting code,
maxim umdistance separable
mean, 1
mean eldtheory ,422,425
melody,201,203
memory ,468
address-based, 468
associative,468,505
content-addressable, 192,469
MemSys, 551
message passing, 187,241,248,283,
324,407,591
BCJR, 330
beliefpropagation, 330
forward{bac kward,330
ingraphs withcycles, 338
loopy,338
sum{pro ductalgorithm, 336
Viterbi, 329
metaco de,104,108
metric, 512
Metrop olismetho d,496,seeMonte
Carlo metho ds
micro-saccades, 554
microsoftus, 458
microwaveoven,126
min{sum algorithm, 245,325,329,578,
581
mine(hole inground), 451
minimax, 455
minimization, 473
minim umdescription length, 351,352
minim umdistance, 206,214,seedis-
tance ofcode
Minka,Thomas, 340
mirror, 529
Mitzenmac her,Michael,568
mixing coecien ts,298,312
mixture
inMarkovchains, 373
mixture distribution, 373
mixture model,437
mixture modelling, 282,284,303
mixture ofGaussians, 312
MLP,seemultilayerperceptron
MML, seeminim umdescription length
mobile phone, 186
model
latentvariable, 437
modelcomparison, 198,346,347,349typical behaviour ofevidence, 54,
60
modelsofimages, 524
moderation, 29,498,seemarginaliza-
tion
molecules, 201
Molesw orth,241
momen tum,387,479
MonteCarlo metho ds,357,498
acceptance rate,394
acceptance ratiometho d,379
annealed importance sampling,
379
coalescence, 413
dependence ondimensionalit y,
358
exact sampling, 413
forvisualization, 551
Gibbs sampling, 370,391,418
Hamiltonian MonteCarlo, 387,
496
hybrid MonteCarlo, seeHamilto-
nianMonteCarlo
importance sampling, 361,379
weakness of,382
Langevin metho d,498
MarkovchainMonteCarlo, 365,
366
Metrop olismetho d
dumbMetrop olis,394,496
Metrop olis{Hastings, 365
multi-state, 393,395,398
overrelaxation
ordered, 391
perfect simulation, 413
random walksuppression, 370
random-w alkMetrop olis,388
rejection sampling, 364
adaptiv e,370
reversible jump, 379
simulated annealing, 379,392
thermo dynamic integration, 379
umbrella sampling, 379
MontyHallproblem, 57
Morse, 256
motorcycle, 110
movie,551
multilayerperceptron, 529,535
multiple access channel, 237
multiterminal networks,239
multivariate Gaussian, 176
Munro{Robbins theorem, 441
murder, 26,58,61,354
music,201,203
mutation rate,446
mutual information, 139,146,150,151
howtocompute, 149
myth,347
compression, 74
nat(unit), 264,601
natural gradien t,443
natural selection, 269
navigation, 594Neal, Radford, 111,121,187,374,379,
391,392,397,419,420,429,
432,496
needle, Buon's, 38
network,529
neural network,468,470
capacit y,483
learning ascomm unication, 483
learning asinference, 492
neuron, 471
capacit y,483
Newton algorithm, 441
Newton, Isaac, 200,552
Newton{Raphson, 303
noise, 3,seechannel
coloured, 179
spectral densit y,177
white, 179
noisy channel, seechannel
Gaussian, 177
noisy typewriter, 148,152,154
noisy-c hannel codingtheorem, 15,152,
162,171,229
poorman's version, 216
noisy-or, 294
non-confusable inputs, 152
noninformativ e,319
nonlinear, 535
nonlinear code,187
nonparametric datamodelling, 537
nonrecursiv e,575
noodle,Buon's, 38
normal, 312,seeGaussian
normal graph, 219,584
normalizing constan t,seepartition
function
not-sum, 335
notation, 598,seeconventions
absolute value,33,599
conventionsofthisbook,147
expectation, 37
intervals,90
logarithms, 2
matrices, 147
setsize,33,599
vectors, 147
NP-complete, 184,325,517
nucleotide, 201,204
nuisance parameters, 319
numerology ,208
objectivefunction, 473
Occam factor, 322,345,348,350,352
Occam's razor, 343,343
octal,575
octave ,478
OdetoJoy,203
Oliver,56
one-w ayhashfunction, 200
optic nerve,491
optimal decoder,152
optimal input distribution, 150,162
optimal linear lter, 549
Optimization, 531
optimization, 169,392,429,479,505,
516,531

<<<PAGE 638>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
626 Index
gradien tdescen t,476
Newton algorithm, 441
ordered overrelaxation, 391
orthodoxstatistics, 320,seesampling
theory
outer code,184
overtting, 306,322,529
overrelaxation, 390
ordered, 391
p-value,64,457,462
packet,188
parado x
Allias, 454
parado xes,107
paramagnetic, seeIsingmodel
paranormal, 233
parasite, 278
parent,559
parity,9
paritycheckbits,203
paritychecknodes,567,568,583
parity-checkbits,9
parity-checkconstrain ts,20
parity-checkmatrix, 12,229
generalized, 581
parity-checknodes,19,219
parse, 119,448
partial order, 418
partial partition functions, 407
particle lter, 396
partition, 174
partition function, 401,407,409,422,
423,601,603
analogy withlake,360
partitioned inverse,543
Pasco,111
pattern recognition, 156,179,201
pentagonful code,21,221
perfect code,208,210,211,219,589
perfect simulation, 413,seeMonte
Carlo metho ds,exact sam-
pling
periodicvariable, 315
permutation, 18,268
phase transition, 361,601
philosoph y,26,119,384
phone, 594
phone directory ,193
phone number,58,128
photon, 307
photon counter,307,342,448
physics, 85
pigeon-hole, 573
pigeon-hole principle, 86
pitchfork bifurcation, 291
plaintext,265
plankton, 359
pointestimate, 432
pointspread function, 549
pointer,119
Poisson distribution, 2,175,307,311,
342
Poisson process,39,46,448
Poisson ville,39,313
polymer, 257poorman's codingtheorem, 216
porridge, 280
positivedenite, 539
positivit y,551
posterior probabilit y,6,152
powercost,180
powerlaw,584
practical, 183, seeerror-correcting
code,practical
precision, 176,181,312,320,383
prediction, 29,52
predictiv edistribution, 111
prex code,92,95
prior, 6,308,529
assigning, 308
improp er,353
subjectivit y,30
priorequivalence, 447
priorit yofbitsinamessage, 239
prize, ongame show,57
probabilistic model,111,120
probabilistic movie,551
probabilit y,26,38
Bayesian, 50
contrasted withlikelihood,28
probabilit ydistributions, 311
probabilit yofblockerror, 152
probabilit ypropagation, seesum{
productalgorithm
productcode,184,214
prole, 569
prole, ofrandom graph, 568
pronunciation, 34
proper,539
proposaldensit y,364,365
Propp, JimG.,413,419
prosecutor's fallacy ,25
prospecting, 451
protein, 201,204
regulatory ,201,204
protein synthesis, 280
protocol,589
pseudoin verse,550
Punch,448
puncturing, 222,580
puzzle, seegame
chessboard, 520
delityofDNA replication, 280
hat,222,223
life,520
magic trick,233,234
poisoned glass, 103
southeast ,520
transatlan ticcable, 173
weighing 12balls, 68
quantumerror-correction, 572
QWER TY,118
R3,seerepetition code
race,354
radial basisfunction, 535,536
RAID, 188,190,220
random, 357
random code,156,161,164,165,184,
192,195,214,565forcompression, 231
random variable, 26
random walk,367
suppression, 370
random-co dingexponent,171
random-w alkMetrop olismetho d,388
rant,seesermon
condence level,465
p-value,463
Raptor codes,594
rate,152
rate-distortion theory ,167
reading aloud, 529
receiv eroperating characteristic, 533
recognition, 204
record breaking, 446
rectangular code,184
reducible, 373
redundancy ,4,33
inchannel code,146
redundan tarrayofindependen tdisks,
188,190
redundan tconstrain tsincode,20
Reed{Solomon code,185,186,571,589
regression, 342,536
regret, 455
regular, 557
regularization, 529,550
regularization constan t,479
Reinforcemen tlearning, 453
rejection, 364,366,533
rejection sampling, 364,seeMonte
Carlo metho ds
relativ eentropy,34,98,102,142,422,
429,435,475
reliabilit yfunction, 171
repeat{accum ulate code
connection tolow-densit yparity-
checkcode,587
repetition code,5,13,15,16,46,183
responsibilit y,289
retransmission, 589
reverse,110
reversible jump, 379
Richardson, Thomas J.,570,595
Rissanen, Jorma, 111
Roberts,Gareth O.,394
ROC,533
roman, 126
ruleofthumb,380
runlength, 256
runlength-limited channel, 249
saccades, 554
saddle-p ointapproximation, 341
sample, 312
fromGaussian, 312
sampler densit y,362
sampling distribution, 459
sampling theory ,38,320
criticisms, 32
satellite, 594
satellite comm unications, 186
scaling, 203
Schonberg,203
Schottky anomaly ,404

<<<PAGE 639>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
Index 627
secret, 200
securit y,199
seektime, 593
Sejnowski, TerryJ.,522
self-delimiting, 132
self-dual, 218
self-orthogonal, 218
self-punctuating, 92
sequence, 344
sequen tialdecoding, 581
sequen tialprobabilit yratiotest,464
sermon, seecaution
classical statistics, 64
condence level,465
gradien tdescen t,441
importance sampling, 382
interleaving,189
MAP metho d,283
maxim umentropy,308
maxim umlikelihood,306
maxim umaposteriori metho d,
306
mostprobable isatypical, 283
p-value,463
sampling theory ,64
sphere-pac king,209,212
stopping rule,463
turbocodes,581
unbiased estimator, 307
worst-case-ism, 207
set,66
Shannon, seenoisy-c hannel codingthe-
orem, source coding theo-
rem,information content
shannon (unit), 265
Shannon information content,67,91,
115
Shannon, Claude, 14,15,152,164,212,
215,262
Shevelev,Vladimir, 572
shifter ensem ble,524
Shokrollahi, M.Amin, 568
shortening, 222
Siegel, Paul,262
sigmoid, 473,527
signal tonoise ratio, 177
signicance, 463
signicance level,51,457
signicance levels,64
simplex, 173,316
Simpson's parado x,355
Simpson, O.J.,seewife-b eaters
simulated annealing, 379,392,seean-
nealing
Skilling, John, 392
slicesampling, 374,seeMonteCarlo
metho ds
multi-dimensional, 378
softK-means clustering, 289
softmax, 339
softmax, softmin, 289
software
arithmetic coding, 121
solarsystem, 346
soliton distribution, 592sound, 187
source code,73,75
blockcode,76
Burro ws{Wheeler transform, 121
forintegers, 132
Human, seeHuman code
implicit probabilities, 102
optimal lengths, 97,102
prex code,95
stream codes,110{130
supermark et,96,104,112
symbolcode,91
optimal, 91
uniquely decodeable, 94
variable symboldurations, 125,
256
source coding
block-sorting compression, 121
forcomplex sources, 353
source codingtheorem, 78,91,229,231
southeast puzzle, 520
span, 331
sparse graph code,556
densit yevolution, 566
sparsiers, 255
species, 269
speculation
earlyvision, 554
spell,201
sphere-pac kingexponent,172
Spielman, Daniel A.,568
spinsystem, 400
spines, 524
spline, 538
spring, 291
square, 38
staircase, 569,587
stalactite, 214
standard deviation, 320
stars, 307
statediagram, 251
statistic, 458
sucien t,300
statistical physics, 257,401
statistical test,51,458
steepestdescen ts,441
stereoscopic vision, 524
stiness, 289
Stirling's approximation, 1,7
stochastic, 472
stochastic gradien t,476
stop-when-it's-done, 561,583
stopping rule,463
straws,drawing, 233
stream codes,110{130
studen t,125
Studen t-tdistribution, 312,323
subjectiveprobabilit y,26,30
submarine ,71
subscrib er,593
subset, 66
substring, 119
sucien tstatistics, 300
sumrule,39,46sum{pro ductalgorithm, 187,245,326,
334,336,407,434,556,557,
572,578
summary ,335
summary state, 419
summation convention,438
super-channel, 184
supermark etforcodewords, 96,104,
112
supportvector, 548
surprise value,264
surveypropagation, 340
suspicious coincidences, 350
symbolcode,91
budget, 94,96
codeword,92
disadv antages, 100
optimal, 91
self-delimiting, 132
supermark et,112
symmetric channel, 171
symmetry argumen t,151
synchronization, 249
synchronization errors, 187
syndrome, 10{12, 20
syndrome decoding, 11,216,229,371
system, 4
systematic, 575
t-distribution, seeStuden t-t
tail,85,312,313,440,446,503,584
tamperdetection, 199
Tank,DavidW.,517
Tanner productcode,571
Tanner, Michael,569
Tanzanite, 451
tap,575
telephone, 125
telephone directory ,193
telephone number,58,128
telescop e,529
temperature, 392,601
termination, 579
terminology ,598
MonteCarlo metho ds,372
test
uctuation, 446
statistical, 51,458
textentry,118
thermal distribution, 88
thermo dynamic integration, 379
thermo dynamics, 404,601
Thiele, T.N., 547
thinshell, 37,124
Thitima jshima, P.,186
three cards, 141
three doors,57
threshold, 567
tiling, 420
time-division, 237
timing, 187
transatlan tic,173
transfer matrix metho d,407
transition, 251
transition probabilit ymatrix, 147,607
translation-in variant,409

<<<PAGE 640>>>

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.
628 Index
travelling salesman problem, 246,517
tree,242,336,343,350
trellis, 251
termination, 579
trellis section, 251,257
triangle, 307
truth function, 211,600
tube,257
turbocode,186,556
turboproductcode,571
Turing, Alan, 265
twentyquestions, 70,103
twin,111
typical set,80,154,363
forcompression, 80
fornoisy channel, 154
typical setdecoder,165,230
typicalit y,78,80,162
behaviour ofevidence, 54,60
umbrella sampling, 379
unbiased estimator, 307,321,449
uncompression, 231
union, 66
union bound, 216
uniquely decodeable, 93,94
units, 264
universal, 110,120,135
universalit y,400
Urbank e,Rudiger, 570,595
urn,31
userinterfaces, 118utility,451
vaccination, 458
Vapnik{Cherv onenkis dimension, 489
variable-length code,249,255
variable-rate error-correcting codes,
238,590
variance, 1,27,88,321
variance{co variance matrix, 176
variational Bayes,429
variational freeenergy ,422,423
minimization, 423
variational metho ds,422,433,496,508
typical properties, 435
variational Gaussian processclas-
sier, 547
VCdimension, 489
vectorquantization, 284,290
verygood,seeerror-correcting code,
verygood
VGC(variational Gaussian process
classier), 547
Virtak allio,Juhani, 209
Viterbi algorithm, 245,329,340,578
volume, 42
VonMises distribution, 315
Wainwright,Martin, 340
waiting forabus,39,46
warning, seecaution
Watson{Cric kbasepairing, 280
weighing babies, 164weighing problem, 66,68
weight
importance sampling, 362
inneural net,471
ofbinary vector, 20
weightdecay,479,529
weightenumerator, 206,211,214,216
typical, 572
weightspace, 473,474,487
Wenglish, 72,260
what numbercomes next?, 344
white, 354
Wiberg,Niclas, 187
Wiener process,535
Wiener, Norbert,548
wife-b eater, 58,61
Wilson, DavidB.,413,419
windo w,307
Winfree, Erik,520
Wolf,Jack,262
word-English, 260
worldrecord, 446
worst-case-ism, 207,213
writing, 118
Yedidia, Jonathan, 340
Zchannel, 148,149{151, 155
Zecchina, R.,340
Zipfplot,317
Zipf'slaw,40
Zipf,George K.,262