
<<<PAGE 1>>>

To Isa

<<<PAGE 2>>>



<<<PAGE 3>>>

Preface
Taken literally, the title “All of Statistics” is an exaggeration. But in spirit,
the title is apt, as the book does cover a much broader range of topics than a
typical introductory book on mathematical statistics.
This book is for people who want to learn probability and statistics quickly.
It is suitable for graduate or advanced undergraduate students in computerscience, mathematics, statistics, and related disciplines. The book includes
modern topics like nonparametric curve estimation, bootstrapping, and clas-
siﬁcation, topics that are usually relegated to follow-up courses. The reader ispresumed to know calculus and a little linear algebra. No previous knowledge
of probability and statistics is required.
Statistics ,data mining ,a n d machine learning are all concerned with
collecting and analyzing data. For some time, statistics research was con-ducted in statistics departments while data mining and machine learning re-search was conducted in computer science departments. Statisticians thought
that computer scientists were reinventing the wheel. Computer scientists
thought that statistical theory didn’t apply to their problems.
Things are changing. Statisticians now recognize that computer scientists
are making novel contributions while computer scientists now recognize thegenerality of statistical theory and methodology. Clever data mining algo-
rithms are more scalable than statisticians ever thought possible. Formal sta-
tistical theory is more pervasive than computer scientists had realized.
Students who analyze data, or who aspire to develop new methods for
analyzing data, should be well grounded in basic probability and mathematicalstatistics. Using fancy tools like neural nets, boosting, and support vector

<<<PAGE 4>>>

viii Preface
machines without understanding basic statistics is like doing brain surgery
before knowing how to use a band-aid.
But where can students learn basic probability and statistics quickly? Nowhere.
At least, that was my conclusion when my computer science colleagues keptasking me: “Where can I send my students to get a good understanding ofmodern statistics quickly?” The typical mathematical statistics course spendstoo much time on tedious and uninspiring topics (counting methods, two di-mensional integrals, etc.) at the expense of covering modern concepts (boot-strapping, curve estimation, graphical models, etc.). So I set out to redesignour undergraduate honors course on probability and mathematical statistics.This book arose from that course. Here is a summary of the main features ofthis book.
1. The book is suitable for graduate students in computer science and
honors undergraduates in math, statistics, and computer science. It isalso useful for students beginning graduate work in statistics who needto ﬁll in their background on mathematical statistics.
2. I cover advanced topics that are traditionally not taught in a ﬁrst course.
For example, nonparametric regression, bootstrapping, density estima-tion, and graphical models.
3. I have omitted topics in probability that do not play a central role in
statistical inference. For example, counting methods are virtually ab-sent.
4. Whenever possible, I avoid tedious calculations in favor of emphasizing
concepts.
5. I cover nonparametric inference before parametric inference.6. I abandon the usual “First Term = Probability” and “Second Term
= Statistics” approach. Some students only take the ﬁrst half and itwould be a crime if they did not see any statistical theory. Furthermore,probability is more engaging when students can see it put to work in thecontext of statistics. An exception is the topic of stochastic processeswhich is included in the later material.
7. The course moves very quickly and covers much material. My colleagues
joke that I cover all of statistics in this course and hence the title. Thecourse is demanding but I have worked hard to make the material asintuitive as possible so that the material is very understandable despitethe fast pace.
8. Rigor and clarity are not synonymous. I have tried to strike a good
balance. To avoid getting bogged down in uninteresting technical details,many results are stated without proof. The bibliographic references atthe end of each chapter point the student to appropriate sources.

<<<PAGE 5>>>

Preface ix
Data generating process Observed dataProbability
Inference and Data Mining
FIGURE 1. Probability and inference.
9. On my website are ﬁles with R code which students can use for doing
all the computing. The website is:
http://www.stat.cmu.edu/ ∼larry/all-of-statistics
However, the book is not tied to R and any computing language can be
used.
Part I of the text is concerned with probability theory, the formal language
of uncertainty which is the basis of statistical inference. The basic problem
that we study in probability is:
Given a data generating process, what are the properties of the out-
comes?
Part II is about statistical inference and its close cousins, data mining and
machine learning. The basic problem of statistical inference is the inverse ofprobability:
Given the outcomes, what can we say about the process that gener-
ated the data?
These ideas are illustrated in Figure 1. Prediction, classiﬁcation, clustering,
and estimation are all special cases of statistical inference. Data analysis,
machine learning and data mining are various names given to the practice of
statistical inference, depending on the context.

<<<PAGE 6>>>

xP r e f a c e
Part III applies the ideas from Part II to speciﬁc problems such as regres-
sion, graphical models, causation, density estimation, smoothing, classiﬁca-
tion, and simulation. Part III contains one more chapter on probability that
covers stochastic processes including Markov chains.
I have drawn on other books in many places. Most chapters contain a section
called Bibliographic Remarks which serves both to acknowledge my debt to
other authors and to point readers to other useful references. I would especially
like to mention the books by DeGroot and Schervish (2002) and Grimmett
and Stirzaker (1982) from which I adapted many examples and exercises.
As one develops a book over several yea rs it is easy to lose track of where pre-
sentation ideas and, especially, homework problems originated. Some I made
up. Some I remembered from my education. Some I borrowed from otherbooks. I hope I do not oﬀend anyone if I have used a problem from their book
and failed to give proper credit. As my colleague Mark Schervish wrote in his
book (Schervish (1995)),
“...t h ep rob l e m satt h ee n d sofe ac hc h ap t e rh a v ec om ef romm an y
sou rc e s. ...T h e se p rob l e m s, i n t u rn , c am e f rom v ari ou s sou rc e s
u n kn o w nt om e...I fIh a v eu se dap rob l e mw i t h ou tgi vi n gp rope rcredit, please take it as a compliment.”
I am indebted to many people without whose help I could not have written
this book. First and foremost, the many students who used earlier versions
of this text and provided much feedback. In particular, Liz Prather and Jen-nifer Bakal read the book carefully. Rob Reeder valiantly read through the
entire book in excruciating detail and gave me countless suggestions for im-
provements. Chris Genovese deserves special mention. He not only providedhelpful ideas about intellectual content, but also spent many, many hours
writing L
ATEXcode for the book. The best aspects of the book’s layout are due
to his hard work; any stylistic deﬁcien cies are due to my lack of expertise.
David Hand, Sam Roweis, and David Scott read the book very carefully and
made numerous suggestions that greatly improved the book. John Laﬀertyand Peter Spirtes also provided helpfu l feedback. John Kimmel has been sup-
portive and helpful throughout the writing process. Finally, my wife Isabella
Verdinelli has been an invaluable source of love, support, and inspiration.
Larry Wasserman
Pittsburgh, Pennsylvania
July 2003

<<<PAGE 7>>>

Preface xi
Statistics/Data Mining Dictionary
Statisticians and computer scientists often use diﬀerent language for the
same thing. Here is a dictionary that the reader may want to return tothroughout the course.
Statistics
Computer Science Meaning
estimation learning using data to estimate
an unknown quantity
classiﬁcation supervised learning predicting a discrete Y
fromX
clustering unsupervised learning putting data into groupsdata training sample ( X
1,Y1),...,(Xn,Yn)
covariates features the Xi’s
classiﬁer hypothesis a map from covariates
to outcomes
hypothesis — subset of a parameter
space Θ
conﬁdence interval — interval that contains an
unknown quantitywith given frequency
directed acyclic graph Bayes net multivariate distribution
with given conditionalindependence relations
Bayesian inference Bayesian inference statistical methods for
using data toupdate beliefs
frequentist inference — statistical methods
with guaranteedfrequency behavior
large deviation bounds PAC learning uniform bounds on
probability of errors

<<<PAGE 8>>>



<<<PAGE 9>>>

Contents
I Probability
1 Probability 3
1.1 Introduction ............................. 3
1.2 Sample Spaces and Events ..................... 3
1.3 Probability ............................. 5
1.4 Probability on Finite Sample Spaces ............... 7
1.5 Independent Events ........................ 8
1.6 Conditional Probability ...................... 1 0
1.7 Bayes’ Theorem ........................... 1 2
1.8 Bibliographic Remarks ....................... 1 3
1.9 Appendix .............................. 1 3
1.10 Exercises .............................. 1 3
2 Random Variables 19
2.1 Introduction ............................. 1 9
2.2 Distribution Functions and Probability Functions ........ 2 0
2.3 Some Important Discrete Random Variables ........... 2 5
2.4 Some Important Continuous Random Variables ......... 2 7
2.5 Bivariate Distributions ....................... 3 1
2.6 Marginal Distributions ....................... 3 3
2.7 Independent Random Variables .................. 3 4
2.8 Conditional Distributions ..................... 3 6

<<<PAGE 10>>>

xiv Contents
2.9 Multivariate Distributions and iidSamples ........... 3 8
2.10 Two Important Multivariate Distributions ............ 3 9
2.11 Transformations of Random Variables .............. 4 1
2.12 Transformations of Several Random Variables .......... 4 2
2.13 Appendix .............................. 4 3
2.14 Exercises .............................. 4 3
3 Expectation 47
3.1 Expectation of a Random Variable ................ 4 7
3.2 Properties of Expectations ..................... 5 0
3.3 Variance and Covariance ...................... 5 0
3.4 Expectation and Variance of Important Random Variables . . . 523.5 Conditional Expectation ...................... 5 4
3.6 Moment Generating Functions .................. 5 6
3.7 Appendix .............................. 5 8
3.8 Exercises .............................. 5 8
4 Inequalities 63
4.1 Probability Inequalities ...................... 6 3
4.2 Inequalities For Expectations ................... 6 6
4.3 Bibliographic Remarks ....................... 6 6
4.4 Appendix .............................. 6 7
4.5 Exercises .............................. 6 8
5 Convergence of Random Variables 71
5.1 Introduction ............................. 7 1
5.2 Types of Convergence ....................... 7 2
5.3 The Law of Large Numbers .................... 7 6
5.4 The Central Limit Theorem .................... 7 7
5.5 The Delta Method ......................... 7 9
5.6 Bibliographic Remarks ....................... 8 0
5.7 Appendix .............................. 8 1
5.7.1 Almost Sure and L
1Convergence ............. 8 1
5.7.2 Proof of the Central Limit Theorem ........... 8 1
5.8 Exercises .............................. 8 2
II Statistical Inference
6 Models, Statistical Inference and Learning 87
6.1 Introduction ............................. 8 7
6.2 Parametric and Nonparametric Models .............. 8 7
6.3 Fundamental Concepts in Inference ................ 9 0
6.3.1 Point Estimation ...................... 9 0
6.3.2 Conﬁdence Sets ....................... 9 2

<<<PAGE 11>>>

Contents xv
6.3.3 Hypothesis Testing ..................... 9 4
6.4 Bibliographic Remarks ....................... 9 5
6.5 Appendix .............................. 9 5
6.6 Exercises .............................. 9 5
7 Estimating the cdfand Statistical Functionals 97
7.1 The Empirical Distribution Function ............... 9 7
7.2 Statistical Functionals ....................... 9 9
7.3 Bibliographic Remarks .......................1 0 4
7.4 Exercises ..............................1 0 4
8 The Bootstrap 107
8.1 Simulation ..............................1 0 8
8.2 Bootstrap Variance Estimation ..................1 0 8
8.3 Bootstrap Conﬁdence Intervals ..................1 1 0
8.4 Bibliographic Remarks .......................1 1 5
8.5 Appendix ..............................1 1 5
8.5.1 The Jackknife ........................1 1 5
8.5.2 Justiﬁcation For The Percentile Interval .........1 1 6
8.6 Exercises ..............................1 1 6
9 Parametric Inference 119
9.1 Parameter of Interest ........................1 2 0
9.2 The Method of Moments ......................1 2 0
9.3 Maximum Likelihood ........................1 2 2
9.4 Properties of Maximum Likelihood Estimators .........1 2 4
9.5 Consistency of Maximum Likelihood Estimators .........1 2 6
9.6 Equivariance of the mle......................1 2 7
9.7 Asymptotic Normality .......................1 2 8
9.8 Optimality .............................1 3 0
9.9 The Delta Method .........................1 3 1
9.10 Multiparameter Models ......................1 3 3
9.11 The Parametric Bootstrap .....................1 3 4
9.12 Checking Assumptions .......................1 3 5
9.13 Appendix ..............................1 3 5
9.13.1 Proofs ............................1 3 5
9.13.2 Suﬃciency ..........................1 3 7
9.13.3 Exponential Families ....................1 4 0
9.13.4 Computing Maximum Likelihood Estimates .......1 4 2
9.14 Exercises ..............................1 4 6
10 Hypothesis Testing and p-values 149
10.1 The Wald Test ...........................1 5 2
10.2 p-values ...............................1 5 6
10.3 The χ2Distribution ........................1 5 9

<<<PAGE 12>>>

xvi Contents
10.4 Pearson’s χ2Test For Multinomial Data .............1 6 0
10.5 The Permutation Test .......................1 6 1
10.6 The Likelihood Ratio Test .....................1 6 4
10.7 Multiple Testing ..........................1 6 5
10.8 Goodness-of-ﬁt Tests ........................1 6 8
10.9 Bibliographic Remarks .......................1 6 9
10.10Appendix ..............................1 7 0
10.10.1The Neyman-Pearson Lemma ...............1 7 0
10.10.2The t-test ..........................1 7 0
10.11Exercises ..............................1 7 0
11 Bayesian Inference 175
11.1 The Bayesian Philosophy .....................1 7 5
11.2 The Bayesian Method .......................1 7 6
11.3 Functions of Parameters ......................1 8 0
11.4 Simulation ..............................1 8 0
11.5 Large Sample Properties of Bayes’ Procedures ..........1 8 1
11.6 Flat Priors, Improper Priors, and “Noninformative” Priors . . . 18111.7 Multiparameter Problems .....................1 8 3
11.8 Bayesian Testing ..........................1 8 4
11.9 Strengths and Weaknesses of Bayesian Inference ........1 8 5
11.10Bibliographic Remarks .......................1 8 9
11.11Appendix ..............................1 9 0
11.12Exercises ..............................1 9 0
12 Statistical Decision Theory 193
12.1 Preliminaries ............................1 9 3
12.2 Comparing Risk Functions .....................1 9 4
12.3 Bayes Estimators ..........................1 9 7
12.4 Minimax Rules ...........................1 9 8
12.5 Maximum Likelihood, Minimax, and Bayes ...........2 0 1
12.6 Admissibility ............................2 0 2
12.7 Stein’s Paradox ...........................2 0 4
12.8 Bibliographic Remarks .......................2 0 4
12.9 Exercises ..............................2 0 4
III Statistical Models and Methods
13 Linear and Logistic Regression 209
13.1 Simple Linear Regression .....................2 0 9
13.2 Least Squares and Maximum Likelihood .............2 1 2
13.3 Properties of the Least Squares Estimators ...........2 1 4
13.4 Prediction ..............................2 1 5
13.5 Multiple Regression ........................2 1 6

<<<PAGE 13>>>

Contents xvii
13.6 Model Selection ...........................2 1 8
13.7 Logistic Regression .........................2 2 3
13.8 Bibliographic Remarks .......................2 2 5
13.9 Appendix ..............................2 2 5
13.10Exercises ..............................2 2 6
14 Multivariate Models 231
14.1 Random Vectors ..........................2 3 2
14.2 Estimating the Correlation ....................2 3 3
14.3 Multivariate Normal ........................2 3 4
14.4 Multinomial .............................2 3 5
14.5 Bibliographic Remarks .......................2 3 7
14.6 Appendix ..............................2 3 7
14.7 Exercises ..............................2 3 8
15 Inference About Independence 239
15.1 Two Binary Variables .......................2 3 9
15.2 Two Discrete Variables .......................2 4 3
15.3 Two Continuous Variables .....................2 4 4
15.4 One Continuous Variable and One Discrete ...........2 4 4
15.5 Appendix ..............................2 4 5
15.6 Exercises ..............................2 4 8
16 Causal Inference 251
16.1 The Counterfactual Model .....................2 5 1
16.2 Beyond Binary Treatments ....................2 5 5
16.3 Observational Studies and Confounding .............2 5 7
16.4 Simpson’s Paradox .........................2 5 9
16.5 Bibliographic Remarks .......................2 6 1
16.6 Exercises ..............................2 6 1
17 Directed Graphs and Conditional Independence 263
17.1 Introduction .............................2 6 3
17.2 Conditional Independence .....................2 6 4
17.3 DAGs ................................2 6 4
17.4 Probability and DAGs .......................2 6 6
17.5 More Independence Relations ...................2 6 7
17.6 Estimation for DAGs ........................2 7 2
17.7 Bibliographic Remarks .......................2 7 2
17.8 Appendix ..............................2 7 2
17.9 Exercises ..............................2 7 6
18 Undirected Graphs 281
18.1 Undirected Graphs .........................2 8 1
18.2 Probability and Graphs ......................2 8 2

<<<PAGE 14>>>

xviii Contents
18.3 Cliques and Potentials .......................2 8 5
18.4 Fitting Graphs to Data ......................2 8 6
18.5 Bibliographic Remarks .......................2 8 6
18.6 Exercises ..............................2 8 6
19 Log-Linear Models 291
19.1 The Log-Linear Model .......................2 9 1
19.2 Graphical Log-Linear Models ...................2 9 4
19.3 Hierarchical Log-Linear Models ..................2 9 6
19.4 Model Generators ..........................2 9 7
19.5 Fitting Log-Linear Models to Data ................2 9 8
19.6 Bibliographic Remarks .......................3 0 0
19.7 Exercises ..............................3 0 1
20 Nonparametric Curve Estimation 303
20.1 The Bias-Variance Tradeoﬀ ....................3 0 4
20.2 Histograms .............................3 0 5
20.3 Kernel Density Estimation .....................3 1 2
20.4 Nonparametric Regression .....................3 1 9
20.5 Appendix ..............................3 2 4
20.6 Bibliographic Remarks .......................3 2 5
20.7 Exercises ..............................3 2 5
21 Smoothing Using Orthogonal Functions 327
21.1 Orthogonal Functions and L2Spaces ...............3 2 7
21.2 Density Estimation .........................3 3 1
21.3 Regression ..............................3 3 5
21.4 Wavelets ...............................3 4 0
21.5 Appendix ..............................3 4 5
21.6 Bibliographic Remarks .......................3 4 6
21.7 Exercises ..............................3 4 6
22 Classiﬁcation 349
22.1 Introduction ............................3 4 9
22.2 Error Rates and the Bayes Classiﬁer ...............3 5 0
22.3 Gaussian and Linear Classiﬁers ..................3 5 3
22.4 Linear Regression and Logistic Regression ...........3 5 6
22.5 Relationship Between Logistic Regression and LDA ......3 5 8
22.6 Density Estimation and Naive Bayes ...............3 5 9
22.7 Trees ................................3 6 0
22.8 Assessing Error Rates and Choosing a Good Classiﬁer .....3 6 2
22.9 Support Vector Machines .....................3 6 8
22.10 Kernelization ............................3 7 1
22.11 Other Classiﬁers ..........................3 7 5
22.12 Bibliographic Remarks ......................3 7 7

<<<PAGE 15>>>

Contents xix
22.13 Exercises ..............................3 7 7
23 Probability Redux: Stochastic Processes 381
23.1 Introduction .............................3 8 1
23.2 Markov Chains ...........................3 8 3
23.3 Poisson Processes ..........................3 9 4
23.4 Bibliographic Remarks .......................3 9 7
23.5 Exercises ..............................3 9 8
24 Simulation Methods 403
24.1 Bayesian Inference Revisited ....................4 0 3
24.2 Basic Monte Carlo Integration ..................4 0 4
24.3 Importance Sampling ........................4 0 8
24.4 MCMC Part I: The Metropolis–Hastings Algorithm ......4 1 1
24.5 MCMC Part II: Diﬀerent Flavors .................4 1 5
24.6 Bibliographic Remarks .......................4 2 0
24.7 Exercises ..............................4 2 0
Index 434

<<<PAGE 16>>>



<<<PAGE 17>>>

Part I
Probability

<<<PAGE 18>>>



<<<PAGE 19>>>

1
Probability
1.1 Introduction
Probability is a mathematical language for quantifying uncertainty. In this
Chapter we introduce the basic concepts underlying probability theory. Webegin with the sample space, which is the set of possible outcomes.
1.2 Sample Spaces and Events
Thesample space Ω is the set of possible outcomes of an experiment. Points
ωin Ω are called sample outcomes ,realizations ,o relements . Subsets of
Ω are called Events .
1.1 Example. If we toss a coin twice then Ω = {HH,HT,TH,TT }. The event
that the ﬁrst toss is heads is A={HH,HT }./squaresolid
1.2 Example. Letωbe the outcome of a measurement of some physical quan-
tity, for example, temperature. Then Ω = R=(−∞,∞). One could argue that
taking Ω = Ris not accurate since temperature has a lower bound. But there
is usually no harm in taking the sample space to be larger than needed. Theevent that the measurement is larger than 10 but less than or equal to 23 isA= (10,23].
/squaresolid

<<<PAGE 20>>>

4 1. Probability
1.3 Example. If we toss a coin forever, then the sample space is the inﬁnite
set
Ω=braceleftBig
ω=(ω1,ω2,ω3,...,):ωi∈{H,T}bracerightBig
.
LetEbe the event that the ﬁrst head appears on the third toss. Then
E=braceleftBig
(ω1,ω2,ω3,...,):ω1=T,ω2=T,ω3=H, ω i∈{H,T}fori>3bracerightBig
./squaresolid
Given an event A, letAc={ω∈Ω:ω/∈A}denote the complement of
A. Informally, Accan be read as “not A.” The complement of Ω is the empty
set∅. The union of events AandBis deﬁned
Auniondisplay
B={ω∈Ω:ω∈Aorω∈Borω∈both}
which can be thought of as “ AorB.” IfA1,A2,...is a sequence of sets then
∞uniondisplay
i=1Ai=braceleftBig
ω∈Ω:ω∈Aifor at least one ibracerightBig
.
The intersection of AandBis
Aintersectiondisplay
B={ω∈Ω:ω∈Aandω∈B}
read “ AandB.” Sometimes we write AintersectiontextBasABor (A, B). IfA1,A2,...is
a sequence of sets then
∞intersectiondisplay
i=1Ai=braceleftBig
ω∈Ω:ω∈Aifor all ibracerightBig
.
The set diﬀerence is deﬁned by A−B={ω:ω∈A, ω /∈B}. If every element
ofAis also contained in Bwe write A⊂Bor, equivalently, B⊃A.I fAis a
ﬁnite set, let |A|denote the number of elements in A. See the following table
for a summary.
Summary of Terminology
Ω sample space
ω outcome (point or element)
A event (subset of Ω)
Accomplement of A(notA)
AuniontextB union ( AorB)
AintersectiontextBorAB intersection ( AandB)
A−B set diﬀerence ( ωinAbut not in B)
A⊂B set inclusion
∅ null event (always false)
Ω true event (always true)

<<<PAGE 21>>>

1.3 Probability 5
We say that A1,A2,...aredisjoint or aremutually exclusive ifAiintersectiontextAj=
∅whenever i/negationslash=j. For example, A1=[ 0,1),A2=[ 1,2),A3=[ 2,3),...are
disjoint. A partition of Ω is a sequence of disjoint sets A1,A2,...such thatuniontext∞
i=1Ai= Ω. Given an event A, deﬁne the indicator function of Aby
IA(ω)=I(ω∈A)=braceleftbigg
1i f ω∈A
0i fω/∈A.
A sequence of sets A1,A2,...ismonotone increasing ifA1⊂A2⊂
···and we deﬁne lim n→∞An=uniontext∞
i=1Ai. A sequence of sets A1,A2,...is
monotone decreasing ifA1⊃A2⊃ ··· and then we deﬁne lim n→∞An=intersectiontext∞
i=1Ai. In either case, we will write An→A.
1.4 Example. Let Ω = Rand let Ai=[ 0,1/i) fori=1,2,.... Thenuniontext∞
i=1Ai=
[0,1) andintersectiontext∞
i=1Ai={0}. If instead we deﬁne Ai=( 0,1/i) thenuniontext∞
i=1Ai=
(0,1) andintersectiontext∞
i=1Ai=∅./squaresolid
1.3 Probability
We will assign a real number P(A) to every event A, called the probability of
A.1We also call Paprobability distribution or aprobability measure.
To qualify as a probability, Pmust satisfy three axioms:
1.5 Deﬁnition. A function Pthat assigns a real number P(A)to each
event Ais aprobability distribution or aprobability measure if it
satisﬁes the following three axioms:Axiom 1 :P(A)≥0for every A
Axiom 2 :P( Ω )=1
Axiom 3 :I fA
1,A2,...are disjoint then
PparenleftBigg∞uniondisplay
i=1AiparenrightBigg
=∞summationdisplay
i=1P(Ai).
1It is not always possible to assign a probability to every event Aif the sample space is large,
such as the whole real line. Instead, we assign probabilities to a limited class of set called aσ-ﬁeld. See the appendix for details.

<<<PAGE 22>>>

6 1. Probability
There are many interpretations of P(A). The two common interpretations
are frequencies and degrees of beliefs. In the frequency interpretation, P(A)
is the long run proportion of times that Ais true in repetitions. For example,
if we say that the probability of heads is 1/2, we mean that if we ﬂip thecoin many times then the proportion of times we get heads tends to 1/2 asthe number of tosses increases. An inﬁnitely long, unpredictable sequence oftosses whose limiting proportion tends to a constant is an idealization, muchlike the idea of a straight line in geometry. The degree-of-belief interpretationis that P(A) measures an observer’s strength of belief that Ais true. In either
interpretation, we require that Axioms 1 to 3 hold. The diﬀerence in inter-pretation will not matter much until we deal with statistical inference. There,the diﬀering interpretations lead to two schools of inference: the frequentistand the Bayesian schools. We defer discussion until Chapter 11.
One can derive many properties of Pfrom the axioms, such as:
P(∅)=0
A⊂B=⇒ P(A)≤P(B)
0≤P(A)≤1
P(A
c)=1 −P(A)
Aintersectiondisplay
B=∅=⇒ PparenleftBig
Auniondisplay
BparenrightBig
=P(A)+P(B). (1.1)
A less obvious property is given in the following Lemma.
1.6 Lemma. For any events AandB,
PparenleftBig
Auniondisplay
BparenrightBig
=P(A)+P(B)−P(AB).
Proof. Write AuniontextB=(ABc)uniontext(AB)uniontext(AcB) and note that these events
are disjoint. Hence, making repeated use of the fact that Pis additive for
disjoint events, we see that
PparenleftBig
Auniondisplay
BparenrightBig
=PparenleftBig
(ABc)uniondisplay
(AB)uniondisplay
(AcB)parenrightBig
=P(ABc)+P(AB)+P(AcB)
=P(ABc)+P(AB)+P(AcB)+P(AB)−P(AB)
=PparenleftBig
(ABc)uniondisplay
(AB)parenrightBig
+PparenleftBig
(AcB)uniondisplay
(AB)parenrightBig
−P(AB)
=P(A)+P(B)−P(AB)./squaresolid
1.7 Example. Two coin tosses. Let H1be the event that heads occurs on
toss 1 and let H2be the event that heads occurs on toss 2. If all outcomes are

<<<PAGE 23>>>

1.4 Probability on Finite Sample Spaces 7
equally likely, then P(H1uniontextH2)=P(H1)+P(H2)−P(H1H2)=1
2+1
2−1
4=3/4.
/squaresolid
1.8 Theorem (Continuity of Probabilities) .IfAn→Athen
P(An)→P(A)
asn→∞.
Proof. Suppose that Anis monotone increasing so that A1⊂A2⊂ ···.
LetA= lim n→∞An=uniontext∞
i=1Ai. Deﬁne B1=A1,B2={ω∈Ω:ω∈
A2,ω /∈A1},B3={ω∈Ω: ω∈A3,ω /∈A2,ω /∈A1},...It can be
shown that B1,B2,...are disjoint, An=uniontextn
i=1Ai=uniontextn
i=1Bifor each nanduniontext∞
i=1Bi=uniontext∞
i=1Ai. (See exercise 1.) From Axiom 3,
P(An)=PparenleftBiggnuniondisplay
i=1BiparenrightBigg
=nsummationdisplay
i=1P(Bi)
and hence, using Axiom 3 again,
lim
n→∞P(An) = lim
n→∞nsummationdisplay
i=1P(Bi)=∞summationdisplay
i=1P(Bi)=PparenleftBigg∞uniondisplay
i=1BiparenrightBigg
=P(A)./squaresolid
1.4 Probability on Finite Sample Spaces
Suppose that the sample space Ω = {ω1,...,ω n}is ﬁnite. For example, if we
toss a die twice, then Ω has 36 elements: Ω = {(i, j);i, j∈{1,...6}}. If each
outcome is equally likely, then P(A)=|A|/36 where |A|denotes the number
of elements in A. The probability that the sum of the dice is 11 is 2 /36 since
there are two outcomes that correspond to this event.
If Ω is ﬁnite and if each outcome is equally likely, then
P(A)=|A|
|Ω|,
which is called the uniform probability distribution. To compute prob-
abilities, we need to count the number of points in an event A. Methods for
counting points are called combinatorial methods. We needn’t delve into thesein any great detail. We will, however, need a few facts from counting theorythat will be useful later. Given nobjects, the number of ways of ordering

<<<PAGE 24>>>

8 1. Probability
these objects is n!=n(n−1)(n−2)···3·2·1. For convenience, we deﬁne
0! = 1. We also deﬁne parenleftbiggn
kparenrightbigg
=n!
k!(n−k)!, (1.2)
read “ nchoose k”, which is the number of distinct ways of choosing kobjects
fromn. For example, if we have a class of 20 people and we want to select a
committee of 3 students, then there are
parenleftbigg20
3parenrightbigg
=20!
3!17!=20×19×18
3×2×1= 1140
possible committees. We note the following properties:
parenleftbiggn
0parenrightbigg
=parenleftbiggn
nparenrightbigg
= 1 andparenleftbiggn
kparenrightbigg
=parenleftbiggn
n−kparenrightbigg
.
1.5 Independent Events
If we ﬂip a fair coin twice, then the probability of two heads is1
2×1
2.W e
multiply the probabilities because we regard the two tosses as independent.The formal deﬁnition of independence is as follows:
1.9 Deﬁnition. Two events AandBareindependent if
P(AB)=P(A)P(B) (1.3)
and we write A/coproductB. A set of events {Ai:i∈I}is independent if
PparenleftBiggintersectiondisplay
i∈JAiparenrightBigg
=productdisplay
i∈JP(Ai)
for every ﬁnite subset JofI.I fAandBare not independent, we write
A/gluonbelement/gluonelement/gluonelement/gluonelement/gluonelement/gluoneelementB
Independence can arise in two distinct ways. Sometimes, we explicitly as-
sume that two events are independent. For example, in tossing a coin twice,
we usually assume the tosses are independent which reﬂects the fact that thecoin has no memory of the ﬁrst toss. In other instances, we derive indepen-
dence by verifying that P(AB)=P(A)P(B) holds. For example, in tossing
a fair die, let A={2,4,6}and let B={1,2,3,4}. Then, AintersectiontextB={2,4},

<<<PAGE 25>>>

1.5 Independent Events 9
P(AB)=2/6=P(A)P(B)=( 1 /2)×(2/3) and so AandBare independent.
In this case, we didn’t assume that AandBare independent — it just turned
out that they were.
Suppose that AandBare disjoint events, each with positive probability.
Can they be independent? No. This follows since P(A)P(B)>0y e t P(AB)=
P(∅) = 0. Except in this special case, there is no way to judge independence
by looking at the sets in a Venn diagram.
1.10 Example. Toss a fair coin 10 times. Let A=“at least one head.” Let Tj
be the event that tails occurs on the jthtoss. Then
P(A)=1 −P(Ac)
=1 −P(all tails)
=1 −P(T1T2···T10)
=1 −P(T1)P(T2)···P(T10) using independence
=1 −parenleftbigg1
2parenrightbigg10
≈.999./squaresolid
1.11 Example. Two people take turns trying to sink a basketball into a net.
Person 1 succeeds with probability 1/3 while person 2 succeeds with proba-bility 1/4. What is the probability that person 1 succeeds before person 2?LetEdenote the event of interest. Let A
jbe the event that the ﬁrst success
is by person 1 and that it occurs on trial number j. Note that A1,A2,...are
disjoint and that E=uniontext∞
j=1Aj. Hence,
P(E)=∞summationdisplay
j=1P(Aj).
Now, P(A1)=1/3.A2occurs if we have the sequence person 1 misses, person
2 misses, person 1 succeeds. This has probability P(A2)=( 2 /3)(3/4)(1/3) =
(1/2)(1/3). Following this logic we see that P(Aj)=( 1 /2)j−1(1/3). Hence,
P(E)=∞summationdisplay
j=11
3parenleftbigg1
2parenrightbiggj−1
=1
3∞summationdisplay
j=1parenleftbigg1
2parenrightbiggj−1
=2
3.
Here we used that fact that, if 0 <r< 1 thensummationtext∞
j=krj=rk/(1−r)./squaresolid

<<<PAGE 26>>>

10 1. Probability
Summary of Independence
1.AandBare independent if and only if P(AB)=P(A)P(B).
2. Independence is sometimes assumed and sometimes derived.3. Disjoint events with positive probability are not independent.
1.6 Conditional Probability
Assuming that P(B)>0, we deﬁne the conditional probability of Agiven
thatBhas occurred as follows:
1.12 Deﬁnition. IfP(B)>0then the conditional probability ofA
given Bis
P(A|B)=P(AB)
P(B). (1.4)
Think of P(A|B) as the fraction of times Aoccurs among those in which
Boccurs. For any ﬁxed Bsuch that P(B)>0,P(·|B) is a probability (i.e., it
satisﬁes the three axioms of probability). In particular, P(A|B)≥0,P(Ω|B)=
1 and if A1,A2,...are disjoint then P(uniontext∞
i=1Ai|B)=summationtext∞
i=1P(Ai|B). But it
is in general nottrue that P(A|BuniontextC)=P(A|B)+P(A|C). The rules of
probability apply to events on the left of the bar. In general it is notthe case
thatP(A|B)=P(B|A). People get this confused all the time. For example,
the probability of spots given you have measles is 1 but the probability thatyou have measles given that you have spots is not 1. In this case, the diﬀerencebetween P(A|B) and P(B|A) is obvious but there are cases where it is less
obvious. This mistake is made often enough in legal cases that it is sometimescalled the prosecutor’s fallacy.
1.13 Example. A medical test for a disease Dhas outcomes + and −. The
probabilities are:
DDc
+.009 .099
−.001 .891

<<<PAGE 27>>>

1.6 Conditional Probability 11
From the deﬁnition of conditional probability,
P(+|D)=P(+intersectiontextD)
P(D)=.009
.009 + .001=.9
and
P(−|Dc)=P(−intersectiontextDc)
P(Dc)=.891
.891 + .099≈.9.
Apparently, the test is fairly accurate. Sick people yield a positive 90 percent
of the time and healthy people yield a negative about 90 percent of the time.Suppose you go for a test and get a positive. What is the probability you havethe disease? Most people answer .90. The correct answer is
P(D|+) =P(+intersectiontextD)
P(+)=.009
.009 + .099≈.08.
The lesson here is that you need to compute the answer numerically. Don’t
trust your intuition. /squaresolid
The results in the next lemma follow directly from the deﬁnition of condi-
tional probability.
1.14 Lemma. IfAandBare independent events then P(A|B)=P(A). Also,
for any pair of events AandB,
P(AB)=P(A|B)P(B)=P(B|A)P(A).
From the last lemma, we see that another interpretation of independence is
that knowing Bdoesn’t change the probability of A. The formula P(AB)=
P(A)P(B|A) is sometimes helpful for calculating probabilities.
1.15 Example. Draw two cards from a deck, without replacement. Let Abe
the event that the ﬁrst draw is the Ace of Clubs and let Bbe the event that
the second draw is the Queen of Diamonds. Then P(AB)=P(A)P(B|A)=
(1/52)×(1/51)./squaresolid
Summary of Conditional Probability
1. If P(B)>0, then
P(A|B)=P(AB)
P(B).
2.P(·|B) satisﬁes the axioms of probability, for ﬁxed B. In general,
P(A|·) does not satisfy the axioms of probability, for ﬁxed A.
3. In general, P(A|B)/negationslash=P(B|A).

<<<PAGE 28>>>

12 1. Probability
4.AandBare independent if and only if P(A|B)=P(A).
1.7 Bayes’ Theorem
Bayes’ theorem is the basis of “expert systems” and “Bayes’ nets,” which are
discussed in Chapter 17. First, we need a preliminary result.
1.16 Theorem (The Law of Total Probability) .LetA1,...,A kbe a partition
ofΩ. Then, for any event B,
P(B)=ksummationdisplay
i=1P(B|Ai)P(Ai).
Proof. Deﬁne Cj=BAjand note that C1,...,C kare disjoint and that
B=uniontextk
j=1Cj. Hence,
P(B)=summationdisplay
jP(Cj)=summationdisplay
jP(BAj)=summationdisplay
jP(B|Aj)P(Aj)
since P(BAj)=P(B|Aj)P(Aj) from the deﬁnition of conditional probability.
/squaresolid
1.17 Theorem (Bayes’ Theorem) .LetA1,...,A kbe a partition of Ωsuch
thatP(Ai)>0for each i.I fP(B)>0then, for each i=1,...,k ,
P(Ai|B)=P(B|Ai)P(Ai)summationtext
jP(B|Aj)P(Aj). (1.5)
1.18 Remark. We call P(Ai) theprior probability of AandP(Ai|B) the
posterior probability of A.
Proof. We apply the deﬁnition of conditional probability twice, followed
by the law of total probability:
P(Ai|B)=P(AiB)
P(B)=P(B|Ai)P(Ai)
P(B)=P(B|Ai)P(Ai)summationtext
jP(B|Aj)P(Aj)./squaresolid
1.19 Example. I divide my email into three categories: A1= “spam,” A2=
“low priority” and A3= “high priority.” From previous experience I ﬁnd that

<<<PAGE 29>>>

1.8 Bibliographic Remarks 13
P(A1)=.7,P(A2)=.2 and P(A3)=.1. Of course, .7+.2+.1=1 .L e t Bbe
the event that the email contains the word “free.” From previous experience,P(B|A
1)=.9,P(B|A2)=.01,P(B|A1)=.01. (Note: .9+.01 +.01/negationslash= 1.) I
receive an email with the word “free.” What is the probability that it is spam?Bayes’ theorem yields,
P(A
1|B)=.9×.7
(.9×.7 )+( .01×.2 )+( .01×.1)=.995./squaresolid
1.8 Bibliographic Remarks
The material in this chapter is standard. Details can be found in any number
of books. At the introductory level, there is DeGroot and Schervish (2002);at the intermediate level, Grimmett and Stirzaker (1982) and Karr (1993); atthe advanced level there are Billingsley (1979) and Breiman (1992). I adaptedmany examples and exercises from DeGroot and Schervish (2002) and Grim-mett and Stirzaker (1982).
1.9 Appendix
Generally, it is not feasible to assign probabilities to all subsets of a samplespace Ω. Instead, one restricts attention to a set of events called a σ-algebra
or aσ-ﬁeld which is a class Athat satisﬁes:
(i)∅∈A ,
(ii) if A
1,A2,...,∈Athenuniontext∞
i=1Ai∈Aand
(iii)A∈Aimplies that Ac∈A.
The sets in Aare said to be measurable . We call (Ω ,A)ameasurable
space. IfPis a probability measure deﬁned on A, then (Ω ,A,P) is called a
probability space. When Ω is the real line, we take Ato be the smallest
σ-ﬁeld that contains all the open subsets, which is called the Borel σ-ﬁeld .
1.10 Exercises
1. Fill in the details of the proof of Theorem 1.8. Also, prove the monotone
decreasing case.
2. Prove the statements in equation (1.1).

<<<PAGE 30>>>

14 1. Probability
3. Let Ω be a sample space and let A1,A2,...,be events. Deﬁne Bn=uniontext∞
i=nAiandCn=intersectiontext∞
i=nAi.
(a) Show that B1⊃B2⊃··· and that C1⊂C2⊂···.
(b) Show that ω∈intersectiontext∞
n=1Bnif and only if ωbelongs to an inﬁnite
number of the events A1,A2,....
(c) Show that ω∈uniontext∞
n=1Cnif and only if ωbelongs to all the events
A1,A2,...except possibly a ﬁnite number of those events.
4. Let {Ai:i∈I}be a collection of events where Iis an arbitrary index
set. Show that
parenleftBigguniondisplay
i∈IAiparenrightBiggc
=intersectiondisplay
i∈IAc
iandparenleftBiggintersectiondisplay
i∈IAiparenrightBiggc
=uniondisplay
i∈IAc
i
Hint: First prove this for I={1,...,n }.
5. Suppose we toss a fair coin until we get exactly two heads. Describe
the sample space S. What is the probability that exactly ktosses are
required?
6. Let Ω = {0,1,...,}. Prove that there does not exist a uniform distri-
bution on Ω (i.e., if P(A)=P(B) whenever |A|=|B|, then Pcannot
satisfy the axioms of probability).
7. Let A1,A2,...be events. Show that
PparenleftBigg∞uniondisplay
n=1AnparenrightBigg
≤∞summationdisplay
n=1P(An).
Hint: Deﬁne Bn=An−uniontextn−1
i=1Ai. Then show that the Bnare disjoint
and thatuniontext∞
n=1An=uniontext∞
n=1Bn.
8. Suppose that P(Ai) = 1 for each i. Prove that
PparenleftBigg∞intersectiondisplay
i=1AiparenrightBigg
=1.
9. For ﬁxed Bsuch that P(B)>0, show that P(·|B) satisﬁes the axioms
of probability.
10. You have probably heard it before. Now you can solve it rigorously.
It is called the “Monty Hall Problem.” A prize is placed at random

<<<PAGE 31>>>

1.10 Exercises 15
behind one of three doors. You pick a door. To be concrete, let’s suppose
you always pick door 1. Now Monty Hall chooses one of the other twodoors, opens it and shows you that it is empty. He then gives you theopportunity to keep your door or switch to the other unopened door.Should you stay or switch? Intuition suggests it doesn’t matter. Thecorrect answer is that you should switch. Prove it. It will help to specifythe sample space and the relevant events carefully. Thus write Ω ={(ω
1,ω2):ωi∈{1,2,3}}where ω1is where the prize is and ω2is the
door Monty opens.
11. Suppose that AandBare independent events. Show that AcandBc
are independent events.
12. There are three cards. The ﬁrst is green on both sides, the second is red
on both sides and the third is green on one side and red on the other. Wechoose a card at random and we see one side (also chosen at random).If the side we see is green, what is the probability that the other side isalso green? Many people intuitively answer 1/2. Show that the correctanswer is 2/3.
13. Suppose that a fair coin is tossed repeatedly until both a head and tail
have appeared at least once.
(a) Describe the sample space Ω.(b) What is the probability that three tosses will be required?
14. Show that if P(A)=0o r P(A) = 1 then Ais independent of every other
event. Show that if Ais independent of itself then P(A) is either 0 or 1.
15. The probability that a child has blue eyes is 1/4. Assume independence
between children. Consider a family with 3 children.
(a) If it is known that at least one child has blue eyes, what is the
probability that at least two children have blue eyes?
(b) If it is known that the youngest child has blue eyes, what is the
probability that at least two children have blue eyes?
16. Prove Lemma 1.14.17. Show that
P(ABC)=P(A|BC)P(B|C)P(C).

<<<PAGE 32>>>

16 1. Probability
18. Suppose kevents form a partition of the sample space Ω, i.e., they
are disjoint anduniontextk
i=1Ai= Ω. Assume that P(B)>0. Prove that if
P(A1|B)<P(A1) then P(Ai|B)>P(Ai) for some i=2,...,k .
19. Suppose that 30 percent of computer owners use a Macintosh, 50 percent
use Windows, and 20 percent use Linux. Suppose that 65 percent ofthe Mac users have succumbed to a computer virus, 82 percent of theWindows users get the virus, and 50 percent of the Linux users getthe virus. We select a person at random and learn that her system wasinfected with the virus. What is the probability that she is a Windowsuser?
20. A box contains 5 coins and each has a diﬀerent probability of show-
ing heads. Let p
1,...,p 5denote the probability of heads on each coin.
Suppose that
p1=0,p2=1/4,p3=1/2,p4=3/4 and p5=1.
LetHdenote “heads is obtained” and let Cidenote the event that coin
iis selected.
(a) Select a coin at random and toss it. Suppose a head is obtained.
What is the posterior probability that coin iwas selected ( i=1,...,5)?
In other words, ﬁnd P(Ci|H) fori=1,...,5.
(b) Toss the coin again. What is the probability of another head? In
other words ﬁnd P(H2|H1) where Hj= “heads on toss j.”
Now suppose that the experiment was carried out as follows: We select
a coin at random and toss it until a head is obtained.
(c) Find P(Ci|B4) where B4= “ﬁrst head is obtained on toss 4.”
21. (Computer Experiment .) Suppose a coin has probability pof falling heads
up. If we ﬂip the coin many times, we would expect the proportion ofheads to be near p. We will make this formal later. Take p=.3 and
n=1,000 and simulate ncoin ﬂips. Plot the proportion of heads as a
function of n. Repeat for p=.03.
22. (Computer Experiment .) Suppose we ﬂip a coin ntimes and let pdenote
the probability of heads. Let Xbe the number of heads. We call X
a binomial random variable, which is discussed in the next chapter.Intuition suggests that Xwill be close to np. To see if this is true, we
can repeat this experiment many times and average the Xvalues. Carry

<<<PAGE 33>>>

1.10 Exercises 17
out a simulation and compare the average of the X’s tonp. Try this for
p=.3 and n= 10, n= 100, and n=1,000.
23. (Computer Experiment .) Here we will get some experience simulating
conditional probabilities. Consider tossing a fair die. Let A={2,4,6}
andB={1,2,3,4}. Then, P(A)=1/2,P(B)=2/3 and P(AB)=1/3.
Since P(AB)=P(A)P(B), the events AandBare independent. Simu-
late draws from the sample space and verify that hatwideP(AB)=hatwideP(A)hatwideP(B)
wherehatwideP(A) is the proportion of times Aoccurred in the simulation and
similarly for hatwideP(AB) andhatwideP(B). Now ﬁnd two events AandBthat are not
independent. Compute hatwideP(A),hatwideP(B) andhatwideP(AB). Compare the calculated
values to their theoretical values. Report your results and interpret.

<<<PAGE 34>>>



<<<PAGE 35>>>

2
Random Variables
2.1 Introduction
Statistics and data mining are concerned with data. How do we link sample
spaces and events to data? The link is provided by the concept of a randomvariable.
2.1 Deﬁnition. Arandom variable is a mapping1
X:Ω→R
that assigns a real number X(ω)to each outcome ω.
At a certain point in most probability courses, the sample space is rarely
mentioned anymore and we work directly with random variables. But youshould keep in mind that the sample space is really there, lurking in thebackground.
2.2 Example. Flip a coin ten times. Let X(ω) be the number of heads in the
sequence ω. For example, if ω=HHTHHTHHTT , then X(ω)=6 .
/squaresolid
1Technically, a random variable must be measurable. See the appendix for details.

<<<PAGE 36>>>

20 2. Random Variables
2.3 Example. Let Ω =braceleftbigg
(x, y);x2+y2≤1bracerightbigg
be the unit disk. Consider
drawing a point at random from Ω. (We will make this idea more precise
later.) A typical outcome is of the form ω=(x, y). Some examples of random
variables are X(ω)=x,Y(ω)=y,Z(ω)=x+y, and W(ω)=radicalbig
x2+y2./squaresolid
Given a random variable Xand a subset Aof the real line, deﬁne X−1(A)=
{ω∈Ω:X(ω)∈A}and let
P(X∈A)= P(X−1(A)) =P({ω∈Ω;X(ω)∈A})
P(X=x)= P(X−1(x)) =P({ω∈Ω;X(ω)=x}).
Notice that Xdenotes the random variable and xdenotes a particular value
ofX.
2.4 Example. Flip a coin twice and let Xbe the number of heads. Then,
P(X=0 )= P({TT})=1 /4,P(X=1 )= P({HT,TH })=1 /2 and
P(X=2 )= P({HH})=1 /4. The random variable and its distribution
can be summarized as follows:
ω P({ω})X(ω)
TT 1/4 0
TH 1/4 1
HT 1/4 1
HH 1/4 2xP(X=x)
01/4
11/2
21/4
Try generalizing this to nﬂips. /squaresolid
2.2 Distribution Functions and Probability Functions
Given a random variable X, we deﬁne the cumulative distribution function
(or distribution function) as follows.
2.5 Deﬁnition. Thecumulative distribution function ,o r cdf,i st h e
function FX:R→[0,1]deﬁned by
FX(x)=P(X≤x). (2.1)

<<<PAGE 37>>>

2.2 Distribution Functions and Probability Functions 21
0121
/Bullet/Bullet/BulletFX(x)
x.25.50.75
FIGURE 2.1. cdffor ﬂipping a coin twice (Example 2.6.)
We will see later that the cdfeﬀectively contains all the information about
the random variable. Sometimes we write the cdfasFinstead of FX.
2.6 Example. Flip a fair coin twice and let Xbe the number of heads. Then
P(X=0 )= P(X=2 )=1 /4 and P(X=1 )=1 /2. The distribution function
is
FX(x)=

0 x<0
1/40≤x<1
3/41≤x<2
1 x≥2.
The cdfis shown in Figure 2.1. Although this example is simple, study it
carefully. cdf’s can be very confusing. Notice that the function is right contin-
uous, non-decreasing, and that it is deﬁned for all x, even though the random
variable only takes values 0 ,1, and 2. Do you see why F
X(1.4) =.75?/squaresolid
The following result shows that the cdfcompletely determines the distri-
bution of a random variable.
2.7 Theorem. LetXhave cdfFand let Yhave cdfG.I fF(x)=G(x)for
allx, then P(X∈A)=P(Y∈A)for all A.2
2.8 Theorem. A function Fmapping the real line to [0,1]is acdffor some
probability Pif and only if Fsatisﬁes the following three conditions:
(i)Fis non-decreasing: x1<x2implies that F(x1)≤F(x2).
(ii)Fis normalized:
lim
x→−∞F(x)=0
2Technically, we only have that P(X∈A)= P(Y∈A)for every measurable event A.

<<<PAGE 38>>>

22 2. Random Variables
and
lim
x→∞F(x)=1.
(iii)Fis right-continuous: F(x)=F(x+)for all x, where
F(x+) = lim
y→x
y>xF(y).
Proof. Suppose that Fis a cdf. Let us show that (iii) holds. Let xbe
a real number and let y1,y2,...be a sequence of real numbers such that
y1>y2>···and lim iyi=x. LetAi=(−∞,yi] and let A=(−∞,x]. Note
thatA=intersectiontext∞
i=1Aiand also note that A1⊃A2⊃···. Because the events are
monotone, lim iP(Ai)=P(intersectiontext
iAi). Thus,
F(x)=P(A)=PparenleftBiggintersectiondisplay
iAiparenrightBigg
= lim
iP(Ai) = lim
iF(yi)=F(x+).
Showing (i) and (ii) is similar. Proving the other direction — namely, that if
Fsatisﬁes (i), (ii), and (iii) then it is a cdffor some random variable — uses
some deep tools in analysis. /squaresolid
2.9 Deﬁnition. Xisdiscrete if it takes countably3many values
{x1,x2,...}. We deﬁne the probability function orprobability mass
function forXbyfX(x)=P(X=x).
Thus, fX(x)≥0 for all x∈Randsummationtext
ifX(xi) = 1. Sometimes we write f
instead of fX. The cdfofXis related to fXby
FX(x)=P(X≤x)=summationdisplay
xi≤xfX(xi).
2.10 Example. The probability function for Example 2.6 is
fX(x)=

1/4x=0
1/2x=1
1/4x=2
0 otherwise .
See Figure 2.2.
/squaresolid
3A set is countable if it is ﬁnite or it can be put in a one-to-one correspondence with the
integers. The even numbers, the odd numbers, and the rationals are countable; the set of realnumbers between 0 and 1 is not countable.

<<<PAGE 39>>>

2.2 Distribution Functions and Probability Functions 23
0121
/Bullet/Bullet
/BulletfX(x)
x.25.5.75
FIGURE 2.2. Probability function for ﬂipping a coin twice (Example 2.6).
2.11 Deﬁnition. A random variable Xiscontinuous if there exists a
function fXsuch that fX(x)≥0for all x,integraltext∞
−∞fX(x)dx=1and for
every a≤b,
P(a<X<b )=integraldisplayb
afX(x)dx. (2.2)
The function fXis called the probability density function ( pdf).We
have that
FX(x)=integraldisplayx
−∞fX(t)dt
andfX(x)=F/prime
X(x)at all points xat which FXis diﬀerentiable.
Sometimes we writeintegraltext
f(x)dxorintegraltext
fto meanintegraltext∞
−∞f(x)dx.
2.12 Example. Suppose that Xhaspdf
fX(x)=braceleftbigg1 for 0 ≤x≤1
0 otherwise .
Clearly, fX(x)≥0 andintegraltext
fX(x)dx= 1. A random variable with this density
is said to have a Uniform (0,1) distribution. This is meant to capture the ideaof choosing a point at random between 0 and 1. The cdfis given by
F
X(x)=

0x<0
x0≤x≤1
1x>1.
See Figure 2.3. /squaresolid

<<<PAGE 40>>>

24 2. Random Variables
011FX(x)
x
FIGURE 2.3. cdffor Uniform (0,1).
2.13 Example. Suppose that Xhaspdf
f(x)=braceleftbigg0 for x<0
1
(1+x)2otherwise .
Sinceintegraltext
f(x)dx= 1, this is a well-deﬁned pdf./squaresolid
Warning! Continuous random variables can lead to confusion. First, note
that if Xis continuous then P(X=x) = 0 for every x. Don’t try to think
off(x)a sP(X=x). This only holds for discrete random variables. We get
probabilities from a pdfby integrating. A pdfcan be bigger than 1 (unlike
a mass function). For example, if f(x)=5f o r x∈[0,1/5] and 0 otherwise,
thenf(x)≥0 andintegraltext
f(x)dx= 1 so this is a well-deﬁned pdfeven though
f(x) = 5 in some places. In fact, a pdfcan be unbounded. For example, if
f(x)=( 2 /3)x−1/3for 0<x< 1 and f(x) = 0 otherwise, thenintegraltext
f(x)dx=1
even though fis not bounded.
2.14 Example. Let
f(x)=braceleftbigg0 for x<0
1
(1+x)otherwise .
This is not a pdfsinceintegraltext
f(x)dx=integraltext∞
0dx/(1+x)=integraltext∞
1du/u= log( ∞)=∞.
/squaresolid
2.15 Lemma. LetFbe the cdffor a random variable X. Then:
1.P(X=x)=F(x)−F(x−)where F(x−) = lim y↑xF(y);

<<<PAGE 41>>>

2.3 Some Important Discrete Random Variables 25
2.P(x<X ≤y)=F(y)−F(x);
3.P(X>x )=1−F(x);
4. IfXis continuous then
F(b)−F(a)= P(a<X<b )=P(a≤X<b )
=P(a<X ≤b)=P(a≤X≤b).
It is also useful to deﬁne the inverse cdf(or quantile function).
2.16 Deﬁnition. LetXbe a random variable with cdfF. Theinverse
CDF orquantile function is deﬁned by4
F−1(q) = infbraceleftBig
x:F(x)>qbracerightBig
forq∈[0,1].I fFis strictly increasing and continuous then F−1(q)is the
unique real number xsuch that F(x)=q.
We call F−1(1/4) the ﬁrst quartile ,F−1(1/2) the median (or second
quartile), and F−1(3/4) the third quartile .
Two random variables XandYareequal in distribution — written
Xd=Y—i fFX(x)=FY(x) for all x. This does not mean that XandYare
equal. Rather, it means that all probability statements about XandYwill
be the same. For example, suppose that P(X=1 )= P(X=−1 )=1 /2. Let
Y=−X. Then P(Y=1 )= P(Y=−1 )=1 /2 and so Xd=Y. But XandY
are not equal. In fact, P(X=Y)=0 .
2.3 Some Important Discrete Random Variables
Warning About Notation! It is traditional to write X∼Fto indicate
thatXhas distribution F. This is unfortunate notation since the symbol ∼
is also used to denote an approximation. The notation X∼Fis so pervasive
that we are stuck with it. Read X∼Fas “Xhas distribution F”notas “X
is approximately F”.
4If you are unfamiliar with “inf”, just think of it as the minimum.

<<<PAGE 42>>>

26 2. Random Variables
The Point Mass Distribution. Xhas a point mass distribution at a,
written X∼δa,i fP(X=a) = 1 in which case
F(x)=braceleftbigg
0x<a
1x≥a.
The probability mass function is f(x)=1f o r x=aand 0 otherwise.
The Discrete Uniform Distribution. Letk>1 be a given integer.
Suppose that Xhas probability mass function given by
f(x)=braceleftbigg
1/kforx=1,...,k
0 otherwise .
We say that Xhas a uniform distribution on {1,...,k }.
The Bernoulli Distribution. LetXrepresent a binary coin ﬂip. Then
P(X=1 )= pandP(X=0 )=1 −pfor some p∈[0,1]. We say that Xhas a
Bernoulli distribution written X∼Bernoulli( p). The probability function is
f(x)=px(1−p)1−xforx∈{0,1}.
The Binomial Distribution. Suppose we have a coin which falls heads
up with probability pfor some 0 ≤p≤1. Flip the coin ntimes and let
Xbe the number of heads. Assume that the tosses are independent. Let
f(x)=P(X=x) be the mass function. It can be shown that
f(x)=braceleftBiggparenleftbign
xparenrightbig
px(1−p)n−xforx=0,...,n
0 otherwise .
A random variable with this mass function is called a Binomial random
variable and we write X∼Binomial( n, p). IfX1∼Binomial( n1,p) and
X2∼Binomial( n2,p) then X1+X2∼Binomial( n1+n2,p).
Warning! Let us take this opportunity to prevent some confusion. Xis a
random variable; xdenotes a particular value of the random variable; nandp
areparameters , that is, ﬁxed real numbers. The parameter pis usually un-
known and must be estimated from data; that’s what statistical inference is allabout. In most statistical models, there are random variables and parameters:don’t confuse them.
The Geometric Distribution. Xhas a geometric distribution with
parameter p∈(0,1), written X∼Geom( p), if
P(X=k)=p(1−p)
k−1,k≥1.

<<<PAGE 43>>>

2.4 Some Important Continuous Random Variables 27
We have that
∞summationdisplay
k=1P(X=k)=p∞summationdisplay
k=1(1−p)k=p
1−(1−p)=1.
Think of Xas the number of ﬂips needed until the ﬁrst head when ﬂipping a
coin.
The Poisson Distribution. Xhas a Poisson distribution with parameter
λ, written X∼Poisson( λ)i f
f(x)=e−λλx
x!x≥0.
Note that∞summationdisplay
x=0f(x)=e−λ∞summationdisplay
x=0λx
x!=e−λeλ=1.
The Poisson is often used as a model for counts of rare events like radioactive
decay and traﬃc accidents. If X1∼Poisson( λ1) and X2∼Poisson( λ2) then
X1+X2∼Poisson( λ1+λ2).
Warning! We deﬁned random variables to be mappings from a sample
space Ω to Rbut we did not mention the sample space in any of the distri-
butions above. As I mentioned earlier, the sample space often “disappears”but it is really there in the background. Let’s construct a sample space ex-plicitly for a Bernoulli random variable. Let Ω = [0 ,1] and deﬁne Pto satisfy
P([a, b]) =b−afor 0≤a≤b≤1. Fix p∈[0,1] and deﬁne
X(ω)=braceleftbigg1ω≤p
0ω>p .
Then P(X=1 )= P(ω≤p)=P([0,p]) =pandP(X=0 )=1 −p. Thus,
X∼Bernoulli( p). We could do this for all the distributions deﬁned above. In
practice, we think of a random variable like a random number but formally itis a mapping deﬁned on some sample space.
2.4 Some Important Continuous Random Variables
The Uniform Distribution .Xhas a Uniform( a, b) distribution, written
X∼Uniform( a, b), if
f(x)=braceleftbigg1
b−aforx∈[a, b]
0 otherwise

<<<PAGE 44>>>

28 2. Random Variables
where a<b. The distribution function is
F(x)=

0 x<a
x−a
b−ax∈[a, b]
1 x>b .
Normal (Gaussian). Xhas a Normal (or Gaussian) distribution with
parameters µandσ, denoted by X∼N(µ, σ2), if
f(x)=1
σ√
2πexpbraceleftbigg
−1
2σ2(x−µ)2bracerightbigg
,x∈R (2.3)
where µ∈Randσ>0. The parameter µis the “center” (or mean) of the
distribution and σis the “spread” (or standard deviation) of the distribu-
tion. (The mean and standard deviation will be formally deﬁned in the nextchapter.) The Normal plays an important role in probability and statistics.Many phenomena in nature have approximately Normal distributions. Later,we shall study the Central Limit Theorem which says that the distribution ofa sum of random variables can be approximated by a Normal distribution.
We say that Xhas astandard Normal distribution ifµ= 0 and σ=1 .
Tradition dictates that a standard Normal random variable is denoted by Z.
The pdfand cdfof a standard Normal are denoted by φ(z) and Φ( z). The
pdfis plotted in Figure 2.4. There is no closed-form expression for Φ. Here
are some useful facts:
(i) IfX∼N(µ, σ
2), then Z=(X−µ)/σ∼N(0,1).
(ii) If Z∼N(0,1), then X=µ+σZ∼N(µ, σ2).
(iii) If Xi∼N(µi,σ2
i),i=1,...,n are independent, then
nsummationdisplay
i=1Xi∼NparenleftBiggnsummationdisplay
i=1µi,nsummationdisplay
i=1σ2
iparenrightBigg
.
It follows from (i) that if X∼N(µ, σ2), then
P(a<X<b )= Pparenleftbigga−µ
σ<Z<b−µ
σparenrightbigg
=Φparenleftbiggb−µ
σparenrightbigg
−Φparenleftbigga−µ
σparenrightbigg
.
Thus we can compute any probabilities we want as long as we can compute
thecdfΦ(z) of a standard Normal. All statistical computing packages will

<<<PAGE 45>>>

2.4 Some Important Continuous Random Variables 29
012 −1 −2
zFIGURE 2.4. Density of a standard Normal.
compute Φ( z) and Φ−1(q). Most statistics texts, including this one, have a
table of values of Φ( z).
2.17 Example. Suppose that X∼N(3,5). Find P(X>1). The solution is
P(X>1 )=1 −P(X<1 )=1 −Pparenleftbigg
Z<1−3√
5parenrightbigg
=1−Φ(−0.8944) = 0 .81.
Now ﬁnd q=Φ−1(0.2). This means we have to ﬁnd qsuch that P(X<q )=
0.2. We solve this by writing
0.2=P(X<q )=Pparenleftbigg
Z<q−µ
σparenrightbigg
=Φparenleftbiggq−µ
σparenrightbigg
.
From the Normal table, Φ( −0.8416) = 0 .2. Therefore,
−0.8416 =q−µ
σ=q−3√
5
and hence q=3−0.8416√
5=1.1181./squaresolid
Exponential Distribution. Xhas an Exponential distribution with
parameter β, denoted by X∼Exp(β), if
f(x)=1
βe−x/β,x > 0
where β>0. The exponential distribution is used to model the lifetimes of
electronic components and the waiting times between rare events.
Gamma Distribution. Forα>0, the Gamma function is deﬁned by
Γ(α)=integraltext∞
0yα−1e−ydy.Xhas a Gamma distribution with parameters αand

<<<PAGE 46>>>

30 2. Random Variables
β, denoted by X∼Gamma( α,β), if
f(x)=1
βαΓ(α)xα−1e−x/β,x > 0
where α,β > 0. The exponential distribution is just a Gamma(1 ,β) distribu-
tion. If Xi∼Gamma( αi,β) are independent, thensummationtextn
i=1Xi∼Gamma(summationtextn
i=1αi,β).
The Beta Distribution. Xhas a Beta distribution with parameters
α>0 and β>0, denoted by X∼Beta(α,β), if
f(x)=Γ(α+β)
Γ(α)Γ(β)xα−1(1−x)β−1,0<x< 1.
tand Cauchy Distribution. Xhas a tdistribution with νdegrees of
freedom — written X∼tν—i f
f(x)=Γparenleftbigν+1
2parenrightbig
Γparenleftbigν
2parenrightbig1
parenleftbig
1+x2
νparenrightbig(ν+1)/2.
Thetdistribution is similar to a Normal but it has thicker tails. In fact, the
Normal corresponds to a twithν=∞. The Cauchy distribution is a special
case of the tdistribution corresponding to ν= 1. The density is
f(x)=1
π(1 +x2).
To see that this is indeed a density:
integraldisplay∞
−∞f(x)dx=1
πintegraldisplay∞
−∞dx
1+x2=1
πintegraldisplay∞
−∞dtan−1(x)
dx
=1
πbracketleftbig
tan−1(∞)−tan−1(−∞)bracketrightbig
=1
πbracketleftBigπ
2−parenleftBig
−π
2parenrightBigbracketrightBig
=1.
Theχ2distribution. Xhas a χ2distribution with pdegrees of freedom
— written X∼χ2
p—i f
f(x)=1
Γ(p/2)2p/2x(p/2)−1e−x/2,x > 0.
IfZ1,...,Z pare independent standard Normal random variables thensummationtextp
i=1Z2
i∼
χ2
p.

<<<PAGE 47>>>

2.5 Bivariate Distributions 31
2.5 Bivariate Distributions
Given a pair of discrete random variables XandY, deﬁne the joint mass
function byf(x, y)=P(X=xandY=y). From now on, we write P(X=
xandY=y)a sP(X=x, Y=y). We write fasfX,Ywhen we want to be
more explicit.
2.18 Example. Here is a bivariate distribution for two random variables X
andYeach taking values 0 or 1:
Y=0 Y=1
X=0 1/9 2/9 1/3
X=1 2/9 4/9 2/3
1/3 2/3 1
Thus, f(1,1) =P(X=1,Y=1 )=4 /9./squaresolid
2.19 Deﬁnition. In the continuous case, we call a function f(x, y)apdf
for the random variables (X,Y)if
(i)f(x, y)≥0for all (x, y),
(ii)integraltext∞
−∞integraltext∞
−∞f(x, y)dxdy=1and,
(iii) for any set A⊂R×R,P((X,Y)∈A)=integraltextintegraltext
Af(x, y)dxdy.
In the discrete or continuous case we deﬁne the joint cdfasFX,Y(x, y)=
P(X≤x, Y≤y).
2.20 Example. Let (X,Y) be uniform on the unit square. Then,
f(x, y)=braceleftbigg1i f 0 ≤x≤1,0≤y≤1
0 otherwise .
Find P(X<1/2,Y < 1/2). The event A={X<1/2,Y < 1/2}corresponds
to a subset of the unit square. Integrating fover this subset corresponds, in
this case, to computing the area of the set Awhich is 1/4. So, P(X<1/2,Y <
1/2 )=1 /4./squaresolid

<<<PAGE 48>>>

32 2. Random Variables
2.21 Example. Let (X,Y) have density
f(x, y)=braceleftbigg
x+yif 0≤x≤1,0≤y≤1
0 otherwise .
Then
integraldisplay1
0integraldisplay1
0(x+y)dxdy =integraldisplay1
0bracketleftbiggintegraldisplay1
0xd xbracketrightbigg
dy+integraldisplay1
0bracketleftbiggintegraldisplay1
0yd xbracketrightbigg
dy
=integraldisplay1
01
2dy+integraldisplay1
0yd y=1
2+1
2=1
which veriﬁes that this is a pdf /squaresolid
2.22 Example. If the distribution is deﬁned over a non-rectangular region,
then the calculations are a bit more complicated. Here is an example which Iborrowed from DeGroot and Schervish (2002). Let ( X,Y) have density
f(x, y)=braceleftbiggcx
2yifx2≤y≤1
0 otherwise .
Note ﬁrst that −1≤x≤1. Now let us ﬁnd the value of c. The trick here is
to be careful about the range of integration. We pick one variable, xsay, and
let it range over its values. Then, for each ﬁxed value of x,w el e t yvary over
its range, which is x2≤y≤1. It may help if you look at Figure 2.5. Thus,
1=integraldisplayintegraldisplay
f(x, y)dydx=cintegraldisplay1
−1integraldisplay1
x2x2yd yd x
=cintegraldisplay1
−1x2bracketleftbiggintegraldisplay1
x2yd ybracketrightbigg
dx=cintegraldisplay1
−1x21−x4
2dx=4c
21.
Hence, c=2 1/4.Now let us compute P(X≥Y). This corresponds to the set
A={(x, y); 0≤x≤1,x2≤y≤x}. (You can see this by drawing a diagram.)
So,
P(X≥Y)=21
4integraldisplay1
0integraldisplayx
x2x2yd yd x =21
4integraldisplay1
0x2bracketleftbiggintegraldisplayx
x2yd ybracketrightbigg
dx
=21
4integraldisplay1
0x2parenleftbiggx2−x4
2parenrightbigg
dx=3
20./squaresolid

<<<PAGE 49>>>

2.6 Marginal Distributions 33
011y=x2
y=x
xy
FIGURE 2.5. The light shaded region is x2≤y≤1. The density is positive over
this region. The hatched region is the event X≥Yintersected with x2≤y≤1.
2.6 Marginal Distributions
2.23 Deﬁnition. If(X,Y)have joint distribution with mass function
fX,Y, then the marginal mass function for Xis deﬁned by
fX(x)=P(X=x)=summationdisplay
yP(X=x, Y=y)=summationdisplay
yf(x, y) (2.4)
and the marginal mass function for Yis deﬁned by
fY(y)=P(Y=y)=summationdisplay
xP(X=x, Y=y)=summationdisplay
xf(x, y). (2.5)
2.24 Example. Suppose that fX,Yis given in the table that follows. The
marginal distribution for Xcorresponds to the row totals and the marginal
distribution for Ycorresponds to the columns totals.
Y=0 Y=1
X=0 1/10 2/10 3/10
X=1 3/10 4/10 7/10
4/10 6/10 1
For example, fX( 0 )=3 /10 and fX( 1 )=7 /10./squaresolid

<<<PAGE 50>>>

34 2. Random Variables
2.25 Deﬁnition. For continuous random variables, the marginal densities
are
fX(x)=integraldisplay
f(x, y)dy, and fY(y)=integraldisplay
f(x, y)dx. (2.6)
The corresponding marginal distribution functions are denoted by FXand
FY.
2.26 Example. Suppose that
fX,Y(x, y)=e−(x+y)
forx, y≥0. Then fX(x)=e−xintegraltext∞
0e−ydy=e−x./squaresolid
2.27 Example. Suppose that
f(x, y)=braceleftbiggx+yif 0≤x≤1,0≤y≤1
0 otherwise .
Then
fY(y)=integraldisplay1
0(x+y)dx=integraldisplay1
0xd x+integraldisplay1
0yd x=1
2+y./squaresolid
2.28 Example. Let (X,Y) have density
f(x, y)=braceleftbigg21
4x2yifx2≤y≤1
0 otherwise .
Thus,
fX(x)=integraldisplay
f(x, y)dy=21
4x2integraldisplay1
x2yd y=21
8x2(1−x4)
for−1≤x≤1 and fX(x) = 0 otherwise. /squaresolid
2.7 Independent Random Variables
2.29 Deﬁnition. Two random variables XandYareindependent if,
for every AandB,
P(X∈A, Y∈B)=P(X∈A)P(Y∈B) (2.7)
and we write X/coproductY. Otherwise we say that XandYaredependent
and we write X/gluonbelement/gluonelement/gluonelement/gluonelement/gluonelement/gluoneelementY.

<<<PAGE 51>>>

2.7 Independent Random Variables 35
In principle, to check whether XandYare independent we need to check
equation (2.7) for all subsets AandB. Fortunately, we have the following
result which we state for continuous random variables though it is true fordiscrete random variables too.
2.30 Theorem. LetXandYhave joint pdff
X,Y. Then X/coproductYif and only
iffX,Y(x, y)=fX(x)fY(y)for all values xandy.5
2.31 Example. LetXandYhave the following distribution:
Y=0 Y=1
X=0 1/4 1/4 1/2
X=1 1/4 1/4 1/2
1/2 1/2 1
Then, fX(0) = fX( 1 )=1 /2 and fY(0) = fY( 1 )=1 /2.XandYare inde-
pendent because fX(0)fY(0) = f(0,0),fX(0)fY(1) = f(0,1),fX(1)fY(0) =
f(1,0),fX(1)fY(1) = f(1,1). Suppose instead that XandYhave the follow-
ing distribution:
Y=0 Y=1
X=0 1/2 0 1/2
X=1 0 1/2 1/2
1/2 1/2 1
These are not independent because fX(0)fY( 1 )=( 1 /2)(1/2 )=1 /4y e t
f(0,1 )=0 . /squaresolid
2.32 Example. Suppose that XandYare independent and both have the
same density
f(x)=braceleftbigg2xif 0≤x≤1
0 otherwise .
Let us ﬁnd P(X+Y≤1). Using independence, the joint density is
f(x, y)=fX(x)fY(y)=braceleftbigg4xyif 0≤x≤1,0≤y≤1
0 otherwise .
5The statement is not rigorous because the density is deﬁned only up to sets of
measure 0.

<<<PAGE 52>>>

36 2. Random Variables
Now,
P(X+Y≤1) =integraldisplayintegraldisplay
x+y≤1f(x, y)dydx
=4integraldisplay1
0xbracketleftbiggintegraldisplay1−x
0ydybracketrightbigg
dx
=4integraldisplay1
0x(1−x)2
2dx=1
6./squaresolid
The following result is helpful for verifying independence.
2.33 Theorem. Suppose that the range of XandYis a (possibly inﬁnite)
rectangle. If f(x, y)=g(x)h(y)for some functions gandh(not necessarily
probability density functions) then XandYare independent.
2.34 Example. LetXandYhave density
f(x, y)=braceleftbigg
2e−(x+2y)ifx>0 and y>0
0 otherwise .
The range of XandYis the rectangle (0 ,∞)×(0,∞). We can write f(x, y)=
g(x)h(y) where g(x)=2e−xandh(y)=e−2y. Thus, X/coproductY./squaresolid
2.8 Conditional Distributions
IfXandYare discrete, then we can compute the conditional distribution of
Xgiven that we have observed Y=y. Speciﬁcally, P(X=x|Y=y)=P(X=
x, Y=y)/P(Y=y). This leads us to deﬁne the conditional probability mass
function as follows.
2.35 Deﬁnition. Theconditional probability mass function is
fX|Y(x|y)=P(X=x|Y=y)=P(X=x, Y=y)
P(Y=y)=fX,Y(x, y)
fY(y)
iffY(y)>0.
For continuous distributions we use the same deﬁnitions.6The interpre-
tation diﬀers: in the discrete case, fX|Y(x|y)i sP(X=x|Y=y), but in the
continuous case, we must integrate to get a probability.
6We are treading in deep water here. When we compute P(X∈A|Y=y)in the
continuous case we are conditioning on the event {Y=y}which has probability 0. We

<<<PAGE 53>>>

2.8 Conditional Distributions 37
2.36 Deﬁnition. For continuous random variables, the conditional
probability density function is
fX|Y(x|y)=fX,Y(x, y)
fY(y)
assuming that fY(y)>0. Then,
P(X∈A|Y=y)=integraldisplay
AfX|Y(x|y)dx.
2.37 Example. LetXandYhave a joint uniform distribution on the unit
square. Thus, fX|Y(x|y) = 1 for 0 ≤x≤1 and 0 otherwise. Given Y=y,X
is Uniform(0 ,1). We can write this as X|Y=y∼Uniform(0 ,1)./squaresolid
From the deﬁnition of the conditional density, we see that fX,Y(x, y)=
fX|Y(x|y)fY(y)=fY|X(y|x)fX(x). This can sometimes be useful as in exam-
ple 2.39.
2.38 Example. Let
f(x, y)=braceleftbiggx+yif 0≤x≤1,0≤y≤1
0 otherwise .
Let us ﬁnd P(X< 1/4|Y=1/3). In example 2.27 we saw that fY(y)=
y+( 1/2). Hence,
fX|Y(x|y)=fX,Y(x, y)
fY(y)=x+y
y+1
2.
So,
PparenleftBigg
X<1
4vextendsinglevextendsinglevextendsinglevextendsinglevextendsingleY=1
3parenrightBigg
=integraldisplay1/4
0fX|YparenleftBigg
xvextendsinglevextendsinglevextendsinglevextendsinglevextendsingle1
3parenrightBigg
dx
=integraldisplay1/4
0x+1
3
1
3+1
2dx=1
32+1
12
1
3+1
2=11
80./squaresolid
2.39 Example. Suppose that X∼Uniform(0 ,1). After obtaining a value of
Xwe generate Y|X=x∼Uniform( x,1). What is the marginal distribution
avoid this problem by deﬁning things in terms of the pdf . The fact that this leads to
a well-deﬁned theory is proved in more advanced courses. Here, we simply take it as adeﬁnition.

<<<PAGE 54>>>

38 2. Random Variables
ofY? First note that,
fX(x)=braceleftbigg
1i f 0 ≤x≤1
0 otherwise
and
fY|X(y|x)=braceleftbigg1
1−xif 0<x<y< 1
0 otherwise .
So,
fX,Y(x, y)=fY|X(y|x)fX(x)=braceleftbigg1
1−xif 0<x<y< 1
0 otherwise .
The marginal for Yis
fY(y)=integraldisplayy
0fX,Y(x, y)dx=integraldisplayy
0dx
1−x=−integraldisplay1−y
1du
u=−log(1−y)
for 0<y< 1./squaresolid
2.40 Example. Consider the density in Example 2.28. Let’s ﬁnd fY|X(y|x).
When X=x,ymust satisfy x2≤y≤1. Earlier, we saw that fX(x)=
(21/8)x2(1−x4). Hence, for x2≤y≤1,
fY|X(y|x)=f(x, y)
fX(x)=21
4x2y
21
8x2(1−x4)=2y
1−x4.
Now let us compute P(Y≥3/4|X=1/2). This can be done by ﬁrst noting
thatfY|X(y|1/2 )=3 2 y/15. Thus,
P(Y≥3/4|X=1/2) =integraldisplay1
3/4f(y|1/2)dy=integraldisplay1
3/432y
15dy=7
15./squaresolid
2.9 Multivariate Distributions and iidSamples
LetX=(X1,...,X n) where X1,...,X nare random variables. We call Xa
random vector . Let f(x1,...,x n) denote the pdf. It is possible to deﬁne
their marginals, conditionals etc. much the same way as in the bivariate case.We say that X
1,...,X nare independent if, for every A1,...,A n,
P(X1∈A1,...,X n∈An)=nproductdisplay
i=1P(Xi∈Ai). (2.8)
It suﬃces to check that f(x1,...,x n)=producttextn
i=1fXi(xi).

<<<PAGE 55>>>

2.10 Two Important Multivariate Distributions 39
2.41 Deﬁnition. IfX1,...,X nare independent and each has the same
marginal distribution with cdfF, we say that X1,...,X nareiid
(independent and identically distributed) and we write
X1,...X n∼F.
IfFhas density fwe also write X1,...X n∼f. We also call X1,...,X n
arandom sample of size nfrom F.
Much of statistical theory and practice begins with iidobservations and we
shall study this case in detail when we discuss statistics.
2.10 Two Important Multivariate Distributions
Multinomial . The multivariate version of a Binomial is called a Multino-
mial. Consider drawing a ball from an urn which has balls with kdiﬀerent
colors labeled “color 1, color 2, ...,color k.” Let p=(p1,...,p k) where
pj≥0 andsummationtextk
j=1pj= 1 and suppose that pjis the probability of drawing
a ball of color j. Draw ntimes (independent draws with replacement) and
letX=(X1,...,X k) where Xjis the number of times that color jappears.
Hence, n=summationtextk
j=1Xj. We say that Xhas a Multinomial ( n,p) distribution
written X∼Multinomial( n, p). The probability function is
f(x)=parenleftbiggn
x1...x kparenrightbigg
px1
1···pxk
k(2.9)
where parenleftbiggn
x1...x kparenrightbigg
=n!
x1!···xk!.
2.42 Lemma. Suppose that X∼Multinomial( n, p)where X=(X1,...,X k)
andp=(p1,...,p k). The marginal distribution of Xjis Binomial ( n,pj).
Multivariate Normal . The univariate Normal has two parameters, µ
andσ. In the multivariate version, µis a vector and σis replaced by a matrix
Σ. To begin, let
Z=
Z1
...
Zk


<<<PAGE 56>>>

40 2. Random Variables
where Z1,...,Z k∼N(0,1) are independent. The density of Zis7
f(z)=kproductdisplay
i=1f(zi)=1
(2π)k/2exp

−1
2ksummationdisplay
j=1z2
j


=1
(2π)k/2expbraceleftbigg
−1
2zTzbracerightbigg
.
We say that Zhas a standard multivariate Normal distribution written Z∼
N(0,I) where it is understood that 0 represents a vector of kzeroes and Iis
thek×kidentity matrix.
More generally, a vector Xhas a multivariate Normal distribution, denoted
byX∼N(µ,Σ), if it has density8
f(x;µ,Σ) =1
(2π)k/2|(Σ)|1/2expbraceleftbigg
−1
2(x−µ)TΣ−1(x−µ)bracerightbigg
(2.10)
where |Σ|denotes the determinant of Σ, µis a vector of length ka n dΣi sa
k×ksymmetric, positive deﬁnite matrix.9Setting µ=0a n dΣ= Igives
back the standard Normal.
Since Σ is symmetric and positive deﬁnite, it can be shown that there exists
a matrix Σ1/2— called the square root o f Σ — with the following properties:
(i) Σ1/2is symmetric, (ii) Σ = Σ1/2Σ1/2and (iii) Σ1/2Σ−1/2=Σ−1/2Σ1/2=I
where Σ−1/2=( Σ1/2)−1.
2.43 Theorem. IfZ∼N(0,I)andX=µ+Σ1/2ZthenX∼N(µ,Σ).
Conversely, if X∼N(µ,Σ), then Σ−1/2(X−µ)∼N(0,I).
Suppose we partition a random Normal vector XasX=(Xa,Xb)W ec a n
similarly partition µ=(µa,µb) and
Σ=parenleftbiggΣaaΣab
ΣbaΣbbparenrightbigg
.
2.44 Theorem. LetX∼N(µ,Σ). Then:
(1) The marginal distribution of XaisXa∼N(µa,Σaa).
(2) The conditional distribution of Xbgiven Xa=xais
Xb|Xa=xa∼Nparenleftbig
µb+ΣbaΣ−1
aa(xa−µa),Σbb−ΣbaΣ−1
aaΣabparenrightbig
.
(3) If ais a vector then aTX∼N(aTµ, aTΣa).
(4)V=(X−µ)TΣ−1(X−µ)∼χ2
k.
7Ifaandbare vectors then aTb=/summationtextk
i=1aibi.
8Σ−1is the inverse of the matrix Σ.
9A matrix Σis positive deﬁnite if, for all nonzero vectors x,xTΣx>0.

<<<PAGE 57>>>

2.11 Transformations of Random Variables 41
2.11 Transformations of Random Variables
Suppose that Xis a random variable with pdffXandcdfFX. LetY=r(X)
be a function of X, for example, Y=X2orY=eX. We call Y=r(X)a
transformation of X. How do we compute the pdfand cdfofY?I nt h e
discrete case, the answer is easy. The mass function of Yis given by
fY(y)= P(Y=y)=P(r(X)=y)
=P({x;r(x)=y})=P(X∈r−1(y)).
2.45 Example. Suppose that P(X=−1) =P(X=1 )=1 /4 and P(X=0 )=
1/2. Let Y=X2. Then, P(Y=0 )= P(X=0 )=1 /2 and P(Y=1 )= P(X=
1) +P(X=−1 )=1 /2. Summarizing:
xf X(x)
-1 1/4
0 1/21 1/4yf
Y(y)
0 1/2
1 1/2
Ytakes fewer values than Xbecause the transformation is not one-to-one. /squaresolid
The continuous case is harder. There are three steps for ﬁnding fY:
Three Steps for Transformations
1. For each y, ﬁnd the set Ay={x:r(x)≤y}.
2. Find the cdf
FY(y)= P(Y≤y)=P(r(X)≤y)
=P({x;r(x)≤y})
=integraldisplay
AyfX(x)dx. (2.11)
3. The pdfisfY(y)=F/prime
Y(y).
2.46 Example. LetfX(x)=e−xforx>0. Hence, FX(x)=integraltextx
0fX(s)ds=
1−e−x. LetY=r(X) = log X. Then, Ay={x:x≤ey}and
FY(y)= P(Y≤y)=P(logX≤y)
=P(X≤ey)=FX(ey)=1−e−ey.
Therefore, fY(y)=eye−eyfory∈R./squaresolid

<<<PAGE 58>>>

42 2. Random Variables
2.47 Example. LetX∼Uniform( −1,3). Find the pdfofY=X2. The
density of Xis
fX(x)=braceleftbigg
1/4i f −1<x< 3
0 otherwise .
Ycan only take values in (0 ,9). Consider two cases: (i) 0 <y< 1 and (ii) 1 ≤
y<9. For case (i), Ay=[−√y,√y] and FY(y)=integraltext
AyfX(x)dx=( 1/2)√y.
For case (ii), Ay=[−1,√y] and FY(y)=integraltext
AyfX(x)dx=( 1/4)(√y+ 1).
Diﬀerentiating Fwe get
fY(y)=

1
4√yif 0<y< 1
1
8√yif 1<y< 9
0 otherwise ./squaresolid
When ris strictly monotone increasing or strictly monotone decreasing then
rhas an inverse s=r−1and in this case one can show that
fY(y)=fX(s(y))vextendsinglevextendsinglevextendsinglevextendsingleds(y)
dyvextendsinglevextendsinglevextendsinglevextendsingle. (2.12)
2.12 Transformations of Several Random Variables
In some cases we are interested in transformations of several random variables.
For example, if XandYare given random variables, we might want to know
the distribution of X/Y,X+Y, max{X,Y}or min {X,Y}. LetZ=r(X,Y)
be the function of interest. The steps for ﬁnding fZare the same as before:
Three Steps for Transformations
1. For each z, ﬁnd the set Az={(x, y):r(x, y)≤z}.
2. Find the cdf
FZ(z)= P(Z≤z)=P(r(X,Y)≤z)
=P({(x, y);r(x, y)≤z})=integraldisplayintegraldisplay
AzfX,Y(x, y)dx dy.
3. Then fZ(z)=F/prime
Z(z).

<<<PAGE 59>>>

2.13 Appendix 43
2.48 Example. LetX1,X2∼Uniform(0 ,1) be independent. Find the density
ofY=X1+X2. The joint density of ( X1,X2)i s
f(x1,x2)=braceleftbigg
10<x1<1,0<x2<1
0 otherwise .
Letr(x1,x2)=x1+x2.N o w ,
FY(y)= P(Y≤y)=P(r(X1,X2)≤y)
=P({(x1,x2):r(x1,x2)≤y})=integraldisplayintegraldisplay
Ayf(x1,x2)dx1dx2.
Now comes the hard part: ﬁnding Ay. First suppose that 0 <y≤1. Then Ay
is the triangle with vertices (0 ,0),(y,0) and (0 ,y). See Figure 2.6. In this case,integraltextintegraltext
Ayf(x1,x2)dx1dx2is the area of this triangle which is y2/2. If 1 <y< 2,
thenAyis everything in the unit square except the triangle with vertices
(1,y−1),(1,1),(y−1,1). This set has area 1 −(2−y)2/2. Therefore,
FY(y)=

0 y<0
y2
20≤y<1
1−(2−y)2
21≤y<2
1 y≥2.
By diﬀerentiation, the pdfis
fY(y)=

y 0≤y≤1
2−y1≤y≤2
0 otherwise ./squaresolid
2.13 Appendix
Recall that a probability measure Pis deﬁned on a σ-ﬁeld Aof a sample
space Ω. A random variable Xis ameasurable mapX:Ω→R. Measurable
means that, for every x,{ω:X(ω)≤x}∈A .
2.14 Exercises
1. Show that
P(X=x)=F(x+)−F(x−).

<<<PAGE 60>>>

44 2. Random Variables
0101
(0,y)
(y,0)/Bullet
/Bullet
This is the case 0 ≤y<1.0101
(1,y−1)(y−1,1)
/Bullet/Bullet
This is the case 1 ≤y≤2.
FIGURE 2.6. The set Ayfor example 2.48. Ayconsists of all points ( x1,x2)i nt h e
square below the line x2=y−x1.
2. Let Xbe such that P(X=2 )= P(X=3 )=1 /10 and P(X=5 )=8 /10.
Plot the cdfF. Use Fto ﬁnd P(2<X≤4.8) and P(2≤X≤4.8).
3. Prove Lemma 2.15.4. Let Xhave probability density function
f
X(x)=

1/40<x< 1
3/83<x< 5
0 otherwise .
(a) Find the cumulative distribution function of X.
(b) Let Y=1/X. Find the probability density function fY(y) forY.
Hint: Consider three cases:1
5≤y≤1
3,1
3≤y≤1, and y≥1.
5. Let XandYbe discrete random variables. Show that XandYare
independent if and only if fX,Y(x, y)=fX(x)fY(y) for all xandy.
6. Let Xhave distribution Fand density function fand let Abe a subset
of the real line. Let IA(x) be the indicator function for A:
IA(x)=braceleftbigg1x∈A
0x/∈A.
LetY=IA(X). Find an expression for the cumulative distribution of
Y. (Hint: ﬁrst ﬁnd the probability mass function for Y.)

<<<PAGE 61>>>

2.14 Exercises 45
7. Let XandYbe independent and suppose that each has a Uniform(0 ,1)
distribution. Let Z= min {X,Y}. Find the density fZ(z) forZ. Hint:
It might be easier to ﬁrst ﬁnd P(Z>z ).
8. Let Xhave cdfF. Find the cdfofX+= max {0,X}.
9. Let X∼Exp(β). Find F(x) and F−1(q).
10. Let XandYbe independent. Show that g(X) is independent of h(Y)
where gandhare functions.
11. Suppose we toss a coin once and let pbe the probability of heads. Let
Xdenote the number of heads and let Ydenote the number of tails.
(a) Prove that XandYare dependent.
(b) Let N∼Poisson( λ) and suppose we toss a coin Ntimes. Let Xand
Ybe the number of heads and tails. Show that XandYare independent.
12. Prove Theorem 2.33.13. Let X∼N(0,1) and let Y=e
X.
(a) Find the pdfforY. Plot it.
(b) (Computer Experiment. ) Generate a vector x=(x1,...,x 10,000) con-
sisting of 10,000 random standard Normals. Let y=(y1,...,y 10,000)
where yi=exi. Draw a histogram of yand compare it to the pdfyou
found in part (a).
14. Let ( X,Y) be uniformly distributed on the unit disk {(x, y):x2+y2≤
1}. LetR=√
X2+Y2. Find the cdfand pdfofR.
15. (A universal random number generator. ) LetXhave a continuous, strictly
increasing cdfF. LetY=F(X). Find the density of Y. This is called
the probability integral transform. Now let U∼Uniform(0 ,1) and let
X=F−1(U). Show that X∼F. Now write a program that takes
Uniform (0,1) random variables and generates random variables froman Exponential ( β) distribution.
16. Let X∼Poisson( λ) and Y∼Poisson( µ) and assume that XandYare
independent. Show that the distribution of Xgiven that X+Y=nis
Binomial( n, π) where π=λ/(λ+µ).

<<<PAGE 62>>>

46 2. Random Variables
Hint 1: You may use the following fact: If X∼Poisson( λ) and Y∼
Poisson( µ), and XandYare independent, then X+Y∼Poisson( µ+λ).
Hint 2: Note that {X=x, X+Y=n}={X=x, Y=n−x}.
17. Let
fX,Y(x, y)=braceleftbigg
c(x+y2)0≤x≤1 and 0 ≤y≤1
0 otherwise .
FindPparenleftbig
X<1
2|Y=1
2parenrightbig
.
18. Let X∼N(3,16). Solve the following using the Normal table and using
a computer package.
(a) Find P(X<7).
(b) Find P(X>−2).
(c) Find xsuch that P(X>x )=.05.
(d) Find P(0≤X<4).
(e) Find xsuch that P(|X|>|x|)=.05.
19. Prove formula (2.12).20. Let X,Y∼Uniform(0 ,1) be independent. Find the pdfforX−Yand
X/Y.
21. Let X
1,...,X n∼Exp(β)b e iid. LetY= max {X1,...,X n}. Find the
pdfofY. Hint: Y≤yif and only if Xi≤yfori=1,...,n .

<<<PAGE 63>>>

3
Expectation
3.1 Expectation of a Random Variable
The mean, or expectation, of a random variable Xis the average value of X.
3.1 Deﬁnition. Theexpected value ,o rmean,o rﬁrst moment ,o f
Xis deﬁned to be
E(X)=integraldisplay
xd F(x)=braceleftBiggsummationtext
xxf(x)i f Xis discreteintegraltext
xf(x)dxifXis continuous(3.1)
assuming that the sum (or integral) is well deﬁned. We use the following
notation to denote the expected value of X:
E(X)=EX=integraldisplay
xd F(x)=µ=µX. (3.2)
The expectation is a one-number summary of the distribution. Think of
E(X) as the averagesummationtextn
i=1Xi/nof a large number of iiddraws X1,...,X n.
The fact that E(X)≈summationtextni=1Xi/nis actually more than a heuristic; it is a
theorem called the law of large numbers that we will discuss in Chapter 5.
The notationintegraltext
xd F(x) deserves some comment. We use it merely as a
convenient unifying notation so we don’t have to writesummationtext
xxf(x) for discrete

<<<PAGE 64>>>

48 3. Expectation
random variables andintegraltext
xf(x)dxfor continuous random variables, but you
should be aware thatintegraltext
xd F(x) has a precise meaning that is discussed in real
analysis courses.
To ensure that E(X) is well deﬁned, we say that E(X) exists ifintegraltext
x|x|dFX(x)<
∞. Otherwise we say that the expectation does not exist.
3.2 Example. LetX∼Bernoulli( p). Then E(X)=summationtext1
x=0xf(x)=( 0 ×(1−
p) )+( 1 ×p)=p./squaresolid
3.3 Example. Flip a fair coin two times. Let Xbe the number of heads. Then,
E(X)=integraltext
xdFX(x)=summationtext
xxfX(x)=( 0 ×f(0)) + (1 ×f(1)) + (2 ×f(2)) =
(0×(1/4 ) )+( 1 ×(1/2 ) )+( 2 ×(1/4 ) )=1 ./squaresolid
3.4 Example. LetX∼Uniform( −1,3). Then, E(X)=integraltext
xd F X(x)=integraltext
xfX(x)dx=
1
4integraltext3
−1xd x=1 . /squaresolid
3.5 Example. Recall that a random variable has a Cauchy distribution if it
has density fX(x)={π(1 +x2)}−1. Using integration by parts, (set u=x
andv= tan−1x),
integraldisplay
|x|dF(x)=2
πintegraldisplay∞
0xd x
1+x2=bracketleftbig
xtan−1(x)bracketrightbig∞
0−integraldisplay∞
0tan−1xd x=∞
so the mean does not exist. If you simulate a Cauchy distribution many times
and take the average, you will see that the average never settles down. Thisis because the Cauchy has thick tails and hence extreme observations arecommon.
/squaresolid
From now on, whenever we discuss expectations, we implicitly assume that
they exist.
LetY=r(X). How do we compute E(Y)? One way is to ﬁnd fY(y) and
then compute E(Y)=integraltext
yfY(y)dy. But there is an easier way.
3.6 Theorem (The Rule of the Lazy Statistician) .LetY=r(X). Then
E(Y)=E(r(X)) =integraldisplay
r(x)dFX(x). (3.3)
This result makes intuitive sense. Think of playing a game where we draw
Xat random and then I pay you Y=r(X). Your average income is r(x) times
the chance that X=x, summed (or integrated) over all values of x. Here is

<<<PAGE 65>>>

3.1 Expectation of a Random Variable 49
a special case. Let Abe an event and let r(x)=IA(x) where IA(x)=1i f
x∈AandIA(x)=0i f x/∈A. Then
E(IA(X)) =integraldisplay
IA(x)fX(x)dx=integraldisplay
AfX(x)dx=P(X∈A).
In other words, probability is a special case of expectation.
3.7 Example. LetX∼Unif(0 ,1). Let Y=r(X)=eX. Then,
E(Y)=integraldisplay1
0exf(x)dx=integraldisplay1
0exdx=e−1.
Alternatively, you could ﬁnd fY(y) which turns out to be fY(y)=1 /yfor
1<y<e . Then, E(Y)=integraltexte
1yf(y)dy=e−1./squaresolid
3.8 Example. Take a stick of unit length and break it at random. Let Ybe
the length of the longer piece. What is the mean of Y?I fXis the break point
thenX∼Unif(0 ,1) and Y=r(X) = max {X,1−X}. Thus, r(x)=1 −x
when 0 <x< 1/2 and r(x)=xwhen 1 /2≤x<1. Hence,
E(Y)=integraldisplay
r(x)dF(x)=integraldisplay1/2
0(1−x)dx+integraldisplay1
1/2xd x=3
4./squaresolid
Functions of several variables are handled in a similar way. If Z=r(X,Y)
then
E(Z)=E(r(X,Y)) =integraldisplayintegraldisplay
r(x, y)dF(x, y). (3.4)
3.9 Example. Let (X,Y) have a jointly uniform distribution on the unit
square. Let Z=r(X,Y)=X2+Y2. Then,
E(Z)=integraldisplayintegraldisplay
r(x, y)dF(x, y)=integraldisplay1
0integraldisplay1
0(x2+y2)dxdy
=integraldisplay1
0x2dx+integraldisplay1
0y2dy=2
3./squaresolid
Thekthmoment ofXis deﬁned to be E(Xk) assuming that E(|X|k)<∞.
3.10 Theorem. If the kthmoment exists and if j<k then the jthmoment
exists.
Proof. We have
E|X|j=integraldisplay∞
−∞|x|jfX(x)dx

<<<PAGE 66>>>

50 3. Expectation
=integraldisplay
|x|≤1|x|jfX(x)dx+integraldisplay
|x|>1|x|jfX(x)dx
≤integraldisplay
|x|≤1fX(x)dx+integraldisplay
|x|>1|x|kfX(x)dx
≤1+E(|X|k)<∞./squaresolid
Thekthcentral moment is deﬁned to be E((X−µ)k).
3.2 Properties of Expectations
3.11 Theorem. IfX1,...,X nare random variables and a1,...,a nare con-
stants, then
EparenleftBiggsummationdisplay
iaiXiparenrightBigg
=summationdisplay
iaiE(Xi). (3.5)
3.12 Example. LetX∼Binomial( n, p). What is the mean of X? We could
try to appeal to the deﬁnition:
E(X)=integraldisplay
xd F X(x)=summationdisplay
xxfX(x)=nsummationdisplay
x=0xparenleftbiggn
xparenrightbigg
px(1−p)n−x
but this is not an easy sum to evaluate. Instead, note that X=summationtextn
i=1Xi
where Xi= 1 if the ithtoss is heads and Xi= 0 otherwise. Then E(Xi)=
(p×1) + ((1 −p)×0) =pandE(X)=E(summationtext
iXi)=summationtext
iE(Xi)=np./squaresolid
3.13 Theorem. LetX1,...,X nbe independent random variables. Then,
EparenleftBiggnproductdisplay
i=1XiparenrightBigg
=productdisplay
iE(Xi). (3.6)
Notice that the summation rule does not require independence but the
multiplication rule does.
3.3 Variance and Covariance
The variance measures the “spread” of a distribution.1
1We can’t use E(X−µ)as a measure of spread since E(X−µ)= E(X)−µ=µ−µ=0.
We can and sometimes do use E|X−µ|as a measure of spread but more often we use the
variance.

<<<PAGE 67>>>

3.3 Variance and Covariance 51
3.14 Deﬁnition. LetXbe a random variable with mean µ. Thevariance
ofX— denoted by σ2orσ2
XorV(X)orVX— is deﬁned by
σ2=E(X−µ)2=integraldisplay
(x−µ)2dF(x) (3.7)
assuming this expectation exists. The standard deviation is
sd(X)=radicalbig
V(X)and is also denoted by σandσX.
3.15 Theorem. Assuming the variance is well deﬁned, it has the following
properties:
1.V(X)=E(X2)−µ2.
2. Ifaandbare constants then V(aX+b)=a2V(X).
3. IfX1,...,X nare independent and a1,...,a nare constants, then
VparenleftBiggnsummationdisplay
i=1aiXiparenrightBigg
=nsummationdisplay
i=1a2
iV(Xi). (3.8)
3.16 Example. LetX∼Binomial( n, p). We write X=summationtext
iXiwhere Xi=1
if toss iis heads and Xi= 0 otherwise. Then X=summationtext
iXiand the random
variables are independent. Also, P(Xi=1 )= pandP(Xi=0 )=1 −p. Recall
that
E(Xi)=parenleftbigg
p×1parenrightbigg
+parenleftbigg
(1−p)×0parenrightbigg
=p.
Now,
E(X2
i)=parenleftbigg
p×12parenrightbigg
+parenleftbigg
(1−p)×02parenrightbigg
=p.
Therefore, V(Xi)= E(X2
i)−p2=p−p2=p(1−p). Finally, V(X)=
V(summationtext
iXi)=summationtext
iV(Xi)=summationtext
ip(1−p)=np(1−p). Notice that V(X)=0
ifp=1o r p= 0. Make sure you see why this makes intuitive sense. /squaresolid
IfX1,...,X nare random variables then we deﬁne the sample mean to be
Xn=1
nnsummationdisplay
i=1Xi (3.9)
and the sample variance to be
S2
n=1
n−1nsummationdisplay
i=1parenleftbig
Xi−Xnparenrightbig2. (3.10)

<<<PAGE 68>>>

52 3. Expectation
3.17 Theorem. LetX1,...,X nbeiidand let µ=E(Xi),σ2=V(Xi). Then
E(Xn)=µ,V(Xn)=σ2
nand E(S2
n)=σ2.
IfXandYare random variables, then the covariance and correlation be-
tween XandYmeasure how strong the linear relationship is between Xand
Y.
3.18 Deﬁnition. LetXandYbe random variables with means µXand
µYand standard deviations σXandσY. Deﬁne the covariance between
XandYby
Cov(X,Y)=Eparenleftbigg
(X−µX)(Y−µY)parenrightbigg
(3.11)
and the correlation by
ρ=ρX,Y=ρ(X,Y)=Cov(X,Y)
σXσY. (3.12)
3.19 Theorem. The covariance satisﬁes:
Cov(X,Y)=E(XY)−E(X)E(Y).
The correlation satisﬁes:
−1≤ρ(X,Y)≤1.
IfY=aX+bfor some constants aandbthenρ(X,Y)=1 ifa>0and
ρ(X,Y)=−1ifa<0.I fXandYare independent, then Cov(X,Y)=ρ=0.
The converse is not true in general.
3.20 Theorem. V(X+Y)=V(X)+V(Y)+2Cov(X,Y)andV(X−Y)=
V(X)+V(Y)−2Cov(X,Y). More generally, for random variables X1,...,X n,
VparenleftBiggsummationdisplay
iaiXiparenrightBigg
=summationdisplay
ia2
iV(Xi)+2summationdisplaysummationdisplay
i<jaiajCov(Xi,Xj).
3.4 Expectation and Variance of Important Random
Variables
Here we record the expectation of some important random variables:

<<<PAGE 69>>>

3.4 Expectation and Variance of Important Random Variables 53
Distribution Mean Variance
Point mass at aa 0
Bernoulli( p) pp (1−p)
Binomial( n, p) np np (1−p)
Geometric( p)1 /p (1−p)/p2
Poisson( λ) λλ
Uniform( a, b)( a+b)/2( b−a)2/12
Normal( µ, σ2) µσ2
Exponential( β) ββ2
Gamma( α,β) αβ αβ2
Beta(α,β) α/(α+β)αβ/((α+β)2(α+β+ 1))
tν 0 (ifν>1)ν/(ν−2) (if ν>2)
χ2
p p 2p
Multinomial( n, p) np see below
Multivariate Normal( µ,Σ)µ Σ
We derived E(X) and V(X) for the Binomial in the previous section. The
calculations for some of the others are in the exercises.
The last two entries in the table are multivariate models which involve a
random vector Xof the form
X=
X1
...
Xk
.
The mean of a random vector Xis deﬁned by
µ=
µ1
...
µk
=
E(X1)
...
E(Xk)
.
Thevariance-covariance matrix Σ is deﬁned to be
V(X)=
V(X
1) Cov(X1,X2)···Cov(X1,Xk)
Cov(X2,X1)V(X2) ···Cov(X2,Xk)
............
Cov(X
k,X1)Cov(Xk,X2)···V(Xk)
.
IfX∼Multinomial( n, p) then E(X)=np=n(p
1,...,p k) and
V(X)=
np
1(1−p1)−np1p2 ··· − np1pk
−np2p1 np2(1−p2)··· − np2pk
............
−np
kp1 −npkp2 ···npk(1−pk)
.

<<<PAGE 70>>>

54 3. Expectation
To see this, note that the marginal distribution of any one component of the
vector Xi∼Binomial( n, pi). Thus, E(Xi)=npiandV(Xi)=npi(1−pi).
Note also that Xi+Xj∼Binomial( n, pi+pj). Thus, V(Xi+Xj)=n(pi+
pj)(1−[pi+pj]). On the other hand, using the formula for the variance
of a sum, we have that V(Xi+Xj)=V(Xi)+V(Xj)+2Cov(Xi,Xj)=
npi(1−pi)+npj(1−pj)+2Cov(Xi,Xj). If we equate this formula with
n(pi+pj)(1−[pi+pj]) and solve, we get Cov(Xi,Xj)=−npipj.
Finally, here is a lemma that can be useful for ﬁnding means and variances
of linear combinations of multivariate random vectors.
3.21 Lemma. Ifais a vector and Xis a random vector with mean µand
variance Σ, then E(aTX)=aTµandV(aTX)=aTΣa.I fAis a matrix then
E(AX)=AµandV(AX)=AΣAT.
3.5 Conditional Expectation
Suppose that XandYare random variables. What is the mean of Xamong
those times when Y=y? The answer is that we compute the mean of Xas
before but we substitute fX|Y(x|y) forfX(x) in the deﬁnition of expectation.
3.22 Deﬁnition. The conditional expectation of Xgiven Y=yis
E(X|Y=y)=braceleftBiggsummationtextxfX|Y(x|y)dxdiscrete caseintegraltext
xfX|Y(x|y)dxcontinuous case .(3.13)
Ifr(x, y)is a function of xandythen
E(r(X,Y)|Y=y)=braceleftBiggsummationtextr(x, y)fX|Y(x|y)dxdiscrete caseintegraltext
r(x, y)fX|Y(x|y)dxcontinuous case .
(3.14)
Warning! Here is a subtle point. Whereas E(X)i san u m b e r , E(X|Y=y)
is a function of y. Before we observe Y, we don’t know the value of E(X|Y=y)
so it is a random variable which we denote E(X|Y). In other words, E(X|Y)
is the random variable whose value is E(X|Y=y) when Y=y. Similarly,
E(r(X,Y)|Y) is the random variable whose value is E(r(X,Y)|Y=y) when
Y=y. This is a very confusing point so let us look at an example.
3.23 Example. Suppose we draw X∼Unif(0 ,1). After we observe X=x,
we draw Y|X=x∼Unif(x,1). Intuitively, we expect that E(Y|X=x)=

<<<PAGE 71>>>

3.5 Conditional Expectation 55
(1 +x)/2. In fact, fY|X(y|x)=1/(1−x) forx<y< 1 and
E(Y|X=x)=integraldisplay1
xyfY|X(y|x)dy=1
1−xintegraldisplay1
xyd y=1+x
2
as expected. Thus, E(Y|X)=( 1+ X)/2. Notice that E(Y|X)=( 1+ X)/2i s
a random variable whose value is the number E(Y|X=x) = (1 + x)/2 once
X=xis observed. /squaresolid
3.24 Theorem (The Rule of Iterated Expectations) .For random variables X
andY, assuming the expectations exist, we have that
E[E(Y|X)] =E(Y) and E[E(X|Y)] =E(X). (3.15)
More generally, for any function r(x, y)we have
E[E(r(X,Y)|X)] =E(r(X,Y)). (3.16)
Proof. We’ll prove the ﬁrst equation. Using the deﬁnition of conditional
expectation and the fact that f(x, y)=f(x)f(y|x),
E[E(Y|X)] =integraldisplay
E(Y|X=x)fX(x)dx=integraldisplayintegraldisplay
yf(y|x)dyf(x)dx
=integraldisplayintegraldisplay
yf(y|x)f(x)dxdy=integraldisplayintegraldisplay
yf(x, y)dxdy=E(Y)./squaresolid
3.25 Example. Consider example 3.23. How can we compute E(Y)? One
method is to ﬁnd the joint density f(x, y) and then compute E(Y)=integraltextintegraltext
yf(x, y)dxdy.
An easier way is to do this in two steps. First, we already know that E(Y|X)=
(1 +X)/2. Thus,
E(Y)= EE(Y|X)=Eparenleftbigg(1 +X)
2parenrightbigg
=(1 +E(X))
2=( 1+( 1 /2))
2=3/4./squaresolid
3.26 Deﬁnition. Theconditional variance is deﬁned as
V(Y|X=x)=integraldisplay
(y−µ(x))2f(y|x)dy (3.17)
where µ(x)=E(Y|X=x).
3.27 Theorem. For random variables XandY,
V(Y)=EV(Y|X)+VE(Y|X).

<<<PAGE 72>>>

56 3. Expectation
3.28 Example. Draw a county at random from the United States. Then draw
npeople at random from the county. Let Xbe the number of those people
who have a certain disease. If Qdenotes the proportion of people in that
county with the disease, then Qis also a random variable since it varies from
county to county. Given Q=q, we have that X∼Binomial( n, q). Thus,
E(X|Q=q)=nqandV(X|Q=q)=nq(1−q). Suppose that the random
variable Qhas a Uniform (0,1) distribution. A distribution that is constructed
in stages like this is called a hierarchical model and can be written as
Q∼Uniform(0 ,1)
X|Q=q∼Binomial( n, q).
Now, E(X)=EE(X|Q)=E(nQ)=nE(Q)=n/2. Let us compute the
variance of X.N o w , V(X)=EV(X|Q)+VE(X|Q).Let’s compute these
two terms. First, EV(X|Q)=E[nQ(1−Q)] =nE(Q(1−Q)) =nintegraltext
q(1−
q)f(q)dq=nintegraltext1
0q(1−q)dq=n/6. Next, VE(X|Q)=V(nQ)=n2V(Q)=
n2integraltext
(q−(1/2))2dq=n2/12. Hence, V(X)=(n/6 )+( n2/12)./squaresolid
3.6 Moment Generating Functions
Now we will deﬁne the moment generating function which is used for ﬁnding
moments, for ﬁnding the distribution of sums of random variables and whichis also used in the proofs of some theorems.
3.29 Deﬁnition. Themoment generating function mgf,o rLaplace
transform ,o fXis deﬁned by
ψX(t)=E(etX)=integraldisplay
etxdF(x)
where tvaries over the real numbers.
In what follows, we assume that the mgfis well deﬁned for all tin some
open interval around t=0 .2
When the mgfis well deﬁned, it can be shown that we can interchange the
operations of diﬀerentiation and “taking expectation.” This leads to
ψ/prime(0) =bracketleftbiggd
dtEetXbracketrightbigg
t=0=Ebracketleftbiggd
dtetXbracketrightbigg
t=0=Ebracketleftbig
XetXbracketrightbig
t=0=E(X).
2A related function is the characteristic function, deﬁned by E(eitX)where i=√−1. This
function is always well deﬁned for all t.

<<<PAGE 73>>>

3.6 Moment Generating Functions 57
By taking kderivatives we conclude that ψ(k)(0) = E(Xk). This gives us a
method for computing the moments of a distribution.
3.30 Example. LetX∼Exp(1). For any t<1,
ψX(t)=EetX=integraldisplay∞
0etxe−xdx=integraldisplay∞
0e(t−1)xdx=1
1−t.
The integral is divergent if t≥1. So, ψX(t)=1/(1−t) for all t<1. Now,
ψ/prime(0) = 1 and ψ/prime/prime(0) = 2. Hence, E(X)=1a n d V(X)=E(X2)−µ2=2−1=
1./squaresolid
3.31 Lemma. Properties of the mgf.
(1) If Y=aX+b, then ψY(t)=ebtψX(at).
(2) If X1,...,X nare independent and Y=summationtext
iXi, then ψY(t)=producttext
iψi(t)
where ψiis the mgfofXi.
3.32 Example. LetX∼Binomial( n, p). We know that X=summationtextn
i=1Xiwhere
P(Xi=1 )= pandP(Xi=0 )=1 −p.N o w ψi(t)=EeXit=(p×et) + ((1 −
p)) =pet+qwhere q=1−p. Thus, ψX(t)=producttext
iψi(t)=(pet+q)n./squaresolid
Recall that XandYare equal in distribution if they have the same distri-
bution function and we write Xd=Y.
3.33 Theorem. LetXandYbe random variables. If ψX(t)=ψY(t)for all t
in an open interval around 0, then Xd=Y.
3.34 Example. LetX1∼Binomial( n1,p) and X2∼Binomial( n2,p) be inde-
pendent. Let Y=X1+X2. Then,
ψY(t)=ψ1(t)ψ2(t)=(pet+q)n1(pet+q)n2=(pet+q)n1+n2
and we recognize the latter as the mgfof a Binomial( n1+n2,p) distribu-
tion. Since the mgfcharacterizes the distribution (i.e., there can’t be an-
other random variable which has the same mgf) we conclude that Y∼
Binomial( n1+n2,p)./squaresolid

<<<PAGE 74>>>

58 3. Expectation
Moment Generating Functions for Some Common Distributions
Distribution MGF ψ(t)
Bernoulli( p) pet+( 1−p)
Binomial( n, p)(pet+( 1−p))n
Poisson( λ) eλ(et−1)
Normal( µ,σ) expbraceleftBig
µt+σ2t2
2bracerightBig
Gamma( α,β)parenleftBig
1
1−βtparenrightBigα
fort<1/β
3.35 Example. LetY1∼Poisson( λ1) and Y2∼Poisson( λ2) be independent.
The moment generating function of Y=Y1+Y+2is ψY(t)=ψY1(t)ψY2(t)=
eλ1(et−1)eλ2(et−1)=e(λ1+λ2)(et−1)which is the moment generating function
of a Poisson( λ1+λ2). We have thus proved that the sum of two independent
Poisson random variables has a Poisson distribution. /squaresolid
3.7 Appendix
Expectation as an Integral . The integral of a measurable function r(x)
is deﬁned as follows. First suppose that ris simple, meaning that it takes
ﬁnitely many values a1,...,a kover a partition A1,...,A k. Then deﬁne
integraldisplay
r(x)dF(x)=ksummationdisplay
i=1aiP(r(X)∈Ai).
The integral of a positive measurable function ris deﬁned byintegraltext
r(x)dF(x)=
limiintegraltext
ri(x)dF(x) where riis a sequence of simple functions such that ri(x)≤
r(x) and ri(x)→r(x)a si→∞. This does not depend on the particular se-
quence. The integral of a measurable function ris deﬁned to beintegraltext
r(x)dF(x)=integraltext
r+(x)dF(x)−integraltext
r−(x)dF(x) assuming both integrals are ﬁnite, where r+(x)=
max{r(x),0}andr−(x)=−min{r(x),0}.
3.8 Exercises
1. Suppose we play a game where we start with cdollars. On each play of
the game you either double or halve your money, with equal probability.What is your expected fortune after ntrials?

<<<PAGE 75>>>

3.8 Exercises 59
2. Show that V(X) = 0 if and only if there is a constant csuch that
P(X=c)=1 .
3. Let X1,...,X n∼Uniform(0 ,1) and let Yn= max {X1,...,X n}. Find
E(Yn).
4. A particle starts at the origin of the real line and moves along the line in
jumps of one unit. For each jump the probability is pthat the particle
will jump one unit to the left and the probability is 1 −pthat the particle
will jump one unit to the right. Let Xnbe the position of the particle
afternunits. Find E(Xn) and V(Xn). (This is known as a random
walk.)
5. A fair coin is tossed until a head is obtained. What is the expected
number of tosses that will be required?
6. Prove Theorem 3.6 for discrete random variables.7. Let Xbe a continuous random variable with cdfF. Suppose that
P(X> 0) = 1 and that E(X) exists. Show that E(X)=integraltext
∞
0P(X>
x)dx.
Hint: Consider integrating by parts. The following fact is helpful: if E(X)
exists then lim x→∞x[1−F(x) ]=0 .
8. Prove Theorem 3.17.9. (Computer Experiment .) Let X
1,X2,...,X nbeN(0,1) random variables
and let Xn=n−1summationtextn
i=1Xi. Plot Xnversus nforn=1,...,10,000.
Repeat for X1,X2,...,X n∼Cauchy. Explain why there is such a dif-
ference.
10. Let X∼N(0,1) and let Y=eX. Find E(Y) and V(Y).
11. (Computer Experiment: Simulating the Stock Market .) Let Y1,Y2,...be
independent random variables such that P(Yi=1 )= P(Yi=−1) =
1/2. Let Xn=summationtextn
i=1Yi. Think of Yi= 1 as “the stock price increased
by one dollar”, Yi=−1 as “the stock price decreased by one dollar”,
andXnas the value of the stock on day n.
(a) Find E(Xn) and V(Xn).
(b) Simulate Xnand plot Xnversus nforn=1,2,...,10,000. Repeat
the whole simulation several times. Notice two things. First, it’s easyto “see” patterns in the sequence even though it is random. Second,

<<<PAGE 76>>>

60 3. Expectation
you will ﬁnd that the four runs look very diﬀerent even though they
were generated the same way. How do the calculations in (a) explainthe second observation?
12. Prove the formulas given in the table at the beginning of Section 3.4
for the Bernoulli, Poisson, Uniform, Exponential, Gamma, and Beta.Here are some hints. For the mean of the Poisson, use the fact thate
a=summationtext∞
x=0ax/x!. To compute the variance, ﬁrst compute E(X(X−1)).
For the mean of the Gamma, it will help to multiply and divide byΓ(α+1 )/β
α+1and use the fact that a Gamma density integrates to 1.
For the Beta, multiply and divide by Γ( α+ 1)Γ( β)/Γ(α+β+ 1).
13. Suppose we generate a random variable X in the following way. First
we ﬂip a fair coin. If the coin is heads, take X to have a Unif(0,1)distribution. If the coin is tails, take X to have a Unif(3,4) distribution.
(a) Find the mean of X.(b) Find the standard deviation of X.
14. Let X
1,...,X mandY1,...,Y nbe random variables and let a1,...,a m
andb1,...,b nbe constants. Show that
Cov
msummationdisplay
i=1aiXi,nsummationdisplay
j=1bjYj
=msummationdisplay
i=1nsummationdisplay
j=1aibjCov(Xi,Yj).
15. Let
fX,Y(x, y)=braceleftbigg1
3(x+y)0≤x≤1,0≤y≤2
0 otherwise .
Find V(2X−3Y+ 8).
16. Let r(x) be a function of x and let s(y) be a function of y. Show that
E(r(X)s(Y)|X)=r(X)E(s(Y)|X).
Also, show that E(r(X)|X)=r(X).
17. Prove that
V(Y)=EV(Y|X)+VE(Y|X).
Hint: Let m=E(Y) and let b(x)=E(Y|X=x). Note that E(b(X)) =
EE(Y|X)=E(Y)=m.Bear in mind that bis a function of x.N o w
write V(Y)=E(Y−m)2=E((Y−b(X) )+(b(X)−m))2.Expand the

<<<PAGE 77>>>

3.8 Exercises 61
square and take the expectation. You then have to take the expectation
of three terms. In each case, use the rule of the iterated expectation:E(stuﬀ) = E(E(stuﬀ|X)).
18. Show that if E(X|Y=y)=cfor some constant c, then XandYare
uncorrelated.
19. This question is to help you understand the idea of a sampling dis-
tribution. LetX
1,...,X nbeiidwith mean µand variance σ2. Let
Xn=n−1summationtextn
i=1Xi. Then Xnis astatistic , that is, a function of the
data. Since Xnis a random variable, it has a distribution. This distri-
bution is called the sampling distribution of the statistic. Recall from
Theorem 3.17 that E(Xn)=µandV(Xn)=σ2/n. Don’t confuse the
distribution of the data fXand the distribution of the statistic fXn.T o
make this clear, let X1,...,X n∼Uniform(0 ,1). Let fXbe the density
of the Uniform(0 ,1). Plot fX. Now let Xn=n−1summationtextn
i=1Xi. Find E(Xn)
andV(Xn). Plot them as a function of n. Interpret. Now simulate the
distribution of Xnforn=1,5,25,100. Check that the simulated values
ofE(Xn) and V(Xn) agree with your theoretical calculations. What do
you notice about the sampling distribution of Xnasnincreases?
20. Prove Lemma 3.21.21. Let XandYbe random variables. Suppose that E(Y|X)=X. Show
thatCov(X,Y)=V(X).
22. Let X∼Uniform(0 ,1). Let 0 <a<b< 1. Let
Y=braceleftbigg10<x<b
0 otherwise
and let
Z=braceleftbigg1a<x< 1
0 otherwise
(a) Are YandZindependent? Why/Why not?
(b) Find E(Y|Z). Hint: What values zcanZtake? Now ﬁnd E(Y|Z=z).
23. Find the moment generating function for the Poisson, Normal, and
Gamma distributions.
24. Let X
1,...,X n∼Exp(β). Find the moment generating function of Xi.
Prove thatsummationtextn
i=1Xi∼Gamma( n, β).

<<<PAGE 78>>>



<<<PAGE 79>>>

4
Inequalities
4.1 Probability Inequalities
Inequalities are useful for bounding quantities that might otherwise be hard
to compute. They will also be used in the theory of convergence which isdiscussed in the next chapter. Our ﬁrst inequality is Markov’s inequality.
4.1 Theorem (Markov’s inequality) .LetXbe a non-negative random
variable and suppose that E(X)exists. For any t>0,
P(X>t )≤E(X)
t. (4.1)
Proof. Since X>0,
E(X)=integraldisplay∞
0xf(x)dx=integraldisplayt
0xf(x)dx+integraldisplay∞
txf(x)dx
≥integraldisplay∞
txf(x)dx≥tintegraldisplay∞
tf(x)dx=tP(X>t )/squaresolid

<<<PAGE 80>>>

64 4. Inequalities
4.2 Theorem (Chebyshev’s inequality) .Letµ=E(X)andσ2=V(X).
Then,
P(|X−µ|≥t)≤σ2
t2and P(|Z|≥k)≤1
k2(4.2)
where Z=(X−µ)/σ. In particular, P(|Z|>2)≤1/4and
P(|Z|>3)≤1/9.
Proof. We use Markov’s inequality to conclude that
P(|X−µ|≥t)=P(|X−µ|2≥t2)≤E(X−µ)2
t2=σ2
t2.
The second part follows by setting t=kσ./squaresolid
4.3 Example. Suppose we test a prediction method, a neural net for example,
on a set of nnew test cases. Let Xi= 1 if the predictor is wrong and Xi=0
if the predictor is right. Then Xn=n−1summationtextn
i=1Xiis the observed error rate.
Each Ximay be regarded as a Bernoulli with unknown mean p. We would
like to know the true — but unknown — error rate p. Intuitively, we expect
thatXnshould be close to p. How likely is Xnto not be within /epsilon1ofp?W e
have that V(Xn)=V(X1)/n=p(1−p)/nand
P(|Xn−p|>/epsilon1)≤V(Xn)
/epsilon12=p(1−p)
n/epsilon12≤1
4n/epsilon12
sincep(1−p)≤1
4for all p.F o r/epsilon1=.2 and n= 100 the bound is .0625. /squaresolid
Hoeﬀding’s inequality is similar in spirit to Markov’s inequality but it is a
sharper inequality. We present the result here in two parts.
4.4 Theorem (Hoeﬀding’s Inequality) .LetY1,...,Y nbe independent
observations such thatE(Y
i)=0andai≤Yi≤bi.L e t/epsilon1>0. Then, for any t>0,
PparenleftBiggnsummationdisplay
i=1Yi≥/epsilon1parenrightBigg
≤e−t/epsilon1nproductdisplay
i=1et2(bi−ai)2/8. (4.3)

<<<PAGE 81>>>

4.1 Probability Inequalities 65
4.5 Theorem. LetX1,...,X n∼Bernoulli( p). Then, for any /epsilon1>0,
Pparenleftbig
|Xn−p|>/epsilon1parenrightbig
≤2e−2n/epsilon12(4.4)
where Xn=n−1summationtextn
i=1Xi.
4.6 Example. LetX1,...,X n∼Bernoulli( p). Let n= 100 and /epsilon1=.2. We
saw that Chebyshev’s inequality yielded
P(|Xn−p|>/epsilon1)≤.0625.
According to Hoeﬀding’s inequality,
P(|Xn−p|>.2)≤2e−2(100)( .2)2=.00067
which is much smaller than .0625. /squaresolid
Hoeﬀding’s inequality gives us a simple way to create a conﬁdence inter-
valfor a binomial parameter p. We will discuss conﬁdence intervals in detail
later (see Chapter 6) but here is the basic idea. Fix α>0 and let
/epsilon1n=radicalBigg
1
2nlogparenleftbigg2
αparenrightbigg
.
By Hoeﬀding’s inequality,
Pparenleftbig
|Xn−p|>/epsilon1nparenrightbig
≤2e−2n/epsilon12
n=α.
LetC=(Xn−/epsilon1n,Xn+/epsilon1n). Then, P(p/∈C)=P(|Xn−p|>/epsilon1n)≤α. Hence,
P(p∈C)≥1−α, that is, the random interval Ctraps the true parameter
value pwith probability 1 −α; we call Ca1−αconﬁdence interval. More on
this later.
The following inequality is useful for bounding probability statements about
Normal random variables.
4.7 Theorem (Mill’s Inequality) .LetZ∼N(0,1). Then,
P(|Z|>t)≤radicalbigg
2
πe−t2/2
t.

<<<PAGE 82>>>

66 4. Inequalities
4.2 Inequalities For Expectations
This section contains two inequalities on expected values.
4.8 Theorem (Cauchy-Schwartz inequality) .IfXandYhave ﬁnite
variances then
E|XY|≤radicalbig
E(X2)E(Y2). (4.5)
Recall that a function gisconvex if for each x, yand each α∈[0,1],
g(αx+( 1−α)y)≤αg(x)+( 1 −α)g(y).
Ifgis twice diﬀerentiable and g/prime/prime(x)≥0 for all x, then gis convex. It can
be shown that if gis convex, then glies above any line that touches gat
some point, called a tangent line. A function gisconcave if−gis convex.
Examples of convex functions are g(x)=x2andg(x)=ex. Examples of
concave functions are g(x)=−x2andg(x) = log x.
4.9 Theorem (Jensen’s inequality) .Ifgis convex, then
Eg(X)≥g(EX). (4.6)
Ifgis concave, then
Eg(X)≤g(EX). (4.7)
Proof. LetL(x)=a+bxbe a line, tangent to g(x) at the point E(X).
Since gis convex, it lies above the line L(x). So,
Eg(X)≥EL(X)=E(a+bX)=a+bE(X)=L(E(X)) =g(EX)./squaresolid
From Jensen’s inequality we see that E(X2)≥(EX)2and if Xis positive,
then E(1/X)≥1/E(X). Since log is concave, E(logX)≤logE(X).
4.3 Bibliographic Remarks
Devroye et al. (1996) is a good reference on probability inequalities and their
use in statistics and pattern recognition. The following proof of Hoeﬀding’sinequality is from that text.

<<<PAGE 83>>>

4.4 Appendix 67
4.4 Appendix
Proof of Hoeffding’s Inequality. We will make use of the exact form of
Taylor’s theorem: if gis a smooth function, then there is a number ξ∈(0,u)
such that g(u)=g(0) + ug/prime(0) +u2
2g/prime/prime(ξ).
Proof of Theorem 4.4. For any t>0, we have, from Markov’s inequality,
that
PparenleftBiggnsummationdisplay
i=1Yi≥/epsilon1parenrightBigg
=PparenleftBigg
tnsummationdisplay
i=1Yi≥t/epsilon1parenrightBigg
=PparenleftBig
et/summationtextn
i=1Yi≥et/epsilon1parenrightBig
≤e−t/epsilon1EparenleftBig
et/summationtextn
i=1YiparenrightBig
=e−t/epsilon1productdisplay
iE(etYi). (4.8)
Since ai≤Yi≤bi, we can write Yias a convex combination of aiandbi,
namely, Yi=αbi+( 1−α)aiwhere α=(Yi−ai)/(bi−ai). So, by the
convexity of etywe have
etYi≤Yi−ai
bi−aietbi+bi−Yi
bi−aietai.
Take expectations of both sides and use the fact that E(Yi) = 0 to get
EetYi≤−ai
bi−aietbi+bi
bi−aietai=eg(u)(4.9)
where u=t(bi−ai),g(u)=−γu+ log(1 −γ+γeu) and γ=−ai/(bi−ai).
Note that g(0) = g/prime(0) = 0. Also, g/prime/prime(u)≤1/4 for all u>0. By Taylor’s
theorem, there is a ξ∈(0,u) such that
g(u)= g(0) + ug/prime(0) +u2
2g/prime/prime(ξ)
=u2
2g/prime/prime(ξ)≤u2
8=t2(bi−ai)2
8.
Hence,
EetYi≤eg(u)≤et2(bi−ai)2/8.
The result follows from (4.8). /squaresolid
Proof of Theorem 4.5. Let Yi=( 1/n)(Xi−p). Then E(Yi) = 0 and
a≤Yi≤bwhere a=−p/nandb=( 1−p)/n. Also, ( b−a)2=1/n2.
Applying Theorem 4.4 we get
P(Xn−p>/epsilon1)=PparenleftBiggsummationdisplay
iYi>/epsilon1parenrightBigg
≤e−t/epsilon1et2/(8n).
The above holds for any t>0. In particular, take t=4n/epsilon1and we get P(Xn−
p>/epsilon1)≤e−2n/epsilon12. By a similar argument we can show that P(Xn−p<−/epsilon1)≤
e−2n/epsilon12. Putting these together we get Pparenleftbig
|Xn−p|>/epsilon1parenrightbig
≤2e−2n/epsilon12./squaresolid

<<<PAGE 84>>>

68 4. Inequalities
4.5 Exercises
1. Let X∼Exponential( β). Find P(|X−µX|≥kσX) fork>1. Compare
this to the bound you get from Chebyshev’s inequality.
2. Let X∼Poisson( λ). Use Chebyshev’s inequality to show that P(X≥
2λ)≤1/λ.
3. Let X1,...,X n∼Bernoulli( p) and Xn=n−1summationtextn
i=1Xi. Bound P(|Xn−
p|>/epsilon1) using Chebyshev’s inequality and using Hoeﬀding’s inequality.
Show that, when nis large, the bound from Hoeﬀding’s inequality is
smaller than the bound from Chebyshev’s inequality.
4. Let X1,...,X n∼Bernoulli( p).
(a) Let α>0 be ﬁxed and deﬁne
/epsilon1n=radicalBigg
1
2nlogparenleftbigg2
αparenrightbigg
.
Lethatwidepn=n−1summationtextn
i=1Xi. Deﬁne Cn=(hatwidepn−/epsilon1n,hatwidepn+/epsilon1n). Use Hoeﬀding’s
inequality to show that
P(Cncontains p)≥1−α.
In practice, we truncate the interval so it does not go below 0 or above
1.
(b) (Computer Experiment. ) Let’s examine the properties of this conﬁ-
dence interval. Let α=0.05 and p=0.4. Conduct a simulation study
to see how often the interval contains p(called the coverage). Do this
for various values of nbetween 1 and 10000. Plot the coverage versus n.
(c) Plot the length of the interval versus n. Suppose we want the length
of the interval to be no more than .05. How large should nbe?
5. Prove Mill’s inequality, Theorem 4.7. Hint. Note that P(|Z|>t)=
2P(Z>t). Now write out what P(Z>t) means and note that x/t > 1
whenever x>t.
6. Let Z∼N(0,1). Find P(|Z|>t) and plot this as a function of t. From
Markov’s inequality, we have the bound P(|Z|>t)≤E|Z|k
tkfor any
k>0. Plot these bounds for k=1,2,3,4,5 and compare them to the
true value of P(|Z|>t). Also, plot the bound from Mill’s inequality.

<<<PAGE 85>>>

4.5 Exercises 69
7. Let X1,...,X n∼N(0,1). Bound P(|Xn|>t) using Mill’s inequality,
where Xn=n−1summationtextn
i=1Xi. Compare to the Chebyshev bound.

<<<PAGE 86>>>



<<<PAGE 87>>>

5
Convergence of Random Variables
5.1 Introduction
The most important aspect of probability theory concerns the behavior of
sequences of random variables. This part of probability is called large sample
theory ,o rlimit theory ,o rasymptotic theory . The basic question is this:
what can we say about the limiting behavior of a sequence of random variablesX
1,X2,X3,...? Since statistics and data mining are all about gathering data,
we will naturally be interested in what happens as we gather more and moredata.
In calculus we say that a sequence of real numbers x
nconverges to a limit
xif, for every /epsilon1>0,|xn−x|</epsilon1for all large n. In probability, convergence is
more subtle. Going back to calculus for a moment, suppose that xn=xfor
alln. Then, trivially, lim n→∞xn=x. Consider a probabilistic version of this
example. Suppose that X1,X2,...is a sequence of random variables which
are independent and suppose each has a N(0,1) distribution. Since these all
have the same distribution, we are tempted to say that Xn“converges” to
X∼N(0,1). But this can’t quite be right since P(Xn=X) = 0 for all n.
(Two continuous random variables are equal with probability zero.)
Here is another example. Consider X1,X2,...where Xi∼N(0,1/n). Intu-
itively, Xnis very concentrated around 0 for large nso we would like to say
thatXnconverges to 0. But P(Xn= 0) = 0 for all n. Clearly, we need to

<<<PAGE 88>>>

72 5. Convergence of Random Variables
develop some tools for discussing convergence in a rigorous way. This chapter
develops the appropriate methods.
There are two main ideas in this chapter which we state informally here:
1. The law of large numbers says that the sample average Xn=n−1summationtextn
i=1Xi
converges in probability to the expectation µ=E(Xi). This means
thatXnis close to µwith high probability.
2. The central limit theorem says that√n(Xn−µ)converges in dis-
tribution to a Normal distribution. This means that the sample average
has approximately a Normal distribution for large n.
5.2 Types of Convergence
The two main types of convergence are deﬁned as follows.
5.1 Deﬁnition. LetX1,X2,...be a sequence of random variables and let
Xbe another random variable. Let Fndenote the cdfofXnand let F
denote the cdfofX.
1.Xnconverges to Xin probability , written XnP−→X, if, for every
/epsilon1>0,
P(|Xn−X|>/epsilon1)→0 (5.1)
asn→∞.
2.Xnconverges to Xin distribution , written Xn/squigglerightX,i f
lim
n→∞Fn(t)=F(t) (5.2)
at all tfor which Fis continuous.
When the limiting random variable is a point mass, we change the notation
slightly. If P(X=c)=1a n d XnP−→Xthen we write XnP−→c. Similarly, if
Xn/squigglerightXwe write Xn/squigglerightc.
There is another type of convergence which we introduce mainly because it
is useful for proving convergence in probability.

<<<PAGE 89>>>

5.2 Types of Convergence 73
tFn(t)
tF(t)
FIGURE 5.1. Example 5.3. Xnconverges in distribution to Xbecause Fn(t) con-
verges to F(t) at all points except t= 0. Convergence is not required at t=0
because t= 0 is not a point of continuity for F.
5.2 Deﬁnition. Xnconverges to Xin quadratic mean (also called
convergence in L2), written Xnqm−→X,i f
E(Xn−X)2→0 (5.3)
asn→∞.
Again, if Xis a point mass at cwe write Xnqm−→cinstead of Xnqm−→X.
5.3 Example. LetXn∼N(0,1/n). Intuitively, Xnis concentrating at 0 so
we would like to say that Xnconverges to 0. Let’s see if this is true. Let Fbe
the distribution function for a point mass at 0. Note that√nXn∼N(0,1).
LetZdenote a standard normal random variable. For t<0,Fn(t)=P(Xn<
t)=P(√nXn<√nt)=P(Z<√nt)→0 since√nt→− ∞ .F o r t>0,
Fn(t)=P(Xn<t)=P(√nXn<√nt)=P(Z<√nt)→1 since√nt→∞.
Hence, Fn(t)→F(t) for all t/negationslash= 0 and so Xn/squiggleright0. Notice that Fn( 0 )=1 /2/negationslash=
F(1/2) = 1 so convergence fails at t= 0. That doesn’t matter because t=0
is not a continuity point of Fand the deﬁnition of convergence in distribution
only requires convergence at continuity points. See Figure 5.1. Now considerconvergence in probability. For any /epsilon1>0, using Markov’s inequality,
P(|X
n|>/epsilon1)= P(|Xn|2>/epsilon12)
≤E(X2
n)
/epsilon12=1
n
/epsilon12→0
asn→∞. Hence, XnP−→0./squaresolid
The next theorem gives the relationship between the types of convergence.
The results are summarized in Figure 5.2.
5.4 Theorem. The following relationships hold:
(a)Xnqm−→Ximplies that XnP−→X.
(b)XnP−→Ximplies that Xn/squigglerightX.
(c) If Xn/squigglerightXand if P(X=c)=1for some real number c, then XnP−→X.

<<<PAGE 90>>>

74 5. Convergence of Random Variables
In general, none of the reverse implications hold except the special case in
(c).
Proof. We start by proving (a). Suppose that Xnqm−→X. Fix/epsilon1>0. Then,
using Markov’s inequality,
P(|Xn−X|>/epsilon1)=P(|Xn−X|2>/epsilon12)≤E|Xn−X|2
/epsilon12→0.
Proof of (b). This proof is a little more complicated. You may skip it if you
wish. Fix /epsilon1>0 and let xbe a continuity point of F. Then
Fn(x)= P(Xn≤x)=P(Xn≤x, X≤x+/epsilon1)+P(Xn≤x, X > x +/epsilon1)
≤P(X≤x+/epsilon1)+P(|Xn−X|>/epsilon1)
=F(x+/epsilon1)+P(|Xn−X|>/epsilon1).
Also,F(x−/epsilon1)= P(X≤x−/epsilon1)=P(X≤x−/epsilon1, X
n≤x)+P(X≤x−/epsilon1, Xn>x)
≤Fn(x)+P(|Xn−X|>/epsilon1).
Hence,
F(x−/epsilon1)−P(|Xn−X|>/epsilon1)≤Fn(x)≤F(x+/epsilon1)+P(|Xn−X|>/epsilon1).
Take the limit as n→∞ to conclude that
F(x−/epsilon1)≤lim inf
n→∞Fn(x)≤lim sup
n→∞Fn(x)≤F(x+/epsilon1).
This holds for all /epsilon1>0. Take the limit as /epsilon1→0 and use the fact that Fis
continuous at xand conclude that lim nFn(x)=F(x).
Proof of (c). Fix /epsilon1>0. Then,
P(|Xn−c|>/epsilon1)= P(Xn<c−/epsilon1)+P(Xn>c+/epsilon1)
≤P(Xn≤c−/epsilon1)+P(Xn>c+/epsilon1)
=Fn(c−/epsilon1)+1−Fn(c+/epsilon1)
→F(c−/epsilon1)+1−F(c+/epsilon1)
= 0+1 −1=0.
Let us now show that the reverse implications do not hold.
Convergence in probability does not imply convergence in quadratic
mean. LetU∼Unif(0 ,1) and let Xn=√nI(0,1/n)(U). Then P(|Xn|>/epsilon1)=

<<<PAGE 91>>>

5.2 Types of Convergence 75
quadratic mean probability distributionpoint-mass distribution
FIGURE 5.2. Relationship between types of convergence.
P(√nI(0,1/n)(U)>/epsilon1)=P(0≤U<1/n)=1/n→0. Hence, XnP−→0. But
E(X2
n)=nintegraltext1/n
0du= 1 for all nsoXndoes not converge in quadratic mean.
Convergence in distribution does not imply convergence in prob-
ability. LetX∼N(0,1). Let Xn=−Xforn=1,2,3,...; hence Xn∼
N(0,1).Xnhas the same distribution function as Xfor all nso, trivially,
limnFn(x)=F(x) for all x. Therefore, Xn/squigglerightX. But P(|Xn−X|>/epsilon1)=
P(|2X|>/epsilon1)=P(|X|>/epsilon1 /2)/negationslash=0 .S o Xndoes not converge to Xin probability.
/squaresolid
Warning! One might conjecture that if XnP−→b, then E(Xn)→b. This is
not1true. Let Xnbe a random variable deﬁned by P(Xn=n2)=1/nand
P(Xn=0 )=1 −(1/n). Now, P(|Xn|</epsilon1)=P(Xn=0 )=1 −(1/n)→1.
Hence, XnP−→0. However, E(Xn)=[n2×(1/n)]+[0 ×(1−(1/n))] =n. Thus,
E(Xn)→∞.
Summary. Stare at Figure 5.2.
Some convergence properties are preserved under transformations.
5.5 Theorem. LetXn,X,Y n,Ybe random variables. Let gbe a continuous
function.
(a) If XnP−→XandYnP−→Y, then Xn+YnP−→X+Y.
(b) If Xnqm−→XandYnqm−→Y, then Xn+Ynqm−→X+Y.
(c) If Xn/squigglerightXandYn/squigglerightc, then Xn+Yn/squigglerightX+c.
(d) If XnP−→XandYnP−→Y, then XnYnP−→XY.
(e) If Xn/squigglerightXandYn/squigglerightc, then XnYn/squigglerightcX.
(f) If XnP−→X, then g(Xn)P−→g(X).
(g) If Xn/squigglerightX, then g(Xn)/squigglerightg(X).
Parts (c) and (e) are know as Slutzky’s theorem . It is worth noting that
Xn/squigglerightXandYn/squigglerightYdoes not in general imply that Xn+Yn/squigglerightX+Y.
1We can conclude that E(Xn)→bifXnis uniformly integrable. See the appendix.

<<<PAGE 92>>>

76 5. Convergence of Random Variables
5.3 The Law of Large Numbers
Now we come to a crowning achievement in probability, the law of large num-
bers. This theorem says that the mean of a large sample is close to the meanof the distribution. For example, the proportion of heads of a large numberof tosses is expected to be close to 1/2. We now make this more precise.
LetX
1,X2,...be an iidsample, let µ=E(X1) and2σ2=V(X1). Recall
that the sample mean is deﬁned as Xn=n−1summationtextn
i=1Xiand that E(Xn)=µ
andV(Xn)=σ2/n.
5.6 Theorem (The Weak Law of Large Numbers (WLLN)) .3
IfX1,...,X nareiid, then XnP−→µ.
Interpretation of the WLLN: The distribution of Xnbecomes more
concentrated around µasngets large.
Proof. Assume that σ<∞. This is not necessary but it simpliﬁes the
proof. Using Chebyshev’s inequality,
Pparenleftbig
|Xn−µ|>/epsilon1parenrightbig
≤V(Xn)
/epsilon12=σ2
n/epsilon12
which tends to 0 as n→∞./squaresolid
5.7 Example. Consider ﬂipping a coin for which the probability of heads is
p. Let Xidenote the outcome of a single toss (0 or 1). Hence, p=P(Xi=
1) =E(Xi). The fraction of heads after ntosses is Xn. According to the law
of large numbers, Xnconverges to pin probability. This does not mean that
Xnwill numerically equal p. It means that, when nis large, the distribution
ofXnis tightly concentrated around p. Suppose that p=1/2. How large
should nbe so that P(.4≤Xn≤.6)≥.7? First, E(Xn)=p=1/2 and
V(Xn)=σ2/n=p(1−p)/n=1/(4n). From Chebyshev’s inequality,
P(.4≤Xn≤.6) = P(|Xn−µ|≤.1)
=1 −P(|Xn−µ|>.1)
≥1−1
4n(.1)2=1−25
n.
The last expression will be larger than .7 if n= 84. /squaresolid
2Note that µ=E(Xi)is the same for all iso we can deﬁne µ=E(Xi)for any i.B y
convention, we often write µ=E(X1).
3There is a stronger theorem in the appendix called the strong law of large numbers.

<<<PAGE 93>>>

5.4 The Central Limit Theorem 77
5.4 The Central Limit Theorem
The law of large numbers says that the distribution of Xnpiles up near µ.
This isn’t enough to help us approximate probability statements about Xn.
For this we need the central limit theorem.
Suppose that X1,...,X nareiidwith mean µand variance σ2. The central
limit theorem (CLT) says that Xn=n−1summationtext
iXihas a distribution which is
approximately Normal with mean µand variance σ2/n. This is remarkable
since nothing is assumed about the distribution of Xi, except the existence of
the mean and variance.
5.8 Theorem (The Central Limit Theorem (CLT)) .LetX1,...,X nbeiid
with mean µand variance σ2.L e tXn=n−1summationtextn
i=1Xi. Then
Zn≡Xn−µradicalBig
V(Xn)=√n(Xn−µ)
σ/squigglerightZ
where Z∼N(0,1). In other words,
lim
n→∞P(Zn≤z)=Φ ( z)=integraldisplayz
−∞1√
2πe−x2/2dx.
Interpretation: Probability statements about Xncan be approximated
using a Normal distribution. It’s the probability statements that weare approximating, not the random variable itself.
In addition to Z
n/squigglerightN(0,1), there are several forms of notation to denote
the fact that the distribution of Znis converging to a Normal. They all mean
the same thing. Here they are:
Zn≈N(0,1)
Xn≈Nparenleftbigg
µ,σ2
nparenrightbigg
Xn−µ≈Nparenleftbigg
0,σ2
nparenrightbigg
√n(Xn−µ)≈Nparenleftbig
0,σ2parenrightbig
√n(Xn−µ)
σ≈N(0,1).
5.9 Example. Suppose that the number of errors per computer program has a
Poisson distribution with mean 5. We get 125 programs. Let X1,...,X 125be

<<<PAGE 94>>>

78 5. Convergence of Random Variables
the number of errors in the programs. We want to approximate P(Xn<5.5).
Letµ=E(X1)=λ= 5 and σ2=V(X1)=λ= 5. Then,
P(Xn<5.5) = Pparenleftbigg√n(Xn−µ)
σ<√n(5.5−µ)
σparenrightbigg
≈P(Z<2.5) =.9938./squaresolid
The central limit theorem tells us that Zn=√n(Xn−µ)/σis approximately
N(0,1). However, we rarely know σ. Later, we will see that we can estimate
σ2fromX1,...,X nby
S2
n=1
n−1nsummationdisplay
i=1(Xi−Xn)2.
This raises the following question: if we replace σwithSn, is the central limit
theorem still true? The answer is yes.
5.10 Theorem. Assume the same conditions as the CLT. Then,
√n(Xn−µ)
Sn/squigglerightN(0,1).
You might wonder, how accurate the normal approximation is. The answer
is given in the Berry-Ess` een theorem.
5.11 Theorem (The Berry-Ess` een Inequality) .Suppose that E|X1|3<∞. Then
sup
z|P(Zn≤z)−Φ(z)|≤33
4E|X1−µ|3
√nσ3. (5.4)
There is also a multivariate version of the central limit theorem.
5.12 Theorem (Multivariate central limit theorem) .LetX1,...,X nbeiidran-
dom vectors where
Xi=
X
1i
X2i
...
Xki

with mean
µ=
µ
1
µ2
...
µk
=
E(X
1i)
E(X2i)
...
E(Xki)


<<<PAGE 95>>>

5.5 The Delta Method 79
and variance matrix Σ.L e t
X=
X1
X2
...
Xk
.
whereXj=n−1summationtextn
i=1Xji. Then,
√n(X−µ)/squigglerightN(0,Σ).
5.5 The Delta Method
IfYnhas a limiting Normal distribution then the delta method allows us to
ﬁnd the limiting distribution of g(Yn) where gis any smooth function.
5.13 Theorem (The Delta Method) .Suppose that
√n(Yn−µ)
σ/squigglerightN(0,1)
and that gis a diﬀerentiable function such that g/prime(µ)/negationslash=0. Then
√n(g(Yn)−g(µ))
|g/prime(µ)|σ/squigglerightN(0,1).
In other words,
Yn≈NparenleftBigg
µ,σ2
nparenrightBigg
implies that g(Yn)≈NparenleftBigg
g(µ),(g/prime(µ))2σ2
nparenrightBigg
.
5.14 Example. LetX1,...,X nbeiidwith ﬁnite mean µand ﬁnite variance
σ2. By the central limit theorem,√n(Xn−µ)/σ/squigglerightN(0,1). Let Wn=eXn.
Thus, Wn=g(Xn) where g(s)=es. Since g/prime(s)=es, the delta method
implies that Wn≈N(eµ,e2µσ2/n)./squaresolid
There is also a multivariate version of the delta method.
5.15 Theorem (The Multivariate Delta Method) .Suppose that Yn=(Yn1,...,Y nk)
is a sequence of random vectors such that
√n(Yn−µ)/squigglerightN(0,Σ).

<<<PAGE 96>>>

80 5. Convergence of Random Variables
Letg:Rk→Rand let
∇g(y)=
∂g
∂y1...
∂g
∂yk
.
Let∇µdenote ∇g(y)evaluated at y=µand assume that the elements of ∇µ
are nonzero. Then
√n(g(Yn)−g(µ))/squigglerightNparenleftbig
0,∇T
µΣ∇µparenrightbig
.
5.16 Example. Let
parenleftbigg
X11
X21parenrightbigg
,parenleftbigg
X12
X22parenrightbigg
, ...,parenleftbigg
X1n
X2nparenrightbigg
beiidrandom vectors with mean µ=(µ1,µ2)Tand variance Σ. Let
X1=1
nnsummationdisplay
i=1X1i,X2=1
nnsummationdisplay
i=1X2i
and deﬁne Yn=X1X2. Thus, Yn=g(X1,X2) where g(s1,s2)=s1s2.B yt h e
central limit theorem,
√nparenleftbiggX1−µ1
X2−µ2parenrightbigg
/squigglerightN(0,Σ).
Now
∇g(s)=parenleftBigg
∂g
∂s1∂g
∂s2parenrightBigg
=parenleftbiggs2
s1parenrightbigg
and so
∇T
µΣ∇µ=(µ2µ1)parenleftbiggσ11σ12
σ12σ22parenrightbiggparenleftbiggµ2
µ1parenrightbigg
=µ2
2σ11+2µ1µ2σ12+µ2
1σ22.
Therefore,
√n(X1X2−µ1µ2)/squigglerightNparenleftbigg
0,µ2
2σ11+2µ1µ2σ12+µ2
1σ22parenrightbigg
./squaresolid
5.6 Bibliographic Remarks
Convergence plays a central role in modern probability theory. For more de-
tails, see Grimmett and Stirzaker (1982), Karr (1993), and Billingsley (1979).Advanced convergence theory is explained in great detail in van der Vaartand Wellner (1996) and and van der Vaart (1998).

<<<PAGE 97>>>

5.7 Appendix 81
5.7 Appendix
5.7.1 Almost Sure and L1Convergence
We say that Xnconverges almost surely to X, written Xnas−→X,i f
P({s:Xn(s)→X(s)})=1.
We say that Xnconverges in L1toX, written XnL1−→X,i f
E|Xn−X|→0
asn→∞.
5.17 Theorem. LetXnandXbe random variables. Then:
(a)Xnas−→Ximplies that XnP−→X.
(b)Xnqm−→Ximplies that XnL1−→X.
(c)XnL1−→Ximplies that XnP−→X.
The weak law of large numbers says that Xnconverges to E(X1) in proba-
bility. The strong law asserts that this is also true almost surely.
5.18 Theorem (The Strong Law of Large Numbers) .LetX1,X2,...beiid.I f
µ=E|X1|<∞thenXnas−→µ.
A sequence Xnisasymptotically uniformly integrable if
lim
M→∞lim sup
n→∞E(|Xn|I(|Xn|>M)) = 0 .
5.19 Theorem. IfXnP−→bandXnis asymptotically uniformly integrable,
thenE(Xn)→b.
5.7.2 Proof of the Central Limit Theorem
Recall that if Xis a random variable, its moment generating function ( mgf)
isψX(t)=EetX. Assume in what follows that the mgfis ﬁnite in a neigh-
borhood around t=0 .
5.20 Lemma. LetZ1,Z2,...be a sequence of random variables. Let ψnbe the
mgfofZn.L e t Zbe another random variable and denote its mgfbyψ.I f
ψn(t)→ψ(t)for all tin some open interval around 0, then Zn/squigglerightZ.

<<<PAGE 98>>>

82 5. Convergence of Random Variables
proof of the central limit theorem. LetYi=(Xi−µ)/σ. Then,
Zn=n−1/2summationtext
iYi. Let ψ(t)b et h e mgfofYi. The mgfofsummationtext
iYiis (ψ(t))n
and mgfofZnis [ψ(t/√n)]n≡ξn(t). Now ψ/prime(0) = E(Y1)=0 , ψ/prime/prime(0) =
E(Y2
1)=V(Y1)=1 .S o ,
ψ(t)= ψ(0) + tψ/prime(0) +t2
2!ψ/prime/prime(0) +t3
3!ψ/prime/prime/prime(0) + ···
= 1+0+t2
2+t3
3!ψ/prime/prime/prime(0) + ···
=1 +t2
2+t3
3!ψ/prime/prime/prime(0) + ···
Now,
ξn(t)=bracketleftbigg
ψparenleftbiggt√nparenrightbiggbracketrightbiggn
=bracketleftbigg
1+t2
2n+t3
3!n3/2ψ/prime/prime/prime(0) + ···bracketrightbiggn
=bracketleftBigg
1+t2
2+t3
3!n1/2ψ/prime/prime/prime(0) + ···
nbracketrightBiggn
→et2/2
which is the mgfof a N(0,1). The result follows from the previous Theorem.
In the last step we used the fact that if an→athen
parenleftBig
1+an
nparenrightBign
→ea./squaresolid
5.8 Exercises
1. Let X1,...,X nbeiidwith ﬁnite mean µ=E(X1) and ﬁnite variance
σ2=V(X1). Let Xnbe the sample mean and let S2
nbe the sample
variance.
(a) Show that E(S2
n)=σ2.
(b) Show that S2
nP−→σ2. Hint: Show that S2
n=cnn−1summationtextn
i=1X2
i−dnX2
n
where cn→1 and dn→1. Apply the law of large numbers to n−1summationtextn
i=1X2
i
and to Xn. Then use part (e) of Theorem 5.5.
2. Let X1,X2,...be a sequence of random variables. Show that Xnqm−→b
if and only if
lim
n→∞E(Xn)=band lim
n→∞V(Xn)=0.

<<<PAGE 99>>>

5.8 Exercises 83
3. Let X1,...,X nbeiidand let µ=E(X1). Suppose that the variance is
ﬁnite. Show that Xnqm−→µ.
4. Let X1,X2,...be a sequence of random variables such that
Pparenleftbigg
Xn=1
nparenrightbigg
=1−1
n2and P(Xn=n)=1
n2.
DoesXnconverge in probability? Does Xnconverge in quadratic mean?
5. Let X1,...,X n∼Bernoulli( p). Prove that
1
nnsummationdisplay
i=1X2
iP−→pand1
nnsummationdisplay
i=1X2
iqm−→p.
6. Suppose that the height of men has mean 68 inches and standard de-
viation 2.6 inches. We draw 100 men at random. Find (approximately)the probability that the average height of men in our sample will be atleast 68 inches.
7. Let λ
n=1/nforn=1,2,.... LetXn∼Poisson( λn).
(a) Show that XnP−→0.
(b) Let Yn=nXn. Show that YnP−→0.
8. Suppose we have a computer program consisting of n= 100 pages of
code. Let Xibe the number of errors on the ithpage of code. Suppose
that the X/prime
isare Poisson with mean 1 and that they are independent.
LetY=summationtextn
i=1Xibe the total number of errors. Use the central limit
theorem to approximate P(Y<90).
9. Suppose that P(X=1 )= P(X=−1 )=1 /2.Deﬁne
Xn=braceleftbiggXwith probability 1 −1
n
enwith probability1
n.
DoesXnconverge to Xin probability? Does Xnconverge to Xin dis-
tribution? Does E(X−Xn)2converge to 0?
10. Let Z∼N(0,1). Let t>0. Show that, for any k>0,
P(|Z|>t)≤E|Z|k
tk.
Compare this to Mill’s inequality in Chapter 4.

<<<PAGE 100>>>

84 5. Convergence of Random Variables
11. Suppose that Xn∼N(0,1/n) and let Xbe a random variable with
distribution F(x)=0i f x<0 and F(x)=1i f x≥0. Does Xnconverge
toXin probability? (Prove or disprove). Does Xnconverge to Xin
distribution? (Prove or disprove).
12. Let X,X 1,X2,X3,...be random variables that are positive and integer
valued. Show that Xn/squigglerightXif and only if
lim
n→∞P(Xn=k)=P(X=k)
for every integer k.
13. Let Z1,Z2,...beiidrandom variables with density f. Suppose that
P(Zi>0) = 1 and that λ= lim x↓0f(x)>0. Let
Xn=nmin{Z1,...,Z n}.
Show that Xn/squigglerightZwhere Zhas an exponential distribution with mean
1/λ.
14. Let X1,...,X n∼Uniform(0 ,1). Let Yn=X2
n. Find the limiting distri-
bution of Yn.
15. Let parenleftbiggX11
X21parenrightbigg
,parenleftbiggX12
X22parenrightbigg
, ...,parenleftbiggX1n
X2nparenrightbigg
beiidrandom vectors with mean µ=(µ1,µ2) and variance Σ. Let
X1=1
nnsummationdisplay
i=1X1i,X2=1
nnsummationdisplay
i=1X2i
and deﬁne Yn=X1/X2. Find the limiting distribution of Yn.
16. Construct an example where Xn/squigglerightXandYn/squigglerightYbutXn+Yndoes
not converge in distribution to X+Y.

<<<PAGE 101>>>

Part II
Statistical Inference

<<<PAGE 102>>>



<<<PAGE 103>>>

6
Models, Statistical Inference and
Learning
6.1 Introduction
Statistical inference, or “learning” as it is called in computer science, is the
process of using data to infer the distribution that generated the data. Atypical statistical inference question is:
Given a sample X
1,...,X n∼F, how do we infer F?
In some cases, we may want to infer only some feature of Fsuch as its
mean.
6.2 Parametric and Nonparametric Models
Astatistical model Fis a set of distributions (or densities or regression
functions). A parametric model is a set Fthat can be parameterized by a
ﬁnite number of parameters. For example, if we assume that the data comefrom a Normal distribution, then the model is
F=braceleftbigg
f(x;µ, σ)=1
σ√
2πexpbraceleftbigg
−1
2σ2(x−µ)2bracerightbigg
,µ∈R,σ > 0bracerightbigg
.(6.1)
This is a two-parameter model. We have written the density as f(x;µ, σ)t o
show that xis a value of the random variable whereas µandσare parameters.

<<<PAGE 104>>>

88 6. Models, Statistical Inference and Learning
In general, a parametric model takes the form
F=braceleftBigg
f(x;θ):θ∈ΘbracerightBigg
(6.2)
where θis an unknown parameter (or vector of parameters) that can take
values in the parameter space Θ. Ifθis a vector but we are only interested in
one component of θ, we call the remaining parameters nuisance parameters .
Anonparametric model is a set Fthat cannot be parameterized by a ﬁnite
number of parameters. For example, FALL={allcdf/primes}is nonparametric.1
6.1 Example (One-dimensional Parametric Estimation) .LetX1,...,Xnbe in-
dependent Bernoulli( p) observations. The problem is to estimate the param-
eterp./squaresolid
6.2 Example (Two-dimensional Parametric Estimation) .Suppose that X1,...,
Xn∼Fand we assume that the pdff∈Fwhere Fis given in (6.1). In
this case there are two parameters, µandσ. The goal is to estimate the
parameters from the data. If we are only interested in estimating µ, then µis
the parameter of interest and σis a nuisance parameter. /squaresolid
6.3 Example (Nonparametric estimation of the cdf).LetX1,...,Xnbe inde-
pendent observations from a cdfF. The problem is to estimate Fassuming
only that F∈FALL={allcdf/primes}./squaresolid
6.4 Example (Nonparametric density estimation) .LetX1,...,X nbe indepen-
dent observations from a cdfFand let f=F/primebe the pdf. Suppose we want
to estimate the pdff. It is not possible to estimate fassuming only that
F∈FALL. We need to assume some smoothness on f. For example, we might
assume that f∈F=FDENSintersectiontextFSOBwhere FDENSis the set of all probability
density functions and
FSOB=braceleftbigg
f:integraldisplay
(f/prime/prime(x))2dx <∞bracerightbigg
.
The class FSOBis called a Sobolev space ; it is the set of functions that are
not “too wiggly.” /squaresolid
6.5 Example (Nonparametric estimation of functionals) .LetX1,...,Xn∼F.
Suppose we want to estimate µ=E(X1)=integraltext
xd F(x) assuming only that
1The distinction between parametric and nonparametric is more subtle than this but we don’t
need a rigorous deﬁnition for our purposes.

<<<PAGE 105>>>

6.2 Parametric and Nonparametric Models 89
µexists. The mean µmay be thought of as a function of F: we can write
µ=T(F)=integraltext
xd F(x). In general, any function of Fis called a statis-
tical functional . Other examples of functionals are the variance T(F)=integraltext
x2dF(x)−parenleftbigintegraltext
xdF(x)parenrightbig2and the median T(F)=F−1(1/2)./squaresolid
6.6 Example (Regression, prediction, and classiﬁcation) .Suppose we observe pairs
of data ( X1,Y1),...(Xn,Yn). Perhaps Xiis the blood pressure of subject i
andYiis how long they live. Xis called a predictor orregressor orfea-
tureorindependent variable .Yis called the outcome or the response
variable or the dependent variable . We call r(x)=E(Y|X=x) there-
gression function . If we assume that r∈Fwhere Fis ﬁnite dimensional —
the set of straight lines for example — then we have a parametric regres-
sion model . If we assume that r∈Fwhere Fis not ﬁnite dimensional then
we have a nonparametric regression model . The goal of predicting Yfor
a new patient based on their Xvalue is called prediction .I fYis discrete
(for example, live or die) then prediction is instead called classiﬁcation. If
our goal is to estimate the function r, then we call this regression orcurve
estimation . Regression models are sometimes written as
Y=r(X)+/epsilon1 (6.3)
where E(/epsilon1) = 0. We can always rewrite a regression model this way. To see
this, deﬁne /epsilon1=Y−r(X) and hence Y=Y+r(X)−r(X)=r(X)+/epsilon1.
Moreover, E(/epsilon1)=EE(/epsilon1|X)=E(E(Y−r(X))|X)=E(E(Y|X)−r(X)) =
E(r(X)−r(X) )=0 . /squaresolid
What’s Next? It is traditional in most introductory courses to start with
parametric inference. Instead, we will start with nonparametric inference andthen we will cover parametric inference. In some respects, nonparametric in-ference is easier to understand and is more useful than parametric inference.
Frequentists and Bayesians. There are many approaches to statistical
inference. The two dominant approaches are called frequentist inference
andBayesian inference . We’ll cover both but we will start with frequentist
inference. We’ll postpone a discussion of the pros and cons of these two untillater.
Some Notation. IfF={f(x;θ):θ∈Θ}is a parametric model, we write
P
θ(X∈A)=integraltext
Af(x;θ)dxandEθ(r(X)) =integraltext
r(x)f(x;θ)dx. The subscript θ
indicates that the probability or expectation is with respect to f(x;θ); it does
not mean we are averaging over θ. Similarly, we write Vθfor the variance.

<<<PAGE 106>>>

90 6. Models, Statistical Inference and Learning
6.3 Fundamental Concepts in Inference
Many inferential problems can be identiﬁed as being one of three types: es-
timation, conﬁdence sets, or hypothesis testing. We will treat all of theseproblems in detail in the rest of the book. Here, we give a brief introductionto the ideas.
6.3.1 Point Estimation
Point estimation refers to providing a single “best guess” of some quantity
of interest. The quantity of interest could be a parameter in a parametricmodel, a cdfF, a probability density function f, a regression function r,o r
a prediction for a future value Yof some random variable.
By convention, we denote a point estimate of θbyhatwideθorhatwideθ
n. Remember
thatθis a ﬁxed, unknown quantity. The estimate hatwideθdepends on the
data sohatwideθis a random variable.
More formally, let X1,...,X nbeniiddata points from some distribution
F. A point estimator hatwideθnof a parameter θis some function of X1,...,X n:
hatwideθn=g(X1,...,X n).
The bias of an estimator is deﬁned by
bias(hatwideθn)=Eθ(hatwideθn)−θ. (6.4)
We say that hatwideθnisunbiased ifE(hatwideθn)=θ. Unbiasedness used to receive much
attention but these days is considered less important; many of the estimatorswe will use are biased. A reasonable requirement for an estimator is that itshould converge to the true parameter value as we collect more and moredata. This requirement is quantiﬁed by the following deﬁnition:
6.7 Deﬁnition. A point estimator hatwideθnof a parameter θisconsistent if
hatwideθnP−→θ.
The distribution of hatwideθnis called the sampling distribution . The standard
deviation of hatwideθnis called the standard error , denoted by se:
se=se(hatwideθn)=radicalBig
V(hatwideθn). (6.5)
Often, the standard error depends on the unknown F. In those cases, seis
an unknown quantity but we usually can estimate it. The estimated standarderror is denoted by hatwidese.

<<<PAGE 107>>>

6.3 Fundamental Concepts in Inference 91
6.8 Example. LetX1,...,X n∼Bernoulli( p) and let hatwidepn=n−1summationtext
iXi. Then
E(hatwidepn)=n−1summationtext
iE(Xi)=psohatwidepnis unbiased. The standard error is se=radicalbig
V(hatwidepn)=radicalbig
p(1−p)/n. The estimated standard error is hatwidese=radicalbig
hatwidep(1−hatwidep)/n.
/squaresolid
The quality of a point estimate is sometimes assessed by the mean squared
error ,o rmsedeﬁned by
mse=Eθ(hatwideθn−θ)2. (6.6)
Keep in mind that Eθ(·) refers to expectation with respect to the distribution
f(x1,...,x n;θ)=nproductdisplay
i=1f(xi;θ)
that generated the data. It does not mean we are averaging over a distribution
forθ.
6.9 Theorem. The msecan be written as
mse=bias2(hatwideθn)+Vθ(hatwideθn). (6.7)
Proof. Letθn=Eθ(hatwideθn). Then
Eθ(hatwideθn−θ)2=Eθ(hatwideθn−θn+θn−θ)2
=Eθ(hatwideθn−θn)2+2 (θn−θ)Eθ(hatwideθn−θn)+Eθ(θn−θ)2
=(θn−θ)2+Eθ(hatwideθn−θn)2
=bias2(hatwideθn)+V(hatwideθn)
where we have used the fact that Eθ(hatwideθn−θn)=θn−θn=0 . /squaresolid
6.10 Theorem. Ifbias→0andse→0asn→∞thenhatwideθnis consistent, that
is,hatwideθnP−→θ.
Proof. Ifbias→0 and se→0 then, by Theorem 6.9, MSE →0. It
follows that hatwideθnqm−→θ. (Recall Deﬁnition 5.2.) The result follows from part (b)
of Theorem 5.4. /squaresolid
6.11 Example. Returning to the coin ﬂipping example, we have that Ep(hatwidepn)=
pso the bias = p−p= 0 and se=radicalbig
p(1−p)/n→0. Hence, hatwidepnP−→p, that is,
hatwidepnis a consistent estimator. /squaresolid
Many of the estimators we will encounter will turn out to have, approxi-
mately, a Normal distribution.

<<<PAGE 108>>>

92 6. Models, Statistical Inference and Learning
6.12 Deﬁnition. An estimator is asymptotically Normal if
hatwideθn−θ
se/squigglerightN(0,1). (6.8)
6.3.2 Conﬁdence Sets
A1−αconﬁdence interval for a parameter θis an interval Cn=(a, b)
where a=a(X1,...,X n) and b=b(X1,...,X n) are functions of the data
such that
Pθ(θ∈Cn)≥1−α,for all θ∈Θ. (6.9)
In words, ( a, b) traps θwith probability 1 −α. We call 1 −αthecoverage of
the conﬁdence interval.
Warning! Cnis random and θis ﬁxed.
Commonly, people use 95 percent conﬁdence intervals, which corresponds
to choosing α=0.05. If θis a vector then we use a conﬁdence set (such as
a sphere or an ellipse) instead of an interval.
Warning! There is much confusion about how to interpret a conﬁdence
interval. A conﬁdence interval is not a probability statement about θsince
θis a ﬁxed quantity, not a random variable. Some texts interpret conﬁdence
intervals as follows: if I repeat the experiment over and over, the interval willcontain the parameter 95 percent of the time. This is correct but useless sincewe rarely repeat the same experiment over and over. A better interpretationis this:
On day 1, you collect data and construct a 95 percent conﬁdence
interval for a parameter θ
1. On day 2, you collect new data and con-
struct a 95 percent conﬁdence interval for an unrelated parameter θ2.
On day 3, you collect new data and construct a 95 percent conﬁ-dence interval for an unrelated parameter θ
3. You continue this way
constructing conﬁdence intervals for a sequence of unrelated param-etersθ
1,θ2,...Then 95 percent of your intervals will trap the true
parameter value. There is no need to introduce the idea of repeatingthe same experiment over and over.
6.13 Example. Every day, newspapers report opinion polls. For example, they
might say that “83 percent of the population favor arming pilots with guns.”Usually, you will see a statement like “this poll is accurate to within 4 points

<<<PAGE 109>>>

6.3 Fundamental Concepts in Inference 93
95 percent of the time.” They are saying that 83 ±4 is a 95 percent conﬁdence
interval for the true but unknown proportion pof people who favor arming
pilots with guns. If you form a conﬁdence interval this way every day for therest of your life, 95 percent of your intervals will contain the true parameter.This is true even though you are estimating a diﬀerent quantity (a diﬀerentpoll question) every day.
/squaresolid
6.14 Example. The fact that a conﬁdence interval is not a probability state-
ment about θis confusing. Consider this example from Berger and Wolpert
(1984). Let θbe a ﬁxed, known real number and let X1,X2be independent
random variables such that P(Xi=1 )= P(Xi=−1 )=1 /2. Now deﬁne
Yi=θ+Xiand suppose that you only observe Y1andY2. Deﬁne the follow-
ing “conﬁdence interval” which actually only contains one point:
C=braceleftBigg
{Y1−1} ifY1=Y2
{(Y1+Y2)/2}ifY1/negationslash=Y2.
You can check that, no matter what θis, we have Pθ(θ∈C)=3/4 so this
is a 75 percent conﬁdence interval. Suppose we now do the experiment andwe get Y
1= 15 and Y2= 17. Then our 75 percent conﬁdence interval is {16}.
However, we are certain that θ= 16. If you wanted to make a probability
statement about θyou would probably say that P(θ∈C|Y1,Y2) = 1. There is
nothing wrong with saying that {16}is a 75 percent conﬁdence interval. But
is it not a probability statement about θ./squaresolid
In Chapter 11 we will discuss Bayesian methods in which we treat θas if it
were a random variable and we do make probability statements about θ.I n
particular, we will make statements like “the probability that θis inCn, given
the data, is 95 percent.” However, these Bayesian intervals refer to degree-of-belief probabilities. These Bayesian intervals will not, in general, trap theparameter 95 percent of the time.
6.15 Example. In the coin ﬂipping setting, let C
n=(hatwidepn−/epsilon1n,hatwidepn+/epsilon1n) where
/epsilon12
n= log(2 /α)/(2n). From Hoeﬀding’s inequality (4.4) it follows that
P(p∈Cn)≥1−α
for every p. Hence, Cni sa1 −αconﬁdence interval. /squaresolid
As mentioned earlier, point estimators often have a limiting Normal dis-
tribution, meaning that equation (6.8) holds, that is, hatwideθn≈N(θ,hatwidese2). In this
case we can construct (approximate) conﬁdence intervals as follows.

<<<PAGE 110>>>

94 6. Models, Statistical Inference and Learning
6.16 Theorem (Normal-based Conﬁdence Interval) .Suppose that hatwideθn≈N(θ,hatwidese2).
LetΦbe the cdfof a standard Normal and let zα/2=Φ−1(1−(α/2)), that
is,P(Z>z α/2)=α/2andP(−zα/2<Z<z α/2)=1−αwhere Z∼N(0,1).
Let
Cn=(hatwideθn−zα/2hatwidese,hatwideθn+zα/2hatwidese). (6.10)
Then
Pθ(θ∈Cn)→1−α. (6.11)
Proof. LetZn=(hatwideθn−θ)/hatwidese. By assumption Zn/squigglerightZwhere Z∼N(0,1).
Hence,
Pθ(θ∈Cn)= PθparenleftBig
hatwideθn−zα/2hatwidese<θ<hatwideθn+zα/2hatwideseparenrightBig
=PθparenleftBigg
−zα/2<hatwideθn−θ
hatwidese<zα/2parenrightBigg
→Pparenleftbig
−zα/2<Z<z α/2parenrightbig
=1 −α./squaresolid
For 95 percent conﬁdence intervals, α=0.05 and zα/2=1.96≈2 leading
to the approximate 95 percent conﬁdence interval hatwideθn±2hatwidese.
6.17 Example. LetX1,...,X n∼Bernoulli( p) and let hatwidepn=n−1summationtextn
i=1Xi.
Then V(hatwidepn)=n−2summationtextn
i=1V(Xi)=n−2summationtextn
i=1p(1−p)=n−2np(1−p)=p(1−
p)/n. Hence, se=radicalbig
p(1−p)/nandhatwidese=radicalbig
hatwidepn(1−hatwidepn)/n. By the Central
Limit Theorem, hatwidepn≈N(p,hatwidese2). Therefore, an approximate 1 −αconﬁdence
interval is
hatwidepn±zα/2hatwidese=hatwidepn±zα/2radicalbigg
hatwidepn(1−hatwidepn)
n.
Compare this with the conﬁdence interval in example 6.15. The Normal-based
interval is shorter but it only has approximately (large sample) correct cover-age.
/squaresolid
6.3.3 Hypothesis Testing
Inhypothesis testing , we start with some default theory — called a null
hypothesis — and we ask if the data provide suﬃcient evidence to reject the
theory. If not we retain the null hypothesis.2
2The term “retaining the null hypothesis” is due to Chris Genovese. Other terminology is
“accepting the null” or “failing to reject the null.”

<<<PAGE 111>>>

6.4 Bibliographic Remarks 95
6.18 Example (Testing if a Coin is Fair) .Let
X1,...,X n∼Bernoulli( p)
benindependent coin ﬂips. Suppose we want to test if the coin is fair. Let H0
denote the hypothesis that the coin is fair and let H1denote the hypothesis
that the coin is not fair. H0is called the null hypothesis andH1is called
thealternative hypothesis . We can write the hypotheses as
H0:p=1/2 versus H1:p/negationslash=1/2.
It seems reasonable to reject H0ifT=|hatwidepn−(1/2)|is large. When we discuss
hypothesis testing in detail, we will be more precise about how large Tshould
be to reject H0./squaresolid
6.4 Bibliographic Remarks
Statistical inference is covered in many texts. Elementary texts include DeG-
root and Schervish (2002) and Larsen and Marx (1986). At the intermediatelevel I recommend Casella and Berger (2002), Bickel and Doksum (2000), andRice (1995). At the advanced level, Cox and Hinkley (2000), Lehmann andCasella (1998), Lehmann (1986), and van der Vaart (1998).
6.5 Appendix
Our deﬁnition of conﬁdence interval requires that Pθ(θ∈Cn)≥1−α
for all θ∈Θ. Apointwise asymptotic conﬁdence interval requires that
lim inf n→∞Pθ(θ∈Cn)≥1−αfor all θ∈Θ. Auniform asymptotic con-
ﬁdence interval requires that lim inf n→∞infθ∈ΘPθ(θ∈Cn)≥1−α. The
approximate Normal-based interval is a pointwise asymptotic conﬁdence in-terval.
6.6 Exercises
1. Let X1,...,X n∼Poisson( λ) and let hatwideλ=n−1summationtextn
i=1Xi. Find the bias,
se, and mseof this estimator.
2. Let X1,...,X n∼Uniform(0 ,θ) and let hatwideθ= max {X1,...,X n}. Find the
bias,se, and mseof this estimator.

<<<PAGE 112>>>

96 6. Models, Statistical Inference and Learning
3. Let X1,...,X n∼Uniform(0 ,θ) and let hatwideθ=2Xn. Find the bias,se, and
mseof this estimator.

<<<PAGE 113>>>

7
Estimating the cdf and Statistical
Functionals
The ﬁrst inference problem we will consider is nonparametric estimation of the
cdfF. Then we will estimate statistical functionals, which are functions of
cdf, such as the mean, the variance, and the correlation. The nonparametric
method for estimating functionals is called the plug-in method.
7.1 The Empirical Distribution Function
LetX1,...,X n∼Fbe an iidsample where Fis a distribution function on
the real line. We will estimate Fwith the empirical distribution function,
which is deﬁned as follows.
7.1 Deﬁnition. Theempirical distribution function hatwideFnis the cdf
that puts mass 1/nat each data point Xi. Formally,
hatwideFn(x)=summationtextn
i=1I(Xi≤x)
n(7.1)
where
I(Xi≤x)=braceleftbigg1i f Xi≤x
0i f Xi>x .

<<<PAGE 114>>>

98 7. Estimating the cdfand Statistical Functionals
0.0 0.5 1.0 1.50.0 0.5 1.0
FIGURE 7.1. Nerve data. Each vertical line represents one data point. The solid
line is the empirical distribution function. The lines above and below the middleline are a 95 percent conﬁdence band.
7.2 Example (Nerve Data) .Cox and Lewis (1966) reported 799 waiting times
between successive pulses along a nerve ﬁber. Figure 7.1 shows the empiricalcdfhatwideF
n. The data points are shown as small vertical lines at the bottom of
the plot. Suppose we want to estimate the fraction of waiting times between.4 and .6 seconds. The estimate is hatwideF
n(.6)−hatwideFn(.4) =.93−.84 =.09./squaresolid
7.3 Theorem. At any ﬁxed value of x,
EparenleftBig
hatwideFn(x)parenrightBig
=F(x),
VparenleftBig
hatwideFn(x)parenrightBig
=F(x)(1−F(x))
n,
mse =F(x)(1−F(x))
n→0,
hatwideFn(x)P−→ F(x).
7.4 Theorem (The Glivenko-Cantelli Theorem) .LetX1,...,X n∼F. Then1
sup
x|hatwideFn(x)−F(x)|P−→0.
Now we give an inequality that will be used to construct a conﬁdence band.
1More precisely, supx|/hatwideFn(x)−F(x)|converges to 0 almost surely.

<<<PAGE 115>>>

7.2 Statistical Functionals 99
7.5 Theorem (The Dvoretzky-Kiefer-Wolfowitz (DKW) Inequality) .LetX1,...,
Xn∼F. Then, for any /epsilon1>0,
PparenleftBigg
sup
x|F(x)−hatwideFn(x)|>/epsilon1parenrightBigg
≤2e−2n/epsilon12. (7.2)
From the DKW inequality, we can construct a conﬁdence set as follows:
A Nonparametric 1−αConﬁdence Band for F
Deﬁne,
L(x) = max {hatwideFn(x)−/epsilon1n,0}
U(x) = min {hatwideFn(x)+/epsilon1n,1}
where /epsilon1n=radicalBigg
1
2nlogparenleftbigg2
αparenrightbigg
.
It follows from (7.2) that for any F,
Pparenleftbigg
L(x)≤F(x)≤U(x) for all xparenrightbigg
≥1−α. (7.3)
7.6 Example. The dashed lines in Figure 7.1 give a 95 percent conﬁdence
band using /epsilon1n=radicalBig
1
2nlogparenleftbig2
.05parenrightbig
=.048. /squaresolid
7.2 Statistical Functionals
Astatistical functional T(F) is any function of F. Examples are the mean
µ=integraltext
xd F(x), the variance σ2=integraltext
(x−µ)2dF(x) and the median m=
F−1(1/2).
7.7 Deﬁnition. Theplug-in estimator ofθ=T(F)is deﬁned by
hatwideθn=T(hatwideFn).
In other words, just plug in hatwideFnfor the unknown F.
7.8 Deﬁnition. IfT(F)=integraltext
r(x)dF(x)for some function r(x)thenTis
called a linear functional.

<<<PAGE 116>>>

100 7. Estimating the cdfand Statistical Functionals
The reason T(F)=integraltext
r(x)dF(x) is called a linear functional is because T
satisﬁes
T(aF+bG)=aT(F)+bT(G),
hence Tis linear in its arguments. Recall thatintegraltext
r(x)dF(x) is deﬁned to beintegraltext
r(x)f(x)dxin the continuous case andsummationtext
jr(xj)f(xj) in the discrete. The
empirical cdf hatwideFn(x) is discrete, putting mass 1 /nat each Xi. Hence, if T(F)=integraltext
r(x)dF(x) is a linear functional then we have:
7.9 Theorem. The plug-in estimator for linear functional
T(F)=integraltext
r(x)dF(x)is:
T(hatwideFn)=integraldisplay
r(x)dhatwideFn(x)=1
nnsummationdisplay
i=1r(Xi). (7.4)
Sometimes we can ﬁnd the estimated standard error seofT(hatwideFn) by doing
some calculations. However, in other cases it is not obvious how to estimatethe standard error. In the next chapter, we will discuss a general method forﬁndinghatwidese. For now, let us just assume that somehow we can ﬁnd hatwidese.
In many cases, it turns out that
T(hatwideF
n)≈N(T(F),hatwidese2). (7.5)
By equation (6.11), an approximate 1 −αconﬁdence interval for T(F) is then
T(hatwideFn)±zα/2hatwidese. (7.6)
We will call this the Normal-based interval. For a 95 percent conﬁdence
interval, zα/2=z.05/2=1.96≈2 so the interval is
T(hatwideFn)±2hatwidese.
7.10 Example (The mean) .Letµ=T(F)=integraltext
xd F(x). The plug-in estima-
tor ishatwideµ=integraltext
xdhatwideFn(x)=Xn. The standard error is se=radicalBig
V(Xn)=σ/√n.I f
hatwideσdenotes an estimate of σ, then the estimated standard error is hatwideσ/√n. (In
the next example, we shall see how to estimate σ.) A Normal-based conﬁdence
interval for µisXn±zα/2hatwidese./squaresolid
7.11 Example (The Variance) .Letσ2=T(F)=V(X)=integraltext
x2dF(x)−parenleftbigintegraltext
xdF(x)parenrightbig2.
The plug-in estimator is
hatwideσ2=integraldisplay
x2dhatwideFn(x)−parenleftbiggintegraldisplay
xdhatwideFn(x)parenrightbigg2

<<<PAGE 117>>>

7.2 Statistical Functionals 101
=1
nnsummationdisplay
i=1X2
i−parenleftBigg
1
nnsummationdisplay
i=1XiparenrightBigg2
=1
nnsummationdisplay
i=1(Xi−Xn)2.
Another reasonable estimator of σ2is the sample variance
S2
n=1
n−1nsummationdisplay
i=1(Xi−Xn)2.
In practice, there is little diﬀerence between hatwideσ2andS2
nand you can use either
one. Returning to the last example, we now see that the estimated standarderror of the estimate of the mean is hatwidese=hatwideσ/√
n./squaresolid
7.12 Example (The Skewness) .Letµandσ2denote the mean and variance
of a random variable X. The skewness is deﬁned to be
κ=E(X−µ)3
σ3=integraltext
(x−µ)3dF(x)
braceleftbigintegraltext
(x−µ)2dF(x)bracerightbig3/2.
The skewness measures the lack of symmetry of a distribution. To ﬁnd the
plug-in estimate, ﬁrst recall that hatwideµ=n−1summationtext
iXiandhatwideσ2=n−1summationtext
i(Xi−hatwideµ)2.
The plug-in estimate of κis
hatwideκ=integraltext
(x−µ)3dhatwideFn(x)
braceleftBigintegraltext
(x−µ)2dhatwideFn(x)bracerightBig3/2=1
nsummationtext
i(Xi−hatwideµ)3
hatwideσ3./squaresolid
7.13 Example (Correlation) .LetZ=(X,Y) and let ρ=T(F)=E(X−
µX)(Y−µY)/(σxσy) denote the correlation between XandY, where F(x, y)
is bivariate. We can write
T(F)=a(T1(F),T2(F),T3(F),T4(F),T5(F))
where
T1(F)=integraltext
xd F(z),T 2(F)=integraltext
yd F(z),T 3(F)=integraltext
xy dF (z),
T4(F)=integraltext
x2dF(z),T5(F)=integraltext
y2dF(z),
and
a(t1,...,t 5)=t3−t1t2radicalbig
(t4−t2
1)(t5−t2
2).
Replace FwithhatwideFninT1(F) , ..., T5(F), and take
hatwideρ=a(T1(hatwideFn),T2(hatwideFn),T3(hatwideFn),T4(hatwideFn),T5(hatwideFn)).

<<<PAGE 118>>>

102 7. Estimating the cdfand Statistical Functionals
We get
hatwideρ=summationtext
i(Xi−Xn)(Yi−Yn)radicalBigsummationtext
i(Xi−Xn)2radicalBigsummationtext
i(Yi−Yn)2
which is called the sample correlation ./squaresolid
7.14 Example (Quantiles) .LetFbe strictly increasing with density f.F o r
0<p< 1, the pthquantile is deﬁned by T(F)=F−1(p). The estimate if
T(F)i shatwideF−1
n(p). We have to be a bit careful since hatwideFnis not invertible. To
avoid ambiguity we deﬁne
hatwideF−1
n(p) = inf {x:hatwideFn(x)≥p}.
We call T(hatwideFn)=hatwideF−1
n(p) thepthsample quantile ./squaresolid
Only in the ﬁrst example did we compute a standard error or a conﬁdence
interval. How shall we handle the other examples? When we discuss parametricmethods, we will develop formulas for standard errors and conﬁdence intervals.But in our nonparametric setting we need something else. In the next chapter,we will introduce the bootstrap for getting standard errors and conﬁdenceintervals.
7.15 Example (Plasma Cholesterol) .Figure 7.2 shows histograms for plasma
cholesterol (in mg/dl) for 371 patients with chest pain (Scott et al. (1978)).The histograms show the percentage of patients in 10 bins. The ﬁrst histogramis for 51 patients who had no evidence of heart disease while the secondhistogram is for 320 patients who had narrowing of the arteries. Is the meancholesterol diﬀerent in the two groups? Let us regard these data as samplesfrom two distributions F
1andF2. Let µ1=integraltext
xdF1(x) and µ2=integraltext
xdF2(x)
denote the means of the two populations. The plug-in estimates are hatwideµ1=integraltext
xdhatwideFn,1(x)=Xn,1= 195 .27 andhatwideµ2=integraltext
xdhatwideFn,2(x)=Xn,2= 216 .19.Recall
that the standard error of the sample mean hatwideµ=1
nsummationtextn
i=1Xiis
se(hatwideµ)=radicaltpradicalvertexradicalvertexradicalbt
VparenleftBigg
1
nnsummationdisplay
i=1XiparenrightBigg
=radicaltpradicalvertexradicalvertexradicalbt1
n2nsummationdisplay
i=1V(Xi)=radicalbigg
nσ2
n2=σ√n
which we estimate by
hatwidese(hatwideµ)=hatwideσ√n
where
hatwideσ=radicaltpradicalvertexradicalvertexradicalbt1
nnsummationdisplay
i=1(Xi−X)2.

<<<PAGE 119>>>

7.2 Statistical Functionals 103
For the two groups this yields hatwidese(hatwideµ1)=5.0 andhatwidese(hatwideµ2)=2.4. Approximate 95
percent conﬁdence intervals for µ1andµ2arehatwideµ1±2hatwidese(hatwideµ1) = (185 ,205) and
hatwideµ2±2hatwidese(hatwideµ2) = (211 ,221).
Now, consider the functional θ=T(F2)−T(F1) whose plug-in estimate is
hatwideθ=hatwideµ2−hatwideµ1= 216 .19−195.2 7=2 0 .92.The standard error of hatwideθis
se=radicalbig
V(hatwideµ2−hatwideµ1)=radicalbig
V(hatwideµ2)+V(hatwideµ1)=radicalbig
(se(hatwideµ1))2+(se(hatwideµ2))2
and we estimate this by
hatwidese=radicalbig
(hatwidese(hatwideµ1))2+(hatwidese(hatwideµ2))2=5.55.
An approximate 95 percent conﬁdence interval for θishatwideθ±2hatwidese(hatwideθn)=( 9 .8,32.0).
This suggests that cholesterol is higher among those with narrowed arteries.We should not jump to the conclusion (from these data) that cholesterol causesheart disease. The leap from statistical evidence to causation is very subtleand is discussed in Chapter 16.
/squaresolid
plasma cholesterol for patients without heart disease100 150 200 250 300 350 400
plasma cholesterol for patients with heart disease100 150 200 250 300 350 400
FIGURE 7.2. Plasma cholesterol for 51 patients with no heart disease and 320
patients with narrowing of the arteries.

<<<PAGE 120>>>

104 7. Estimating the cdfand Statistical Functionals
7.3 Bibliographic Remarks
The Glivenko-Cantelli theorem is the tip of the iceberg. The theory of dis-
tribution functions is a special case of what are called empirical processeswhich underlie much of modern statistical theory. Some references on empiri-cal processes are Shorack and Wellner (1986) and van der Vaart and Wellner(1996).
7.4 Exercises
1. Prove Theorem 7.3.
2. Let X1,...,X n∼Bernoulli( p) and let Y1,...,Y m∼Bernoulli( q). Find
the plug-in estimator and estimated standard error for p. Find an ap-
proximate 90 percent conﬁdence interval for p. Find the plug-in esti-
mator and estimated standard error for p−q. Find an approximate 90
percent conﬁdence interval for p−q.
3. (Computer Experiment .) Generate 100 observations from a N(0,1) dis-
tribution. Compute a 95 percent conﬁdence band for the cdfF(as
described in the appendix). Repeat this 1000 times and see how oftenthe conﬁdence band contains the true distribution function. Repeat us-ing data from a Cauchy distribution.
4. Let X
1,...,X n∼Fand let hatwideFn(x) be the empirical distribution func-
tion. For a ﬁxed x, use the central limit theorem to ﬁnd the limiting
distribution of hatwideFn(x).
5. Let xandybe two distinct points. Find Cov( hatwideFn(x),hatwideFn(y)).
6. Let X1,...,X n∼Fand let hatwideFbe the empirical distribution function.
Leta<b be ﬁxed numbers and deﬁne θ=T(F)=F(b)−F(a). Let
hatwideθ=T(hatwideFn)=hatwideFn(b)−hatwideFn(a). Find the estimated standard error of hatwideθ.
Find an expression for an approximate 1 −αconﬁdence interval for θ.
7. Data on the magnitudes of earthquakes near Fiji are available on the
website for this book. Estimate the cdfF(x). Compute and plot a 95
percent conﬁdence envelope for F(as described in the appendix). Find
an approximate 95 percent conﬁdence interval for F(4.9)−F(4.3).

<<<PAGE 121>>>

7.4 Exercises 105
8. Get the data on eruption times and waiting times between eruptions of
the Old Faithful geyser from the website. Estimate the mean waitingtime and give a standard error for the estimate. Also, give a 90 percentconﬁdence interval for the mean waiting time. Now estimate the medianwaiting time. In the next chapter we will see how to get the standarderror for the median.
9. 100 people are given a standard antibiotic to treat an infection and
another 100 are given a new antibiotic. In the ﬁrst group, 90 peoplerecover; in the second group, 85 people recover. Let p
1be the probability
of recovery under the standard treatment and let p2be the probability of
recovery under the new treatment. We are interested in estimating θ=
p1−p2. Provide an estimate, standard error, an 80 percent conﬁdence
interval, and a 95 percent conﬁdence interval for θ.
10. In 1975, an experiment was conducted to see if cloud seeding produced
rainfall. 26 clouds were seeded with silver nitrate and 26 were not. Thedecision to seed or not was made at random. Get the data from
http://lib.stat.cmu.edu/DASL/Stories/CloudSeeding.htmlLetθbe the diﬀerence in the mean precipitation from the two groups.
Estimate θ. Estimate the standard error of the estimate and produce a
95 percent conﬁdence interval.

<<<PAGE 122>>>



<<<PAGE 123>>>

8
The Bootstrap
Thebootstrap is a method for estimating standard errors and computing
conﬁdence intervals. Let Tn=g(X1,...,X n)b eastatistic , that is, Tnis any
function of the data. Suppose we want to know VF(Tn), the variance of Tn.
We have written VFto emphasize that the variance usually depends on the
unknown distribution function F. For example, if Tn=Xnthen VF(Tn)=
σ2/nwhere σ2=integraltext
(x−µ)2dF(x) and µ=integraltext
xdF(x). Thus the variance of Tn
is a function of F. The bootstrap idea has two steps:
Step 1 : Estimate VF(Tn) with V/hatwideFn(Tn).
Step 2 : Approximate V/hatwideFn(Tn) using simulation.
ForTn=Xn, we have for Step 1 that V/hatwideFn(Tn)=hatwideσ2/nwherehatwideσ2=n−1summationtextn
i=1(Xi−
Xn). In this case, Step 1 is enough. However, in more complicated cases we
cannot write down a simple formula for V/hatwideFn(Tn) which is why we need Step
2. Before proceeding, let us discuss the idea of simulation.

<<<PAGE 124>>>

108 8. The Bootstrap
8.1 Simulation
Suppose we draw an iidsample Y1,...,Y Bfrom a distribution G. By the law
of large numbers,
Yn=1
BBsummationdisplay
j=1YjP−→integraldisplay
yd G(y)=E(Y)
asB→∞. So if we draw a large sample from G, we can use the sample
mean Ynto approximate E(Y). In a simulation, we can make Bas large as
we like, in which case, the diﬀerence between YnandE(Y) is negligible. More
generally, if his any function with ﬁnite mean then
1
BBsummationdisplay
j=1h(Yj)P−→integraldisplay
h(y)dG(y)=E(h(Y))
asB→∞. In particular,
1
BBsummationdisplay
j=1(Yj−Y)2=1
BBsummationdisplay
j=1Y2
j−parenleftBigg
1
BBsummationdisplay
j=1YjparenrightBigg2
P−→integraldisplay
y2dF(y)−parenleftBiggintegraldisplay
ydF(y)parenrightBigg2
=V(Y).
Hence, we can use the sample variance of the simulated values to approximate
V(Y).
8.2 Bootstrap Variance Estimation
According to what we just learned, we can approximate V/hatwideFn(Tn) by simula-
tion. Now V/hatwideFn(Tn) means “the variance of Tnif the distribution of the data
ishatwideFn.” How can we simulate from the distribution of Tnwhen the data are
assumed to have distribution hatwideFn? The answer is to simulate X∗
1,...,X∗
nfrom
hatwideFnand then compute T∗
n=g(X∗
1,...,X∗
n). This constitutes one draw from
the distribution of Tn. The idea is illustrated in the following diagram:
Real world F=⇒X1,...,X n=⇒Tn=g(X1,...,X n)
Bootstrap world hatwideFn=⇒X∗
1,...,X∗
n=⇒T∗
n=g(X∗
1,...,X∗
n)
How do we simulate X∗
1,...,X∗
nfromhatwideFn? Notice that hatwideFnputs mass 1/n at
each data point X1,...,X n. Therefore,

<<<PAGE 125>>>

8.2 Bootstrap Variance Estimation 109
drawing an observation from hatwideFnis equivalent to drawing
one point at random from the original data set.
Thus, to simulate X∗
1,...,X∗
n∼hatwideFnit suﬃces to draw nobservations with
replacement from X1,...,X n. Here is a summary:
Bootstrap Variance Estimation
1. Draw X∗
1,...,X∗
n∼hatwideFn.
2. Compute T∗
n=g(X∗
1,...,X∗
n).
3. Repeat steps 1 and 2, Btimes, to get T∗
n,1,...,T∗
n,B.
4. Let
vboot=1
BBsummationdisplay
b=1parenleftBigg
T∗
n,b−1
BBsummationdisplay
r=1T∗
n,rparenrightBigg2
. (8.1)
8.1 Example. The following pseudocode shows how to use the bootstrap to
estimate the standard error of the median.
Bootstrap for The Median
Given dat a X = (X(1), ..., X(n)):
T <- median(X)
Tboot <- vector of length Bfor(i in 1:B){
Xstar <- sample of size n from X (with replacement)Tboot[i] <- median(Xstar)}
se <- sqrt(variance(Tboot))
The following schematic diagram will remind you that we are using two
approximations:
V
F(Tn)not so smallbracehtipdownleftbracehtipuprightbracehtipupleftbracehtipdownright≈ V/hatwideFn(Tn)smallbracehtipdownleftbracehtipuprightbracehtipupleftbracehtipdownright≈vboot.
8.2 Example. Consider the nerve data. Let θ=T(F)=integraltext
(x−µ)3dF(x)/σ3be
the skewness. The skewness is a measure of asymmetry. A Normal distribution,

<<<PAGE 126>>>

110 8. The Bootstrap
for example, has skewness 0. The plug-in estimate of the skewness is
hatwideθ=T(hatwideFn)=integraltext
(x−µ)3dhatwideFn(x)
hatwideσ3=1
nsummationtextn
i=1(Xi−Xn)3
hatwideσ3=1.76.
To estimate the standard error with the bootstrap we follow the same steps
as with the median example except we compute the skewness from eachbootstrap sample. When applied to the nerve data, the bootstrap, based onB=1,000 replications, yields a standard error for the estimated skewness of
.16.
/squaresolid
8.3 Bootstrap Conﬁdence Intervals
There are several ways to construct bootstrap conﬁdence intervals. Here wediscuss three methods.
Method 1: The Normal Interval. The simplest method is the Normal interval
T
n±zα/2hatwideseboot (8.2)
wherehatwideseboot=√vbootis the bootstrap estimate of the standard error. This
interval is not accurate unless the distribution of Tnis close to Normal.
Method 2: Pivotal Intervals. Letθ=T(F) andhatwideθn=T(hatwideFn) and deﬁne the
pivot Rn=hatwideθn−θ. Lethatwideθ∗
n,1,...,hatwideθ∗
n,Bdenote bootstrap replications of hatwideθn. Let
H(r) denote the cdfof the pivot:
H(r)=PF(Rn≤r). (8.3)
Deﬁne C⋆
n=(a, b) where
a=hatwideθn−H−1parenleftBig
1−α
2parenrightBig
and b=hatwideθn−H−1parenleftBigα
2parenrightBig
. (8.4)
It follows that
P(a≤θ≤b)= P(a−hatwideθn≤θ−hatwideθn≤b−hatwideθn)
=P(hatwideθn−b≤hatwideθn−θ≤hatwideθn−a)
=P(hatwideθn−b≤Rn≤hatwideθn−a)
=H(hatwideθn−a)−H(hatwideθn−b)
=HparenleftBig
H−1parenleftBig
1−α
2parenrightBigparenrightBig
−HparenleftBig
H−1parenleftBigα
2parenrightBigparenrightBig
=1 −α
2−α
2=1−α.

<<<PAGE 127>>>

8.3 Bootstrap Conﬁdence Intervals 111
Hence, C⋆
nis an exact 1 −αconﬁdence interval for θ. Unfortunately, aandb
depend on the unknown distribution Hbut we can form a bootstrap estimate
ofH:
hatwideH(r)=1
BBsummationdisplay
b=1I(R∗
n,b≤r) (8.5)
where R∗
n,b=hatwideθ∗
n,b−hatwideθn. Letr∗
βdenote the βsample quantile of ( R∗
n,1,...,R∗
n,B)
and let θ∗
βdenote the βsample quantile of ( hatwideθ∗
n,1,...,hatwideθ∗
n,B). Note that r∗
β=
θ∗
β−hatwideθn. It follows that an approximate 1 −αconﬁdence interval is Cn=(hatwidea,hatwideb)
where
hatwidea=hatwideθn−hatwideH−1parenleftBig
1−α
2parenrightBig
=hatwideθn−r∗
1−α/2=2hatwideθn−θ∗
1−α/2
hatwideb=hatwideθn−hatwideH−1parenleftBigα
2parenrightBig
=hatwideθn−r∗
α/2=2hatwideθn−θ∗
α/2.
In summary, the 1 −αbootstrap pivotal conﬁdence interval is
Cn=parenleftBig
2hatwideθn−hatwideθ∗
1−α/2,2hatwideθn−hatwideθ∗
α/2parenrightBig
. (8.6)
8.3 Theorem. Under weak conditions on T(F),
PF(T(F)∈Cn)→1−α
asn→∞, where Cnis given in (8.6).
Method 3: Percentile Intervals . Thebootstrap percentile interval is de-
ﬁned by
Cn=parenleftBig
θ∗
α/2,θ∗
1−α/2parenrightBig
.
The justiﬁcation for this interval is given in the appendix.
8.4 Example. For estimating the skewness of the nerve data, here are the
various conﬁdence intervals.
Method 95% Interval
Normal (1.44, 2.09)
Pivotal (1.48, 2.11)Percentile (1.42, 2.03)
All these conﬁdence intervals are approximate. The probability that T(F)
is in the interval is not exactly 1 −α. All three intervals have the same level
of accuracy. There are more accurate bootstrap conﬁdence intervals but theyare more complicated and we will not discuss them here.

<<<PAGE 128>>>

112 8. The Bootstrap
8.5 Example (The Plasma Cholesterol Data) .Let us return to the cholesterol
data. Suppose we are interested in the diﬀerence of the medians. Pseudocodefor the bootstrap analysis is as follows:
x1 <- first sample
x2 <- second samplen1 <- length(x1)n2 <- length(x2)th.hat <- median(x2) - median(x1)B <- 1000Tboot <- vector of length Bfor(i in 1:B){
xx1 <- sample of size n1 with replacement from x1xx2 <- sample of size n2 with replacement from x2Tboot[i] <- median(xx2) - median(xx1)}
se <- sqrt(variance(Tboot))Normal <- (th.hat - 2*se, th.hat + 2*se)percentile <- (quantile(Tboot,.025), quantile(Tboot,.975))pivotal <- ( 2*th.hat-quantile(Tboot,.975),
2*th.hat-quantile(Tboot,.025) )
The point estimate is 18.5, the bootstrap standard error is 7.42 and the re-
sulting approximate 95 percent conﬁdence intervals are as follows:
Method 95% Interval
Normal (3.7, 33.3)
Pivotal (5.0, 34.0)Percentile (5.0, 33.3)
Since these intervals exclude 0, it appears that the second group has higher
cholesterol although there is considerable uncertainty about how much higheras reﬂected in the width of the intervals.
/squaresolid
The next two examples are based on small sample sizes. In practice, sta-
tistical methods based on very small sample sizes might not be reliable. Weinclude the examples for their pedagogical value but we do want to sound anote of caution about interpreting the results with some skepticism.
8.6 Example. Here is an example that was one of the ﬁrst used to illustrate
the bootstrap by Bradley Efron, the inventor of the bootstrap. The data areLSAT scores (for entrance to law school) and GPA.

<<<PAGE 129>>>

8.3 Bootstrap Conﬁdence Intervals 113
LSAT 576 635 558 578 666 580 555 661
651 605 653 575 545 572 594
GPA 3.39 3.30 2.81 3.03 3.44 3.07 3.00 3.43
3.36 3.13 3.12 2.74 2.76 2.88 3.96
Each data point is of the form Xi=(Yi,Zi) where Yi= LSAT iandZi=
GPA i. The law school is interested in the correlation
θ=integraltextintegraltext
(y−µY)(z−µZ)dF(y,z)radicalBigintegraltext
(y−µY)2dF(y)integraltext
(z−µZ)2dF(z).
The plug-in estimate is the sample correlation
hatwideθ=summationtext
i(Yi−Y)(Zi−Z)radicalBigsummationtext
i(Yi−Y)2summationtext
i(Zi−Z)2.
The estimated correlation is hatwideθ=.776. The bootstrap based on B= 1000
giveshatwidese=.137. Figure 8.1 shows the data and a histogram of the bootstrap
replications hatwideθ∗
1,...,hatwideθ∗
B. This histogram is an approximation to the sampling
distribution of hatwideθ. The Normal-based 95 percent conﬁdence interval is .78±
2hatwidese=(.51,1.00) while the percentile interval is (.46,.96). In large samples, the
two methods will show closer agreement. /squaresolid
8.7 Example. This example is from Efron and Tibshirani (1993). When drug
companies introduce new medications, they are sometimes required to showbioequivalence. This means that the new drug is not substantially diﬀerentthan the current treatment. Here are data on eight subjects who used medi-cal patches to infuse a hormone into the blood. Each subject received threetreatments: placebo, old-patch, new-patch.
subject placebo old new old −placebo new −old
1 9243 17649 16449 8406 -1200
2 9671 12013 14614 2342 26013 11792 19979 17274 8187 -27054 13357 21816 23798 8459 19825 9055 13850 12560 4795 -12906 6290 9806 10157 3516 3517 12412 17208 16570 4796 -6388 18806 29044 26325 10238 -2719

<<<PAGE 130>>>

114 8. The Bootstrap






 

LSATGPA
560 580 600 620 640 6602.8 3.0 3.2 3.4
0.2 0.4 0.6 0.8 1.00 50 100 150
Bootstrap Samples
FIGURE 8.1. Law school data. The top panel shows the raw data. The bottom panel
is a histogram of the correlations computed from each bootstrap sample.
LetZ= old −placebo and Y= new −old. The Food and Drug Adminis-
tration (FDA) requirement for bioequivalence is that |θ|≤.20 where
θ=EF(Y)
EF(Z).
The plug-in estimate of θis
hatwideθ=Y
Z=−452.3
6342=−0.0713.
The bootstrap standard error is hatwidese=0.105. To answer the bioequivalence
question, we compute a conﬁdence interval. From B= 1000 bootstrap repli-
cations we get the 95 percent interval (-0.24,0.15). This is not quite contained

<<<PAGE 131>>>

8.4 Bibliographic Remarks 115
in (-0.20,0.20) so at the 95 percent level we have not demonstrated bioequiv-
alence. Figure 8.2 shows the histogram of the bootstrap values. /squaresolid
-0.3 -0.2 -0.1 0.0 0.1 0.20 2 04 06 08 0
Bootstrap Samples
FIGURE 8.2. Patch data.
8.4 Bibliographic Remarks
The bootstrap was invented by Efron (1979). There are several books on thesetopics including Efron and Tibshirani (1993), Davison and Hinkley (1997),Hall (1992) and Shao and Tu (1995). Also, see section 3.6 of van der Vaartand Wellner (1996).
8.5 Appendix
8.5.1 The Jackknife
There is another method for computing standard errors called the jackknife ,
due to Quenouille (1949). It is less computationally expensive than the boot-

<<<PAGE 132>>>

116 8. The Bootstrap
strap but is less general. Let Tn=T(X1,...,X n) be a statistic and T(−i)de-
note the statistic with the ithobservation removed. Let Tn=n−1summationtextn
i=1T(−i).
The jackknife estimate of var( Tn)i s
vjack=n−1
nnsummationdisplay
i=1(T(−i)−Tn)2
and the jackknife estimate of the standard error is hatwidesejack=√vjack. Under
suitable conditions on T, it can be shown that vjackconsistently estimates
var(Tn) in the sense that vjack/var(T n)P−→1. However, unlike the bootstrap,
the jackknife does not produce consistent estimates of the standard error ofsample quantiles.
8.5.2 Justiﬁcation For The Percentile Interval
Suppose there exists a monotone transformation U=m(T) such that U∼
N(φ, c2) where φ=m(θ). We do not suppose we know the transformation,
only that one exists. Let U∗
b=m(θ∗
n,b). Let u∗
βbe the βsample quantile of
theU∗
b’s. Since a monotone transformation preserves quantiles, we have that
u∗
α/2=m(θ∗
α/2). Also, since U∼N(φ, c2), the α/2 quantile of Uisφ−zα/2c.
Hence u∗
α/2=φ−zα/2c. Similarly, u∗
1−α/2=φ+zα/2c. Therefore,
P(θ∗
α/2≤θ≤θ∗
1−α/2)= P(m(θ∗
α/2)≤m(θ)≤m(θ∗
1−α/2))
=P(u∗
α/2≤φ≤u∗
1−α/2)
=P(U−czα/2≤φ≤U+czα/2)
=P(−zα/2≤U−φ
c≤zα/2)
=1 −α.
An exact normalizing transformation will rarely exist but there may exist
approximate normalizing transformations.
8.6 Exercises
1. Consider the data in Example 8.6. Find the plug-in estimate of the
correlation coeﬃcient. Estimate the standard error using the bootstrap.Find a 95 percent conﬁdence interval using the Normal, pivotal, andpercentile methods.

<<<PAGE 133>>>

8.6 Exercises 117
2. (Computer Experiment. ) Conduct a simulation to compare the various
bootstrap conﬁdence interval methods. Let n= 50 and let T(F)=integraltext
(x−µ)3dF(x)/σ3be the skewness. Draw Y1,...,Y n∼N(0,1) and
setXi=eYi,i=1,...,n . Construct the three types of bootstrap 95
percent intervals for T(F) from the data X1,...,X n. Repeat this whole
thing many times and estimate the true coverage of the three intervals.
3. Let
X1,...,X n∼t3
where n= 25. Let θ=T(F)=(q.75−q.25)/1.34 where qpdenotes the
pthquantile. Do a simulation to compare the coverage and length of the
following conﬁdence intervals for θ: (i) Normal interval with standard
error from the bootstrap, (ii) bootstrap percentile interval, and (iii)pivotal bootstrap interval.
4. Let X
1,...,X nbe distinct observations (no ties). Show that there are
parenleftbigg2n−1
nparenrightbigg
distinct bootstrap samples.
Hint: Imagine putting nballs into nbuckets.
5. Let X1,...,X nbe distinct observations (no ties). Let X∗
1,...,X∗
ndenote
a bootstrap sample and let X∗
n=n−1summationtextn
i=1X∗
i. Find: E(X∗
n|X1,...,X n),
V(X∗
n|X1,...,X n),E(X∗
n) and V(X∗
n).
6. (Computer Experiment. ) LetX1, ..., X nNormal( µ,1).Letθ=eµand let
hatwideθ=eX. Create a data set (using µ= 5) consisting of n=100 observa-
tions.
(a) Use the bootstrap to get the seand 95 percent conﬁdence interval
forθ.
(b) Plot a histogram of the bootstrap replications. This is an estimate
of the distribution of hatwideθ. Compare this to the true sampling distribution
ofhatwideθ.
7. Let X1, ..., X n∼Uniform(0 ,θ).Lethatwideθ=Xmax= max {X1, ..., X n}. Gen-
erate a data set of size 50 with θ=1.
(a) Find the distribution of hatwideθ. Compare the true distribution of hatwideθto the
histograms from the bootstrap.

<<<PAGE 134>>>

118 8. The Bootstrap
(b) This is a case where the bootstrap does very poorly. In fact, we can
prove that this is the case. Show that P(hatwideθ=hatwideθ) = 0 and yet P(hatwideθ∗=
hatwideθ)≈.632. Hint: show that, P(hatwideθ∗=hatwideθ)=1−(1−(1/n))nthen take the
limit as ngets large.
8. Let Tn=X2
n,µ=E(X1),αk=integraltext
|x−µ|kdF(x) andhatwideαk=n−1summationtextn
i=1|Xi−
Xn|k. Show that
vboot=4X2
nhatwideα2
n+4Xnhatwideα3
n2+hatwideα4
n3.

<<<PAGE 135>>>

9
Parametric Inference
We now turn our attention to parametric models, that is, models of the form
F=braceleftbigg
f(x;θ):θ∈Θbracerightbigg
(9.1)
where the Θ ⊂Rkis the parameter space and θ=(θ1,...,θ k) is the param-
eter. The problem of inference then reduces to the problem of estimating the
parameter θ.
Students learning statistics often ask: how would we ever know that the
distribution that generated the data is in some parametric model? This isan excellent question. Indeed, we would rarely have such knowledge whichis why nonparametric methods are preferable. Still, studying methods forparametric models is useful for two reasons. First, there are some cases wherebackground knowledge suggests that a parametric model provides a reasonableapproximation. For example, counts of traﬃc accidents are known from priorexperience to follow approximately a Poisson model. Second, the inferentialconcepts for parametric models provide background for understanding certainnonparametric methods.
We begin with a brief discussion about parameters of interest and nuisance
parameters in the next section, then we will discuss two methods for estimat-ingθ, the method of moments and the method of maximum likelihood.

<<<PAGE 136>>>

120 9. Parametric Inference
9.1 Parameter of Interest
Often, we are only interested in some function T(θ). For example, if X∼
N(µ, σ2) then the parameter is θ=(µ, σ). If our goal is to estimate µthen
µ=T(θ) is called the parameter of interest andσis called a nuisance
parameter . The parameter of interest might be a complicated function of θ
as in the following example.
9.1 Example. LetX1,...,X n∼Normal( µ, σ2). The parameter is θ=(µ, σ)
and the parameter space is Θ = {(µ, σ):µ∈R,σ > 0}. Suppose that Xiis
the outcome of a blood test and suppose we are interested in τ, the fraction
of the population whose test score is larger than 1. Let Zdenote a standard
Normal random variable. Then
τ=P(X>1 )=1 −P(X<1 )=1 −PparenleftbiggX−µ
σ<1−µ
σparenrightbigg
=1 −Pparenleftbigg
Z<1−µ
σparenrightbigg
=1−Φparenleftbigg1−µ
σparenrightbigg
.
The parameter of interest is τ=T(µ, σ)=1−Φ((1−µ)/σ)./squaresolid
9.2 Example. Recall that Xhas a Gamma( α,β) distribution if
f(x;α,β)=1
βαΓ(α)xα−1e−x/β,x > 0
where α,β > 0 and
Γ(α)=integraldisplay∞
0yα−1e−ydy
is the Gamma function. The parameter is θ=(α,β). The Gamma distri-
bution is sometimes used to model lifetimes of people, animals, and elec-tronic equipment. Suppose we want to estimate the mean lifetime. ThenT(α,β)=E
θ(X1)=αβ./squaresolid
9.2 The Method of Moments
The ﬁrst method for generating parametric estimators that we will study
is called the method of moments. We will see that these estimators are notoptimal but they are often easy to compute. They are are also useful as startingvalues for other methods that require iterative numerical routines.

<<<PAGE 137>>>

9.2 The Method of Moments 121
Suppose that the parameter θ=(θ1,...,θ k) has kcomponents. For 1 ≤
j≤k, deﬁne the jthmoment
αj≡αj(θ)=Eθ(Xj)=integraldisplay
xjdFθ(x) (9.2)
and the jthsample moment
hatwideαj=1
nnsummationdisplay
i=1Xj
i. (9.3)
9.3 Deﬁnition. Themethod of moments estimator hatwideθnis deﬁned to be
the value of θsuch that
α1(hatwideθn)=hatwideα1
α2(hatwideθn)=hatwideα2
.........
α
k(hatwideθn)=hatwideαk. (9.4)
Formula (9.4) deﬁnes a system of kequations with kunknowns.
9.4 Example. LetX1,...,X n∼Bernoulli( p). Then α1=Ep(X)=pand
hatwideα1=n−1summationtextn
i=1Xi. By equating these we get the estimator
hatwidepn=1
nnsummationdisplay
i=1Xi./squaresolid
9.5 Example. LetX1,...,X n∼Normal( µ, σ2). Then, α1=Eθ(X1)=µ
andα2=Eθ(X2
1)=Vθ(X1)+(Eθ(X1))2=σ2+µ2. We need to solve the
equations1
hatwideµ=1
nnsummationdisplay
i=1Xi
hatwideσ2+hatwideµ2=1
nnsummationdisplay
i=1X2
i.
This is a system of 2 equations with 2 unknowns. The solution is
hatwideµ=Xn
1Recall that V(X)= E(X2)−(E(X))2. Hence, E(X2)= V(X)+( E(X))2.

<<<PAGE 138>>>

122 9. Parametric Inference
hatwideσ2=1
nnsummationdisplay
i=1(Xi−Xn)2./squaresolid
9.6 Theorem. Lethatwideθndenote the method of moments estimator. Under appro-
priate conditions on the model, the following statements hold:
1. The estimate hatwideθnexists with probability tending to 1.
2. The estimate is consistent: hatwideθnP−→θ.
3. The estimate is asymptotically Normal:
√n(hatwideθn−θ)/squigglerightN(0,Σ)
where
Σ=gEθ(YYT)gT,
Y=(X,X2,...,Xk)T,g=(g1,...,g k)andgj=∂α−1
j(θ)/∂θ.
The last statement in the theorem above can be used to ﬁnd standard errors
and conﬁdence intervals. However, there is an easier way: the bootstrap. Wedefer discussion of this until the end of the chapter.
9.3 Maximum Likelihood
The most common method for estimating parameters in a parametric model isthemaximum likelihood method . LetX
1,...,Xnbeiidwith pdff(x;θ).
9.7 Deﬁnition. Thelikelihood function is deﬁned by
Ln(θ)=nproductdisplay
i=1f(Xi;θ). (9.5)
Thelog-likelihood function is deﬁned by /lscriptn(θ) = log Ln(θ).
The likelihood function is just the joint density of the data, except that we
treat it is a function of the parameter θ.Thus, Ln:Θ→[0,∞). The
likelihood function is not a density function: in general, it is nottrue that
Ln(θ) integrates to 1 (with respect to θ).
9.8 Deﬁnition. Themaximum likelihood estimator mle, denoted by
hatwideθn, is the value of θthat maximizes Ln(θ).

<<<PAGE 139>>>

9.3 Maximum Likelihood 123
0.0 0.2 0.4 0.6 0.8 1.0phatwidepn
FIGURE 9.1. Likelihood function for Bernoulli with n= 20 and/summationtextn
i=1Xi= 12. The
mleis/hatwidepn=1 2/2 0=0 .6.
The maximum of /lscriptn(θ) occurs at the same place as the maximum of Ln(θ),
so maximizing the log-likelihood leads to the same answer as maximizing thelikelihood. Often, it is easier to work with the log-likelihood.
9.9 Remark. If we multiply L
n(θ) by any positive constant c(not depending
onθ) then this will not change the mle. Hence, we shall often drop constants
in the likelihood function.
9.10 Example. Suppose that X1,...,X n∼Bernoulli( p). The probability func-
tion is f(x;p)=px(1−p)1−xforx=0,1. The unknown parameter is p. Then,
Ln(p)=nproductdisplay
i=1f(Xi;p)=nproductdisplay
i=1pXi(1−p)1−Xi=pS(1−p)n−S
where S=summationtext
iXi. Hence,
/lscriptn(p)=Slogp+(n−S) log(1 −p).
Take the derivative of /lscriptn(p), set it equal to 0 to ﬁnd that the mleishatwidepn=S/n.
See Figure 9.1. /squaresolid
9.11 Example. LetX1,...,X n∼N(µ, σ2). The parameter is θ=(µ, σ) and
the likelihood function (ignoring some constants) is:
Ln(µ, σ)=productdisplay
i1
σexpbraceleftbigg
−1
2σ2(Xi−µ)2bracerightbigg
=σ−nexpbraceleftBigg
−1
2σ2summationdisplay
i(Xi−µ)2bracerightBigg

<<<PAGE 140>>>

124 9. Parametric Inference
=σ−nexpbraceleftbigg
−nS2
2σ2bracerightbigg
expbraceleftbigg
−n(X−µ)2
2σ2bracerightbigg
where X=n−1summationtext
iXiis the sample mean and S2=n−1summationtext
i(Xi−X)2. The
last equality above follows from the fact thatsummationtext
i(Xi−µ)2=nS2+n(X−µ)2
which can be veriﬁed by writingsummationtext
i(Xi−µ)2=summationtext
i(Xi−X+X−µ)2and
then expanding the square. The log-likelihood is
/lscript(µ, σ)=−nlogσ−nS2
2σ2−n(X−µ)2
2σ2.
Solving the equations
∂/lscript(µ, σ)
∂µ= 0 and∂/lscript(µ, σ)
∂σ=0,
we conclude that hatwideµ=Xandhatwideσ=S. It can be veriﬁed that these are indeed
global maxima of the likelihood. /squaresolid
9.12 Example (A Hard Example) .Here is an example that many people ﬁnd
confusing. Let X1,...,X n∼Unif(0,θ). Recall that
f(x;θ)=braceleftbigg1/θ0≤x≤θ
0 otherwise .
Consider a ﬁxed value of θ. Suppose θ<X ifor some i. Then, f(Xi;θ)=0
and hence Ln(θ)=producttext
if(Xi;θ) = 0. It follows that Ln(θ)=0i fa n y Xi>θ.
Therefore, Ln(θ)=0i f θ<X (n)where X(n)= max {X1,...,X n}.N o w
consider any θ≥X(n). For every Xiwe then have that f(Xi;θ)=1/θso that
Ln(θ)=producttext
if(Xi;θ)=θ−n. In conclusion,
Ln(θ)=braceleftbiggparenleftbig1
θparenrightbignθ≥X(n)
0 θ<X (n).
See Figure 9.2. Now Ln(θ) is strictly decreasing over the interval [ X(n),∞).
Hence,hatwideθn=X(n)./squaresolid
The maximum likelihood estimators for the multivariate Normal and the
multinomial can be found in Theorems 14.5 and 14.3.
9.4 Properties of Maximum Likelihood Estimators
Under certain conditions on the model, the maximum likelihood estimator hatwideθn
possesses many properties that make it an appealing choice of estimator. The
main properties of the mleare:

<<<PAGE 141>>>

9.4 Properties of Maximum Likelihood Estimators 125
0.0 0.5 1.0 1.5 2.00.0 0.5 1.0 1.5
0.0 0.5 1.0 1.5 2.00.0 0.5 1.0 1.5
0.0 0.5 1.0 1.5 2.00.0 0.5 1.0 1.5
0.6 0.8 1.0 1.2 1.40.0 0.5 1.0 1.5θ=.75 θ=1
θ=1.25xx
xθf(x;θ)
f(x;θ)f(x;θ)
Ln(θ)
FIGURE 9.2. Likelihood function for Uniform (0 ,θ). The vertical lines show the
observed data. The ﬁrst three plots show f(x;θ) for three diﬀerent values of θ.
When θ<X (n)= max {X1,...,X n}, as in the ﬁrst plot, f(X(n);θ)=0a n d
hence Ln(θ)=/producttextn
i=1f(Xi;θ) = 0. Otherwise f(Xi;θ)=1/θfor each iand hence
Ln(θ)=/producttextn
i=1f(Xi;θ)=( 1 /θ)n. The last plot shows the likelihood function.

<<<PAGE 142>>>

126 9. Parametric Inference
1. The mleisconsistent :hatwideθnP−→θ⋆where θ⋆denotes the true value of the
parameter θ;
2. The mleisequivariant :i fhatwideθnis the mleofθtheng(hatwideθn)i st h e mleof
g(θ);
3. The mleisasymptotically Normal :(hatwideθ−θ⋆)/hatwidese/squigglerightN(0,1); also, the
estimated standard error hatwidesecan often be computed analytically;
4. The mleisasymptotically optimal oreﬃcient : roughly, this means
that among all well-behaved estimators, the mlehas the smallest vari-
ance, at least for large samples;
5. The mleis approximately the Bayes estimator. (This point will be ex-
plained later.)
We will spend some time explaining what these properties mean and why
they are good things. In suﬃciently complicated problems, these propertieswill no longer hold and the mlewill no longer be a good estimator. For now
we focus on the simpler situations where the mleworks well. The properties
we discuss only hold if the model satisﬁes certain regularity conditions.
These are essentially smoothness conditions on f(x;θ).Unless otherwise
stated, we shall tacitly assume that these conditions hold.
9.5 Consistency of Maximum Likelihood Estimators
Consistency means that the mleconverges in probability to the true value.
To proceed, we need a deﬁnition. If fandgarepdf’s, deﬁne the Kullback-
Leibler distance2between fandgto be
D(f,g)=integraldisplay
f(x) logparenleftbiggf(x)
g(x)parenrightbigg
dx. (9.6)
It can be shown that D(f,g)≥0 and D(f,f) = 0. For any θ,ψ∈Θ write
D(θ,ψ) to mean D(f(x;θ),f(x;ψ)).
We will say that the model Fisidentiﬁable ifθ/negationslash=ψimplies that D(θ,ψ)>
0. This means that diﬀerent values of the parameter correspond to diﬀerentdistributions. We will assume from now on the the model is identiﬁable.
2This is not a distance in the formal sense because D(f,g)is not symmetric.

<<<PAGE 143>>>

9.6 Equivariance of the mle 127
Letθ⋆denote the true value of θ. Maximizing /lscriptn(θ) is equivalent to maxi-
mizing
Mn(θ)=1
nsummationdisplay
ilogf(Xi;θ)
f(Xi;θ⋆).
This follows since Mn(θ)=n−1(/lscriptn(θ)−/lscriptn(θ⋆)) and /lscriptn(θ⋆) is a constant (with
respect to θ). By the law of large numbers, Mn(θ) converges to
Eθ⋆parenleftBigg
logf(Xi;θ)
f(Xi;θ⋆)parenrightBigg
=integraldisplay
logparenleftbiggf(x;θ)
f(x;θ⋆)parenrightbigg
f(x;θ⋆)dx
=−integraldisplay
logparenleftbiggf(x;θ⋆)
f(x;θ)parenrightbigg
f(x;θ⋆)dx
=−D(θ⋆,θ).
Hence, Mn(θ)≈−D(θ⋆,θ) which is maximized at θ⋆since −D(θ⋆,θ⋆)=0
and−D(θ⋆,θ)<0 forθ/negationslash=θ⋆. Therefore, we expect that the maximizer will
tend to θ⋆. To prove this formally, we need more than Mn(θ)P−→ − D(θ⋆,θ).
We need this convergence to be uniform over θ. We also have to make sure
that the function D(θ⋆,θ) is well behaved. Here are the formal details.
9.13 Theorem. Letθ⋆denote the true value of θ. Deﬁne
Mn(θ)=1
nsummationdisplay
ilogf(Xi;θ)
f(Xi;θ⋆)
andM(θ)=−D(θ⋆,θ). Suppose that
sup
θ∈Θ|Mn(θ)−M(θ)|P−→0 (9.7)
and that, for every /epsilon1>0,
sup
θ:|θ−θ⋆|≥/epsilon1M(θ)<M(θ⋆). (9.8)
Lethatwideθndenote the mle. ThenhatwideθnP−→θ⋆.
The proof is in the appendix.
9.6 Equivariance of the mle
9.14 Theorem. Letτ=g(θ)be a function of θ.L e thatwideθnbe the mleofθ. Then
hatwideτn=g(hatwideθn)is the mleofτ.

<<<PAGE 144>>>

128 9. Parametric Inference
Proof. Leth=g−1denote the inverse of g. Thenhatwideθn=h(hatwideτn). For any τ,
L(τ)=producttext
if(xi;h(τ)) =producttext
if(xi;θ)=L(θ) where θ=h(τ). Hence, for any τ,
Ln(τ)=L(θ)≤L(hatwideθ)=Ln(hatwideτ)./squaresolid
9.15 Example. LetX1,...,X n∼N(θ,1). The mleforθishatwideθn=Xn. Let
τ=eθ. Then, the mleforτishatwideτ=e/hatwideθ=eX./squaresolid
9.7 Asymptotic Normality
It turns out that the distribution of hatwideθnis approximately Normal and we can
compute its approximate variance analytically. To explore this, we ﬁrst needa few deﬁnitions.
9.16 Deﬁnition. Thescore function is deﬁned to be
s(X;θ)=∂logf(X;θ)
∂θ. (9.9)
TheFisher information is deﬁned to be
In(θ)= VθparenleftBiggnsummationdisplay
i=1s(Xi;θ)parenrightBigg
=nsummationdisplay
i=1Vθ(s(Xi;θ)). (9.10)
Forn= 1 we will sometimes write I(θ) instead of I1(θ). It can be shown
thatEθ(s(X;θ)) = 0. It then follows that Vθ(s(X;θ)) =Eθ(s2(X;θ)). In fact,
a further simpliﬁcation of In(θ) is given in the next result.
9.17 Theorem. In(θ)=nI(θ). Also,
I(θ)= −EθparenleftBigg
∂2logf(X;θ)
∂θ2parenrightBigg
=−integraldisplayparenleftBigg
∂2logf(x;θ)
∂θ2parenrightBigg
f(x;θ)dx. (9.11)

<<<PAGE 145>>>

9.7 Asymptotic Normality 129
9.18 Theorem (Asymptotic Normality of the mle).Letse=radicalBig
V(hatwideθn).
Under appropriate regularity conditions, the following hold:
1.se≈radicalbig
1/In(θ)and
(hatwideθn−θ)
se/squigglerightN(0,1). (9.12)
2. Lethatwidese=radicalBig
1/In(hatwideθn). Then,
(hatwideθn−θ)
hatwidese/squigglerightN(0,1). (9.13)
The proof is in the appendix. The ﬁrst statement says that hatwideθn≈N(θ,se)
where the approximate standard error of hatwideθnisse=radicalbig
1/In(θ). The second
statement says that this is still true even if we replace the standard error by
its estimated standard error hatwidese=radicalBig
1/In(hatwideθn).
Informally, the theorem says that the distribution of the mlecan be ap-
proximated with N(θ,hatwidese2). From this fact we can construct an (asymptotic)
conﬁdence interval.
9.19 Theorem. Let
Cn=parenleftbigg
hatwideθn−zα/2hatwidese,hatwideθn+zα/2hatwideseparenrightbigg
.
Then, Pθ(θ∈Cn)→1−αasn→∞.
Proof. LetZdenote a standard normal random variable. Then,
Pθ(θ∈Cn)= PθparenleftBig
hatwideθn−zα/2hatwidese≤θ≤hatwideθn+zα/2hatwideseparenrightBig
=PθparenleftBigg
−zα/2≤hatwideθn−θ
hatwidese≤zα/2parenrightBigg
→P(−zα/2<Z<z α/2)=1−α./squaresolid
Forα=.05,zα/2=1.96≈2, so:
hatwideθn±2hatwidese (9.14)
is an approximate 95 percent conﬁdence interval.

<<<PAGE 146>>>

130 9. Parametric Inference
When you read an opinion poll in the newspaper, you often see a statement
like: the poll is accurate to within one point, 95 percent of the time. They aresimply giving a 95 percent conﬁdence interval of the form hatwideθ
n±2hatwidese.
9.20 Example. LetX1,...,X n∼Bernoulli( p). The mleishatwidepn=summationtext
iXi/n
andf(x;p)=px(1−p)1−x, logf(x;p)=xlogp+( 1−x) log(1 −p),
s(X;p)=X
p−1−X
1−p,
and
−s/prime(X;p)=X
p2+1−X
(1−p)2.
Thus,
I(p)=Ep(−s/prime(X;p)) =p
p2+(1−p)
(1−p)2=1
p(1−p).
Hence,
hatwidese=1radicalbig
In(hatwidepn)=1radicalbig
nI(hatwidepn)=braceleftbigghatwidep(1−hatwidep)
nbracerightbigg1/2
.
An approximate 95 percent conﬁdence interval is
hatwidepn±2braceleftbigghatwidepn(1−hatwidepn)
nbracerightbigg1/2
./squaresolid
9.21 Example. LetX1,...,X n∼N(θ,σ2) where σ2is known. The score
function is s(X;θ)=(X−θ)/σ2ands/prime(X;θ)=−1/σ2so that I1(θ)=1/σ2.
The mleishatwideθn=Xn. According to Theorem 9.18, Xn≈N(θ,σ2/n). In this
case, the Normal approximation is actually exact. /squaresolid
9.22 Example. LetX1,...,X n∼Poisson( λ). Then hatwideλn=Xnand some cal-
culations show that I1(λ)=1/λ,s o
hatwidese=1radicalBig
nI(hatwideλn)=radicalBigg
hatwideλn
n.
Therefore, an approximate 1 −αconﬁdence interval for λishatwideλn±zα/2radicalBig
hatwideλn/n.
/squaresolid
9.8 Optimality
Suppose that X1,...,X n∼N(θ,σ2). The mleishatwideθn=Xn. Another reason-
able estimator of θis the sample median tildewideθn. The mlesatisﬁes
√n(hatwideθn−θ)/squigglerightN(0,σ2).

<<<PAGE 147>>>

9.9 The Delta Method 131
It can be proved that the median satisﬁes
√n(tildewideθn−θ)/squigglerightNparenleftBig
0,σ2π
2parenrightBig
.
This means that the median converges to the right value but has a larger
variance than the mle.
More generally, consider two estimators TnandUnand suppose that
√n(Tn−θ)/squigglerightN(0,t2),
and that√n(Un−θ)/squigglerightN(0,u2).
We deﬁne the asymptotic relative eﬃciency of UtoTbyare(U, T)=t2/u2.
In the Normal example, are(tildewideθn,hatwideθn)=2/π=.63. The interpretation is that
if you use the median, you are eﬀectively using only a fraction of the data.
9.23 Theorem. Ifhatwideθnis the mleandtildewideθnis any other estimator then3
are(tildewideθn,hatwideθn)≤1.
Thus, the mlehas the smallest (asymptotic) variance and we say that the
mleiseﬃcient orasymptotically optimal.
This result is predicated upon the assumed model being correct. If the model
is wrong, the mlemay no longer be optimal. We will discuss optimality in
more generality when we discuss decision theory in Chapter 12.
9.9 The Delta Method
Letτ=g(θ) where gis a smooth function. The maximum likelihood esti-
mator of τishatwideτ=g(hatwideθ). Now we address the following question: what is the
distribution of hatwideτ?
9.24 Theorem (The Delta Method) .Ifτ=g(θ)where gis diﬀerentiable
andg/prime(θ)/negationslash=0then
(hatwideτn−τ)
hatwidese(hatwideτ)/squigglerightN(0,1) (9.15)
3The result is actually more subtle than this but the details are too complicated to consider
here.

<<<PAGE 148>>>

132 9. Parametric Inference
wherehatwideτn=g(hatwideθn)and
hatwidese(hatwideτn)=|g/prime(hatwideθ)|hatwidese(hatwideθn) (9.16)
Hence, if
Cn=parenleftbigg
hatwideτn−zα/2hatwidese(hatwideτn),hatwideτn+zα/2hatwidese(hatwideτn)parenrightbigg
(9.17)
thenPθ(τ∈Cn)→1−αasn→∞.
9.25 Example. LetX1,...,X n∼Bernoulli( p) and let ψ=g(p) = log( p/(1−
p)). The Fisher information function is I(p)=1/(p(1−p)) so the estimated
standard error of the mlehatwidepnis
hatwidese=radicalbigg
hatwidepn(1−hatwidepn)
n.
The mleofψishatwideψ= loghatwidep/(1−hatwidep). Since, g/prime(p)=1/(p(1−p)), according to
the delta method
hatwidese(hatwideψn)=|g/prime(hatwidepn)|hatwidese(hatwidepn)=1radicalbig
nhatwidepn(1−hatwidepn).
An approximate 95 percent conﬁdence interval is
hatwideψn±2radicalbig
nhatwidepn(1−hatwidepn)./squaresolid
9.26 Example. LetX1,...,X n∼N(µ, σ2). Suppose that µis known, σis
unknown and that we want to estimate ψ= log σ. The log-likelihood is /lscript(σ)=
−nlogσ−1
2σ2summationtext
i(xi−µ)2. Diﬀerentiate and set equal to 0 and conclude that
hatwideσn=radicalbiggsummationtext
i(Xi−µ)2
n.
To get the standard error we need the Fisher information. First,
logf(X;σ)=−logσ−(X−µ)2
2σ2
with second derivative
1
σ2−3(X−µ)2
σ4,
and hence
I(σ)=−1
σ2+3σ2
σ4=2
σ2.

<<<PAGE 149>>>

9.10 Multiparameter Models 133
Therefore, hatwidese=hatwideσn/√
2n.Letψ=g(σ) = log σ. Then, hatwideψn= loghatwideσn. Since
g/prime=1/σ,
hatwidese(hatwideψn)=1
hatwideσnhatwideσn√
2n=1√
2n,
and an approximate 95 percent conﬁdence interval is hatwideψn±2/√
2n./squaresolid
9.10 Multiparameter Models
These ideas can directly be extended to models with several parameters. Let
θ=(θ1,...,θ k) and let hatwideθ=(hatwideθ1,...,hatwideθk)b et h e mle. Let/lscriptn=summationtextn
i=1logf(Xi;θ),
Hjj=∂2/lscriptn
∂θ2
jand Hjk=∂2/lscriptn
∂θj∂θk.
Deﬁne the Fisher Information Matrix by
In(θ)=−
E
θ(H11)Eθ(H12)···Eθ(H1k)
Eθ(H21)Eθ(H22)···Eθ(H2k)
............
E
θ(Hk1)Eθ(Hk2)···Eθ(Hkk)
. (9.18)
LetJ
n(θ)=I−1
n(θ) be the inverse of In.
9.27 Theorem. Under appropriate regularity conditions,
(hatwideθ−θ)≈N(0,Jn).
Also, if hatwideθjis the jthcomponent of hatwideθ, then
(hatwideθj−θj)
hatwidesej/squigglerightN(0,1) (9.19)
wherehatwidese2
j=Jn(j,j)is the jthdiagonal element of Jn. The approximate co-
variance of hatwideθjandhatwideθkisCov(hatwideθj,hatwideθk)≈Jn(j,k).
There is also a multiparameter delta method. Let τ=g(θ1,...,θ k)b ea
function and let
∇g=
∂g
∂θ1...
∂g
∂θk

be the gradient of g.

<<<PAGE 150>>>

134 9. Parametric Inference
9.28 Theorem (Multiparameter delta method) .Suppose that ∇gevaluated at
hatwideθis not 0. Let hatwideτ=g(hatwideθ). Then
(hatwideτ−τ)
hatwidese(hatwideτ)/squigglerightN(0,1)
where
hatwidese(hatwideτ)=radicalBig
(hatwide∇g)ThatwideJn(hatwide∇g), (9.20)
hatwideJn=Jn(hatwideθn)andhatwide∇gis∇gevaluated at θ=hatwideθ.
9.29 Example. LetX1,...,X n∼N(µ, σ2). Let τ=g(µ, σ)=σ/µ. In Excer-
cise 8 you will show that
In(µ, σ)=bracketleftbiggn
σ20
02n
σ2bracketrightbigg
.
Hence,
Jn=I−1
n(µ, σ)=1
nbracketleftbiggσ20
0σ2
2bracketrightbigg
.
The gradient of gis
∇g=parenleftBigg−σ
µ2
1
µparenrightBigg
.
Thus,
hatwidese(hatwideτ)=radicalBig
(hatwide∇g)ThatwideJn(hatwide∇g)=1√nradicalBigg
1
hatwideµ4+hatwideσ2
2hatwideµ2./squaresolid
9.11 The Parametric Bootstrap
For parametric models, standard errors and conﬁdence intervals may also be
estimated using the bootstrap. There is only one change. In the nonparametricbootstrap, we sampled X
∗
1,...,X∗
nfrom the empirical distribution hatwideFn.I nt h e
parametric bootstrap we sample instead from f(x;hatwideθn). Here, hatwideθncould be the
mleor the method of moments estimator.
9.30 Example. Consider example 9.29. To get the bootstrap standard er-
ror, simulate X1,...,X∗
n∼N(hatwideµ,hatwideσ2), compute hatwideµ∗=n−1summationtext
iX∗
iandhatwideσ2∗=
n−1summationtext
i(X∗
i−hatwideµ∗)2. Then compute hatwideτ∗=g(hatwideµ∗,hatwideσ∗)=hatwideσ∗/hatwideµ∗. Repeating this B
times yields bootstrap replications
hatwideτ∗
1,...,hatwideτ∗
B

<<<PAGE 151>>>

9.12 Checking Assumptions 135
and the estimated standard error is
hatwideseboot=radicalBiggsummationtextB
b=1(hatwideτ∗
b−hatwideτ)2
B./squaresolid
The bootstrap is much easier than the delta method. On the other hand,
the delta method has the advantage that it gives a closed form expression forthe standard error.
9.12 Checking Assumptions
If we assume the data come from a parametric model, then it is a good idea tocheck that assumption. One possibility is to check the assumptions informallyby inspecting plots of the data. For example, if a histogram of the data looksvery bimodal, then the assumption of Normality might be questionable. Aformal way to test a parametric model is to use a goodness-of-ﬁt test. See
Section 10.8.
9.13 Appendix
9.13.1 Proofs
Proof of Theorem 9.13. Sincehatwideθnmaximizes Mn(θ), we have Mn(hatwideθn)≥
Mn(θ⋆). Hence,
M(θ⋆)−M(hatwideθn)= Mn(θ⋆)−M(hatwideθn)+M(θ⋆)−Mn(θ⋆)
≤Mn(hatwideθn)−M(hatwideθn)+M(θ⋆)−Mn(θ⋆)
≤sup
θ|Mn(θ)−M(θ)|+M(θ⋆)−Mn(θ⋆)
P−→ 0
where the last line follows from (9.7). It follows that, for any δ>0,
PparenleftBig
M(hatwideθn)<M(θ⋆)−δparenrightBig
→0.
Pick any /epsilon1>0. By (9.8), there exists δ>0 such that |θ−θ⋆|≥/epsilon1implies that
M(θ)<M(θ⋆)−δ. Hence,
P(|hatwideθn−θ⋆|>/epsilon1)≤PparenleftBig
M(hatwideθn)<M(θ⋆)−δparenrightBig
→0./squaresolid
Next we want to prove Theorem 9.18. First we need a lemma.

<<<PAGE 152>>>

136 9. Parametric Inference
9.31 Lemma. The score function satisﬁes
Eθ[s(X;θ)] = 0 .
Proof. Note that 1 =integraltext
f(x;θ)dx. Diﬀerentiate both sides of this equation
to conclude that
0=∂
∂θintegraldisplay
f(x;θ)dx=integraldisplay∂
∂θf(x;θ)dx
=integraldisplay∂f(x;θ)
∂θ
f(x;θ)f(x;θ)dx=integraldisplay∂logf(x;θ)
∂θf(x;θ)dx
=integraldisplay
s(x;θ)f(x;θ)dx=Eθs(X;θ)./squaresolid
Proof of Theorem 9.18. Let/lscript(θ) = log L(θ). Then,
0=/lscript/prime(hatwideθ)≈/lscript/prime(θ)+(hatwideθ−θ)/lscript/prime/prime(θ).
Rearrange the above equation to get hatwideθ−θ=−/lscript/prime(θ)//lscript/prime/prime(θ) or, in other words,
√n(hatwideθ−θ)=1√n/lscript/prime(θ)
−1
n/lscript/prime/prime(θ)≡TOP
BOTTOM.
LetYi=∂logf(Xi;θ)/∂θ. Recall that E(Yi) = 0 from the previous lemma
and also V(Yi)=I(θ). Hence,
TOP = n−1/2summationdisplay
iYi=√nY=√n(Y−0)/squigglerightW∼N(0,I(θ))
by the central limit theorem. Let Ai=−∂2logf(Xi;θ)/∂θ2. Then E(Ai)=
I(θ) and
BOTTOM = AP−→I(θ)
by the law of large numbers. Apply Theorem 5.5 part (e), to conclude that
√n(hatwideθ−θ)/squigglerightW
I(θ)d=Nparenleftbigg
0,1
I(θ)parenrightbigg
.
Assuming that I(θ) is a continuous function of θ, it follows that I(hatwideθn)P−→I(θ).
Now
hatwideθn−θ
hatwidese=√nI1/2(hatwideθn)(hatwideθn−θ)
=braceleftBig√nI1/2(θ)(hatwideθn−θ)bracerightBigradicalBigg
I(hatwideθn)
I(θ).

<<<PAGE 153>>>

9.13 Appendix 137
The ﬁrst term tends in distribution to N(0,1). The second term tends in
probability to 1. The result follows from Theorem 5.5 part (e). /squaresolid
Outline of Proof of Theorem 9.24. Write
hatwideτn=g(hatwideθn)≈g(θ)+(hatwideθn−θ)g/prime(θ)=τ+(hatwideθn−θ)g/prime(θ).
Thus,√n(hatwideτn−τ)≈√n(hatwideθn−θ)g/prime(θ),
and hence radicalbig
nI(θ)(hatwideτn−τ)
g/prime(θ)≈radicalbig
nI(θ)(hatwideθn−θ).
Theorem 9.18 tells us that the right-hand side tends in distribution to a N(0,1).
Hence,radicalbig
nI(θ)(hatwideτn−τ)
g/prime(θ)/squigglerightN(0,1)
or, in other words,
hatwideτn≈Nparenleftbig
τ,se2(hatwideτn)parenrightbig
,
where
se2(hatwideτn)=(g/prime(θ))2
nI(θ).
The result remains true if we substitute hatwideθnforθby Theorem 5.5 part (e). /squaresolid
9.13.2 Suﬃciency
Astatistic is a function T(Xn) of the data. A suﬃcient statistic is a statistic
that contains all the information in the data. To make this more formal, weneed some deﬁnitions.
9.32 Deﬁnition. Write xn↔yniff(xn;θ)=cf(yn;θ)for some constant
cthat might depend on xnandynbut not θ. A statistic T(xn)is
suﬃcient ifT(xn)↔T(yn)implies that xn↔yn.
Notice that if xn↔yn, then the likelihood function based on xnhas the
same shape as the likelihood function based on yn. Roughly speaking, a statis-
tic is suﬃcient if we can calculate the likelihood function knowing only T(Xn).
9.33 Example. LetX1,...,X n∼Bernoulli( p). Then L(p)=pS(1−p)n−S
where S=summationtext
iXi,s oSis suﬃcient. /squaresolid

<<<PAGE 154>>>

138 9. Parametric Inference
9.34 Example. LetX1,...,X n∼N(µ, σ) and let T=(X,S). Then
f(Xn;µ, σ)=parenleftbigg1
σ√
2πparenrightbiggn
expbraceleftbigg
−nS2
2σ2bracerightbigg
expbraceleftbigg
−n(X−µ)2
2σ2bracerightbigg
where S2is the sample variance. The last expression depends on the data
only through Tand therefore, T=(X,S) is a suﬃcient statistic. Note that
U= (17 X,S) is also a suﬃcient statistic. If I tell you the value of Uthen you
can easily ﬁgure out Tand then compute the likelihood. Suﬃcient statistics
are far from unique. Consider the following statistics for the N(µ, σ2) model:
T1(Xn)=( X1,...,X n)
T2(Xn)=( X,S)
T3(Xn)= X
T4(Xn)=( X,S,X 3).
The ﬁrst statistic is just the whole data set. This is suﬃcient. The second
is also suﬃcient as we proved above. The third is not suﬃcient: you can’tcompute L(µ, σ) if I only tell you
X. The fourth statistic T4is suﬃcient. The
statistics T1andT4are suﬃcient but they contain redundant information.
Intuitively, there is a sense in which T2is a “more concise” suﬃcient statistic
than either T1orT4. We can express this formally by noting that T2is a
function of T1and similarly, T2is a function of T4. For example, T2=g(T4)
where g(a1,a2,a3)=(a1,a2)./squaresolid
9.35 Deﬁnition. A statistic Tisminimal suﬃcient if (i) it is
suﬃcient; and (ii) it is a function of every other suﬃcient statistic.
9.36 Theorem. Tis minimal suﬃcient if the following is true:
T(xn)=T(yn) if and only if xn↔yn.
A statistic induces a partition on the set of outcomes. We can think of
suﬃciency in terms of these partitions.
9.37 Example. LetX1,X2∼Bernoulli (θ). Let V=X1,T=summationtext
iXiand
U=(T,X1). Here is the set of outcomes and the statistics:
X1X2VTU
00 0 0 (0,0)
01 0 1 (1,0)
10 1 1 (1,1)
11 1 2 (2,1)

<<<PAGE 155>>>

9.13 Appendix 139
The partitions induced by these statistics are:
V−→ { (0,0),(0,1)},{(1,0),(1,1)}
T−→ { (0,0)},{(0,1),(1,0)},{(1,1)}
U−→ { (0,0)},{(0,1)},{(1,0)},{(1,1)}.
Then Vis not suﬃcient but TandUare suﬃcient. Tis minimal suﬃcient;
Uis not minimal since if xn=( 1,0) and yn=( 0,1), then xn↔ynyet
U(xn)/negationslash=U(yn). The statistic W=1 7Tgenerates the same partition as T.I t
is also minimal suﬃcient. /squaresolid
9.38 Example. For a N(µ, σ2) model, T=(X,S) is a minimal suﬃcient
statistic. For the Bernoulli model, T=summationtext
iXiis a minimal suﬃcient statistic.
For the Poisson model, T=summationtext
iXiis a minimal suﬃcient statistic. Check that
T=(summationtext
iXi,X1) is suﬃcient but not minimal suﬃcient. Check that T=X1
is not suﬃcient. /squaresolid
I did not give the usual deﬁnition of suﬃciency. The usual deﬁnition is this:
Tis suﬃcient if the distribution of Xngiven T(Xn)=tdoes not depend on
θ. In other words, Tis suﬃcient if f(x1,...,x n|t;θ)=h(x1,...,x n,t) where
his some function that does not depend on θ.
9.39 Example. Two coin ﬂips. Let X=(X1,X2)∼Bernoulli( p). Then T=
X1+X2is suﬃcient. To see this, we need the distribution of ( X1,X2) given
T=t. Since Tcan take 3 possible values, there are 3 conditional distributions
to check. They are: (i) the distribution of ( X1,X2) given T=0 :
P(X1=0,X2=0|t=0 )=1 ,P(X1=0,X2=1|t=0 )=0 ,
P(X1=1,X2=0|t=0 )=0 ,P(X1=1,X2=1|t=0 )=0 ;
(ii) the distribution of ( X1,X2) given T=1 :
P(X1=0,X2=0|t=1 )=0 ,P(X1=0,X2=1|t=1 )=1
2,
P(X1=1,X2=0|t=1 )=1
2,P(X1=1,X2=1|t=1 )=0 ; a n d
(iii) the distribution of ( X1,X2) given T=2 :
P(X1=0,X2=0|t=2 )=0 ,P(X1=0,X2=1|t=2 )=0 ,
P(X1=1,X2=0|t=2 )=0 ,P(X1=1,X2=1|t=2 )=1 .
None of these depend on the parameter p. Thus, the distribution of X1,X2|T
does not depend on θ,s oTis suﬃcient. /squaresolid

<<<PAGE 156>>>

140 9. Parametric Inference
9.40 Theorem (Factorization Theorem) .Tis suﬃcient if and only if there are
functions g(t, θ)andh(x)such that f(xn;θ)=g(t(xn),θ)h(xn).
9.41 Example. Return to the two coin ﬂips. Let t=x1+x2. Then
f(x1,x2;θ)= f(x1;θ)f(x2;θ)
=θx1(1−θ)1−x1θx2(1−θ)1−x2
=g(t, θ)h(x1,x2)
where g(t, θ)=θt(1−θ)2−tandh(x1,x2) = 1. Therefore, T=X1+X2is
suﬃcient. /squaresolid
Now we discuss an implication of suﬃciency in point estimation. Let hatwideθbe
an estimator of θ. The Rao-Blackwell theorem says that an estimator should
only depend on the suﬃcient statistic, otherwise it can be improved. LetR(θ,hatwideθ)=E
θ(θ−hatwideθ)2denote the mseof the estimator.
9.42 Theorem (Rao-Blackwell) .Lethatwideθbe an estimator and let Tbe a suﬃcient
statistic. Deﬁne a new estimator by
tildewideθ=E(hatwideθ|T).
Then, for every θ,R(θ,tildewideθ)≤R(θ,hatwideθ).
9.43 Example. Consider ﬂipping a coin twice. Let hatwideθ=X1. This is a well-
deﬁned (and unbiased) estimator. But it is not a function of the suﬃcientstatistic T=X
1+X2. However, note that tildewideθ=E(X1|T)=(X1+X2)/2. By
the Rao-Blackwell Theorem, tildewideθhas MSE at least as small as hatwideθ=X1. The
same applies with ncoin ﬂips. Again deﬁne hatwideθ=X1andT=summationtext
iXi. Then
tildewideθ=E(X1|T)=n−1summationtext
iXihas improved mse./squaresolid
9.13.3 Exponential Families
Most of the parametric models we have studied so far are special cases of
a general class of models called exponential families. We say that {f(x;θ):
θ∈Θ}is aone-parameter exponential family if there are functions η(θ),
B(θ),T(x) and h(x) such that
f(x;θ)=h(x)eη(θ)T(x)−B(θ).
It is easy to see that T(X) is suﬃcient. We call Tthenatural suﬃcient
statistic .

<<<PAGE 157>>>

9.13 Appendix 141
9.44 Example. LetX∼Poisson( θ). Then
f(x;θ)=θxe−θ
x!=1
x!exlogθ−θ
and hence, this is an exponential family with η(θ) = log θ,B(θ)=θ,T(x)=x,
h(x)=1/x!./squaresolid
9.45 Example. LetX∼Binomial( n, θ). Then
f(x;θ)=parenleftbiggn
xparenrightbigg
θx(1−θ)n−x=parenleftbiggn
xparenrightbigg
expbraceleftbigg
xlogparenleftbiggθ
1−θparenrightbigg
+nlog(1−θ)bracerightbigg
.
In this case,
η(θ) = logparenleftbiggθ
1−θparenrightbigg
,B(θ)=−nlog(θ)
and
T(x)=x, h(x)=parenleftbiggn
xparenrightbigg
.
/squaresolid
We can rewrite an exponential family as
f(x;η)=h(x)eηT(x)−A(η)
where η=η(θ) is called the natural parameter and
A(η) = logintegraldisplay
h(x)eηT(x)dx.
For example a Poisson can be written as f(x;η)=eηx−eη/x! where the natural
parameter is η= log θ.
LetX1,...,X nbeiidfrom an exponential family. Then f(xn;θ)i sa n
exponential family:
f(xn;θ)=hn(xn)hn(xn)eη(θ)Tn(xn)−Bn(θ)
where hn(xn)=producttext
ih(xi),Tn(xn)=summationtext
iT(xi) and Bn(θ)=nB(θ). This
implies thatsummationtext
iT(Xi) is suﬃcient.
9.46 Example. LetX1,...,X n∼Uniform(0 ,θ). Then
f(xn;θ)=1
θnI(x(n)≤θ)
where Iis 1 if the term inside the brackets is true and 0 otherwise, and
x(n)=max{x1,...,x n}.T h u s T(Xn)=max{X1,...,X n}is suﬃcient. But
sinceT(Xn)/negationslash=summationtext
iT(Xi), this cannot be an exponential family. /squaresolid

<<<PAGE 158>>>

142 9. Parametric Inference
9.47 Theorem. LetXhave density in an exponential family. Then,
E(T(X)) =A/prime(η),V(T(X)) =A/prime/prime(η).
Ifθ=(θ1,...,θ k) is a vector, then we say that f(x;θ) has exponential
family form if
f(x;θ)=h(x) exp

ksummationdisplay
j=1ηj(θ)Tj(x)−B(θ)

.
Again, T=(T1,...,T k) is suﬃcient. An iidsample of size nalso has expo-
nential form with suﬃcient statistic (summationtext
iT1(Xi),...,summationtext
iTk(Xi)).
9.48 Example. Consider the normal family with θ=(µ, σ). Now,
f(x;θ) = expbraceleftbiggµ
σ2x−x2
2σ2−1
2parenleftbiggµ2
σ2+ log(2 πσ2)parenrightbiggbracerightbigg
.
This is exponential with
η1(θ)=µ
σ2,T1(x)=x
η2(θ)=−1
2σ2,T2(x)=x2
B(θ)=1
2parenleftbiggµ2
σ2+ log(2 πσ2)parenrightbigg
,h(x)=1.
Hence, with niidsamples, (summationtext
iXi,summationtext
iX2
i) is suﬃcient. /squaresolid
As before we can write an exponential family as
f(x;η)=h(x) expbraceleftbig
TT(x)η−A(η)bracerightbig
,
where A(η) = logintegraltext
h(x)eTT(x)ηdx. It can be shown that
E(T(X)) =˙A(η)V(T(X)) =¨A(η),
where the ﬁrst expression is the vector of partial derivatives and the second
is the matrix of second derivatives.
9.13.4 Computing Maximum Likelihood Estimates
In some cases we can ﬁnd the mlehatwideθanalytically. More often, we need to
ﬁnd the mleby numerical methods. We will brieﬂy discuss two commonly

<<<PAGE 159>>>

9.13 Appendix 143
used methods: (i) Newton-Raphson, and (ii) the EM algorithm. Both are
iterative methods that produce a sequence of values θ0,θ1,...that, under
ideal conditions, converge to the mlehatwideθ. In each case, it is helpful to use a
good starting value θ0. Often, the method of moments estimator is a good
starting value.
Newton-Raphson. To motivate Newton-Raphson, let’s expand the deriva-
tive of the log-likelihood around θj:
0=/lscript/prime(hatwideθ)≈/lscript/prime(θj)+(hatwideθ−θj)/lscript/prime/prime(θj).
Solving for hatwideθgives
hatwideθ≈θj−/lscript/prime(θj)
/lscript/prime/prime(θj).
This suggests the following iterative scheme:
hatwideθj+1=θj−/lscript/prime(θj)
/lscript/prime/prime(θj).
In the multiparameter case, the mle hatwideθ=(hatwideθ1,...,hatwideθk) is a vector and the
method becomes
hatwideθj+1=θj−H−1/lscript/prime(θj)
where /lscript/prime(θj) is the vector of ﬁrst derivatives and His the matrix of second
derivatives of the log-likelihood.
The EM Algorithm. The letters EM stand for Expectation-Maximization.
The idea is to iterate between taking an expectation then maximizing. Sup-pose we have data Ywhose density f(y;θ) leads to a log-likelihood that is
hard to maximize. But suppose we can ﬁnd another random variable Zsuch
thatf(y;θ)=integraltext
f(y,z;θ)dzand such that the likelihood based on f(y,z;θ)
is easy to maximize. In other words, the model of interest is the marginal of amodel with a simpler likelihood. In this case, we call Ythe observed data and
Zthe hidden (or latent or missing) data. If we could just “ﬁll in” the missing
data, we would have an easy problem. Conceptually, the EM algorithm worksby ﬁlling in the missing data, maximizing the log-likelihood, and iterating.
9.49 Example (Mixture of Normals) .Sometimes it is reasonable to assume that
the distribution of the data is a mixture of two normals. Think of heights ofpeople being a mixture of men and women’s heights. Let φ(y;µ, σ) denote
a normal density with mean µand standard deviation σ. The density of a
mixture of two Normals is
f(y;θ)=( 1 −p)φ(y;µ
0,σ0)+pφ(y;µ1,σ1).

<<<PAGE 160>>>

144 9. Parametric Inference
The idea is that an observation is drawn from the ﬁrst normal with probability
pand the second with probability 1 −p. However, we don’t know which Normal
it was drawn from. The parameters are θ=(µ0,σ0,µ1,σ1,p). The likelihood
function is
L(θ)=nproductdisplay
i=1[(1−p)φ(yi;µ0,σ0)+pφ(yi;µ1,σ1)].
Maximizing this function over the ﬁve parameters is hard. Imaging that we
were given extra information telling us which of the two normals every observa-tion came from. These “complete” data are of the form ( Y
1,Z1),...,(Yn,Zn),
where Zi= 0 represents the ﬁrst normal and Zi= 1 represents the second.
Note that P(Zi=1 )= p. We shall soon see that the likelihood for the com-
plete data ( Y1,Z1),...,(Yn,Zn) is much simpler than the likelihood for the
observed data Y1,...,Y n./squaresolid
Now we describe the EM algorithm.
The EM Algorithm
(0) Pick a starting value θ0. Now for j=1,2,...,repeat steps 1 and 2
below:(1) (The E-step): Calculate
J(θ|θ
j)=Eθjparenleftbigg
logf(Yn,Zn;θ)
f(Yn,Zn;θj)vextendsinglevextendsinglevextendsinglevextendsingleY
n=ynparenrightbigg
.
The expectation is over the missing data Zntreating θiand the observed
dataYnas ﬁxed.
(2) Find θj+1to maximize J(θ|θj).
We now show that the EM algorithm always increases the likelihood, that
is,L(θj+1)≥L(θj). Note that
J(θj+1|θj)= Eθjparenleftbigg
logf(Yn,Zn;θj+1)
f(Yn,Zn;θj)vextendsinglevextendsinglevextendsinglevextendsingleY
n=ynparenrightbigg
= logf(yn;θj+1)
f(yn;θj)+Eθjparenleftbigg
logf(Zn|Yn;θj+1)
f(Zn|Yn;θj)vextendsinglevextendsinglevextendsinglevextendsingleY
n=ynparenrightbigg
and hence
L(θj+1)
L(θj)= logf(yn;θj+1)
f(yn;θj)

<<<PAGE 161>>>

9.13 Appendix 145
=J(θj+1|θj)−Eθjparenleftbigg
logf(Zn|Yn;θj+1)
f(Zn|Yn;θj)vextendsinglevextendsinglevextendsinglevextendsingleY
n=ynparenrightbigg
=J(θj+1|θj)+K(fj,fj+1)
where fj=f(yn;θj) andfj+1=f(yn;θj+1) andK(f,g)=integraltext
f(x) log(f(x)/g(x))dx
is the Kullback-Leibler distance. Now, θj+1was chosen to maximize J(θ|θj).
Hence, J(θj+1|θj)≥J(θj|θj) = 0. Also, by the properties of Kullback-Leibler
divergence, K(fj,fj+1)≥0. Hence, L(θj+1)≥L(θj) as claimed.
9.50 Example (Continuation of Example 9.49) .Consider again the mixture of
two normals but, for simplicity assume that p=1/2,σ1=σ2= 1. The density
is
f(y;µ1,µ2)=1
2φ(y;µ0,1) +1
2φ(y;µ1,1).
Directly maximizing the likelihood is hard. Introduce latent variables Z1,...,Z n
where Zi=0i f Yiis from φ(y;µ0,1), and Zi=1i f Yiis from φ(y;µ1,1),
P(Zi=1 )= P(Zi=0 )=1 /2,f(yi|Zi=0 )= φ(y;µ0,1) and f(yi|Zi=1 )=
φ(y;µ1,1). So f(y)=summationtext1
z=0f(y,z) where we have dropped the parameters
from the density to avoid notational overload. We can write
f(z,y)=f(z)f(y|z)=1
2φ(y;µ0,1)1−zφ(y;µ1,1)z.
Hence, the complete likelihood is
nproductdisplay
i=1φ(yi;µ0,1)1−ziφ(yi;µ1,1)zi.
The complete log-likelihood is then
tildewide/lscript=−1
2nsummationdisplay
i=1(1−zi)(yi−µ0)−1
2nsummationdisplay
i=1zi(yi−µ1).
And so
J(θ|θj)=−1
2nsummationdisplay
i=1(1−E(Zi|yn,θj))(yi−µ0)−1
2nsummationdisplay
i=1E(Zi|yn,θj))(yi−µ1).
Since Ziis binary, E(Zi|yn,θj)=P(Zi=1|yn,θj) and, by Bayes’ theorem,
P(Zi=1|yn,θi)=f(yn|Zi=1 ;θj)P(Zi=1 )
f(yn|Zi=1 ;θj)P(Zi=1 )+ f(yn|Zi=0 ;θj)P(Zi=0 )
=φ(yi;µj
1,1)1
2
φ(yi;µj
1,1)1
2+φ(yi;µj
0,1)1
2
=φ(yi;µj
1,1)
φ(yi;µj
1,1) +φ(yi;µj
0,1)
=τ(i).

<<<PAGE 162>>>

146 9. Parametric Inference
Take the derivative of J(θ|θj) with respect to µ1andµ2, set them equal to 0
to get
hatwideµj+1
1=summationtextn
i=1τiyisummationtextn
i=1τi
and
hatwideµj+1
0=summationtextn
i=1(1−τi)yisummationtextn
i=1(1−τi).
We then recompute τiusinghatwideµj+1
1andhatwideµj+1
0and iterate. /squaresolid
9.14 Exercises
1. Let X1,...,X n∼Gamma( α,β). Find the method of moments estimator
forαandβ.
2. Let X1,...,X n∼Uniform( a, b) where aandbare unknown parameters
anda<b.
(a) Find the method of moments estimators for aandb.
(b) Find the mlehatwideaandhatwideb.
(c) Let τ=integraltext
xd F(x). Find the mleofτ.
(d) Lethatwideτbe the mleofτ. Lettildewideτbe the nonparametric plug-in estimator
ofτ=integraltext
xd F(x). Suppose that a=1 ,b= 3, and n= 10. Find the mse
ofhatwideτby simulation. Find the mseoftildewideτanalytically. Compare.
3. Let X1,...,X n∼N(µ, σ2). Let τbe the .95 percentile, i.e. P(X<τ )=
.95.
(a) Find the mleofτ.
(b) Find an expression for an approximate 1 −αconﬁdence interval for
τ.
(c) Suppose the data are:
3.23 -2.50 1.88 -0.68 4.43 0.17
1.03 -0.07 -0.01 0.76 1.76 3.180.33 -0.31 0.30 -0.61 1.52 5.431.54 2.28 0.42 2.33 -1.03 4.000.39
Find the mlehatwideτ. Find the standard error using the delta method. Find
the standard error using the parametric bootstrap.

<<<PAGE 163>>>

9.14 Exercises 147
4. Let X1,...,X n∼Uniform(0 ,θ). Show that the mleis consistent. Hint:
LetY= max {X1, ..., X n}. For any c,P(Y< c )=P(X1<c , X 2<
c, ..., X n<c)=P(X1<c)P(X2<c)...P(Xn<c).
5. Let X1,...,X n∼Poisson( λ). Find the method of moments estimator,
the maximum likelihood estimator and the Fisher information I(λ).
6. Let X1, ..., X n∼N(θ,1). Deﬁne
Yi=braceleftbigg
1i fXi>0
0i fXi≤0.
Letψ=P(Y1=1 ).
(a) Find the maximum likelihood estimator hatwideψofψ.
(b) Find an approximate 95 percent conﬁdence interval for ψ.
(c) Deﬁne tildewideψ=( 1/n)summationtext
iYi. Show that tildewideψis a consistent estimator of ψ.
(d) Compute the asymptotic relative eﬃciency of tildewideψtohatwideψ. Hint: Use the
delta method to get the standard error of the mle. Then compute the
standard error (i.e. the standard deviation) of tildewideψ.
(e) Suppose that the data are not really normal. Show that hatwideψis not
consistent. What, if anything, does hatwideψconverge to?
7. (Comparing two treatments.) n1people are given treatment 1 and n2
people are given treatment 2. Let X1be the number of people on treat-
ment 1 who respond favorably to the treatment and let X2be the
number of people on treatment 2 who respond favorably. Assume thatX
1∼Binomial( n1,p1)X2∼Binomial( n2,p2). Let ψ=p1−p2.
(a) Find the mlehatwideψforψ.
(b) Find the Fisher information matrix I(p1,p2).
(c) Use the multiparameter delta method to ﬁnd the asymptotic stan-
dard error of hatwideψ.
(d) Suppose that n1=n2= 200, X1= 160 and X2= 148. Find hatwideψ. Find
an approximate 90 percent conﬁdence interval for ψusing (i) the delta
method and (ii) the parametric bootstrap.
8. Find the Fisher information matrix for Example 9.29.9. Let X
1, ..., X n∼Normal( µ,1).Letθ=eµand lethatwideθ=eXbe the mle.
Create a data set (using µ= 5) consisting of n=100 observations.

<<<PAGE 164>>>

148 9. Parametric Inference
(a) Use the delta method to get hatwideseand a 95 percent conﬁdence interval
forθ. Use the parametric bootstrap to get hatwideseand 95 percent conﬁdence
interval for θ. Use the nonparametric bootstrap to get hatwideseand 95 percent
conﬁdence interval for θ.Compare your answers.
(b) Plot a histogram of the bootstrap replications for the parametric
and nonparametric bootstraps. These are estimates of the distributionofhatwideθ. The delta method also gives an approximation to this distribution
namely, Normal( hatwideθ,se
2). Compare these to the true sampling distribu-
tion ofhatwideθ(which you can get by simulation). Which approximation —
parametric bootstrap, bootstrap, or delta method — is closer to the truedistribution?
10. Let X
1, ..., X n∼Uniform(0 ,θ).The mleishatwideθ=X(n)= max {X1, ..., X n}.
Generate a dataset of size 50 with θ=1.
(a) Find the distribution of hatwideθanalytically. Compare the true distribu-
tion of hatwideθto the histograms from the parametric and nonparametric
bootstraps.
(b) This is a case where the nonparametric bootstrap does very poorly.
Show that for the parametric bootstrap P(hatwideθ∗=hatwideθ) = 0, but for the
nonparametric bootstrap P(hatwideθ∗=hatwideθ)≈.632. Hint: show that, P(hatwideθ∗=
hatwideθ)=1 −(1−(1/n))nthen take the limit as n gets large. What is the
implication of this?

<<<PAGE 165>>>

10
Hypothesis Testing and p-values
Suppose we want to know if exposure to asbestos is associated with lung
disease. We take some rats and randomly divide them into two groups. Weexpose one group to asbestos and leave the second group unexposed. Thenwe compare the disease rate in the two groups. Consider the following twohypotheses:
The Null Hypothesis : The disease rate is the same in the two groups.
The Alternative Hypothesis : The disease rate is not the same in the two
groups.
If the exposed group has a much higher rate of disease than the unexposed
group then we will reject the null hypothesis and conclude that the evidence
favors the alternative hypothesis. This is an example of hypothesis testing.
More formally, suppose that we partition the parameter space Θ into two
disjoint sets Θ
0and Θ 1and that we wish to test
H0:θ∈Θ0versus H1:θ∈Θ1. (10.1)
We call H0thenull hypothesis andH1thealternative hypothesis .
LetXbe a random variable and let Xbe the range of X. We test a hypoth-
esis by ﬁnding an appropriate subset of outcomes R⊂Xcalled the rejection

<<<PAGE 166>>>

150 10. Hypothesis Testing and p-values
Retain Null Reject Null
H0true√type I error
H1true type II error√
TABLE 10.1. Summary of outcomes of hypothesis testing.
region .I fX∈Rwe reject the null hypothesis, otherwise, we do not reject
the null hypothesis:
X∈R=⇒reject H0
X/∈R=⇒retain (do not reject) H0
Usually, the rejection region Ris of the form
R=braceleftbigg
x:T(x)>cbracerightbigg
(10.2)
where Tis atest statistic andcis acritical value . The problem in hy-
pothesis testing is to ﬁnd an appropriate test statistic Tand an appropriate
critical value c.
Warning! There is a tendency to use hypothesis testing methods even
when they are not appropriate. Often, estimation and conﬁdence intervals arebetter tools. Use hypothesis testing only when you want to test a well-deﬁnedhypothesis.
Hypothesis testing is like a legal trial. We assume someone is innocent
unless the evidence strongly suggests that he is guilty. Similarly, we retain H
0
unless there is strong evidence to reject H0. There are two types of errors we
can make. Rejecting H0when H0is true is called a type I error . Retaining
H0when H1is true is called a type II error . The possible outcomes for
hypothesis testing are summarized in Tab. 10.1.
10.1 Deﬁnition. Thepower function of a test with rejection region Ris
deﬁned by
β(θ)=Pθ(X∈R). (10.3)
Thesizeof a test is deﬁned to be
α= sup
θ∈Θ0β(θ). (10.4)
A test is said to have level αif its size is less than or equal to α.

<<<PAGE 167>>>

10. Hypothesis Testing and p-values 151
A hypothesis of the form θ=θ0is called a simple hypothesis . A hypoth-
esis of the form θ>θ 0orθ<θ 0is called a composite hypothesis . A test
of the form
H0:θ=θ0versus H1:θ/negationslash=θ0
is called a two-sided test . A test of the form
H0:θ≤θ0versus H1:θ>θ 0
or
H0:θ≥θ0versus H1:θ<θ 0
is called a one-sided test . The most common tests are two-sided.
10.2 Example. LetX1,...,X n∼N(µ, σ) where σis known. We want to test
H0:µ≤0 versus H1:µ>0. Hence, Θ 0=(−∞,0] and Θ 1=( 0,∞).
Consider the test:
reject H0ifT>c
where T=X. The rejection region is
R=braceleftbigg
(x1,...,x n):T(x1,...,x n)>cbracerightbigg
.
LetZdenote a standard Normal random variable. The power function is
β(µ)= Pµparenleftbig
X>cparenrightbig
=Pµparenleftbigg√n(X−µ)
σ>√n(c−µ)
σparenrightbigg
=Pparenleftbigg
Z>√n(c−µ)
σparenrightbigg
=1 −Φparenleftbigg√n(c−µ)
σparenrightbigg
.
This function is increasing in µ. See Figure 10.1. Hence
size = sup
µ≤0β(µ)=β( 0 )=1 −Φparenleftbigg√nc
σparenrightbigg
.
For a size αtest, we set this equal to αand solve for cto get
c=σΦ−1(1−α)√n.
We reject when X>σ Φ−1(1−α)/√n. Equivalently, we reject when
√n(X−0)
σ>zα.
where zα=Φ−1(1−α)./squaresolid

<<<PAGE 168>>>

152 10. Hypothesis Testing and p-values
α
µβ(µ)
H0 H1
FIGURE 10.1. The power function for Example 10.2. The size of the test is the
largest probability of rejecting H0when H0is true. This occurs at µ= 0 hence the
size is β(0). We choose the critical value cso that β(0) = α.
It would be desirable to ﬁnd the test with highest power under H1, among
all size αtests. Such a test, if it exists, is called most powerful . Finding
most powerful tests is hard and, in many cases, most powerful tests don’teven exist. Instead of going into detail about when most powerful tests exist,we’ll just consider four widely used tests: the Wald test,
1theχ2test, the
permutation test, and the likelihood ratio test.
10.1 The Wald Test
Letθbe a scalar parameter, let hatwideθbe an estimate of θand let hatwidesebe the
estimated standard error of hatwideθ.
1The test is named after Abraham Wald (1902–1950), who was a very inﬂuential mathe-
matical statistician. Wald died in a plane crash in India in 1950.

<<<PAGE 169>>>

10.1 The Wald Test 153
10.3 Deﬁnition. The Wald Test
Consider testing
H0:θ=θ0versus H1:θ/negationslash=θ0.
Assume that hatwideθis asymptotically Normal:
(hatwideθ−θ0)
hatwidese/squigglerightN(0,1).
The size αWald test is: reject H0when |W|>zα/2where
W=hatwideθ−θ0
hatwidese. (10.5)
10.4 Theorem. Asymptotically, the Wald test has size α, that is,
Pθ0parenleftbig
|W|>zα/2parenrightbig
→α
asn→∞.
Proof. Under θ=θ0,(hatwideθ−θ0)/hatwidese/squigglerightN(0,1). Hence, the probability of
rejecting when the null θ=θ0is true is
Pθ0parenleftbig
|W|>zα/2parenrightbig
=Pθ0parenleftBigg
|hatwideθ−θ0|
hatwidese>zα/2parenrightBigg
→Pparenleftbig
|Z|>zα/2parenrightbig
=α
where Z∼N(0,1)./squaresolid
10.5 Remark. An alternative version of the Wald test statistic is W=(hatwideθ−
θ0)/se0where se0is the standard error computed at θ=θ0. Both versions of
the test are valid.
Let us consider the power of the Wald test when the null hypothesis is false.
10.6 Theorem. Suppose the true value of θisθ⋆/negationslash=θ0. The power β(θ⋆)— the
probability of correctly rejecting the null hypothesis — is given (approximately)by
1−Φparenleftbiggθ
0−θ⋆
hatwidese+zα/2parenrightbigg
+Φparenleftbiggθ0−θ⋆
hatwidese−zα/2parenrightbigg
. (10.6)

<<<PAGE 170>>>

154 10. Hypothesis Testing and p-values
Recall that hatwidesetends to 0 as the sample size increases. Inspecting (10.6)
closely we note that: (i) the power is large if θ⋆is far from θ0, and (ii) the
power is large if the sample size is large.
10.7 Example (Comparing Two Prediction Algorithms) .We test a prediction
algorithm on a test set of size mand we test a second prediction algorithm on
a second test set of size n. LetXbe the number of incorrect predictions for
algorithm 1 and let Ybe the number of incorrect predictions for algorithm
2. Then X∼Binomial( m, p1) and Y∼Binomial( n, p2). To test the null
hypothesis that p1=p2write
H0:δ= 0 versus H1:δ/negationslash=0
where δ=p1−p2. The mleishatwideδ=hatwidep1−hatwidep2with estimated standard error
hatwidese=radicalbigg
hatwidep1(1−hatwidep1)
m+hatwidep2(1−hatwidep2)
n.
The size αWald test is to reject H0when |W|>zα/2where
W=hatwideδ−0
hatwidese=hatwidep1−hatwidep2radicalBig
/hatwidep1(1−/hatwidep1)
m+/hatwidep2(1−/hatwidep2)
n.
The power of this test will be largest when p1is far from p2and when the
sample sizes are large.
What if we used the same test set to test both algorithms? The two samples
are no longer independent. Instead we use the following strategy. Let Xi=1
if algorithm 1 is correct on test case iandXi= 0 otherwise. Let Yi=1i f
algorithm 2 is correct on test case i, andYi= 0 otherwise. Deﬁne Di=Xi−Yi.
A typical dataset will look something like this:
Test Case XiYiDi=Xi−Yi
1 10 1
2 11 0
3 11 0
4 01 - 1
5 00 0
............
n
01 - 1
Let
δ=E(Di)=E(Xi)−E(Yi)=P(Xi=1 )−P(Yi=1 ).
The nonparametric plug-in estimate of δishatwideδ=D=n−1summationtextn
i=1Diandhatwidese(hatwideδ)=
S/√n, where S2=n−1summationtextn
i=1(Di−D)2. To test H0:δ= 0 versus H1:δ/negationslash=0

<<<PAGE 171>>>

10.1 The Wald Test 155
we use W=hatwideδ/hatwideseand reject H0if|W|>z α/2. This is called a paired
comparison. /squaresolid
10.8 Example (Comparing Two Means) .LetX1,...,X mandY1,...,Ynbe
two independent samples from populations with means µ1andµ2, respec-
tively. Let’s test the null hypothesis that µ1=µ2. Write this as H0:δ=0
versus H1:δ/negationslash= 0 where δ=µ1−µ2. Recall that the nonparametric plug-in
estimate of δishatwideδ=X−Ywith estimated standard error
hatwidese=radicalbigg
s2
1
m+s2
2
n
where s2
1ands2
2are the sample variances. The size αWald test rejects H0
when |W|>zα/2where
W=hatwideδ−0
hatwidese=X−YradicalBig
s2
1
m+s22
n./squaresolid
10.9 Example (Comparing Two Medians) .Consider the previous example again
but let us test whether the medians of the two distributions are the same.Thus, H
0:δ= 0 versus H1:δ/negationslash= 0 where δ=ν1−ν2where ν1andν2are
the medians. The nonparametric plug-in estimate of δishatwideδ=hatwideν1−hatwideν2wherehatwideν1
andhatwideν2are the sample medians. The estimated standard error hatwideseofhatwideδcan be
obtained from the bootstrap. The Wald test statistic is W=hatwideδ/hatwidese./squaresolid
There is a relationship between the Wald test and the 1 −αasymptotic
conﬁdence interval hatwideθ±hatwidesezα/2.
10.10 Theorem. The size αWald test rejects H0:θ=θ0versus H1:θ/negationslash=θ0
if and only if θ0/∈Cwhere
C=(hatwideθ−hatwidesezα/2,hatwideθ+hatwidesezα/2).
Thus, testing the hypothesis is equivalent to checking whether the null value
is in the conﬁdence interval.
Warning! When we reject H0we often say that the result is statistically
signiﬁcant. A result might be statistically signiﬁcant and yet the size of the
eﬀect might be small. In such a case we have a result that is statistically sig-niﬁcant but not scientiﬁcally or practically signiﬁcant. The diﬀerence betweenstatistical signiﬁcance and scientiﬁc signiﬁcance is easy to understand in lightof Theorem 10.10. Any conﬁdence interval that excludes θ
0corresponds to re-
jecting H0. But the values in the interval could be close to θ0(not scientiﬁcally
signiﬁcant) or far from θ0(scientiﬁcally signiﬁcant). See Figure 10.2.

<<<PAGE 172>>>

156 10. Hypothesis Testing and p-values
θ0θθ0θ
FIGURE 10.2. Scientiﬁc signiﬁcance versus statistical signiﬁcance. A level αtest
rejects H0:θ=θ0if and only if the 1 −αconﬁdence interval does not include
θ0. Here are two diﬀerent conﬁdence intervals. Both exclude θ0so in both cases the
test would reject H0. But in the ﬁrst case, the estimated value of θis close to θ0so
the ﬁnding is probably of little scientiﬁc or practical value. In the second case, theestimated value of θis far from θ
0so the ﬁnding is of scientiﬁc value. This shows
two things. First, statistical signiﬁcance does not imply that a ﬁnding is of scientiﬁcimportance. Second, conﬁdence intervals are often more informative than tests.
10.2 p-values
Reporting “reject H0” or “retain H0” is not very informative. Instead, we
could ask, for every α, whether the test rejects at that level. Generally, if the
test rejects at level αit will also reject at level α/prime>α. Hence, there is a
smallest αat which the test rejects and we call this number the p-value. See
Figure 10.3.
10.11 Deﬁnition. Suppose that for every α∈(0,1)we have a size αtest
with rejection region Rα. Then,
p-value = infbraceleftbigg
α:T(Xn)∈Rαbracerightbigg
.
That is, the p-value is the smallest level at which we can reject H0.
Informally, the p-value is a measure of the evidence against H0: the smaller
the p-value, the stronger the evidence against H0. Typically, researchers use
the following evidence scale:

<<<PAGE 173>>>

10.2 p-values 157
NoYes
Reject?
α 01
p-value
FIGURE 10.3. p-values explained. For each αwe can ask: does our test reject H0
at level α? The p-value is the smallest αat which we do reject H0. If the evidence
against H0is strong, the p-value will be small.
p-value evidence
<.01 very strong evidence against H0
.01 – .05 strong evidence against H0
.05 – .10 weak evidence against H0
>.1 little or no evidence against H0
Warning! A large p-value is not strong evidence in favor of H0. A large
p-value can occur for two reasons: (i) H0is true or (ii) H0is false but the test
has low power.
Warning! Do not confuse the p-value with P(H0|Data).2The p-value is
not the probability that the null hypothesis is true.
The following result explains how to compute the p-value.
2We discuss quantities like P(H0|Data) in the chapter on Bayesian inference.

<<<PAGE 174>>>

158 10. Hypothesis Testing and p-values
10.12 Theorem. Suppose that the size αtest is of the form
reject H0if and only if T(Xn)≥cα.
Then,
p-value = sup
θ∈Θ0Pθ(T(Xn)≥T(xn))
where xnis the observed value of Xn.I fΘ0={θ0}then
p-value = Pθ0(T(Xn)≥T(xn)).
We can express Theorem 10.12 as follows:
The p-value is the probability (under H0) of observing a value of the
test statistic the same as or more extreme than what was actuallyobserved.
10.13 Theorem. Letw=(hatwideθ−θ0)/hatwidesedenote the observed value of the
Wald statistic W. The p-value is given by
p−value = Pθ0(|W|>|w|)≈P(|Z|>|w|) = 2Φ( −|w|) (10.7)
where Z∼N(0,1).
To understand this last theorem, look at Figure 10.4.
Here is an important property of p-values.
10.14 Theorem. If the test statistic has a continuous distribution, then under
H0:θ=θ0, the p-value has a Uniform (0,1) distribution. Therefore, if we
reject H0when the p-value is less than α, the probability of a type I error is
α.
In other words, if H0is true, the p-value is like a random draw from a
Unif(0 ,1) distribution. If H1is true, the distribution of the p-value will tend
to concentrate closer to 0.
10.15 Example. Recall the cholesterol data from Example 7.15. To test if the
means are diﬀerent we compute
W=hatwideδ−0
hatwidese=X−YradicalBig
s2
1
m+s22
n=216.2−195.3√
52+2.42=3.78.

<<<PAGE 175>>>

10.3 The χ2Distribution 159
|w| −|w|α/2 α/2
FIGURE 10.4. The p-value is the smallest αat which you would reject H0.T o
ﬁnd the p-value for the Wald test, we ﬁnd αsuch that |w|and−|w|are just at the
boundary of the rejection region. Here, wis the observed value of the Wald statistic:
w=(/hatwideθ−θ0)//hatwidese. This implies that the p-value is the tail area P(|Z|>|w|) where
Z∼N(0,1).
To compute the p-value, let Z∼N(0,1) denote a standard Normal random
variable. Then,
p-value = P(|Z|>3.7 8 )=2 P(Z<−3.78) = .0002
which is very strong evidence against the null hypothesis. To test if the me-
dians are diﬀerent, let hatwideν1andhatwideν2denote the sample medians. Then,
W=hatwideν1−hatwideν2
hatwidese=212.5−194
7.7=2.4
where the standard error 7.7 was found using the bootstrap. The p-value is
p-value = P(|Z|>2.4 )=2 P(Z<−2.4) =.02
which is strong evidence against the null hypothesis. /squaresolid
10.3 The χ2Distribution
Before proceeding we need to discuss the χ2distribution. Let Z1,...,Z kbe
independent, standard Normals. Let V=summationtextk
i=1Z2
i. Then we say that Vhas
aχ2distribution with kdegrees of freedom, written V∼χ2
k. The probability
density of Vis
f(v)=v(k/2)−1e−v/2
2k/2Γ(k/2)
forv>0. It can be shown that E(V)=kandV(V)=2k. We deﬁne the upper
αquantile χ2
k,α=F−1(1−α) where Fis the cdf. That is, P(χ2
k>χ2k,α)=α.

<<<PAGE 176>>>

160 10. Hypothesis Testing and p-values
tα
FIGURE 10.5. The p-value is the smallest αat which we would reject H0.T oﬁ n d
the p-value for the χ2
k−1test, we ﬁnd αsuch that the observed value tof the test
statistic is just at the boundary of the rejection region. This implies that the p-valueis the tail area P(χ
2
k−1>t).
10.4 Pearson’s χ2Test For Multinomial Data
Pearson’s χ2test is used for multinomial data. Recall that if X=(X1,...,X k)
has a multinomial ( n, p) distribution, then the mleofpishatwidep=(hatwidep1,...,hatwidepk)=
(X1/ n ,...,X k/n).
Letp0=(p01,...,p 0k) be some ﬁxed vector and suppose we want to test
H0:p=p0versus H1:p/negationslash=p0.
10.16 Deﬁnition. Pearson’s χ2statistic is
T=ksummationdisplay
j=1(Xj−np0j)2
np0j=ksummationdisplay
j=1(Xj−Ej)2
Ej
where Ej=E(Xj)=np0jis the expected value of Xjunder H0.
10.17 Theorem. Under H0,T/squigglerightχ2
k−1. Hence the test: reject H0ifT>
χ2
k−1,αhas asymptotic level α. The p-value is P(χ2
k−1>t)where tis the
observed value of the test statistic.
Theorem 10.17 is illustrated in Figure 10.5.

<<<PAGE 177>>>

10.5 The Permutation Test 161
10.18 Example (Mendel’s peas) .Mendel bred peas with round yellow seeds
and wrinkled green seeds. There are four types of progeny: round yellow,wrinkled yellow, round green, and wrinkled green. The number of each typeis multinomial with probability p=(p
1,p2,p3,p4). His theory of inheritance
predicts that pis equal to
p0≡parenleftbigg9
16,3
16,3
16,1
16parenrightbigg
.
Inn= 556 trials he observed X= (315 ,101,108,32). We will test H0:p=p0
versus H1:p/negationslash=p0. Since, np01= 312 .75,n p02=np03= 104 .25, and np04=
34.75, the test statistic is
χ2=(315−312.75)2
312.75+(101−104.25)2
104.25
+(108−104.25)2
104.25+(32−34.75)2
34.75=0.47.
Theα=.05 value for a χ2
3is 7.815. Since 0 .47 is not larger than 7.815 we do
not reject the null. The p-value is
p-value = P(χ2
3>.47) = .93
which is not evidence against H0. Hence, the data do not contradict Mendel’s
theory.3/squaresolid
In the previous example, one could argue that hypothesis testing is not the
right tool. Hypothesis testing is useful to see if there is evidence to reject H0.
This is appropriate when H0corresponds to the status quo. It is not useful for
proving that H0is true. Failure to reject H0might occur because H0is true,
but it might occur just because the test has low power. Perhaps a conﬁdenceset for the distance between pandp
0might be more useful in this example.
10.5 The Permutation Test
The permutation test is a nonparametric method for testing whether two
distributions are the same. This test is “exact,” meaning that it is not basedon large sample theory approximations. Suppose that X
1,...,Xm∼FXand
Y1,...,Yn∼FYare two independent samples and H0is the hypothesis that
3There is some controversy about whether Mendel’s results are “too good.”

<<<PAGE 178>>>

162 10. Hypothesis Testing and p-values
the two samples are identically distributed. This is the type of hypothesis we
would consider when testing whether a treatment diﬀers from a placebo. Moreprecisely we are testing
H
0:FX=FYversus H1:FX/negationslash=FY.
LetT(x1,...,x m,y1,...,y n) be some test statistic, for example,
T(X1,...,X m,Y1,...,Y n)=|Xm−Yn|.
LetN=m+nand consider forming all N! permutations of the data X1,...,
Xm,Y1,...,Yn. For each permutation, compute the test statistic T. Denote
these values by T1,...,T N!. Under the null hypothesis, each of these values is
equally likely.4The distribution P0that puts mass 1 /N! on each Tjis called
thepermutation distribution ofT. Lettobsbe the observed value of the
test statistic. Assuming we reject when Tis large, the p-value is
p-value = P0(T>t obs)=1
N!N!summationdisplay
j=1I(Tj>tobs).
10.19 Example. Here is a toy example to make the idea clear. Suppose the
data are: ( X1,X2,Y1)=( 1 ,9,3). Let T(X1,X2,Y1)=|X−Y|= 2. The
permutations are:
permutation value of Tprobability
(1,9,3) 2 1/6
(9,1,3) 2 1/6(1,3,9) 7 1/6(3,1,9) 7 1/6(3,9,1) 5 1/6(9,3,1) 5 1/6
The p-value is P(T>2 )=4 /6.
/squaresolid
Usually, it is not practical to evaluate all N! permutations. We can approx-
imate the p-value by sampling randomly from the set of permutations. Thefraction of times T
j>tobsamong these samples approximates the p-value.
4More precisely, under the null hypothesis, given the ordered data values,
X1,...,X m,Y1,...,Y nis uniformly distributed over the N!permutations of the data.

<<<PAGE 179>>>

10.5 The Permutation Test 163
Algorithm for Permutation Test
1. Compute the observed value of the test statistic
tobs=T(X1,...,X m,Y1,...,Y n).
2. Randomly permute the data. Compute the statistic again using the
permuted data.
3. Repeat the previous step Btimes and let T1,...,T Bdenote the
resulting values.
4. The approximate p-value is
1
BBsummationdisplay
j=1I(Tj>tobs).
10.20 Example. DNA microarrays allow researchers to measure the expres-
sion levels of thousands of genes. The data are the levels of messenger RNA(mRNA) of each gene, which is thought to provide a measure of how muchprotein that gene produces. Roughly, the larger the number, the more activethe gene. The table below, reproduced from Efron et al. (2001) shows theexpression levels for genes from ten patients with two types of liver cancercells. There are 2,638 genes in this experiment but here we show just the ﬁrsttwo. The data are log-ratios of the intensity levels of two diﬀerent color dyesused on the arrays.
Type I Type II
Patient 1 2 3 4 5 678 9 1 0
Gene 1 230 -1,350 -1,580 -400 -760 970 110 -50 -190 -200
Gene 2 470 -850 -.8 -280 120 390 -1730 -1360 -1 -330
..................
...............
Let’s test whether the median level of gene 1 is diﬀerent between the two
groups. Let ν1denote the median level of gene 1 of Type I and let ν2denote the
median level of gene 1 of Type II. The absolute diﬀerence of sample mediansisT=|hatwideν
1−hatwideν2|= 710. Now we estimate the permutation distribution by
simulation and we ﬁnd that the estimated p-value is .045. Thus, if we use aα=.05 level of signiﬁcance, we would say that there is evidence to reject the
null hypothesis of no diﬀerence.
/squaresolid

<<<PAGE 180>>>

164 10. Hypothesis Testing and p-values
In large samples, the permutation test usually gives similar results to a test
that is based on large sample theory. The permutation test is thus most usefulfor small samples.
10.6 The Likelihood Ratio Test
The Wald test is useful for testing a scalar parameter. The likelihood ratiotest is more general and can be used for testing a vector-valued parameter.
10.21 Deﬁnition. Consider testing
H0:θ∈Θ0versus H1:θ/∈Θ0.
Thelikelihood ratio statistic is
λ= 2 logparenleftbiggsupθ∈ΘL(θ)
supθ∈Θ0L(θ)parenrightbigg
= 2 logparenleftBigg
L(hatwideθ)
L(hatwideθ0)parenrightBigg
wherehatwideθis the mleandhatwideθ0is the mlewhen θis restricted to lie in Θ0.
You might have expected to see the maximum of the likelihood over Θc
0
instead of Θ in the numerator. In practice, replacing Θc0with Θ has little
eﬀect on the test statistic. Moreover, the theoretical properties of λare much
simpler if the test statistic is deﬁned this way.
The likelihood ratio test is most useful when Θ 0consists of all parameter
values θsuch that some coordinates of θare ﬁxed at particular values.
10.22 Theorem. Suppose that θ=(θ1,...,θ q,θq+1,...,θ r).L e t
Θ0={θ:(θq+1,...,θ r)=(θ0,q+1,...,θ 0,r)}.
Letλbe the likelihood ratio test statistic. Under H0:θ∈Θ0,
λ(xn)/squigglerightχ2
r−q,α
where r−qis the dimension of Θminus the dimension of Θ0. The p-value
for the test is P(χ2
r−q>λ).
For example, if θ=(θ1,θ2,θ3,θ4,θ5) and we want to test the null hypothesis
thatθ4=θ5= 0 then the limiting distribution has 5 −3 = 2 degrees of
freedom.

<<<PAGE 181>>>

10.7 Multiple Testing 165
10.23 Example (Mendel’s Peas Revisited) .Consider example 10.18 again. The
likelihood ratio test statistic for H0:p=p0versus H1:p/negationslash=p0is
λ= 2 logparenleftbiggL(hatwidep)
L(p0)parenrightbigg
=24summationdisplay
j=1Xjlogparenleftbigghatwidepj
p0jparenrightbigg
=2parenleftbigg
315 logparenleftbigg315
556
9
16parenrightbigg
+ 101 logparenleftbigg101
556
3
16parenrightbigg
+108 logparenleftbigg108
556
3
16parenrightbigg
+ 32 logparenleftbigg32
556
1
16parenrightbiggparenrightbigg
=0.48.
Under H1there are four parameters. However, the parameters must sum to
one so the dimension of the parameter space is three. Under H0there are no
free parameters so the dimension of the restricted parameter space is zero. Thediﬀerence of these two dimensions is three. Therefore, the limiting distributionofλunder H
0isχ2
3and the p-value is
p-value = P(χ2
3>.48) = .92.
The conclusion is the same as with the χ2test. /squaresolid
When the likelihood ratio test and the χ2test are both applicable, as in the
last example, they usually lead to similar results as long as the sample size islarge.
10.7 Multiple Testing
In some situations we may conduct many hypothesis tests. In example 10.20,there were actually 2,638 genes. If we tested for a diﬀerence for each gene,we would be conducting 2,638 separate hypothesis tests. Suppose each testis conducted at level α. For any one test, the chance of a false rejection of
the null is α. But the chance of at least one false rejection is much higher.
This is the multiple testing problem. The problem comes up in many data
mining situations where one may end up testing thousands or even millions ofhypotheses. There are many ways to deal with this problem. Here we discusstwo methods.

<<<PAGE 182>>>

166 10. Hypothesis Testing and p-values
Consider mhypothesis tests:
H0iversus H1i,i=1,...,m
and let P1,...,P mdenote the mp-values for these tests.
The Bonferroni Method
Given p-values P1,...,P m, reject null hypothesis H0iif
Pi<α
m.
10.24 Theorem. Using the Bonferroni method, the probability of falsely re-
jecting any null hypotheses is less than or equal to α.
Proof. LetRbe the event that at least one null hypothesis is falsely
rejected. Let Ribe the event that the ithnull hypothesis is falsely rejected.
Recall that if A1,...,A kare events then P(uniontextk
i=1Ai)≤summationtextk
i=1P(Ai). Hence,
P(R)=PparenleftBiggmuniondisplay
i=1RiparenrightBigg
≤msummationdisplay
i=1P(Ri)=msummationdisplay
i=1α
m=α
from Theorem 10.14. /squaresolid
10.25 Example. In the gene example, using α=.05, we have that .05/2,638 =
.00001895375. Hence, for any gene with p-value less than .00001895375, we
declare that there is a signiﬁcant diﬀerence. /squaresolid
The Bonferroni method is very conservative because it is trying to make
it unlikely that you would make even one false rejection. Sometimes, a morereasonable idea is to control the false discovery rate (FDR) which is de-
ﬁned as the mean of the number of false rejections divided by the number ofrejections.
Suppose we reject all null hypotheses whose p-values fall below some thresh-
old. Let m
0be the number of null hypotheses that are true and let m1=
m−m0. The tests can be categorized in a 2 ×2 as in Table 10.2.
Deﬁne the False Discovery Proportion (FDP)
FDP =braceleftbiggV/R ifR>0
0i f R=0.
The FDP is the proportion of rejections that are incorrect. Next deﬁne FDR =
E(FDP).

<<<PAGE 183>>>

10.7 Multiple Testing 167
H0Not Rejected H0Rejected Total
H0True U V m0
H0False T S m1
Total m−R R m
TABLE 10.2. Types of outcomes in multiple testing.
The Benjamini-Hochberg (BH) Method
1. Let P(1)<···<P(m)denote the ordered p-values.
2. Deﬁne
/lscripti=iα
Cmm,and R= maxbraceleftbigg
i:P(i)</lscriptibracerightbigg
(10.8)
where Cmis deﬁned to be 1 if the p-values are independent and
Cm=summationtextm
i=1(1/i) otherwise.
3. Let T=P(R); we call TtheBH rejection threshold .
4. Reject all null hypotheses H0ifor which Pi≤T.
10.26 Theorem (Benjamini and Hochberg) .If the procedure above is applied,
then regardless of how many nulls are true and regardless of the distributionof the p-values when the null hypothesis is false,
FDR = E(FDP) ≤m
0
mα≤α.
10.27 Example. Figure 10.6 shows six ordered p-values plotted as vertical
lines. If we tested at level αwithout doing any correction for multiple testing,
we would reject all hypotheses whose p-values are less than α. In this case,
the four hypotheses corresponding to the four smallest p-values are rejected.The Bonferroni method rejects all hypotheses whose p-values are less thanα/m. In this case, this leads to no rejections. The BH threshold corresponds
to the last p-value that falls under the line with slope α. This leads to two
hypotheses being rejected in this case.
/squaresolid
10.28 Example. Suppose that 10 independent hypothesis tests are carried
leading to the following ordered p-values:
0.00017 0.00448 0.00671 0.00907 0.01220
0.33626 0.39341 0.53882 0.58125 0.98617

<<<PAGE 184>>>

168 10. Hypothesis Testing and p-values
/Bullet/Circle
/Bullet/Circle
/Bullet/Circle
/Bullet/Circle
/Bullet/Circle
/Bullet/Circle
Thresholdreject don’t rejectα
T
α/m
FIGURE 10.6. The Benjamini-Hochberg (BH) procedure. For uncorrected testing
we reject when Pi<α. For Bonferroni testing we reject when Pi<α / m . The BH
procedure rejects when Pi≤T. The BH threshold Tcorresponds to the rightmost
undercrossing of the upward sloping line.
With α=0.05, the Bonferroni test rejects any hypothesis whose p-value is
less than α/1 0=0 .005. Thus, only the ﬁrst two hypotheses are rejected. For
the BH test, we ﬁnd the largest isuch that P(i)<i α / m , which in this case is
i= 5. Thus we reject the ﬁrst ﬁve hypotheses. /squaresolid
10.8 Goodness-of-ﬁt Tests
There is another situation where testing arises, namely, when we want to check
whether the data come from an assumed parametric model. There are manysuch tests; here is one.
LetF={f(x;θ):θ∈Θ}be a parametric model. Suppose the data take
values on the real line. Divide the line into kdisjoint intervals I
1,...,I k.F o r
j=1,...,k , let
pj(θ)=integraldisplay
Ijf(x;θ)dx
be the probability that an observation falls into interval Ijunder the assumed
model. Here, θ=(θ1,...,θ s) are the parameters in the assumed model. Let
Njbe the number of observations that fall into Ij. The likelihood for θbased

<<<PAGE 185>>>

10.9 Bibliographic Remarks 169
on the counts N1,...,N kis the multinomial likelihood
Q(θ)=kproductdisplay
j=1pi(θ)Nj.
Maximizing Q(θ) yields estimates tildewideθ=(tildewideθ1,...,tildewideθs)o fθ. Now deﬁne the test
statistic
Q=ksummationdisplay
j=1(Nj−npj(tildewideθ))2
npj(tildewideθ). (10.9)
10.29 Theorem. LetH0be the null hypothesis that the data are iiddraws from
the model F={f(x;θ):θ∈Θ}. Under H−0, the statistic Qdeﬁned in
equation (10.9) converges in distribution to a χ2
k−1−srandom variable. Thus,
the (approximate) p-value for the test is P(χ2
k−1−s>q)where qdenotes the
observed value of Q.
It is tempting to replace tildewideθin (10.9) with the mlehatwideθ. However, this will not
result in a statistic whose limiting distribution is a χ2
k−1−s. However, it can
be shown — due to a theorem of Herman Chernoﬀ and Erich Lehmann from1954 — that the p-value is bounded approximately by the p-values obtainedusing a χ
2
k−1−sand a χ2
k−1.
Goodness-of-ﬁt testing has some serious limitations. If reject H0then we
conclude we should not use the model. But if we do not reject H0we can-
not conclude that the model is correct. We may have failed to reject simplybecause the test did not have enough power. This is why it is better to usenonparametric methods whenever possible rather than relying on parametricassumptions.
10.9 Bibliographic Remarks
The most complete book on testing is Lehmann (1986). See also Chapter 8 ofCasella and Berger (2002) and Chapter 9 of Rice (1995). The FDR method isdue to Benjamini and Hochberg (1995). Some of the exercises are from Rice(1995).

<<<PAGE 186>>>

170 10. Hypothesis Testing and p-values
10.10 Appendix
10.10.1 The Neyman-Pearson Lemma
In the special case of a simple null H0:θ=θ0and a simple alternative
H1:θ=θ1we can say precisely what the most powerful test is.
10.30 Theorem (Neyman-Pearson) .Suppose we test H0:θ=θ0versus H1:
θ=θ1.L e t
T=L(θ1)
L(θ0)=producttextn
i=1f(xi;θ1)producttextn
i=1f(xi;θ0).
Suppose we reject H0when T>k . If we choose kso that Pθ0(T>k )=α
then this test is the most powerful, size αtest. That is, among all tests with
sizeα, this test maximizes the power β(θ1).
10.10.2 The t-test
To test H0:µ=µ0where µ=E(Xi) is the mean, we can use the Wald test.
When the data are assumed to be Normal and the sample size is small, it iscommon instead to use the t-test . A random variable Thas at-distribution
with k degrees of freedom if it has density
f(t)=Γparenleftbig
k+1
2parenrightbig
√
kπΓparenleftbigk
2parenrightbigparenleftbig
1+t2
kparenrightbig(k+1)/2.
When the degrees of freedom k→∞, this tends to a Normal distribution.
When k= 1 it reduces to a Cauchy.
LetX1,...,X n∼N(µ, σ2) where θ=(µ, σ2) are both unknown. Suppose
we want to test µ=µ0versus µ/negationslash=µ0. Let
T=√n(Xn−µ0)
Sn
where S2
nis the sample variance. For large samples T≈N(0,1) under H0.
The exact distribution of Tunder H0istn−1. Hence if we reject when |T|>
tn−1,α/2then we get a size αtest. However, when nis moderately large, the
t-test is essentially identical to the Wald test.
10.11 Exercises
1. Prove Theorem 10.6.

<<<PAGE 187>>>

10.11 Exercises 171
2. Prove Theorem 10.14.
3. Prove Theorem 10.10.4. Prove Theorem 10.12.5. Let X
1, ..., X n∼Uniform(0 ,θ) and let Y= max {X1, ..., X n}.W ew a n t
to test
H0:θ=1/2 versus H1:θ>1/2.
The Wald test is not appropriate since Ydoes not converge to a Normal.
Suppose we decide to test this hypothesis by rejecting H0when Y> c .
(a) Find the power function.(b) What choice of cwill make the size of the test .05?
(c) In a sample of size n= 20 with Y=0.48 what is the p-value? What
conclusion about H
0would you make?
(d) In a sample of size n= 20 with Y=0.52 what is the p-value? What
conclusion about H0would you make?
6. There is a theory that people can postpone their death until after an
important event. To test the theory, Phillips and King (1988) collecteddata on deaths around the Jewish holiday Passover. Of 1919 deaths, 922died the week before the holiday and 997 died the week after. Think ofthis as a binomial and test the null hypothesis that θ=1/2.Report and
interpret the p-value. Also construct a conﬁdence interval for θ.
7. In 1861, 10 essays appeared in the New Orleans Daily Crescent . They
were signed “Quintus Curtius Snodgrass” and some people suspectedthey were actually written by Mark Twain. To investigate this, we willconsider the proportion of three letter words found in an author’s work.From eight Twain essays we have:
.225 .262 .217 .240 .230 .229 .235 .217From 10 Snodgrass essays we have:.209 .205 .196 .210 .202 .207 .224 .223 .220 .201(a) Perform a Wald test for equality of the means. Use the nonparamet-
ric plug-in estimator. Report the p-value and a 95 per cent conﬁdenceinterval for the diﬀerence of means. What do you conclude?
(b) Now use a permutation test to avoid the use of large sample methods.
What is your conclusion? (Brinegar (1963)).

<<<PAGE 188>>>

172 10. Hypothesis Testing and p-values
8. Let X1,...,X n∼N(θ,1). Consider testing
H0:θ= 0 versus θ=1.
Let the rejection region be R={xn:T(xn)>c}where T(xn)=
n−1summationtextn
i=1Xi.
(a) Find cso that the test has size α.
(b) Find the power under H1, that is, ﬁnd β(1).
(c) Show that β(1)→1a sn→∞.
9. Lethatwideθbe the mleof a parameter θand lethatwidese={nI(hatwideθ)}−1/2where I(θ)
is the Fisher information. Consider testing
H0:θ=θ0versus θ/negationslash=θ0.
Consider the Wald test with rejection region R={xn:|Z|>zα/2}
where Z=(hatwideθ−θ0)/hatwidese. Let θ1>θ0be some alternative. Show that
β(θ1)→1.
10. Here are the number of elderly Jewish and Chinese women who died
just before and after the Chinese Harvest Moon Festival.
Week Chinese Jewish
-2 55 141
-1 33 145
1 70 139
2 49 161
Compare the two mortality patterns. (Phillips and Smith (1990)).
11. A randomized, double-blind experiment was conducted to assess the
eﬀectiveness of several drugs for reducing postoperative nausea. Thedata are as follows.
Number of Patients Incidence of Nausea
Placebo 80 45
Chlorpromazine 75 26
Dimenhydrinate 85 52
Pentobarbital (100 mg) 67 35
Pentobarbital (150 mg) 85 37

<<<PAGE 189>>>

10.11 Exercises 173
(a) Test each drug versus the placebo at the 5 per cent level. Also, report
the estimated odds–ratios. Summarize your ﬁndings.
(b) Use the Bonferroni and the FDR method to adjust for multiple
testing. (Beecher (1959)).
12. Let X1, ..., X n∼Poisson( λ).
(a) Let λ0>0. Find the size αWald test for
H0:λ=λ0versus H1:λ/negationslash=λ0.
(b) (Computer Experiment. ) Let λ0=1 ,n= 20 and α=.05. Simulate
X1,...,X n∼Poisson( λ0) and perform the Wald test. Repeat many
times and count how often you reject the null. How close is the type Ierror rate to .05?
13. Let X
1,...,X n∼N(µ, σ2). Construct the likelihood ratio test for
H0:µ=µ0versus H1:µ/negationslash=µ0.
Compare to the Wald test.
14. Let X1,...,X n∼N(µ, σ2). Construct the likelihood ratio test for
H0:σ=σ0versus H1:σ/negationslash=σ0.
Compare to the Wald test.
15. Let X∼Binomial( n, p). Construct the likelihood ratio test for
H0:p=p0versus H1:p/negationslash=p0.
Compare to the Wald test.
16. Let θbe a scalar parameter and suppose we test
H0:θ=θ0versus H1:θ/negationslash=θ0.
LetWbe the Wald test statistic and let λbe the likelihood ratio test
statistic. Show that these tests are equivalent in the sense that
W2
λP−→1
asn→∞. Hint: Use a Taylor expansion of the log-likelihood /lscript(θ)t o
show that
λ≈parenleftbigg√n(hatwideθ−θ0)parenrightbigg2parenleftbigg
−1
n/lscript/prime/prime(hatwideθ)parenrightbigg
.

<<<PAGE 190>>>



<<<PAGE 191>>>

11
Bayesian Inference
11.1 The Bayesian Philosophy
The statistical methods that we have discussed so far are known as frequen-
tist (or classical) methods. The frequentist point of view is based on the
following postulates:
F1 Probability refers to limiting relative frequencies. Probabilities are ob-
jective properties of the real world.
F2 Parameters are ﬁxed, unknown constants. Because they are not ﬂuctu-
ating, no useful probability statements can be made about parameters.
F3 Statistical procedures should be designed to have well-deﬁned long run
frequency properties. For example, a 95 percent conﬁdence interval should
trap the true value of the parameter with limiting frequency at least 95percent.
There is another approach to inference called Bayesian inference. The
Bayesian approach is based on the following postulates:

<<<PAGE 192>>>

176 11. Bayesian Inference
B1 Probability describes degree of belief, not limiting frequency. As such,
we can make probability statements about lots of things, not just datawhich are subject to random variation. For example, I might say that“the probability that Albert Einstein drank a cup of tea on August 1,1948” is .35. This does not refer to any limiting frequency. It reﬂects mystrength of belief that the proposition is true.
B2 We can make probability statements about parameters, even though
they are ﬁxed constants.
B3 We make inferences about a parameter θby producing a probability
distribution for θ. Inferences, such as point estimates and interval esti-
mates, may then be extracted from this distribution.
Bayesian inference is a controversial approach because it inherently em-
braces a subjective notion of probability. In general, Bayesian methods pro-vide no guarantees on long run performance. The ﬁeld of statistics puts moreemphasis on frequentist methods although Bayesian methods certainly havea presence. Certain data mining and machine learning communities seem toembrace Bayesian methods very strongly. Let’s put aside philosophical ar-guments for now and see how Bayesian inference is done. We’ll conclude thischapter with some discussion on the strengths and weaknesses of the Bayesianapproach.
11.2 The Bayesian Method
Bayesian inference is usually carried out in the following way.
1. We choose a probability density f(θ) — called the prior distribution
— that expresses our beliefs about a parameter θbefore we see any
data.
2. We choose a statistical model f(x|θ) that reﬂects our beliefs about x
given θ. Notice that we now write this as f(x|θ) instead of f(x;θ).
3. After observing data X1,...,X n, we update our beliefs and calculate
theposterior distribution f(θ|X1,...,X n).
To see how the third step is carried out, ﬁrst suppose that θis discrete and
that there is a single, discrete observation X. We should use a capital letter

<<<PAGE 193>>>

11.2 The Bayesian Method 177
now to denote the parameter since we are treating it like a random variable,
so let Θ denote the parameter. Now, in this discrete setting,
P(Θ = θ|X=x)=P(X=x,Θ=θ)
P(X=x)
=P(X=x|Θ=θ)P(Θ = θ)summationtext
θP(X=x|Θ=θ)P(Θ = θ)
which you may recognize from Chapter 1 as Bayes’ theorem. The version
for continuous variables is obtained by using density functions:
f(θ|x)=f(x|θ)f(θ)integraltext
f(x|θ)f(θ)dθ. (11.1)
If we have niidobservations X1,...,X n, we replace f(x|θ) with
f(x1,...,x n|θ)=nproductdisplay
i=1f(xi|θ)=Ln(θ).
Notation. We will write Xnto mean ( X1,...,X n) andxnto mean ( x1,...,x n).
Now,
f(θ|xn)=f(xn|θ)f(θ)integraltext
f(xn|θ)f(θ)dθ=Ln(θ)f(θ)
cn∝Ln(θ)f(θ) (11.2)
where
cn=integraldisplay
Ln(θ)f(θ)dθ (11.3)
is called the normalizing constant . Note that cndoes not depend on θ.W e
can summarize by writing:
Posterior is proportional to Likelihood times Prior
or, in symbols,
f(θ|xn)∝L(θ)f(θ).
You might wonder, doesn’t it cause a problem to throw away the constant
cn? The answer is that we can always recover the constant later if we need to.
What do we do with the posterior distribution? First, we can get a point
estimate by summarizing the center of the posterior. Typically, we use themean or mode of the posterior. The posterior mean is
θn=integraldisplay
θf(θ|xn)dθ=integraltext
θLn(θ)f(θ)integraltext
Ln(θ)f(θ)dθ. (11.4)

<<<PAGE 194>>>

178 11. Bayesian Inference
We can also obtain a Bayesian interval estimate. We ﬁnd aandbsuch thatintegraltexta
−∞f(θ|xn)dθ=integraltext∞
bf(θ|xn)dθ=α/2. Let C=(a, b). Then
P(θ∈C|xn)=integraldisplayb
af(θ|xn)dθ=1−α
soCis a1−αposterior interval.
11.1 Example. LetX1,...,X n∼Bernoulli( p). Suppose we take the uniform
distribution f(p) = 1 as a prior. By Bayes’ theorem, the posterior has the
form
f(p|xn)∝f(p)Ln(p)=ps(1−p)n−s=ps+1−1(1−p)n−s+1−1
where s=summationtextn
i=1xiis the number of successes. Recall that a random variable
has a Beta distribution with parameters αandβif its density is
f(p;α,β)=Γ(α+β)
Γ(α)Γ(β)pα−1(1−p)β−1.
We see that the posterior for pis a Beta distribution with parameters s+1
andn−s+ 1. That is,
f(p|xn)=Γ(n+2 )
Γ(s+ 1)Γ( n−s+1 )p(s+1)−1(1−p)(n−s+1)−1.
We write this as
p|xn∼Beta(s+1,n−s+1 ).
Notice that we have ﬁgured out the normalizing constant without actually
doing the integralintegraltext
Ln(p)f(p)dp. The mean of a Beta( α,β) distribution is
α/(α+β) so the Bayes estimator is
p=s+1
n+2. (11.5)
It is instructive to rewrite the estimator as
p=λnhatwidep+( 1−λn)tildewidep (11.6)
wherehatwidep=s/nis the mle,tildewidep=1/2 is the prior mean and λn=n/(n+2)≈1.
A 95 percent posterior interval can be obtained by numerically ﬁnding aand
bsuch thatintegraltextb
af(p|xn)dp=.95.
Suppose that instead of a uniform prior, we use the prior p∼Beta(α,β).
If you repeat the calculations above, you will see that p|xn∼Beta(α+s, β+

<<<PAGE 195>>>

11.2 The Bayesian Method 179
n−s). The ﬂat prior is just the special case with α=β= 1. The posterior
mean is
p=α+s
α+β+n=parenleftbiggn
α+β+nparenrightbigg
hatwidep+parenleftbiggα+β
α+β+nparenrightbigg
p0
where p0=α/(α+β) is the prior mean. /squaresolid
In the previous example, the prior was a Beta distribution and the posterior
was a Beta distribution. When the prior and the posterior are in the samefamily, we say that the prior is conjugate with respect to the model.
11.2 Example. LetX
1,...,X n∼N(θ,σ2). For simplicity, let us assume that
σis known. Suppose we take as a prior θ∼N(a, b2). In problem 1 in the
exercises it is shown that the posterior for θis
θ|Xn∼N(θ,τ2) (11.7)
where
θ=wX+( 1−w)a,
w=1se2
1se2+1
b2,1
τ2=1
se2+1
b2,
andse=σ/√nis the standard error of the mleX. This is another example
of a conjugate prior. Note that w→1 and τ/se→1a sn→∞. So, for large
n, the posterior is approximately N(hatwideθ,se2). The same is true if nis ﬁxed but
b→∞, which corresponds to letting the prior become very ﬂat.
Continuing with this example, let us ﬁnd C=(c, d) such that P(θ∈
C|Xn)=.95. We can do this by choosing canddsuch that P(θ<c|Xn)=
.025 and P(θ>d|Xn)=.025. So, we want to ﬁnd csuch that
P(θ<c|Xn)= PparenleftBigg
θ−θ
τ<c−θ
τvextendsinglevextendsinglevextendsinglevextendsinglevextendsingleX
nparenrightBigg
=Pparenleftbigg
Z<c−θ
τparenrightbigg
=.025.
We know that P(Z<−1.96) = .025. So,
c−θ
τ=−1.96
implying that c=θ−1.96τ.By similar arguments, d=θ+1.96.So a 95 percent
Bayesian interval is θ±1.96τ. Since θ≈hatwideθandτ≈se, the 95 percent Bayesian
interval is approximated by hatwideθ±1.96sewhich is the frequentist conﬁdence
interval. /squaresolid

<<<PAGE 196>>>

180 11. Bayesian Inference
11.3 Functions of Parameters
How do we make inferences about a function τ=g(θ)? Remember in Chapter
3 we solved the following problem: given the density fXforX, ﬁnd the density
forY=g(X). We now simply apply the same reasoning. The posterior cdf
forτis
H(τ|xn)=P(g(θ)≤τ|xn)=integraldisplay
Af(θ|xn)dθ
where A={θ:g(θ)≤τ}. The posterior density is h(τ|xn)=H/prime(τ|xn).
11.3 Example. LetX1,...,X n∼Bernoulli( p) and f(p) = 1 so that p|Xn∼
Beta(s+1,n−s+ 1) with s=summationtextn
i=1xi. Letψ= log( p/(1−p)). Then
H(ψ|xn)= P(Ψ≤ψ|xn)=PparenleftBigg
logparenleftbiggP
1−Pparenrightbigg
≤ψvextendsinglevextendsinglevextendsinglevextendsinglevextendsinglex
nparenrightBigg
=PparenleftBigg
P≤eψ
1+eψvextendsinglevextendsinglevextendsinglevextendsinglevextendsinglex
nparenrightBigg
=integraldisplayeψ/(1+eψ)
0f(p|xn)dp
=Γ(n+2 )
Γ(s+ 1)Γ( n−s+1 )integraldisplayeψ/(1+eψ)
0ps(1−p)n−sdp
and
h(ψ|xn)= H/prime(ψ|xn)
=Γ(n+2 )
Γ(s+ 1)Γ( n−s+1 )parenleftbiggeψ
1+eψparenrightbiggsparenleftbigg1
1+eψparenrightbiggn−s
∂parenleftBig
eψ
1+eψparenrightBig
∂ψ

=Γ(n+2 )
Γ(s+ 1)Γ( n−s+1 )parenleftbiggeψ
1+eψparenrightbiggsparenleftbigg1
1+eψparenrightbiggn−sparenleftbigg1
1+eψparenrightbigg2
=Γ(n+2 )
Γ(s+ 1)Γ( n−s+1 )parenleftbiggeψ
1+eψparenrightbiggsparenleftbigg1
1+eψparenrightbiggn−s+2
forψ∈R./squaresolid
11.4 Simulation
The posterior can often be approximated by simulation. Suppose we draw
θ1,...,θ B∼p(θ|xn). Then a histogram of θ1,...,θ Bapproximates the poste-
rior density p(θ|xn). An approximation to the posterior mean θn=E(θ|xn)i s

<<<PAGE 197>>>

11.5 Large Sample Properties of Bayes’ Procedures 181
B−1summationtextB
j=1θj.The posterior 1 −αinterval can be approximated by ( θα/2,θ1−α/2)
where θα/2is the α/2 sample quantile of θ1,...,θ B.
Once we have a sample θ1,...,θ Bfrom f(θ|xn), let τi=g(θi). Then
τ1,...,τ Bis a sample from f(τ|xn). This avoids the need to do any analytical
calculations. Simulation is discussed in more detail in Chapter 24.
11.4 Example. Consider again Example 11.3. We can approximate the pos-
terior for ψwithout doing any calculus. Here are the steps:
1. Draw P1,...,P B∼Beta(s+1,n−s+ 1).
2. Let ψi= log( Pi/(1−Pi)) for i=1,...,B .
Nowψ1,...,ψ Bareiiddraws from h(ψ|xn). A histogram of these values
provides an estimate of h(ψ|xn)./squaresolid
11.5 Large Sample Properties of Bayes’ Procedures
In the Bernoulli and Normal examples we saw that the posterior mean was
close to the mle. This is true in greater generality.
11.5 Theorem. Lethatwideθnbe the mleand lethatwidese=1/radicalBig
nI(hatwideθn). Under appropriate
regularity conditions, the posterior is approximately Normal with mean hatwideθnand
standard deviation hatwidese. Hence, θn≈hatwideθn. Also, if Cn=(hatwideθn−zα/2hatwidese,hatwideθn+zα/2hatwidese)
is the asymptotic frequentist 1−αconﬁdence interval, then Cnis also an
approximate 1−αBayesian posterior interval:
P(θ∈Cn|Xn)→1−α.
There is also a Bayesian delta method. Let τ=g(θ). Then
τ|Xn≈N(hatwideτ,tildewidese2)
wherehatwideτ=g(hatwideθ) andtildewidese=hatwidese|g/prime(hatwideθ)|.
11.6 Flat Priors, Improper Priors, and
“Noninformative” Priors
An important question in Bayesian inference is: where does one get the prior
f(θ)? One school of thought, called subjectivism says that the prior should

<<<PAGE 198>>>

182 11. Bayesian Inference
reﬂect our subjective opinion about θ(before the data are collected). This may
be possible in some cases but is impractical in complicated problems especiallyif there are many parameters. Moreover, injecting subjective opinion into theanalysis is contrary to the goal of making scientiﬁc inference as objectiveas possible. An alternative is to try to deﬁne some sort of “noninformativeprior.” An obvious candidate for a noninformative prior is to use a ﬂat priorf(θ)∝constant.
In the Bernoulli example, taking f(p) = 1 leads to p|X
n∼Beta(s+1,n−
s+ 1) as we saw earlier, which seemed very reasonable. But unfettered use of
ﬂat priors raises some questions.
Improper Priors. LetX∼N(θ,σ2) with σknown. Suppose we adopt
a ﬂat prior f(θ)∝cwhere c>0 is a constant. Note thatintegraltext
f(θ)dθ=∞so
this is not a probability density in the usual sense. We call such a prior animproper prior. Nonetheless, we can still formally carry out Bayes’ theorem
and compute the posterior density by multiplying the prior and the likelihood:f(θ)∝L
n(θ)f(θ)∝L n(θ). This gives θ|Xn∼N(X,σ2/n) and the resulting
point and interval estimators agree exactly with their frequentist counterparts.In general, improper priors are not a problem as long as the resulting posterioris a well-deﬁned probability distribution.
Flat Priors are Not Invariant. LetX∼Bernoulli( p) and suppose we
use the ﬂat prior f(p) = 1. This ﬂat prior presumably represents our lack of
information about pbefore the experiment. Now let ψ= log( p/(1−p)). This
is a transformation of pand we can compute the resulting distribution for ψ,
namely,
f
Ψ(ψ)=eψ
(1 +eψ)2
which is not ﬂat. But if we are ignorant about pthen we are also ignorant
about ψso we should use a ﬂat prior for ψ. This is a contradiction. In short,
the notion of a ﬂat prior is not well deﬁned because a ﬂat prior on a parameterdoes not imply a ﬂat prior on a transformed version of the parameter. Flatpriors are not transformation invariant.
Jeffreys’ Prior. Jeﬀreys came up with a rule for creating priors. The
rule is: take
f(θ)∝I(θ)
1/2
where I(θ) is the Fisher information function. This rule turns out to be trans-
formation invariant. There are various reasons for thinking that this priormight be a useful prior but we will not go into details here.

<<<PAGE 199>>>

11.7 Multiparameter Problems 183
11.6 Example. Consider the Bernoulli ( p) model. Recall that
I(p)=1
p(1−p).
Jeﬀreys’ rule says to use the prior
f(p)∝radicalbig
I(p)=p−1/2(1−p)−1/2.
This is a Beta (1/2,1/2) density. This is very close to a uniform density. /squaresolid
In a multiparameter problem, the Jeﬀreys’ prior is deﬁned to be f(θ)∝radicalbig
|I(θ)|where |A|denotes the determinant of a matrix AandI(θ)i st h e
Fisher information matrix.
11.7 Multiparameter Problems
Suppose that θ=(θ1,...,θ p). The posterior density is still given by
f(θ|xn)∝Ln(θ)f(θ). (11.8)
The question now arises of how to extract inferences about one parameter.
The key is to ﬁnd the marginal posterior density for the parameter of interest.Suppose we want to make inferences about θ
1. The marginal posterior for θ1
is
f(θ1|xn)=integraldisplay
···integraldisplay
f(θ1,···,θp|xn)dθ2...d θ p. (11.9)
In practice, it might not be feasible to do this integral. Simulation can help.
Draw randomly from the posterior:
θ1,...,θB∼f(θ|xn)
where the superscripts index the diﬀerent draws. Each θjis a vector θj=
(θj
1,...,θj
p). Now collect together the ﬁrst component of each draw:
θ1
1,...,θB
1.
These are a sample from f(θ1|xn) and we have avoided doing any integrals.
11.7 Example (Comparing Two Binomials) .Suppose we have n1control pa-
tients and n2treatment patients and that X1control patients survive while
X2treatment patients survive. We want to estimate τ=g(p1,p2)=p2−p1.
Then,
X1∼Binomial( n1,p1) and X2∼Binomial( n2,p2).

<<<PAGE 200>>>

184 11. Bayesian Inference
Iff(p1,p2) = 1, the posterior is
f(p1,p2|x1,x2)∝px1
1(1−p1)n1−x1px2
2(1−p2)n2−x2.
Notice that ( p1,p2) live on a rectangle (a square, actually) and that
f(p1,p2|x1,x2)=f(p1|x1)f(p2|x2)
where
f(p1|x1)∝px1
1(1−p1)n1−x1andf(p2|x2)∝px2
2(1−p2)n2−x2
which implies that p1andp2are independent under the posterior. Also,
p1|x1∼Beta(x1+1,n1−x1+ 1) and p2|x2∼Beta(x2+1,n2−x2+ 1).
If we simulate P1,1,...,P 1,B∼Beta(x1+1,n1−x1+ 1) and P2,1,...,P 2,B∼
Beta(x2+1,n2−x2+1), then τb=P2,b−P1,b,b=1,...,B , is a sample from
f(τ|x1,x2)./squaresolid
11.8 Bayesian Testing
Hypothesis testing from a Bayesian point of view is a complex topic. We
will only give a brief sketch of the main idea here. The Bayesian approachto testing involves putting a prior on H
0and on the parameter θand then
computing P(H0|Xn). Consider the case where θis scalar and we are testing
H0:θ=θ0versus H1:θ/negationslash=θ0.
It is usually reasonable to use the prior P(H0)=P(H1)=1/2 (although this
is not essential in what follows). Under H1we need a prior for θ. Denote this
prior density by f(θ). From Bayes’ theorem
P(H0|Xn=xn)=f(xn|H0)P(H0)
f(xn|H0)P(H0)+f(xn|H1)P(H1)
=1
2f(xn|θ0)
1
2f(xn|θ0)+1
2f(xn|H1)
=f(xn|θ0)
f(xn|θ0)+integraltext
f(xn|θ)f(θ)dθ
=L(θ0)
L(θ0)+integraltext
L(θ)f(θ)dθ.
We saw that, in estimation problems, the prior was not very inﬂuential and
that the frequentist and Bayesian methods gave similar answers. This is not

<<<PAGE 201>>>

11.9 Strengths and Weaknesses of Bayesian Inference 185
the case in hypothesis testing. Also, one can’t use improper priors in testing
because this leads to an undeﬁned constant in the denominator of the expres-sion above. Thus, if you use Bayesian testing you must choose the prior f(θ)
very carefully. It is possible to get a prior-free bound on P(H
0|Xn=xn).
Notice that 0 ≤integraltext
L(θ)f(θ)dθ≤L(hatwideθ). Hence,
L(θ0)
L(θ0)+L(hatwideθ)≤P(H0|Xn=xn)≤1.
The upper bound is not very interesting, but the lower bound is non-trivial.
11.9 Strengths and Weaknesses of Bayesian Inference
Bayesian inference is appealing when prior information is available since Bayes’
theorem is a natural way to combine prior information with data. Some peo-ple ﬁnd Bayesian inference psychologically appealing because it allows us tomake probability statements about parameters. In contrast, frequentist infer-ence provides conﬁdence sets C
nwhich trap the parameter 95 percent of the
time, but we cannot say that P(θ∈Cn|Xn) is .95. In the frequentist approach
we can make probability statements about Cn, notθ. However, psychological
appeal is not a compelling scientiﬁc argument for using one type of inferenceover another.
In parametric models, with large samples, Bayesian and frequentist methods
give approximately the same inferences. In general, they need not agree.
Here are three examples that illustrate the strengths and weakness of Bayesian
inference. The ﬁrst example is Example 6.14 revisited. This example showsthe psychological appeal of Bayesian inference. The second and third showthat Bayesian methods can fail.
11.8 Example (Example 6.14 revisited) .We begin by reviewing the example.
Letθbe a ﬁxed, known real number and let X
1,X2be independent random
variables such that P(Xi=1 )= P(Xi=−1 )=1 /2. Now deﬁne Yi=θ+Xi
and suppose that you only observe Y1andY2. Let
C=braceleftBigg
{Y1−1} ifY1=Y2
{(Y1+Y2)/2}ifY1/negationslash=Y2.
This is a 75 percent conﬁdence set since, no matter what θis,Pθ(θ∈C)=3/4.
Suppose we observe Y1= 15 and Y2= 17. Then our 75 percent conﬁdence
interval is {16}. However, we are certain, in this case, that θ= 16. So calling

<<<PAGE 202>>>

186 11. Bayesian Inference
this a 75 percent conﬁdence set, bothers many people. Nonetheless, Cis a
valid 75 percent conﬁdence set. It will trap the true value 75 percent of thetime.
The Bayesian solution is more satisfying to many. For simplicity, assume
thatθis an integer. Let f(θ) be a prior mass function such that f(θ)>0 for
every integer θ. When Y=(Y
1,Y2) = (15 ,17), the likelihood function is
L(θ)=braceleftbigg
1/4θ=1 6
0 otherwise .
Applying Bayes’ theorem we see that
P(Θ = θ|Y= (15,17)) =braceleftbigg
1θ=1 6
0 otherwise .
Hence, P(θ∈C|Y= (15 ,17)) = 1. There is nothing wrong with saying that
{16}is a 75 percent conﬁdence interval. But is it not a probability statement
about θ./squaresolid
11.9 Example. This is a simpliﬁed version of the example in Robins and Ritov
(1997). The data consist of niidtriples
(X1,R1,Y1),...,(Xn,Yn,Rn).
LetBbe a ﬁnite but very large number, like B= 100100. Any realistic sample
sizenwill be small compared to B. Let
θ=(θ1,...,θ B)
be a vector of unknown parameters such that 0 ≤θj≤1 for 1 ≤j≤B. Let
ξ=(ξ1,...,ξ B)
be a vector of known numbers such that
0<δ≤ξj≤1−δ<1,1≤j≤B,
where δis some, small, positive number. Each data point ( Xi,Ri,Yi) is drawn
in the following way:
1. Draw Xiuniformly from {1,...,B }.
2. Draw Ri∼Bernoulli( ξXi).
3. IfRi= 1, then draw Yi∼Bernoulli( θXi). IfRi= 0, do not draw Yi.

<<<PAGE 203>>>

11.9 Strengths and Weaknesses of Bayesian Inference 187
The model may seem a little artiﬁcial but, in fact, it is caricature of some
realmissing data problems in which some data points are not observed. In
this example, Ri= 0 can be thought of as meaning “missing.” Our goal is to
estimate
ψ=P(Yi=1 ).
Note that
ψ=P(Yi=1 )=Bsummationdisplay
j=1P(Yi=1|X=j)P(X=j)
=1
BBsummationdisplay
j=1θj≡g(θ)
soψ=g(θ) is a function of θ.
Let us consider a Bayesian analysis ﬁrst. The likelihood of a single obser-
vation is
f(Xi,Ri,Yi)=f(Xi)f(Ri|Xi)f(Yi|Xi)Ri.
The last term is raised to the power Risince, if Ri= 0, then Yiis not observed
and hence that term drops out of the likelihood. Since f(Xi)=1/Band that
YiandRiare Bernoulli,
f(Xi)f(Ri|Xi)f(Yi|Xi)Ri=1
BξRi
Xi(1−ξXi)1−RiθYiRi
Xi(1−θXi)(1−Yi)Ri.
Thus, the likelihood function is
L(θ)=nproductdisplay
i=1f(Xi)f(Ri|Xi)f(Yi|Xi)Ri
=nproductdisplay
i=11
BξRi
Xi(1−ξXi)1−RiθYiRi
Xi(1−θXi)(1−Yi)Ri
∝θYiRi
Xi(1−θXi)(1−Yi)Ri.
We have dropped all the terms involving Band the ξj’s since these are known
constants, not parameters. The log-likelihood is
/lscript(θ)=nsummationdisplay
i=1YiRilogθXi+( 1−Yi)Rilog(1−θXi)
=Bsummationdisplay
j=1njlogθj+Bsummationdisplay
j=1mjlog(1−θj)

<<<PAGE 204>>>

188 11. Bayesian Inference
where
nj=# {i:Yi=1,Ri=1,Xi=j}
mj=# {i:Yi=0,Ri=1,Xi=j}.
Now, nj=mj= 0 for most jsince Bis so much larger than n. This has
several implications. First, the mlefor most θjis not deﬁned. Second, for
most θj, the posterior distribution is equal to the prior distribution, since
those θjdo not appear in the likelihood. Hence, f(θ|Data) ≈f(θ). It follows
thatf(ψ|Data) ≈f(ψ). In other words, the data provide little information
about ψin a Bayesian analysis.
Now we consider a frequentist solution. Deﬁne
hatwideψ=1
nnsummationdisplay
i=1RiYi
ξXi. (11.10)
We will now show that this estimator is unbiased and has small mean-squared
error. It can be shown (see Exercise 7) that
E(hatwideψ)=ψand V(hatwideψ)≤1
nδ2. (11.11)
Therefore, the mseis of order 1 /nwhich goes to 0 fairly quickly as we collect
more data, no matter how large Bis. The estimator deﬁned in (11.10) is called
theHorwitz-Thompson estimator. It cannot be derived from a Bayesian or
likelihood point of view since it involves the terms ξXi. These terms drop
out of the log-likelihood and hence will not show up in any likelihood-basedmethod including Bayesian estimators.
The moral of the story is this. Bayesian methods are tied to the likeli-
hood function. But in high dimensional (and nonparametric) problems, thelikelihood may not yield accurate inferences.
/squaresolid
11.10 Example. Suppose that fis a probability density function and that
f(x)=cg(x)
where g(x)>0 is a known function and cis unknown. In principle we can
compute csinceintegraltext
f(x)dx= 1 implies that c=1/integraltext
g(x)dx. But in many cases
we can’t do the integralintegraltext
g(x)dxsincegmight be a complicated function and
xcould be high dimensional. Despite the fact that cis not known, it is often
possible to draw a sample X1,...,X nfromf; see Chapter 24. Can we use the
sample to estimate the normalizing constant c? Here is a frequentist solution:

<<<PAGE 205>>>

11.10 Bibliographic Remarks 189
Lethatwidefn(x) be a consistent estimate of the density f. Chapter 20 explains how to
construct such an estimate. Choose any point xand note that c=f(x)/g(x).
Hence,hatwidec=hatwidef(x)/g(x) is a consistent estimate of c. Now let us try to solve this
problem from a Bayesian approach. Let π(c) be a prior such that π(c)>0 for
allc>0. The likelihood function is
Ln(c)=nproductdisplay
i=1f(Xi)=nproductdisplay
i=1cg(Xi)=cnnproductdisplay
i=1g(Xi)∝cn.
Hence the posterior is proportional to cnπ(c). The posterior does not depend
onX1,...,X n, so we come to the startling conclusion that, from the Bayesian
point of view, there is no information in the data about c. Moreover, the
posterior mean isintegraltext∞
0cn+1π(c)dcintegraltext∞
0cnπ(c)dc
which tends to inﬁnity as nincreases. /squaresolid
These last two examples illustrate an important point. Bayesians are slaves
to the likelihood function. When the likelihood goes awry, so will Bayesianinference.
What should we conclude from all this? The important thing is to under-
stand that frequentist and Bayesian methods are answering diﬀerent ques-tions. To combine prior beliefs with data in a principled way, use Bayesian in-ference. To construct procedures with guaranteed long run performance, suchas conﬁdence intervals, use frequentist methods. Generally, Bayesian methodsrun into problems when the parameter space is high dimensional. In particu-lar, 95 percent posterior intervals need not contain the true value 95 percentof the time (in the frequency sense).
11.10 Bibliographic Remarks
Some references on Bayesian inference include Carlin and Louis (1996), Gel-man et al. (1995), Lee (1997), Robert (1994), and Schervish (1995). See Cox(1993), Diaconis and Freedman (1986), Freedman (1999), Barron et al. (1999),Ghosal et al. (2000), Shen and Wasserman (2001), and Zhao (2000) for discus-sions of some of the technicalities of nonparametric Bayesian inference. TheRobins-Ritov example is discussed in detail in Robins and Ritov (1997) whereit is cast more properly as a nonparametric problem. Example 11.10 is due toEdward George (personal communication). See Berger and Delampady (1987)

<<<PAGE 206>>>

190 11. Bayesian Inference
and Kass and Raftery (1995) for a discussion of Bayesian testing. See Kass
and Wasserman (1996) for a discussion of noninformative priors.
11.11 Appendix
Proof of Theorem 11.5.
It can be shown that the eﬀect of the prior diminishes as nincreases so
thatf(θ|Xn)∝L n(θ)f(θ)≈L n(θ). Hence, log f(θ|Xn)≈/lscript(θ). Now, /lscript(θ)≈
/lscript(hatwideθ)+(θ−hatwideθ)/lscript/prime(hatwideθ)+[(θ−hatwideθ)2/2]/lscript/prime/prime(hatwideθ)=/lscript(hatwideθ)+[(θ−hatwideθ)2/2]/lscript/prime/prime(hatwideθ) since /lscript/prime(hatwideθ)=0 .
Exponentiating, we get approximately that
f(θ|Xn)∝expbraceleftBigg
−1
2(θ−hatwideθ)2
σ2nbracerightBigg
where σ2
n=−1//lscript/prime/prime(hatwideθn). So the posterior of θis approximately Normal with
meanhatwideθand variance σ2
n. Let/lscripti= log f(Xi|θ), then
1
σ2n=−/lscript/prime/prime(hatwideθn)=summationdisplay
i−/lscript/prime/prime
i(hatwideθn)
=nparenleftbigg1
nparenrightbiggsummationdisplay
i−/lscript/prime/prime
i(hatwideθn)≈nEθbracketleftBig
−/lscript/prime/prime
i(hatwideθn)bracketrightBig
=nI(hatwideθn)
and hence σn≈se(hatwideθ)./squaresolid
11.12 Exercises
1. Verify (11.7).
2. Let X1, ..., X n∼Normal( µ,1).
(a) Simulate a data set (using µ= 5) consisting of n=100 observations.
(b) Take f(µ) = 1 and ﬁnd the posterior density. Plot the density.
(c) Simulate 1,000 draws from the posterior. Plot a histogram of the
simulated values and compare the histogram to the answer in (b).
(d) Let θ=eµ. Find the posterior density for θanalytically and by
simulation.
(e) Find a 95 percent posterior interval for µ.
(f) Find a 95 percent conﬁdence interval for θ.

<<<PAGE 207>>>

11.12 Exercises 191
3. Let X1, ..., X n∼Uniform(0 ,θ).Letf(θ)∝1/θ. Find the posterior
density.
4. Suppose that 50 people are given a placebo and 50 are given a new
treatment. 30 placebo patients show improvement while 40 treated pa-tients show improvement. Let τ=p
2−p1where p2is the probability of
improving under treatment and p1is the probability of improving under
placebo.
(a) Find the mleofτ. Find the standard error and 90 percent conﬁdence
interval using the delta method.
(b) Find the standard error and 90 percent conﬁdence interval using the
parametric bootstrap.
(c) Use the prior f(p1,p2) = 1. Use simulation to ﬁnd the posterior
mean and posterior 90 percent interval for τ.
(d) Let
ψ= logparenleftbiggparenleftbiggp1
1−p1parenrightbigg
÷parenleftbiggp2
1−p2parenrightbiggparenrightbigg
be the log-odds ratio. Note that ψ=0i f p1=p2. Find the mleofψ.
Use the delta method to ﬁnd a 90 percent conﬁdence interval for ψ.
(e) Use simulation to ﬁnd the posterior mean and posterior 90 percent
interval for ψ.
5. Consider the Bernoulli(p) observations
0101000000Plot the posterior for pusing these priors: Beta(1/2,1/2), Beta(1,1),
Beta(10,10), Beta(100,100).
6. Let X
1,...,X n∼Poisson( λ).
(a) Let λ∼Gamma( α,β) be the prior. Show that the posterior is also
a Gamma. Find the posterior mean.
(b) Find the Jeﬀreys’ prior. Find the posterior.
7. In Example 11.9, verify (11.11).8. Let X∼N(µ,1). Consider testing
H
0:µ= 0 versus H1:µ/negationslash=0.

<<<PAGE 208>>>

192 11. Bayesian Inference
Take P(H0)=P(H1)=1 /2. Let the prior for µunder H1beµ∼
N(0,b2). Find an expression for P(H0|X=x). Compare P(H0|X=x)
to the p-value of the Wald test. Do the comparison numerically for avariety of values of xandb. Now repeat the problem using a sample of
sizen. You will see that the posterior probability of H
0can be large even
when the p-value is small, especially when nis large. This disagreement
between Bayesian and frequentist testing is called the Jeﬀreys-Lindleyparadox.

<<<PAGE 209>>>

12
Statistical Decision Theory
12.1 Preliminaries
We have considered several point estimators such as the maximum likelihood
estimator, the method of moments estimator, and the posterior mean. In fact,there are many other ways to generate estimators. How do we choose amongthem? The answer is found in decision theory which is a formal theory for
comparing statistical procedures.
Consider a parameter θwhich lives in a parameter space Θ. Let hatwideθbe an
estimator of θ. In the language of decision theory, an estimator is sometimes
called a decision rule and the possible values of the decision rule are called
actions .
We shall measure the discrepancy between θandhatwideθusing a loss function
L(θ,hatwideθ). Formally, Lmaps Θ ×Θi n t o R. Here are some examples of loss
functions:
L(θ,hatwideθ)=(θ−hatwideθ)
2squared error loss,
L(θ,hatwideθ)=|θ−hatwideθ| absolute error loss,
L(θ,hatwideθ)=|θ−hatwideθ|pLploss,
L(θ,hatwideθ)=0i f θ=hatwideθo r1i f θ/negationslash=hatwideθzero–one loss,
L(θ,hatwideθ)=integraltext
logparenleftBig
f(x;θ)
f(x;/hatwideθ)parenrightBig
f(x;θ)dxKullback–Leibler loss .

<<<PAGE 210>>>

194 12. Statistical Decision Theory
Bear in mind in what follows that an estimator hatwideθis a function of the data.
To emphasize this point, sometimes we will write hatwideθashatwideθ(X). To assess an
estimator, we evaluate the average loss or risk.
12.1 Deﬁnition. Theriskof an estimator hatwideθis
R(θ,hatwideθ)=Eθparenleftbigg
L(θ,hatwideθ)parenrightbigg
=integraldisplay
L(θ,hatwideθ(x))f(x;θ)dx.
When the loss function is squared error, the risk is just the mse(mean
squared error):
R(θ,hatwideθ)=Eθ(hatwideθ−θ)2=mse=Vθ(hatwideθ)+bias2
θ(hatwideθ).
In the rest of the chapter, if we do not state what loss function we are using,
assume the loss function is squared error.
12.2 Comparing Risk Functions
To compare two estimators we can compare their risk functions. However, thisdoes not provide a clear answer as to which estimator is better. Consider thefollowing examples.
12.2 Example. LetX∼N(θ,1) and assume we are using squared error
loss. Consider two estimators: hatwideθ
1=Xandhatwideθ2= 3. The risk functions are
R(θ,hatwideθ1)=Eθ(X−θ)2= 1 and R(θ,hatwideθ2)=Eθ(3−θ)2=( 3−θ)2.If 2<θ< 4
thenR(θ,hatwideθ2)<R(θ,hatwideθ1), otherwise, R(θ,hatwideθ1)<R(θ,hatwideθ2). Neither estimator
uniformly dominates the other; see Figure 12.1. /squaresolid
12.3 Example. LetX1,...,X n∼Bernoulli( p). Consider squared error loss
and lethatwidep1=X. Since this has 0 bias, we have that
R(p,hatwidep1)=V(X)=p(1−p)
n.
Another estimator is
hatwidep2=Y+α
α+β+n
where Y=summationtextn
i=1Xiandαandβare positive constants. This is the posterior
mean using a Beta ( α,β) prior. Now,
R(p,hatwidep2)= Vp(hatwidep2)+(biasp(hatwidep2))2

<<<PAGE 211>>>

12.2 Comparing Risk Functions 195
0123450123R(θ,hatwideθ1)R(θ,hatwideθ2)
θ
FIGURE 12.1. Comparing two risk functions. Neither risk function dominates the
other at all values of θ.
=VpparenleftbiggY+α
α+β+nparenrightbigg
+parenleftbigg
EpparenleftbiggY+α
α+β+nparenrightbigg
−pparenrightbigg2
=np(1−p)
(α+β+n)2+parenleftbiggnp+α
α+β+n−pparenrightbigg2
.
Letα=β=radicalbig
n/4. (In Example 12.12 we will explain this choice.) The
resulting estimator is
hatwidep2=Y+radicalbig
n/4
n+√n
and the risk function is
R(p,hatwidep2)=n
4(n+√n)2.
The risk functions are plotted in ﬁgure 12.2. As we can see, neither estimator
uniformly dominates the other.
These examples highlight the need to be able to compare risk functions.
To do so, we need a one-number summary of the risk function. Two suchsummaries are the maximum risk and the Bayes risk.
12.4 Deﬁnition. Themaximum risk is
R(hatwideθ) = sup
θR(θ,hatwideθ) (12.1)
andthe Bayes risk is
r(f,hatwideθ)=integraldisplay
R(θ,hatwideθ)f(θ)dθ (12.2)
where f(θ)is a prior for θ.

<<<PAGE 212>>>

196 12. Statistical Decision Theory
Risk
p
FIGURE 12.2. Risk functions for /hatwidep1and/hatwidep2in Example 12.3. The solid curve is
R(/hatwidep1). The dotted line is R(/hatwidep2).
12.5 Example. Consider again the two estimators in Example 12.3. We have
R(hatwidep1) = max
0≤p≤1p(1−p)
n=1
4n
and
R(hatwidep2) = max
pn
4(n+√n)2=n
4(n+√n)2.
Based on maximum risk, hatwidep2is a better estimator since R(hatwidep2)<R(hatwidep1). How-
ever, when nis large, R(hatwidep1) has smaller risk except for a small region in the
parameter space near p=1/2. Thus, many people prefer hatwidep1tohatwidep2. This il-
lustrates that one-number summaries like maximum risk are imperfect. Nowconsider the Bayes risk. For illustration, let us take f(p) = 1. Then
r(f,hatwidep
1)=integraldisplay
R(p,hatwidep1)dp=integraldisplayp(1−p)
ndp=1
6n
and
r(f,hatwidep2)=integraldisplay
R(p,hatwidep2)dp=n
4(n+√n)2.
Forn≥20,r(f,hatwidep2)>r(f,hatwidep1) which suggests that hatwidep1is a better estimator.
This might seem intuitively reasonable but this answer depends on the choiceof prior. The advantage of using maximum risk, despite its problems, is thatit does not require one to choose a prior.
/squaresolid
These two summaries of the risk function suggest two diﬀerent methods
for devising estimators: choosing hatwideθto minimize the maximum risk leads to

<<<PAGE 213>>>

12.3 Bayes Estimators 197
minimax estimators; choosing hatwideθto minimize the Bayes risk leads to Bayes
estimators.
12.6 Deﬁnition. A decision rule that minimizes the Bayes risk is called a
Bayes rule . Formally, hatwideθis a Bayes rule with respect to the prior fif
r(f,hatwideθ) = inf
/tildewideθr(f,tildewideθ) (12.3)
where the inﬁmum is over all estimators tildewideθ. An estimator that minimizes
the maximum risk is called a minimax rule . Formally, hatwideθis minimax if
sup
θR(θ,hatwideθ) = inf
/tildewideθsup
θR(θ,tildewideθ) (12.4)
where the inﬁmum is over all estimators tildewideθ.
12.3 Bayes Estimators
Letfbe a prior. From Bayes’ theorem, the posterior density is
f(θ|x)=f(x|θ)f(θ)
m(x)=f(x|θ)f(θ)integraltext
f(x|θ)f(θ)dθ(12.5)
where m(x)=integraltext
f(x, θ)dθ=integraltext
f(x|θ)f(θ)dθis themarginal distribution of
X. Deﬁne the posterior risk of an estimator hatwideθ(x)b y
r(hatwideθ|x)=integraldisplay
L(θ,hatwideθ(x))f(θ|x)dθ. (12.6)
12.7 Theorem. The Bayes risk r(f,hatwideθ)satisﬁes
r(f,hatwideθ)=integraldisplay
r(hatwideθ|x)m(x)dx.
Lethatwideθ(x)be the value of θthat minimizes r(hatwideθ|x). Thenhatwideθis the Bayes estimator.
Proof. We can rewrite the Bayes risk as follows:
r(f,hatwideθ)=integraldisplay
R(θ,hatwideθ)f(θ)dθ=integraldisplayparenleftBiggintegraldisplay
L(θ,hatwideθ(x))f(x|θ)dxparenrightBigg
f(θ)dθ
=integraldisplayintegraldisplay
L(θ,hatwideθ(x))f(x, θ)dxdθ=integraldisplayintegraldisplay
L(θ,hatwideθ(x))f(θ|x)m(x)dxdθ
=integraldisplayparenleftBiggintegraldisplay
L(θ,hatwideθ(x))f(θ|x)dθparenrightBigg
m(x)dx=integraldisplay
r(hatwideθ|x)m(x)dx.

<<<PAGE 214>>>

198 12. Statistical Decision Theory
If we choose hatwideθ(x) to be the value of θthat minimizes r(hatwideθ|x) then we will mini-
mize the integrand at every xand thus minimize the integralintegraltext
r(hatwideθ|x)m(x)dx.
/squaresolid
Now we can ﬁnd an explicit formula for the Bayes estimator for some speciﬁc
loss functions.
12.8 Theorem. IfL(θ,hatwideθ)=(θ−hatwideθ)2then the Bayes estimator is
hatwideθ(x)=integraldisplay
θf(θ|x)dθ=E(θ|X=x). (12.7)
IfL(θ,hatwideθ)=|θ−hatwideθ|then the Bayes estimator is the median of the posterior
f(θ|x).I fL(θ,hatwideθ)is zero–one loss, then the Bayes estimator is the mode of the
posterior f(θ|x).
Proof. We will prove the theorem for squared error loss. The Bayes rule
hatwideθ(x) minimizes r(hatwideθ|x)=integraltext
(θ−hatwideθ(x))2f(θ|x)dθ. Taking the derivative of r(hatwideθ|x)
with respect to hatwideθ(x) and setting it equal to 0 yields the equation 2integraltext
(θ−
hatwideθ(x))f(θ|x)dθ= 0. Solving for hatwideθ(x) we get 12.7. /squaresolid
12.9 Example. LetX1,...,X n∼N(µ, σ2) where σ2is known. Suppose we
use a N(a, b2) prior for µ. The Bayes estimator with respect to squared error
loss is the posterior mean, which is
hatwideθ(X1,...,X n)=b2
b2+σ2
nX+σ2
n
b2+σ2
na./squaresolid
12.4 Minimax Rules
Finding minimax rules is complicated and we cannot attempt a complete
coverage of that theory here but we will mention a few key results. The mainmessage to take away from this section is: Bayes estimators with a constant
risk function are minimax.
12.10 Theorem. Lethatwideθ
fbe the Bayes rule for some prior f:
r(f,hatwideθf) = inf
/hatwideθr(f,hatwideθ). (12.8)
Suppose that
R(θ,hatwideθf)≤r(f,hatwideθf) for all θ. (12.9)
Thenhatwideθfis minimax and fis called a least favorable prior .

<<<PAGE 215>>>

12.4 Minimax Rules 199
Proof. Suppose that hatwideθfis not minimax. Then there is another rule hatwideθ0such
that supθR(θ,hatwideθ0)<supθR(θ,hatwideθf). Since the average of a function is always
less than or equal to its maximum, we have that r(f,hatwideθ0)≤supθR(θ,hatwideθ0).
Hence,
r(f,hatwideθ0)≤sup
θR(θ,hatwideθ0)<sup
θR(θ,hatwideθf)≤r(f,hatwideθf)
which contradicts (12.8). /squaresolid
12.11 Theorem. Suppose that hatwideθis the Bayes rule with respect to some
prior f. Suppose further that hatwideθhas constant risk: R(θ,hatwideθ)=cfor some c.
Thenhatwideθis minimax.
Proof. The Bayes risk is r(f,hatwideθ)=integraltext
R(θ,hatwideθ)f(θ)dθ=cand hence R(θ,hatwideθ)≤
r(f,hatwideθ) for all θ. Now apply the previous theorem. /squaresolid
12.12 Example. Consider the Bernoulli model with squared error loss. In
example 12.3 we showed that the estimator
hatwidep(Xn)=summationtextn
i=1Xi+radicalbig
n/4
n+√n
has a constant risk function. This estimator is the posterior mean, and hence
the Bayes rule, for the prior Beta( α,β) with α=β=radicalbig
n/4. Hence, by the
previous theorem, this estimator is minimax. /squaresolid
12.13 Example. Consider again the Bernoulli but with loss function
L(p,hatwidep)=(p−hatwidep)2
p(1−p).
Let
hatwidep(Xn)=hatwidep=summationtextn
i=1Xi
n.
The risk is
R(p,hatwidep)=Eparenleftbigg(hatwidep−p)2
p(1−p)parenrightbigg
=1
p(1−p)parenleftbiggp(1−p)
nparenrightbigg
=1
n
which, as a function of p, is constant. It can be shown that, for this loss
function, hatwidep(Xn) is the Bayes estimator under the prior f(p) = 1. Hence, hatwidepis
minimax. /squaresolid
A natural question to ask is: what is the minimax estimator for a Normal
model?

<<<PAGE 216>>>

200 12. Statistical Decision Theory
θ
0 0.5 -0.5
FIGURE 12.3. Risk function for constrained Normal with m=.5. The two short
dashed lines show the least favorable prior which puts its mass at two points.
12.14 Theorem. LetX1,...,X n∼N(θ,1)and lethatwideθ=X. Thenhatwideθis minimax
with respect to any well-behaved loss function.1It is the only estimator with
this property.
If the parameter space is restricted, then the theorem above does not apply
as the next example shows.
12.15 Example. Suppose that X∼N(θ,1) and that θis known to lie in the
interval [ −m, m] where 0 <m< 1. The unique, minimax estimator under
squared error loss is
hatwideθ(X)=mtanh(mX)
where tanh( z)=(ez−e−z)/(ez+e−z). It can be shown that this is the Bayes
rule with respect to the prior that puts mass 1/2 at mand mass 1/2 at −m.
Moreover, it can be shown that the risk is not constant but it does satisfyR(θ,hatwideθ)≤r(f,hatwideθ) for all θ; see Figure 12.3. Hence, Theorem 12.10 implies that
hatwideθis minimax.
/squaresolid
1“Well-behaved” means that the level sets must be convex and symmetric about the origin.
The result holds up to sets of measure 0.

<<<PAGE 217>>>

12.5 Maximum Likelihood, Minimax, and Bayes 201
12.5 Maximum Likelihood, Minimax, and Bayes
For parametric models that satisfy weak regularity conditions, the maximum
likelihood estimator is approximately minimax. Consider squared error losswhich is squared bias plus variance. In parametric models with large samples,it can be shown that the variance term dominates the bias so the risk of themlehatwideθroughly equals the variance:
2
R(θ,hatwideθ)=Vθ(hatwideθ)+bias2≈Vθ(hatwideθ).
As we saw in Chapter 9, the variance of the mleis approximately
V(hatwideθ)≈1
nI(θ)
where I(θ) is the Fisher information. Hence,
nR(θ,hatwideθ)≈1
I(θ). (12.10)
For any other estimator θ/prime, it can be shown that for large n,R(θ,θ/prime)≥R(θ,hatwideθ).
More precisely,
lim
/epsilon1→0lim sup
n→∞sup
|θ−θ/prime|</epsilon1nR(θ/prime,hatwideθ)≥1
I(θ). (12.11)
This says that, in a local, large sample sense, the mleis minimax. It can also
be shown that the mleis approximately the Bayes rule.
In summary:
In most parametric models, with large samples, the mleis approxi-
mately minimax and Bayes.
There is a caveat: these results break down when the number of parameters
is large as the next example shows.
12.16 Example (Many Normal means) .LetYi∼N(θi,σ2/n),i=1,...,n .
LetY=(Y1,...,Y n) denote the data and let θ=(θ1,...,θ n) denote the
unknown parameters. Assume that
θ∈Θn≡braceleftBigg
(θ1,...,θ n):nsummationdisplay
i=1θ2
i≤c2bracerightBigg
2Typically, the squared bias is order O(n−2)while the variance is of order O(n−1).

<<<PAGE 218>>>

202 12. Statistical Decision Theory
for some c>0. In this model, there are as many parameters as observations.3
The mleishatwideθ=Y=(Y1,...,Y n). Under the loss function L(θ,hatwideθ)=summationtextn
i=1(hatwideθi−
θi)2, the risk of the mleisR(θ,hatwideθ)=σ2. It can be shown that the minimax risk
is approximately σ2/(σ2+c2) and one can ﬁnd an estimator tildewideθthat achieves
this risk. Since σ2/(σ2+c2)<σ2, we see that tildewideθhas smaller risk than the mle.
In practice, the diﬀerence between the risks can be substantial. This showsthat maximum likelihood is not an optimal estimator in high dimensionalproblems.
/squaresolid
12.6 Admissibility
Minimax estimators and Bayes estimators are “good estimators” in the sensethat they have small risk. It is also useful to characterize bad estimators.
12.17 Deﬁnition. An estimator hatwideθisinadmissible if there exists another
rulehatwideθ/primesuch that
R(θ,hatwideθ/prime)≤R(θ,hatwideθ) for all θand
R(θ,hatwideθ/prime)<R (θ,hatwideθ) for at least one θ.
Otherwise, hatwideθisadmissible .
12.18 Example. LetX∼N(θ,1) and consider estimating θwith squared
error loss. Let hatwideθ(X) = 3. We will show that hatwideθis admissible. Suppose not.
Then there exists a diﬀerent rule hatwideθ/primewith smaller risk. In particular, R(3,hatwideθ/prime)≤
R(3,hatwideθ) = 0. Hence, 0 = R(3,hatwideθ/prime)=integraltext
(hatwideθ/prime(x)−3)2f(x;3 )dx. Thus, hatwideθ/prime(x)=3 .
So there is no rule that beats hatwideθ. Even though hatwideθis admissible it is clearly a
bad decision rule. /squaresolid
12.19 Theorem (Bayes Rules Are Admissible) .Suppose that Θ⊂Rand that
R(θ,hatwideθ)is a continuous function of θfor every hatwideθ.L e tfbe a prior density with
full support, meaning that, for every θand every /epsilon1>0,integraltextθ+/epsilon1
θ−/epsilon1f(θ)dθ >0.L e t
hatwideθfbe the Bayes’ rule. If the Bayes risk is ﬁnite then hatwideθfis admissible.
Proof. Suppose hatwideθfis inadmissible. Then there exists a better rule hatwideθsuch
thatR(θ,hatwideθ)≤R(θ,hatwideθf) for all θandR(θ0,hatwideθ)<R(θ0,hatwideθf) for some θ0. Let
3The many Normal means problem is more general than it looks. Many nonparametric esti-
mation problems are mathematically equivalent to this model.

<<<PAGE 219>>>

12.6 Admissibility 203
ν=R(θ0,hatwideθf)−R(θ0,hatwideθ)>0. Since Ris continuous, there is an /epsilon1>0 such
thatR(θ,hatwideθf)−R(θ,hatwideθ)>ν /2 for all θ∈(θ0−/epsilon1, θ0+/epsilon1). Now,
r(f,hatwideθf)−r(f,hatwideθ)=integraldisplay
R(θ,hatwideθf)f(θ)dθ−integraldisplay
R(θ,hatwideθ)f(θ)dθ
=integraldisplaybracketleftBig
R(θ,hatwideθf)−R(θ,hatwideθ)bracketrightBig
f(θ)dθ
≥integraldisplayθ0+/epsilon1
θ0−/epsilon1bracketleftBig
R(θ,hatwideθf)−R(θ,hatwideθ)bracketrightBig
f(θ)dθ
≥ν
2integraldisplayθ0+/epsilon1
θ0−/epsilon1f(θ)dθ
>0.
Hence, r(f,hatwideθf)>r(f,hatwideθ). This implies that hatwideθfdoes not minimize r(f,hatwideθ) which
contradicts the fact that hatwideθfis the Bayes rule. /squaresolid
12.20 Theorem. LetX1,...,X n∼N(µ, σ2). Under squared error loss, Xis
admissible.
The proof of the last theorem is quite technical and is omitted but the idea
is as follows: The posterior mean is admissible for any strictly positive prior.Take the prior to be N(a, b
2). When b2is very large, the posterior mean is
approximately equal to X.
How are minimaxity and admissibility linked? In general, a rule may be one,
both, or neither. But here are some facts linking admissibility and minimaxity.
12.21 Theorem. Suppose that hatwideθhas constant risk and is admissible. Then it
is minimax.
Proof. The risk is R(θ,hatwideθ)=cfor some c.I fhatwideθwere not minimax then
there exists a rule hatwideθ/primesuch that
R(θ,hatwideθ/prime)≤sup
θR(θ,hatwideθ/prime)<sup
θR(θ,hatwideθ)=c.
This would imply that hatwideθis inadmissible. /squaresolid
Now we can prove a restricted version of Theorem 12.14 for squared error
loss.
12.22 Theorem. LetX1,...,X n∼N(θ,1). Then, under squared error loss,
hatwideθ=Xis minimax.
Proof. According to Theorem 12.20, hatwideθis admissible. The risk of hatwideθis 1/n
which is constant. The result follows from Theorem 12.21. /squaresolid

<<<PAGE 220>>>

204 12. Statistical Decision Theory
Although minimax rules are not guaranteed to be admissible they are “close
to admissible.” Say that hatwideθisstrongly inadmissible if there exists a rule hatwideθ/prime
and an /epsilon1>0 such that R(θ,hatwideθ/prime)<R(θ,hatwideθ)−/epsilon1for all θ.
12.23 Theorem. Ifhatwideθis minimax, then it is not strongly inadmissible.
12.7 Stein’s Paradox
Suppose that X∼N(θ,1) and consider estimating θwith squared error loss.
From the previous section we know that hatwideθ(X)=Xis admissible. Now consider
estimating two, unrelated quantities θ=(θ1,θ2) and suppose that X1∼
N(θ1,1) and X2∼N(θ2,1) independently, with loss L(θ,hatwideθ)=summationtext2
j=1(θj−hatwideθj)2.
Not surprisingly, hatwideθ(X)=Xis again admissible where X=(X1,X2). Now
consider the generalization to knormal means. Let θ=(θ1,...,θ k),X=
(X1,...,X k) with Xi∼N(θi,1) (independent) and loss L(θ,hatwideθ)=summationtextk
j=1(θj−
hatwideθj)2. Stein astounded everyone when he proved that, if k≥3, thenhatwideθ(X)=X
is inadmissible. It can be shown that the James-Stein estimator hatwideθShas
smaller risk, where hatwideθS=(hatwideθS
1,...,hatwideθS
k),
hatwideθS
i(X)=parenleftbigg
1−k−2summationtext
iX2
iparenrightbigg+
Xi (12.12)
and (z)+= max {z,0}. This estimator shrinks the Xi’s towards 0. The message
is that, when estimating many parameters, there is great value in shrinking theestimates. This observation plays an important role in modern nonparametricfunction estimation.
12.8 Bibliographic Remarks
Aspects of decision theory can be found in Casella and Berger (2002), Berger(1985), Ferguson (1967), and Lehmann and Casella (1998).
12.9 Exercises
1. In each of the following models, ﬁnd the Bayes risk and the Bayes esti-
mator, using squared error loss.
(a)X∼Binomial( n, p),p∼Beta(α,β).

<<<PAGE 221>>>

12.9 Exercises 205
(b)X∼Poisson( λ),λ∼Gamma( α,β).
(c)X∼N(θ,σ2) where σ2is known and θ∼N(a, b2).
2. Let X1,...,X n∼N(θ,σ2) and suppose we estimate θwith loss function
L(θ,hatwideθ)=(θ−hatwideθ)2/σ2. Show that Xis admissible and minimax.
3. Let Θ = {θ1,...,θ k}be a ﬁnite parameter space. Prove that the poste-
rior mode is the Bayes estimator under zero–one loss.
4. (Casella and Berger (2002).) Let X1,...,X nbe a sample from a distri-
bution with variance σ2. Consider estimators of the form bS2where S2
is the sample variance. Let the loss function for estimating σ2be
L(σ2,hatwideσ2)=hatwideσ2
σ2−1−logparenleftbigghatwideσ2
σ2parenrightbigg
.
Find the optimal value of bthat minimizes the risk for all σ2.
5. (Berliner (1983).) Let X∼Binomial( n, p) and suppose the loss function
is
L(p,hatwidep)=parenleftbigg
1−hatwidep
pparenrightbigg2
where 0 <p< 1. Consider the estimator hatwidep(X) = 0. This estimator falls
outside the parameter space (0 ,1) but we will allow this. Show that
hatwidep(X) = 0 is the unique, minimax rule.
6. (Computer Experiment. ) Compare the risk of the mleand the James-
Stein estimator (12.12) by simulation. Try various values of nand vari-
ous vectors θ. Summarize your results.

<<<PAGE 222>>>



<<<PAGE 223>>>

Part III
Statistical Models and
Methods

<<<PAGE 224>>>



<<<PAGE 225>>>

13
Linear and Logistic Regression
Regression is a method for studying the relationship between a response
variable Yand a covariate X. The covariate is also called a predictor
variable or afeature.1One way to summarize the relationship between X
andYis through the regression function
r(x)=E(Y|X=x)=integraldisplay
yf(y|x)dy. (13.1)
Our goal is to estimate the regression function r(x) from data of the form
(Y1,X1),...,(Yn,Xn)∼FX,Y.
In this Chapter, we take a parametric approach and assume that ris linear.
In Chapters 20 and 21 we discuss nonparametric regression.
13.1 Simple Linear Regression
The simplest version of regression is when Xiis simple (one-dimensional) and
r(x) is assumed to be linear:
r(x)=β0+β1x.
1The term “regression” is due to Sir Francis Galton (1822-1911) who noticed that tall and
short men tend to have sons with heights closer to the mean. He called this “regression t owards
the mean.”

<<<PAGE 226>>>

210 13. Linear and Logistic Regression
4.0 4.5 5.0 5.54.0 4.1 4.2 4.3 4.4 4.5 4.6log surface temperature (Y)
log light intensity (X)
FIGURE 13.1. Data on nearby stars. The solid line is the least squares line.
This model is called the the simple linear regression model . We will make
the further simplifying assumption that V(/epsilon1i|X=x)=σ2does not depend
onx. We can thus write the linear regression model as follows.
13.1 Deﬁnition. The Simple Linear Regression Model
Yi=β0+β1Xi+/epsilon1i (13.2)
where E(/epsilon1i|Xi)=0andV(/epsilon1i|Xi)=σ2.
13.2 Example. Figure 13.1 shows a plot of log surface temperature (Y) versus
log light intensity (X) for some nearby stars. Also on the plot is an estimatedlinear regression line which will be explained shortly.
/squaresolid
The unknown parameters in the model are the intercept β0and the slope
β1and the variance σ2. Lethatwideβ0andhatwideβ1denote estimates of β0andβ1. The
ﬁtted line is
hatwider(x)=hatwideβ0+hatwideβ1x. (13.3)
Thepredicted values orﬁtted values arehatwideYi=hatwider(Xi) and the residuals
are deﬁned to be
hatwide/epsilon1i=Yi−hatwideYi=Yi−parenleftbigg
hatwideβ0+hatwideβ1Xiparenrightbigg
. (13.4)

<<<PAGE 227>>>

13.1 Simple Linear Regression 211
Theresidual sums of squares orrss, which measures how well the line ﬁts
the data, is deﬁned by rss=summationtextn
i=1hatwide/epsilon12
i.
13.3 Deﬁnition. Theleast squares estimates are the values hatwideβ0andhatwideβ1
that minimize rss=summationtextn
i=1hatwide/epsilon12
i.
13.4 Theorem. The least squares estimates are given by
hatwideβ1=summationtextn
i=1(Xi−Xn)(Yi−Yn)summationtextn
i=1(Xi−Xn)2, (13.5)
hatwideβ0=Yn−hatwideβ1Xn. (13.6)
An unbiased estimate of σ2is
hatwideσ2=parenleftbigg1
n−2parenrightbiggnsummationdisplay
i=1hatwide/epsilon12
i. (13.7)
13.5 Example. Consider the star data from Example 13.2. The least squares
estimates are hatwideβ0=3.58 andhatwideβ1=0.166. The ﬁtted line hatwider(x)=3.5 8+0 .166x
is shown in Figure 13.1. /squaresolid
13.6 Example (The 2001 Presidential Election) .Figure 13.2 shows the plot of
votes for Buchanan (Y) versus votes for Bush (X) in Florida. The least squaresestimates (omitting Palm Beach County) and the standard errors are
hatwideβ
0=6 6 .0991hatwidese(hatwideβ0)=1 7 .2926
hatwideβ1=0.0035 hatwidese(hatwideβ1)=0.0002.
The ﬁtted line is
Buchanan = 66 .0991 + 0 .0035 Bush .
(We will see later how the standard errors were computed.) Figure 13.2 also
shows the residuals. The inferences from linear regression are most accuratewhen the residuals behave like random normal numbers. Based on the residualplot, this is not the case in this example. If we repeat the analysis replacingvotes with log(votes) we get
hatwideβ
0=−2.3298hatwidese(hatwideβ0)=0.3529
hatwideβ1=0.730300 hatwidese(hatwideβ1)=0.0358.

<<<PAGE 228>>>

212 13. Linear and Logistic Regression
0 125000 2500000 1500 3000
0 125000 250000−500 0 500
7 8 9 10 11 12 132345678
7 8 9 10 11 12 13−1 0 1BushBuchanan
Bushresiduals
BushBuchanan
Bushresiduals
FIGURE 13.2. Voting Data for Election 2000. See example 13.6.
This gives the ﬁt
log(Buchanan) = −2.3298 + 0 .7303 log(Bush) .
The residuals look much healthier. Later, we shall address the following ques-
tion: how do we see if Palm Beach County has a statistically plausible out-come?
/squaresolid
13.2 Least Squares and Maximum Likelihood
Suppose we add the assumption that /epsilon1i|Xi∼N(0,σ2), that is,
Yi|Xi∼N(µi,σ2)

<<<PAGE 229>>>

13.2 Least Squares and Maximum Likelihood 213
where µi=β0+β1Xi. The likelihood function is
nproductdisplay
i=1f(Xi,Yi)=nproductdisplay
i=1fX(Xi)fY|X(Yi|Xi)
=nproductdisplay
i=1fX(Xi)×nproductdisplay
i=1fY|X(Yi|Xi)
=L1×L2
where L1=producttextn
i=1fX(Xi) and
L2=nproductdisplay
i=1fY|X(Yi|Xi). (13.8)
The term L1does not involve the parameters β0andβ1. We shall focus on
the second term L2which is called the conditional likelihood , given by
L2≡L(β0,β1,σ)=nproductdisplay
i=1fY|X(Yi|Xi)∝σ−nexpbraceleftBigg
−1
2σ2summationdisplay
i(Yi−µi)2bracerightBigg
.
The conditional log-likelihood is
/lscript(β0,β1,σ)=−nlogσ−1
2σ2nsummationdisplay
i=1parenleftbigg
Yi−(β0+β1Xi)parenrightbigg2
. (13.9)
To ﬁnd the mleof (β0,β1) we maximize /lscript(β0,β1,σ). From (13.9) we see that
maximizing the likelihood is the same as minimizing the rsssummationtextn
i=1parenleftbigg
Yi−(β0+
β1Xi)parenrightbigg2
. Therefore, we have shown the following:
13.7 Theorem. Under the assumption of Normality, the least squares estima-
tor is also the maximum likelihood estimator.
We can also maximize /lscript(β0,β1,σ) over σ, yielding the mle
hatwideσ2=1
nsummationdisplay
ihatwide/epsilon12
i. (13.10)
This estimator is similar to, but not identical to, the unbiased estimator.
Common practice is to use the unbiased estimator (13.7).

<<<PAGE 230>>>

214 13. Linear and Logistic Regression
13.3 Properties of the Least Squares Estimators
We now record the standard errors and limiting distribution of the least
squares estimator. In regression problems, we usually focus on the proper-ties of the estimators conditional on X
n=(X1,...,X n). Thus, we state the
means and variances as conditional means and variances.
13.8 Theorem. LethatwideβT=(hatwideβ0,hatwideβ1)Tdenote the least squares estimators.
Then,
E(hatwideβ|Xn)=parenleftbigg
β0
β1parenrightbigg
V(hatwideβ|Xn)=σ2
ns2
XparenleftBigg1
nsummationtextn
i=1X2
i−Xn
−Xn 1parenrightBigg
(13.11)
where s2
X=n−1summationtextn
i=1(Xi−Xn)2.
The estimated standard errors of hatwideβ0andhatwideβ1are obtained by taking the
square roots of the corresponding diagonal terms of V(hatwideβ|Xn) and inserting
the estimate hatwideσforσ. Thus,
hatwidese(hatwideβ0)=hatwideσ
sX√nradicalbiggsummationtextn
i=1X2
i
n(13.12)
hatwidese(hatwideβ1)=hatwideσ
sX√n. (13.13)
We should really write these as hatwidese(hatwideβ0|Xn) andhatwidese(hatwideβ1|Xn) but we will use the
shorter notation hatwidese(hatwideβ0) andhatwidese(hatwideβ1).
13.9 Theorem. Under appropriate conditions we have:
1. (Consistency): hatwideβ0P−→β0andhatwideβ1P−→β1.
2. (Asymptotic Normality):
hatwideβ0−β0
hatwidese(hatwideβ0)/squigglerightN(0,1) andhatwideβ1−β1
hatwidese(hatwideβ1)/squigglerightN(0,1).
3. Approximate 1−αconﬁdence intervals for β0andβ1are
hatwideβ0±zα/2hatwidese(hatwideβ0) and hatwideβ1±zα/2hatwidese(hatwideβ1). (13.14)

<<<PAGE 231>>>

13.4 Prediction 215
4. The Wald test2for testing H0:β1=0versus H1:β1/negationslash=0is: reject H0
if|W|>zα/2where W=hatwideβ1/hatwidese(hatwideβ1).
13.10 Example. For the election data, on the log scale, a 95 percent conﬁ-
dence interval is .7303±2(.0358) = ( .66,.80). The Wald statistics for testing
H0:β1= 0 versus H1:β1/negationslash=0i s |W|=|.7303−0|/.0358 = 20 .40 with a
p-value of P(|Z|>20.40)≈0. This is strong evidence that that the true slope
is not 0. /squaresolid
13.4 Prediction
Suppose we have estimated a regression model hatwider(x)=hatwideβ0+hatwideβ1xfrom data
(X1,Y1),...,(Xn,Yn). We observe the value X=x∗of the covariate for a
new subject and we want to predict their outcome Y∗. An estimate of Y∗is
hatwideY∗=hatwideβ0+hatwideβ1x∗. (13.15)
Using the formula for the variance of the sum of two random variables,
V(hatwideY∗)=V(hatwideβ0+hatwideβ1x∗)=V(hatwideβ0)+x2
∗V(hatwideβ1)+2x∗Cov(hatwideβ0,hatwideβ1).
Theorem 13.8 gives the formulas for all the terms in this equation. The es-
timated standard error hatwidese(hatwideY∗) is the square root of this variance, with hatwideσ2in
place of σ2. However, the conﬁdence interval for Y∗isnotof the usual form
hatwideY∗±zα/2hatwidese. The reason for this is explained in Exercise 10. The correct form
of the conﬁdence interval is given in the following theorem.
13.11 Theorem (Prediction Interval) .Let
hatwideξ2
n=hatwideσ2parenleftbiggsummationtextn
i=1(Xi−X∗)2
nsummationtext
i(Xi−X)2+1parenrightbigg
. (13.16)
An approximate 1−αprediction interval for Y∗is
hatwideY∗±zα/2hatwideξn. (13.17)
2Recall from equation (10.5) that the Wald statistic for testing H0:β=β0versus H1:
β/negationslash=β0isW=(/hatwideβ−β0)//hatwidese(/hatwideβ).

<<<PAGE 232>>>

216 13. Linear and Logistic Regression
13.12 Example (Election Data Revisited) .On the log scale, our linear regres-
sion gives the following prediction equation:
log(Buchanan) = −2.3298 + 0 .7303 log(Bush) .
In Palm Beach, Bush had 152,954 votes and Buchanan had 3,467 votes. On the
log scale this is 11.93789 and 8.151045. How likely is this outcome, assumingour regression model is appropriate? Our prediction for log Buchanan votes-2.3298 + .7303 (11.93789)=6.388441. Now, 8.151045 is bigger than 6.388441but is it “signiﬁcantly” bigger? Let us compute a conﬁdence interval. Weﬁnd that hatwideξ
n=.093775 and the approximate 95 percent conﬁdence interval is
(6.200,6.578) which clearly excludes 8.151. Indeed, 8.151 is nearly 20 standarderrors from hatwideY
∗. Going back to the vote scale by exponentiating, the conﬁdence
interval is (493,717) compared to the actual number of votes which is 3,467.
/squaresolid
13.5 Multiple Regression
Now suppose that the covariate is a vector of length k. The data are of the
form
(Y1,X1),...,(Yi,Xi),...,(Yn,Xn)
where
Xi=(Xi1,...,X ik).
Here, Xiis the vector of kcovariate values for the ithobservation. The linear
regression model is
Yi=ksummationdisplay
j=1βjXij+/epsilon1i (13.18)
fori=1,...,n , where E(/epsilon1i|X1i,...,X ki) = 0. Usually we want to include an
intercept in the model which we can do by setting Xi1= 1 for i=1,...,n .A t
this point it will be more convenient to express the model in matrix notation.The outcomes will be denoted by
Y=
Y
1
Y2
...
Yn


<<<PAGE 233>>>

13.5 Multiple Regression 217
and the covariates will be denoted by
X=
X
11X12... X 1k
X21X22... X 2k
............
X
n1Xn2... X nk
.
Each row is one observation; the columns correspond to the kcovariates. Thus,
Xi sa(n×k) matrix. Let
β=
β
1
...
βk
and /epsilon1=
/epsilon11
...
/epsilon1n
.
Then we can write (13.18) as
Y=Xβ+/epsilon1. (13.19)
The form of the least squares estimate is given in the following theorem.
13.13 Theorem. Assuming that the (k×k)matrix XTXis invertible,
hatwideβ=(XTX)−1XTY (13.20)
V(hatwideβ|Xn)= σ2(XTX)−1(13.21)
hatwideβ≈N(β,σ2(XTX)−1). (13.22)
The estimate regression function is hatwider(x)=summationtextk
j=1hatwideβjxj. An unbiased esti-
mate of σ2is
hatwideσ2=parenleftbigg1
n−kparenrightbiggnsummationdisplay
i=1hatwide/epsilon12
i
wherehatwide/epsilon1=Xhatwideβ−Yis the vector of residuals. An approximate 1 −αconﬁdence
interval for βjis
hatwideβj±zα/2hatwidese(hatwideβj) (13.23)
wherehatwidese2(hatwideβj)i st h e jthdiagonal element of the matrix hatwideσ2(XTX)−1.
13.14 Example. Crime data on 47 states in 1960 can be obtained from
http://lib.stat.cmu.edu/DASL/Stories/USCrime.html.
If we ﬁt a linear regression of crime rate on 10 variables we get the following:

<<<PAGE 234>>>

218 13. Linear and Logistic Regression
Covariate /hatwideβj/hatwidese(/hatwideβj) t value p-value
(Intercept) -589.39 167.59 -3.51 0.001 **
Age 1.04 0.45 2.33 0.025 *Southern State 11.29 13.24 0.85 0.399Education 1.18 0.68 1.7 0.093Expenditures 0.96 0.25 3.86 0.000 ***Labor 0.11 0.15 0.69 0.493Number of Males 0.30 0.22 1.36 0.181Population 0.09 0.14 0.65 0.518Unemployment (14–24) -0.68 0.48 -1.4 0.165Unemployment (25–39) 2.15 0.95 2.26 0.030 *Wealth -0.08 0.09 -0.91 0.367
This table is typical of the output of a multiple regression program. The “t-
value” is the Wald test statistic for testing H0:βj= 0 versus H1:βj/negationslash= 0. The
asterisks denote “degree of signiﬁcance” and more asterisks denote smallerp-values. The example raises several important questions: (1) should we elim-inate some variables from this model? (2) should we interpret these relation-ships as causal? For example, should we conclude that low crime preventionexpenditures cause high crime rates? We will address question (1) in the nextsection. We will not address question (2) until Chapter 16.
/squaresolid
13.6 Model Selection
Example 13.14 illustrates a problem that often arises in multiple regression.We may have data on many covariates but we may not want to include all ofthem in the model. A smaller model with fewer covariates has two advantages:it might give better predictions than a big model and it is more parsimonious(simpler). Generally, as you add more variables to a regression, the bias of thepredictions decreases and the variance increases. Too few covariates yields highbias; this called underﬁtting . Too many covariates yields high variance; this
called overﬁtting . Good predictions result from achieving a good balance
between bias and variance.
In model selection there are two problems: (i) assigning a “score” to each
model which measures, in some sense, how good the model is, and (ii) search-ing through all the models to ﬁnd the model with the best score.
Let us ﬁrst discuss the problem of scoring models. Let S⊂{1,...,k }and
letX
S={Xj:j∈S}denote a subset of the covariates. Let βSdenote the
coeﬃcients of the corresponding set of covariates and let hatwideβSdenote the least
squares estimate of βS. Also, let XSdenote the Xmatrix for this subset of

<<<PAGE 235>>>

13.6 Model Selection 219
covariates and deﬁne hatwiderS(x) to be the estimated regression function. The pre-
dicted values from model Sare denoted by hatwideYi(S)=hatwiderS(Xi). The prediction
riskis deﬁned to be
R(S)=nsummationdisplay
i=1E(hatwideYi(S)−Y∗
i)2(13.24)
where Y∗
idenotes the value of a future observation of Yiat covariate value
Xi. Our goal is to choose Sto make R(S) small.
Thetraining error is deﬁned to be
hatwideRtr(S)=nsummationdisplay
i=1(hatwideYi(S)−Yi)2.
This estimate is very biased as an estimate of R(S).
13.15 Theorem. The training error is a downward-biased estimate of the pre-
diction risk:
E(hatwideRtr(S))<R(S).
In fact,
bias(hatwideRtr(S)) =E(hatwideRtr(S))−R(S)=−2summationdisplay
i=1Cov(hatwideYi,Yi). (13.25)
The reason for the bias is that the data are being used twice: to estimate
the parameters and to estimate the risk. When we ﬁt a complex model withmany parameters, the covariance Cov(hatwideY
i,Yi) will be large and the bias of the
training error gets worse. Here are some better estimates of risk.
Mallow’s Cpstatistic is deﬁned by
hatwideR(S)=hatwideRtr(S)+2|S|hatwideσ2(13.26)
where |S|denotes the number of terms in Sandhatwideσ2is the estimate of σ2
obtained from the full model (with all covariates in the model). This is simply
the training error plus a bias correction. This estimate is named in honor ofColin Mallows who invented it. The ﬁrst term in (13.26) measures the ﬁt ofthe model while the second measure the complexity of the model. Think oftheC
pstatistic as:
lack of ﬁt + complexity penalty .
Thus,ﬁnding a good model involves trading oﬀ ﬁt and complexity.

<<<PAGE 236>>>

220 13. Linear and Logistic Regression
A related method for estimating risk is AIC (Akaike Information Cri-
terion). The idea is to choose Sto maximize
/lscriptS−|S| (13.27)
where /lscriptSis the log-likelihood of the model evaluated at the mle.3This can
be thought of “goodness of ﬁt” minus “complexity.” In linear regression withNormal errors (and taking σequal to its estimate from the largest model),
maximizing AIC is equivalent to minimizing Mallow’s C
p; see Exercise 8. The
appendix contains more explanation about AIC.
Yet another method for estimating risk is leave-one-out cross-validation .
In this case, the risk estimator is
hatwideRCV(S)=nsummationdisplay
i=1(Yi−hatwideY(i))2(13.28)
wherehatwideY(i)is the prediction for Yiobtained by ﬁtting the model with Yiomit-
ted. It can be shown that
hatwideRCV(S)=nsummationdisplay
i=1parenleftBigg
Yi−hatwideYi(S)
1−Uii(S)parenrightBigg2
(13.29)
where Uii(S)i st h e ithdiagonal element of the matrix
U(S)=XS(XT
SXS)−1XT
S. (13.30)
Thus, one need not actually drop each observation and re-ﬁt the model. A
generalization is k-fold cross-validation. Here we divide the data into k
groups; often people take k= 10. We omit one group of data and ﬁt the
models to the remaining data. We use the ﬁtted model to predict the datain the group that was omitted. We then estimate the risk bysummationtext
i(Yi−hatwideYi)2
where the sum is over the the data points in the omitted group. This process is
repeated for each of the kgroups and the resulting risk estimates are averaged.
For linear regression, Mallows Cpand cross-validation often yield essentially
the same results so one might as well use Mallows’ method. In some of themore complex problems we will discuss later, cross-validation will be moreuseful.
Another scoring method is BIC (Bayesian information criterion). Here we
choose a model to maximize
BIC(S)=/lscript
S−|S|
2logn. (13.31)
3Some texts use a slightly diﬀerent deﬁnition of AIC which involves multiplying the deﬁnition
here by 2 or -2. This has no eﬀect on which model is selected.

<<<PAGE 237>>>

13.6 Model Selection 221
The BIC score has a Bayesian interpretation. Let S={S1,...,S m}denote
a set of models. Suppose we assign the prior P(Sj)=1/mover the models.
Also, assume we put a smooth prior on the parameters within each model. Itcan be shown that the posterior probability for a model is approximately,
P(S
j|data) ≈eBIC(Sj)
summationtext
reBIC(Sr).
Hence, choosing the model with highest BIC is like choosing the model with
highest posterior probability. The BIC score also has an information-theoreticinterpretation in terms of something called minimum description length. TheBIC score is identical to Mallows C
pexcept that it puts a more severe penalty
for complexity. It thus leads one to choose a smaller model than the othermethods.
Now let us turn to the problem of model search. If there are kcovariates
then there are 2
kpossible models. We need to search through all these models,
assign a score to each one, and choose the model with the best score. If kis
not too large we can do a complete search over all the models. When kis large,
this is infeasible. In that case we need to search over a subset of all the models.Two common methods are forward and backward stepwise regression.
In forward stepwise regression, we start with no covariates in the model. Wethen add the one variable that leads to the best score. We continue addingvariables one at a time until the score does not improve. Backwards stepwiseregression is the same except that we start with the biggest model and dropone variable at a time. Both are greedy searches; nether is guaranteed toﬁnd the model with the best score. Another popular method is to do randomsearching through the set of all models. However, there is no reason to expectthis to be superior to a deterministic search.
13.16 Example. We applied backwards stepwise regression to the crime data
using AIC. The following was obtained from the program R. This programuses a slightly diﬀerent deﬁnition of AIC. With their deﬁnition, we seek thesmallest (not largest) possible AIC. This is the same is minimizing MallowsC
p.
The full model (which includes all covariates) has AIC= 310.37. In ascend-
ing order, the AIC scores for deleting one variable are as follows:
variable Pop Labor South Wealth Males U1 Educ. U2 Age Expend
AIC 308 309 309 309 310 310 312 314 315 324
For example, if we dropped Pop from the model and kept the other terms,
then the AIC score would be 308. Based on this information we drop “pop-

<<<PAGE 238>>>

222 13. Linear and Logistic Regression
ulation” from the model and the current AIC score is 308. Now we consider
dropping a variable from the current model. The AIC scores are:
variable South Labor Wealth Males U1 Education U2 Age Expend
AIC 308 308 308 309 309 310 313 313 329
We then drop “Southern” from the model. This process is continued untilthere is no gain in AIC by dropping any variables. In the end, we are left withthe following model:
Crime = 1 .2 Age + .75 Education + .87 Expenditure
+.34 Males −.86 U1 + 2 .31 U2 .
Warning! This does not yet address the question of which variables are
causes of crime.
/squaresolid
There is another method for model selection that avoids having to search
through all possible models. This method, which is due to Zheng and Loh(1995), does not seek to minimize prediction errors. Rather, it assumes somesubset of the β
j’s are exactly equal to 0 and tries to ﬁnd the true model,
that is, the smallest sub-model consisting of nonzero βjterms. The method
is carried out as follows.
Zheng-Loh Model Selection Method4
1. Fit the full model with all kcovariates and let Wj=hatwideβj/hatwidese(hatwideβj) denote
the Wald test statistic for H0:βj= 0 versus H1:βj/negationslash=0 .
2. Order the test statistics from largest to smallest in absolute value:
|W(1)|≥|W(2)|≥···≥| W(k)|.
3. Lethatwidejbe the value of jthat minimizes
rss(j)+jhatwideσ2logn
where rss(j) is the residual sums of squares from the model with
thejlargest Wald statistics.
4. Choose, as the ﬁnal model, the regression with the hatwidejterms with the
largest absolute Wald statistics.

<<<PAGE 239>>>

13.7 Logistic Regression 223
x 01
FIGURE 13.3. The logistic function p=ex/(1 +ex).
Zheng and Loh showed that, under appropriate conditions, this method
chooses the true model with probability tending to one as the sample sizeincreases.
13.7 Logistic Regression
So far we have assumed that Yiis real valued. Logistic regression is a para-
metric method for regression when Yi∈{0,1}is binary. For a k-dimensional
covariate X, the model is
pi≡pi(β)≡P(Yi=1|X=x)=eβ0+/summationtextk
j=1βjxij
1+eβ0+/summationtextk
j=1βjxij(13.32)
or, equivalently,
logit(pi)=β0+ksummationdisplay
j=1βjxij (13.33)
where
logit(p) = logparenleftbiggp
1−pparenrightbigg
. (13.34)
The name “logistic regression” comes from the fact that ex/(1 +ex) is called
the logistic function. A plot of the logistic for a one-dimensional covariate isshown in Figure 13.3.
Because the Y
i’s are binary, the data are Bernoulli:
Yi|Xi=xi∼Bernoulli( pi).
Hence the (conditional) likelihood function is
L(β)=nproductdisplay
i=1pi(β)Yi(1−pi(β))1−Yi. (13.35)
4This is just one version of their method. In particular, the penalty jlognis only one choice
from a set of possible penalty functions.

<<<PAGE 240>>>

224 13. Linear and Logistic Regression
The mlehatwideβhas to be obtained by maximizing L(β) numerically. There is
a fast numerical algorithm called reweighted least squares. The steps are asfollows:
Reweighted Least Squares Algorithm
Choose starting values hatwideβ0=(hatwideβ0
0,...,hatwideβ0
k) and compute p0
iusing equation
(13.32), for i=1,...,n . Sets= 0 and iterate the following steps until
convergence.
1. Set
Zi= logit( ps
i)+Yi−ps
i
psi(1−ps
i),i=1,...,n .
2. Let Wbe a diagonal matrix with ( i, i) element equal to ps
i(1−ps
i).
3. Set
hatwideβs=(XTWX)−1XTWY.
This corresponds to doing a (weighted) linear regression of ZonY.
4. Set s=s+ 1 and go back to the ﬁrst step.
The Fisher information matrix Ican also be obtained numerically. The
estimate standard error of hatwideβjis the ( j,j) element of J=I−1. Model selection
is usually done using the AIC score /lscriptS−|S|.
13.17 Example. The Coronary Risk-Factor Study (CORIS) data involve 462
males between the ages of 15 and 64 from three rural areas in South Africa,(Rousseauw et al. (1983)). The outcome Yis the presence ( Y= 1) or absence
(Y= 0) of coronary heart disease. There are 9 covariates: systolic blood
pressure, cumulative tobacco (kg), ldl (low density lipoprotein cholesterol),adiposity, famhist (family history of heart disease), typea (type-A behavior),obesity, alcohol (current alcohol consumption), and age. A logistic regressionyields the following estimates and Wald statistics W
jfor the coeﬃcients:

<<<PAGE 241>>>

13.8 Bibliographic Remarks 225
Covariate hatwideβjhatwidese Wjp-value
Intercept -6.145 1.300 -4.738 0.000
sbp 0.007 0.006 1.138 0.255tobacco 0.079 0.027 2.991 0.003ldl 0.174 0.059 2.925 0.003adiposity 0.019 0.029 0.637 0.524famhist 0.925 0.227 4.078 0.000typea 0.040 0.012 3.233 0.001obesity -0.063 0.044 -1.427 0.153alcohol 0.000 0.004 0.027 0.979age 0.045 0.012 3.754 0.000
Are you surprised by the fact that systolic blood pressure is not signiﬁcant
or by the minus sign for the obesity coeﬃcient? If yes, then you are confusingassociation and causation. This issue is discussed in Chapter 16. The factthat blood pressure is not signiﬁcant does not mean that blood pressure isnot an important cause of heart disease. It means that it is not an important
predictor of heart disease relative to the other variables in the model.
/squaresolid
13.8 Bibliographic Remarks
A succinct book on linear regression is Weisberg (1985). A data-mining view
of regression is given in Hastie et al. (2001). The Akaike Information Criterion(AIC) is due to Akaike (1973). The Bayesian Information Criterion (BIC) isdue to Schwarz (1978). References on logistic regression include Agresti (1990)and Dobson (2001).
13.9 Appendix
The Akaike Information Criterion (AIC). Consider a set of models
{M1,M2,...}. Lethatwidefj(x) denote the estimated probability function obtained
by using the maximum likelihood estimator of model Mj. Thus, hatwidefj(x)=
hatwidef(x;hatwideβj) where hatwideβjis the mleof the set of parameters βjfor model Mj.W e
will use the loss function D(f,hatwidef) where
D(f,g)=summationdisplay
xf(x) logparenleftbiggf(x)
g(x)parenrightbigg
is the Kullback-Leibler distance between two probability functions. The cor-
responding risk function is R(f,hatwidef)=E(D(f,hatwidef). Notice that D(f,hatwidef)=c−

<<<PAGE 242>>>

226 13. Linear and Logistic Regression
A(f,hatwidef) where c=summationtext
xf(x) logf(x) does not depend on hatwidefand
A(f,hatwidef)=summationdisplay
xf(x) loghatwidef(x).
Thus, minimizing the risk is equivalent to maximizing a(f,hatwidef)≡E(A(f,hatwidef)).
It is tempting to estimate a(f,hatwidef)b ysummationtext
xhatwidef(x) loghatwidef(x) but, just as the train-
ing error in regression is a highly biased estimate of prediction risk, it is alsothe case thatsummationtext
xhatwidef(x) loghatwidef(x) is a highly biased estimate of a(f,hatwidef). In fact,
the bias is approximately equal to |Mj|. Thus:
13.18 Theorem. AIC(Mj)is an approximately unbiased estimate of a(f,hatwidef).
13.10 Exercises
1. Prove Theorem 13.4.
2. Prove the formulas for the standard errors in Theorem 13.8. You should
regard the Xi’s as ﬁxed constants.
3. Consider the regression through the origin model:
Yi=βXi+/epsilon1.
Find the least squares estimate for β. Find the standard error of the
estimate. Find conditions that guarantee that the estimate is consistent.
4. Prove equation (13.25).5. In the simple linear regression model, construct a Wald test for H
0:
β1=1 7β0versus H1:β1/negationslash=1 7β0.
6. Get the passenger car mileage data from
http://lib.stat.cmu.edu/DASL/Dataﬁles/carmpgdat.html(a) Fit a simple linear regression model to predict MPG (miles per
gallon) from HP (horsepower). Summarize your analysis including aplot of the data with the ﬁtted line.
(b) Repeat the analysis but use log(MPG) as the response. Compare
the analyses.

<<<PAGE 243>>>

13.10 Exercises 227
7. Get the passenger car mileage data from
http://lib.stat.cmu.edu/DASL/Dataﬁles/carmpgdat.html(a) Fit a multiple linear regression model to predict MPG (miles per
gallon) from the other variables. Summarize your analysis.
(b) Use Mallow C
pto select a best sub-model. To search through the
models try (i) forward stepwise, (ii) backward stepwise. Summarize yourﬁndings.
(c) Use the Zheng-Loh model selection method and compare to (b).(d) Perform all possible regressions. Compare C
pand BIC. Compare the
results.
8. Assume a linear regression model with Normal errors. Take σknown.
Show that the model with highest AIC (equation (13.27)) is the modelwith the lowest Mallows C
pstatistic.
9. In this question we will take a closer look at the AIC method. Let
X1,...,X nbeiidobservations. Consider two models M0andM1. Un-
derM0the data are assumed to be N(0,1) while under M1the data
are assumed to be N(θ,1) for some unknown θ∈R:
M0:X1,...,X n∼N(0,1)
M1:X1,...,X n∼N(θ,1),θ∈R.
This is just another way to view the hypothesis testing problem: H0:
θ= 0 versus H1:θ/negationslash= 0. Let /lscriptn(θ) be the log-likelihood function.
The AIC score for a model is the log-likelihood at the mleminus the
number of parameters. (Some people multiply this score by 2 but thatis irrelevant.) Thus, the AIC score for M
0isAIC0=/lscriptn(0) and the AIC
score for M1isAIC1=/lscriptn(hatwideθ)−1. Suppose we choose the model with
the highest AIC score. Let Jndenote the selected model:
Jn=braceleftbigg0i fAIC0>A I C 1
1i fAIC1>A I C 0.
(a) Suppose that M0is the true model, i.e. θ= 0. Find
lim
n→∞P(Jn=0 ).
Now compute lim n→∞P(Jn= 0) when θ/negationslash=0 .

<<<PAGE 244>>>

228 13. Linear and Logistic Regression
(b) The fact that lim n→∞P(Jn=0 )/negationslash= 1 when θ= 0 is why some people
say that AIC “overﬁts.” But this is not quite true as we shall now see.Letφ
θ(x) denote a Normal density function with mean θand variance
1. Deﬁne
hatwidefn(x)=braceleftbigg
φ0(x)i fJn=0
φ/hatwideθ(x)i fJn=1.
Ifθ= 0, show that D(φ0,hatwidefn)p→0a sn→∞ where
D(f,g)=integraldisplay
f(x) logparenleftbiggf(x)
g(x)parenrightbigg
dx
is the Kullback-Leibler distance. Show also that D(φθ,hatwidefn)p→0i fθ/negationslash=0 .
Hence, AIC consistently estimates the true density even if it “over-shoots” the correct model.
(c) Repeat this analysis for BIC which is the log-likelihood minus ( p/2) log n
where pis the number of parameters and nis sample size.
10. In this question we take a closer look at prediction intervals. Let θ=
β
0+β1X∗and lethatwideθ=hatwideβ0+hatwideβ1X∗. Thus,hatwideY∗=hatwideθwhile Y∗=θ+/epsilon1.N o w ,
hatwideθ≈N(θ,se2) where
se2=V(hatwideθ)=V(hatwideβ0+hatwideβ1x∗).
Note that V(hatwideθ) is the same as V(hatwideY∗). Now,hatwideθ±2radicalBig
V(hatwideθ) is an approximate
95 percent conﬁdence interval for θ=β0+β1x∗using the usual argument
for a conﬁdence interval. But, as you shall now show, it is not a validconﬁdence interval for Y
∗.
(a) Let s=radicalBig
V(hatwideY∗). Show that
P(hatwideY∗−2s<Y ∗<hatwideY∗+2s)≈Pparenleftbigg
−2<Nparenleftbigg
0,1+σ2
s2parenrightbigg
<2parenrightbigg
/negationslash=0.95.
(b) The problem is that the quantity of interest Y∗is equal to a param-
eterθplus a random variable. We can ﬁx this by deﬁning
ξ2
n=V(hatwideY∗)+σ2=bracketleftbiggsummationtext
i(xi−x∗)2
nsummationtext
i(xi−x)2+1bracketrightbigg
σ2.
In practice, we substitute hatwideσforσand we denote the resulting quantity
byhatwideξn. Now consider the interval hatwideY∗±2hatwideξn. Show that
P(hatwideY∗−2hatwideξn<Y∗<hatwideY∗+2hatwideξn)≈P(−2<N(0,1)<2)≈0.95.

<<<PAGE 245>>>

13.10 Exercises 229
11. Get the Coronary Risk-Factor Study (CORIS) data from the book web
site. Use backward stepwise logistic regression based on AIC to select amodel. Summarize your results.

<<<PAGE 246>>>



<<<PAGE 247>>>

14
Multivariate Models
In this chapter we revisit the Multinomial model and the multivariate Normal.
Let us ﬁrst review some notation from linear algebra. In what follows, xand
yare vectors and Ais a matrix.
Linear Algebra Notation
xTyinner productsummationtext
jxjyj
|A|determinant
ATtranspose of A
A−1inverse of A
I the identity matrix
tr(A) trace of a square matrix; sum of its diagonal elements
A1/2square root matrix
The trace satisﬁes tr( AB) = tr( BA) and tr( A) + tr( B). Also, tr( a)=aifa
is a scalar. A matrix is positive deﬁnite ifxTΣx>0 for all nonzero vectors
x. If a matrix Ais symmetric and positive deﬁnite, its square root A1/2exists
and has the following properties: (1) A1/2is symmetric; (2) A=A1/2A1/2;
(3)A1/2A−1/2=A−1/2A1/2=Iwhere A−1/2=(A1/2)−1.

<<<PAGE 248>>>

232 14. Multivariate Models
14.1 Random Vectors
Multivariate models involve a random vector Xof the form
X=
X1
...
Xk
.
The mean of a random vector Xis deﬁned by
µ=
µ1
...
µk
=
E(X1)
...
E(Xk)
. (14.1)
Thecovariance matrix Σ, also written V(X), is deﬁned to be
Σ=
V(X
1) Cov(X1,X2)···Cov(X1,Xk)
Cov(X2,X1)V(X2) ···Cov(X2,Xk)
............
Cov(X
k,X1)Cov(Xk,X2)···V(Xk)
. (14.2)
This is also called the variance matrix or the variance–covariance matrix. The
inverse Σ −1is called the precision matrix .
14.1 Theorem. Letabe a vector of length kand let Xbe a random vector
of the same length with mean µand variance Σ. Then E(aTX)=aTµand
V(aTX)=aTΣa.I fAis a matrix with kcolumns, then E(AX)=Aµand
V(AX)=AΣAT.
Now suppose we have a random sample of nvectors:

X
11
X21
...
Xk1
,
X
12
X22
...
Xk2
, ...,
X
1n
X2n
...
Xkn
. (14.3)
The sample meanXis a vector deﬁned by
X=
X1
...
Xk


<<<PAGE 249>>>

14.2 Estimating the Correlation 233
where Xi=n−1summationtextn
j=1Xij. The sample variance matrix, also called the co-
variance matrix or the variance–covariance matrix, is
S=
s
11s12···s1k
s12s22···s2k
............
s
1ks2k···skk
(14.4)
where
s
ab=1
n−1nsummationdisplay
j=1(Xaj−Xa)(Xbj−Xb).
It follows that E(X)=µ. and E(S)=Σ .
14.2 Estimating the Correlation
Consider ndata points from a bivariate distribution:
parenleftbiggX11
X21parenrightbigg
,parenleftbiggX12
X22parenrightbigg
,···,parenleftbiggX1n
X2nparenrightbigg
.
Recall that the correlation between X1andX2is
ρ=E((X1−µ1)(X2−µ2))
σ1σ2(14.5)
where σ2
j=V(Xji),j=1,2. The nonparametric plug-in estimator is the
sample correlation1
hatwideρ=summationtextn
i=1(X1i−X1)(X2i−X2)
s1s2(14.6)
where
s2
j=1
n−1nsummationdisplay
i=1(Xji−Xj)2.
We can construct a conﬁdence interval for ρby applying the delta method.
However, it turns out that we get a more accurate conﬁdence interval by ﬁrstconstructing a conﬁdence interval for a function θ=f(ρ) and then applying
1More precisely, the plug-in estimator has nrather than n−1in the formula for sjbut this
diﬀerence is small.

<<<PAGE 250>>>

234 14. Multivariate Models
the inverse function f−1. The method, due to Fisher, is as follows: Deﬁne f
and its inverse by
f(r)=1
2parenleftbigg
log(1 + r)−log(1−r)parenrightbigg
f−1(z)=e2z−1
e2z+1.
Approximate Conﬁdence Interval for The Correlation
1. Compute
hatwideθ=f(hatwideρ)=1
2parenleftbigg
log(1 +hatwideρ)−log(1−hatwideρ)parenrightbigg
.
2. Compute the approximate standard error of hatwideθwhich can be shown to
be
hatwidese(hatwideθ)=1√n−3.
3. An approximate 1 −αconﬁdence interval for θ=f(ρ)i s
(a, b)≡parenleftbigg
hatwideθ−zα/2√n−3,hatwideθ+zα/2√n−3parenrightbigg
.
4. Apply the inverse transformation f−1(z) to get a conﬁdence interval
forρ:parenleftbigge2a−1
e2a+1,e2b−1
e2b+1parenrightbigg
.
Yet another method for getting a conﬁdence interval for ρis to use the
bootstrap.
14.3 Multivariate Normal
Recall that a vector Xhas a multivariate Normal distribution, denoted by
X∼N(µ,Σ), if its density is
f(x;µ,Σ) =1
(2π)k/2|Σ|1/2expbraceleftbigg
−1
2(x−µ)TΣ−1(x−µ)bracerightbigg
(14.7)
where µis a vector of length ka n dΣi sa k×ksymmetric, positive deﬁnite
matrix. Then E(X)=µandV(X)=Σ .

<<<PAGE 251>>>

14.4 Multinomial 235
14.2 Theorem. The following properties hold:
1. IfZ∼N(0,1)andX=µ+Σ1/2Z, then X∼N(µ,Σ).
2. IfX∼N(µ,Σ), then Σ−1/2(X−µ)∼N(0,1).
3. IfX∼N(µ,Σ)ais a vector of the same length as X, then aTX∼
N(aTµ, aTΣa).
4. Let
V=(X−µ)TΣ−1(X−µ).
Then V∼χ2
k.
14.3 Theorem. Given a random sample of size nfrom a N(µ,Σ), the log-
likelihood is (up to a constant not depending on µorΣ) given by
/lscript(µ,Σ) =−n
2(X−µ)TΣ−1(X−µ)−n
2tr(Σ−1S)−n
2log|Σ|.
The mleis
hatwideµ=XandhatwideΣ=parenleftbiggn−1
nparenrightbigg
S. (14.8)
14.4 Multinomial
Let us now review the Multinomial distribution. The data take the form
X=(X1,...,X k) where each Xjis a count. Think of drawing nballs (with
replacement) from an urn which has balls with kdiﬀerent colors. In this case,
Xjis the number of balls of the kthcolor. Let p=(p1,...,p k) where pj≥0
andsummationtextk
j=1pj= 1 and suppose that pjis the probability of drawing a ball of
colorj.
14.4 Theorem. LetX∼Multinomial( n, p). Then the marginal distribution
ofXjisXj∼Binomial( n, pj). The mean and variance of Xare
E(X)=
np1
...
npk

and
V(X)=
np
1(1−p1)−np1p2··· − np1pk
−np1p2np2(1−p2)··· − np2pk
............
−np
1pk −np2pk···npk(1−pk)
.

<<<PAGE 252>>>

236 14. Multivariate Models
Proof. ThatXj∼Binomial( n, pj) follows easily. Hence, E(Xj)=npjand
V(Xj)=npj(1−pj). To compute Cov(Xi,Xj) we proceed as follows: Notice
thatXi+Xj∼Binomial( n, pi+pj) and so V(Xi+Xj)=n(pi+pj)(1−pi−pj).
On the other hand,
V(Xi+Xj)= V(Xi)+V(Xj)+2Cov(Xi,Xj)
=npi(1−pi)+npj(1−pj)+2Cov(Xi,Xj).
Equating this last expression with n(pi+pj)(1−pi−pj) implies that Cov(Xi,Xj)=
−npipj./squaresolid
14.5 Theorem. The maximum likelihood estimator of pis
hatwidep=
hatwidep1
...
hatwidepk
=
X1
n...
Xk
n
=X
n.
Proof. The log-likelihood (ignoring a constant) is
/lscript(p)=ksummationdisplay
j=1Xjlogpj.
When we maximize /lscriptwe have to be careful since we must enforce the con-
straint thatsummationtext
jpj= 1. We use the method of Lagrange multipliers and instead
maximize
A(p)=ksummationdisplay
j=1Xjlogpj+λparenleftbiggsummationdisplay
jpj−1parenrightbigg
.
Now
∂A(p)
∂pj=Xj
pj+λ.
Setting∂A(p)
∂pj= 0 yields hatwidepj=−Xj/λ. Sincesummationtext
jhatwidepj= 1 we see that λ=−n
and hence hatwidepj=Xj/nas claimed. /squaresolid
Next we would like to know the variability of the mle. We can either
compute the variance matrix of hatwidepdirectly or we can approximate the vari-
ability of the mleby computing the Fisher information matrix. These two
approaches give the same answer in this case. The direct approach is easy:V(hatwidep)=V(X/n)=n
−2V(X), and so
V(hatwidep)=1
nΣ

<<<PAGE 253>>>

14.5 Bibliographic Remarks 237
where
Σ=
p
1(1−p1)−p1p2··· − p1pk
−p1p2p2(1−p2)··· − p2pk
............
−p
1pk −p2pk···pk(1−pk)
.
For large n,hatwidephas approximately a multivariate Normal distribution.
14.6 Theorem. Asn→∞,
√
n(hatwidep−p)/squigglerightN(0,Σ).
14.5 Bibliographic Remarks
Some references on multivariate analysis are Johnson and Wichern (1982) and
Anderson (1984). The method for constructing the conﬁdence interval for thecorrelation described in this chapter is due to Fisher (1921).
14.6 Appendix
Proof of Theorem 14.3. Denote the ithrandom vector by Xi. The log-
likelihood is
/lscript(µ,Σ) =nsummationdisplay
i=1f(Xi;µ,Σ)
=−kn
2log(2π)−n
2log|Σ|−1
2nsummationdisplay
i=1(Xi−µ)TΣ−1(Xi−µ).
Now,
nsummationdisplay
i=1(Xi−µ)TΣ−1(Xi−µ)
=nsummationdisplay
i=1[(Xi−X)+(X−µ)]TΣ−1[(Xi−X)+(X−µ)]
=nsummationdisplay
i=1bracketleftbig
(Xi−X)TΣ−1(Xi−X)bracketrightbig
+n(X−µ)TΣ−1(X−µ)
sincesummationtextn
i=1(Xi−X)Σ−1(X−µ) = 0. Also, notice that ( Xi−µ)TΣ−1(Xi−µ)
is a scalar, so
nsummationdisplay
i=1(Xi−µ)TΣ−1(Xi−µ)=nsummationdisplay
i=1trbracketleftbig
(Xi−µ)TΣ−1(Xi−µ)bracketrightbig

<<<PAGE 254>>>

238 14. Multivariate Models
=nsummationdisplay
i=1trbracketleftbig
Σ−1(Xi−µ)(Xi−µ)Tbracketrightbig
=t rbracketleftBigg
Σ−1nsummationdisplay
i=1(Xi−µ)(Xi−µ)TbracketrightBigg
=ntrbracketleftbig
Σ−1Sbracketrightbig
and the conclusion follows. /squaresolid
14.7 Exercises
1. Prove Theorem 14.1.
2. Find the Fisher information matrix for the mleof a Multinomial.
3. (Computer Experiment. ) Write a function to generate nsimobservations
from a Multinomial( n, p) distribution.
4. (Computer Experiment. ) Write a function to generate nsimobservations
from a Multivariate normal with given mean µand covariance matrix
Σ.
5. (Computer Experiment. ) Generate 100 random vectors from a N(µ,Σ)
distribution where
µ=parenleftbigg3
8parenrightbigg
,Σ=parenleftbigg11
12parenrightbigg
.
Plot the simulation as a scatterplot. Estimate the mean and covariance
matrix Σ. Find the correlation ρbetween X1andX2. Compare this
with the sample correlations from your simulation. Find a 95 percentconﬁdence interval for ρ. Use two methods: the bootstrap and Fisher’s
method. Compare.
6. (Computer Experiment. ) Repeat the previous exercise 1000 times. Com-
pare the coverage of the two conﬁdence intervals for ρ.

<<<PAGE 255>>>

15
Inference About Independence
In this chapter we address the following questions:
(1) How do we test if two random variables are independent?
(2) How do we estimate the strength of dependence between two
random variables?
When YandZare not independent, we say that they are dependent or
associated orrelated. IfYandZare associated, it does notimply that Y
causes Zor that Zcauses Y. Causation is discussed in Chapter 16.
Recall that we write Y/coproductZto mean that YandZare independent and we
write Y/gluonbelement/gluonelement/gluonelement/gluonelement/gluonelement/gluoneelementZto mean that YandZare dependent.
15.1 Two Binary Variables
Suppose that YandZare both binary and consider data ( Y1,Z1),...,(Yn,Zn).
We can represent the data as a two-by-two table:
Y=0 Y=1
Z=0 X00 X01 X0·
Z=1 X10 X11 X1·
X·0 X·1n=X··

<<<PAGE 256>>>

240 15. Inference About Independence
where
Xij= number of observations for which Y=iandZ=j.
The dotted subscripts denote sums. Thus,
Xi·=summationdisplay
jXij,X ·j=summationdisplay
iXij,n=X··=summationdisplay
i,jXij.
This is a convention we use throughout the remainder of the book. Denote
the corresponding probabilities by:
Y=0 Y=1
Z=0 p00 p01 p0·
Z=1 p10 p11 p1·
p·0 p·1 1
where pij=P(Z=i, Y=j). Let X=(X00,X01,X10,X11) denote the vector
of counts. Then X∼Multinomial( n, p) where p=(p00,p01,p10,p11). It is now
convenient to introduce two new parameters.
15.1 Deﬁnition. Theodds ratio is deﬁned to be
ψ=p00p11
p01p10. (15.1)
Thelog odds ratio is deﬁned to be
γ= log( ψ). (15.2)
15.2 Theorem. The following statements are equivalent:
1.Y/coproductZ.
2.ψ=1.
3.γ=0.
4. For i, j∈{0,1},pij=pi·p·j.
Now consider testing
H0:Y/coproductZversus H1:Y/gluonbelement/gluonelement/gluonelement/gluonelement/gluonelement/gluoneelementZ. (15.3)
First we consider the likelihood ratio test. Under H1,X∼Multinomial( n, p)
and the mleis the vector hatwidep=X/n. Under H0, we again have that X∼
Multinomial( n, p) but the restricted mleis computed under the constraint
pij=pi·p·jThis leads to the following test:

<<<PAGE 257>>>

15.1 Two Binary Variables 241
15.3 Theorem. The likelihood ratio test statistic for (15.3) is
T=21summationdisplay
i=01summationdisplay
j=0XijlogparenleftbiggXijX··
Xi·X·jparenrightbigg
. (15.4)
Under H0,T/squigglerightχ2
1. Thus, an approximate level αtest is obtained by
rejecting H0when T>χ2
1,α.
Another popular test for independence is Pearson’s χ2test.
15.4 Theorem. Pearson’s χ2test statistic for independence is
U=1summationdisplay
i=01summationdisplay
j=0(Xij−Eij)2
Eij(15.5)
where
Eij=Xi·X·j
n.
Under H0,U/squigglerightχ2
1. Thus, an approximate level αtest is obtained by
rejecting H0when U>χ2
1,α.
Here is the intuition for the Pearson test. Under H0,pij=pi·p·j,s ot h e
maximum likelihood estimator of pijunder H0is
hatwidepij=hatwidepi·hatwidep·j=Xi·
nX·j
n.
Thus, the expected number of observations in the (i,j) cell is
Eij=nhatwidepij=Xi·X·j
n.
The statistic Ucompares the observed and expected counts.
15.5 Example. The following data from Johnson and Johnson (1972) relate
tonsillectomy and Hodgkins disease.1
Hodgkins Disease No Disease
Tonsillectomy 90 165 255
No Tonsillectomy 84 307 391
Total 174 472 646
1The data are actually from a case-control study; see the appendix for an explanation of
case-control studies.

<<<PAGE 258>>>

242 15. Inference About Independence
We would like to know if tonsillectomy is related to Hodgkins disease. The
likelihood ratio statistic is T=1 4.75 and the p-value is P(χ2
1>14.75) = .0001.
Theχ2statistic is U=1 4.96 and the p-value is P(χ2
1>14.96) = .0001. We re-
ject the null hypothesis of independence and conclude that tonsillectomy is as-sociated with Hodgkins disease. This does not mean that tonsillectomies causeHodgkins disease. Suppose, for example, that doctors gave tonsillectomies tothe most seriously ill patients. Then the association between tonsillectomiesand Hodgkins disease may be due to the fact that those with tonsillectomieswere the most ill patients and hence more likely to have a serious disease.
/squaresolid
We can also estimate the strength of dependence by estimating the odds
ratioψand the log-odds ratio γ.
15.6 Theorem. The mle’s ofψandγare
hatwideψ=X00X11
X01X10,hatwideγ= loghatwideψ. (15.6)
The asymptotic standard errors (computed using the delta method) are
hatwidese(hatwideγ)=radicalbigg
1
X00+1
X01+1
X10+1
X11(15.7)
hatwidese(hatwideψ)=hatwideψhatwidese(hatwideγ). (15.8)
15.7 Remark. For small sample sizes, hatwideψandhatwideγcan have a very large variance.
In this case, we often use the modiﬁed estimator
hatwideψ=parenleftbig
X00+1
2parenrightbigparenleftbig
X11+1
2parenrightbig
parenleftbig
X01+1
2parenrightbigparenleftbig
X10+1
2parenrightbig. (15.9)
Another test for independence is the Wald test for γ= 0 given by W=
(hatwideγ−0)/hatwidese(hatwideγ) .A1 −αconﬁdence interval for γishatwideγ±zα/2hatwidese(hatwideγ).
A1−αconﬁdence interval for ψcan be obtained in two ways. First, we
could use hatwideψ±zα/2hatwidese(hatwideψ). Second, since ψ=eγwe could use
expbraceleftbig
hatwideγ±zα/2hatwidese(hatwideγ)bracerightbig
. (15.10)
This second method is usually more accurate.
15.8 Example. In the previous example,
hatwideψ=90×307
165×84=1.99
and
hatwideγ= log(1 .99) = .69.

<<<PAGE 259>>>

15.2 Two Discrete Variables 243
So tonsillectomy patients were twice as likely to have Hodgkins disease. The
standard error of hatwideγis
radicalbigg
1
90+1
84+1
165+1
307=.18.
The Wald statistic is W=.69/.1 8=3 .84 whose p-value is P(|Z|>3.84) =
.0001, the same as the other tests. A 95 per cent conﬁdence interval for γis
hatwideγ±2(.1 8 )=( .33,1.05). A 95 per cent conﬁdence interval for ψis (e.33,e1.05)=
(1.39,2.86)./squaresolid
15.2 Two Discrete Variables
Now suppose that Y∈{1,...,I }andZ∈{1,...,J }are two discrete vari-
ables. The data can be represented as an I×Jtable of counts:
Y=1 Y=2 ···Y=j···Y=J
Z=1 X11 X12 ··· X1j··· X1JX1·
.....................
...
Z=iXi1 Xi2··· Xij··· XiJ Xi·
.....................
...
Z=IXI1 XI2··· XIj ··· XIJ XI·
X·1 X·2··· X·j··· X·J n
where
Xij= number of observations for which Z=iandY=j.
Consider testing
H0:Y/coproductZversus H1:Y/gluonbelement/gluonelement/gluonelement/gluonelement/gluonelement/gluoneelementZ. (15.11)
15.9 Theorem. The likelihood ratio test statistic for (15.11) is
T=2Isummationdisplay
i=1Jsummationdisplay
j=1XijlogparenleftbiggXijX··
Xi·X·jparenrightbigg
. (15.12)
The limiting distribution of Tunder the null hypothesis of independence
isχ2
νwhere ν=(I−1)(J−1). Pearson’s χ2test statistic is
U=Isummationdisplay
i=1Jsummationdisplay
j=1(Xij−Eij)2
Eij. (15.13)

<<<PAGE 260>>>

244 15. Inference About Independence
Asymptotically, under H0,Uhas a χ2
νdistribution where
ν=(I−1)(J−1).
15.10 Example. These data are from Dunsmore et al. (1987). Patients with
Hodgkins disease are classiﬁed by their response to treatment and by histo-logical type.
Type
Positive Response Partial Response No Response
LP 74 18 12 104
NS 68 16 12 96
MC 154 54 58 266
LD 18 10 44 72
Theχ2test statistic is 75.89 with 2 ×3 = 6 degrees of freedom. The p-value
isP(χ2
6>75.89)≈0. The likelihood ratio test statistic is 68.30 with 2 ×3=6
degrees of freedom. The p-value is P(χ2
6>68.30)≈0. Thus there is strong
evidence that response to treatment and histological type are associated. /squaresolid
15.3 Two Continuous Variables
Now suppose that YandZare both continuous. If we assume that the joint
distribution of YandZis bivariate Normal, then we measure the dependence
between YandZby means of the correlation coeﬃcient ρ. Tests, estimates,
and conﬁdence intervals for ρin the Normal case are given in the previous
chapter in Section 14.2. If we do not assume Normality then we can still use themethods in Section 14.2 to draw inferences about the correlation ρ. However,
if we conclude that ρis 0, we cannot conclude that YandZare independent,
only that they are uncorrelated. Fortunately, the reverse direction is valid:if we conclude that YandZare correlated than we can conclude they are
dependent.
15.4 One Continuous Variable and One Discrete
Suppose that Y∈{1,...,I }is discrete and Zis continuous. Let Fi(z)=
P(Z≤z|Y=i) denote the cdfofZconditional on Y=i.

<<<PAGE 261>>>

15.5 Appendix 245
15.11 Theorem. When Y∈{1,...,I }is discrete and Zis continuous, then
Y/coproductZif and only if F1=···=FI.
It follows from the previous theorem that to test for independence, we need
to test
H0:F1=···=FIversus H1: notH0.
For simplicity, we consider the case where I= 2. To test the null hypothesis
thatF1=F2we will use the two sample Kolmogorov-Smirnov test . Let
n1denote the number of observations for which Yi= 1 and let n2denote the
number of observations for which Yi= 2. Let
hatwideF1(z)=1
n1nsummationdisplay
i=1I(Zi≤z)I(Yi=1 )
and
hatwideF2(z)=1
n2nsummationdisplay
i=1I(Zi≤z)I(Yi=2 )
denote the empirical distribution function of Zgiven Y= 1 and Y=2
respectively. Deﬁne the test statistic
D= sup
x|hatwideF1(x)−hatwideF2(x)|.
15.12 Theorem. Let
H(t)=1−2∞summationdisplay
j=1(−1)j−1e−2j2t2. (15.14)
Under the null hypothesis that F1=F2,
lim
n→∞Pparenleftbiggradicalbiggn1n2
n1+n2D≤tparenrightbigg
=H(t).
It follows from the theorem that an approximate level αtest is obtained by
rejecting H0whenradicalbiggn1n2
n1+n2D>H−1(1−α).
15.5 Appendix
Interpreting The Odds Ratios. Suppose event Aas probability P(A).
The odds of Aare deﬁned as odds( A)=P(A)/(1−P(A)). It follows that

<<<PAGE 262>>>

246 15. Inference About Independence
P(A) = odds( A)/(1 + odds( A)). Let Ebe the event that someone is exposed
to something (smoking, radiation, etc) and let Dbe the event that they get
a disease. The odds of getting the disease given that you are exposed are:
odds(D|E)=P(D|E)
1−P(D|E)
and the odds of getting the disease given that you are not exposed are:
odds(D|Ec)=P(D|Ec)
1−P(D|Ec).
Theodds ratio is deﬁned to be
ψ=odds(D|E)
odds(D|Ec).
Ifψ= 1 then disease probability is the same for exposed and unexposed. This
implies that these events are independent. Recall that the log-odds ratio isdeﬁned as γ= log( ψ). Independence corresponds to γ=0 .
Consider this table of probabilities and corresponding table of data:
DcD
Ecp00p01p0·
Ep10p11p1·
p·0p·11DcD
EcX00X01X0·
EX10X11X1·
X·0X·1X··
Now
P(D|E)=p11
p10+p11and P(D|Ec)=p01
p00+p01,
and so
odds(D|E)=p11
p10and odds( D|Ec)=p01
p00,
and therefore,
ψ=p11p00
p01p10.
To estimate the parameters, we have to ﬁrst consider how the data were
collected. There are three methods.
Multinomial Sampling. We draw a sample from the population and,
for each person, record their exposure and disease status. In this case, X=
(X00,X01,X10,X11)∼Multinomial( n, p). We then estimate the probabilities
in the table by hatwidepij=Xij/nand
hatwideψ=hatwidep11hatwidep00
hatwidep01hatwidep10=X11X00
X01X10.

<<<PAGE 263>>>

15.5 Appendix 247
Prospective Sampling. (Cohort Sampling). We get some exposed and
unexposed people and count the number with disease in each group. Thus,
X01∼Binomial( X0·,P(D|Ec))
X11∼Binomial( X1·,P(D|E)).
We should really write x0·andx1·instead of X0·andX1·since in this case,
these are ﬁxed not random, but for notational simplicity I’ll keep using capitalletters. We can estimate P(D|E) and P(D|E
c) but we cannot estimate all the
probabilities in the table. Still, we can estimate ψsince ψis a function of
P(D|E) and P(D|Ec). Now
hatwideP(D|E)=X11
X1·andhatwideP(D|Ec)=X01
X0·.
Thus,
hatwideψ=X11X00
X01X10
just as before.
Case-Control (Retrospective) Sampling. Here we get some diseased
and non-diseased people and we observe how many are exposed. This is muchmore eﬃcient if the disease is rare. Hence,
X
10∼Binomial( X·0,P(E|Dc))
X11∼Binomial( X·1,P(E|D)).
From these data we can estimate P(E|D) and P(E|Dc). Surprisingly, we can
also still estimate ψ. To understand why, note that
P(E|D)=p11
p01+p11,1−P(E|D)=p01
p01+p11,odds(E|D)=p11
p01.
By a similar argument,
odds(E|Dc)=p10
p00.
Hence,
odds(E|D)
odds(E|Dc)=p11p00
p01p10=ψ.
From the data, we form the following estimates:
hatwideP(E|D)=X11
X·1,1−hatwideP(E|D)=X01
X·1,/hatwideodds(E|D)=X11
X01,/hatwideodds(E|Dc)=X10
X00.
Therefore,
hatwideψ=X00X11
X01X10.

<<<PAGE 264>>>

248 15. Inference About Independence
So in all three data collection methods, the estimate of ψturns out to be the
same.
It is tempting to try to estimate P(D|E)−P(D|Ec). In a case-control design,
this quantity is not estimable. To see this, we apply Bayes’ theorem to get
P(D|E)−P(D|Ec)=P(E|D)P(D)
P(E)−P(Ec|D)P(D)
P(Ec).
Because of the way we obtained the data, P(D) is not estimable from the data.
However, we can estimate ξ=P(D|E)/P(D|Ec), which is called the relative
risk, under the rare disease assumption.
15.13 Theorem. Letξ=P(D|E)/P(D|Ec). Then
ψ
ξ→1
asP(D)→0.
Thus, under the rare disease assumption, the relative risk is approximately
the same as the odds ratio and, as we have seen, we can estimate the oddsratio.
15.6 Exercises
1. Prove Theorem 15.2.
2. Prove Theorem 15.3.3. Prove Theorem 15.6.4. The New York Times (January 8, 2003, page A12) reported the following
data on death sentencing and race, from a study in Maryland:
2
Death Sentence No Death Sentence
Black Victim 14 641White Victim 62 594
Analyze the data using the tools from this chapter. Interpret the results.
Explain why, based only on this information, you can’t make causalconclusions. (The authors of the study did use much more informationin their full report.)
2The data here are an approximate re-creation using the information in the article.

<<<PAGE 265>>>

15.6 Exercises 249
5. Analyze the data on the variables Age and Financial Status from:
http://lib.stat.cmu.edu/DASL/Dataﬁles/montanadat.html
6. Estimate the correlation between temperature and latitude using the
data from
http://lib.stat.cmu.edu/DASL/Dataﬁles/USTemperatures.htmlUse the correlation coeﬃcient. Provide estimates, tests, and conﬁdence
intervals.
7. Test whether calcium intake and drop in blood pressure are associated.
Use the data in
http://lib.stat.cmu.edu/DASL/Dataﬁles/Calcium.html

<<<PAGE 266>>>



<<<PAGE 267>>>

16
Causal Inference
Roughly speaking, the statement “ Xcauses Y” means that changing the
value of Xwill change the distribution of Y. When Xcauses Y,XandY
will be associated but the reverse is not, in general, true. Association does not
necessarily imply causation. We will consider two frameworks for discussingcausation. The ﬁrst uses counterfactual random variables. The second, pre-
sented in the next chapter, uses directed acyclic graphs.
16.1 The Counterfactual Model
Suppose that Xis a binary treatment variable where X= 1 means “treated”
andX= 0 means “not treated.” We are using the word “treatment” in a
very broad sense. Treatment might refer to a medication or something likesmoking. An alternative to “treated/not treated” is “exposed/not exposed”but we shall use the former.
LetYbe some outcome variable such as presence or absence of disease.
To distinguish the statement “ Xis associated Y” from the statement “ X
causes Y” we need to enrich our probabilistic vocabulary. Speciﬁcally, we will
decompose the response Yinto a more ﬁne-grained object.
We introduce two new random variables ( C
0,C1), called potential out-
comes with the following interpretation: C0is the outcome if the subject is

<<<PAGE 268>>>

252 16. Causal Inference
not treated ( X= 0) and C1is the outcome if the subject is treated ( X= 1).
Hence,
Y=braceleftbigg
C0ifX=0
C1ifX=1.
We can express the relationship between Yand (C0,C1) more succinctly by
Y=CX. (16.1)
Equation (16.1) is called the consistency relationship .
Here is a toy dataset to make the idea clear:
XYC 0C1
044 *
077 *022 *088 *
13* 315* 518* 819* 9
The asterisks denote unobserved values. When X= 0 we don’t observe C
1,
in which case we say that C1is acounterfactual since it is the outcome
you would have had if, counter to the fact, you had been treated ( X= 1).
Similarly, when X= 1 we don’t observe C0, and we say that C0iscounter-
factual . There are four types of subjects:
Type C0C1
Survivors 1 1
Responders 0 1Anti-responders 1 0Doomed 0 0
Think of the potential outcomes ( C
0,C1) as hidden variables that contain all
the relevant information about the subject.
Deﬁne the average causal eﬀect oraverage treatment eﬀect to be
θ=E(C1)−E(C0). (16.2)
The parameter θhas the following interpretation: θis the mean if everyone
were treated ( X= 1) minus the mean if everyone were not treated ( X= 0).
There are other ways of measuring the causal eﬀect. For example, if C0and
C1are binary, we deﬁne the causal odds ratio
P(C1=1 )
P(C1=0 )÷P(C0=1 )
P(C0=0 )

<<<PAGE 269>>>

16.1 The Counterfactual Model 253
and the causal relative risk
P(C1=1 )
P(C0=1 ).
The main ideas will be the same whatever causal eﬀect we use. For simplicity,
we shall work with the average causal eﬀect θ.
Deﬁne the association to be
α=E(Y|X=1 )−E(Y|X=0 ). (16.3)
Again, we could use odds ratios or other summaries if we wish.
16.1 Theorem (Association Is Not Causation) .In general, θ/negationslash=α.
16.2 Example. Suppose the whole population is as follows:
XYC 0C1
000 0∗
000 0∗
000 0∗
000 0∗
111∗1
111∗1
111∗1
111∗1
Again, the asterisks denote unobserved values. Notice that C0=C1for every
subject, thus, this treatment has no eﬀect. Indeed,
θ=E(C1)−E(C0)=1
88summationdisplay
i=1C1i−1
88summationdisplay
i=1C0i
=0+0+0+0+1+1+1+1
8−0+0+0+0+1+1+1+1
8
=0.
Thus, the average causal eﬀect is 0. The observed data are only the X’s and
Y’s, from which we can estimate the association:
α=E(Y|X=1 )−E(Y|X=0 )
=1+1+1+1
4−0+0+0+0
4=1.
Hence, θ/negationslash=α.
To add some intuition to this example, imagine that the outcome variable
is 1 if “healthy” and 0 if “sick”. Suppose that X= 0 means that the subject

<<<PAGE 270>>>

254 16. Causal Inference
does not take vitamin C and that X= 1 means that the subject does take
vitamin C. Vitamin C has no causal eﬀect since C0=C1for each subject. In
this example there are two types of people: healthy people ( C0,C1)=( 1 ,1)
and unhealthy people ( C0,C1)=( 0 ,0). Healthy people tend to take vitamin
C while unhealthy people don’t. It is this association between ( C0,C1) and
Xthat creates an association between XandY. If we only had data on X
andYwe would conclude that XandYare associated. Suppose we wrongly
interpret this causally and conclude that vitamin C prevents illness. Next wemight encourage everyone to take vitamin C. If most people comply with ouradvice, the population will look something like this:
XYC
0C1
000 0∗
100 0∗
100 0∗
100 0∗
111∗1
111∗1
111∗1
111∗1
Nowα=( 4/7)−(0/1 )=4 /7. We see that αwent down from 1 to 4/7.
Of course, the causal eﬀect never changed but the naive observer who doesnot distinguish association and causation will be confused because his adviceseems to have made things worse instead of better.
/squaresolid
In the last example, θ= 0 and α= 1. It is not hard to create examples in
which α>0 and yet θ<0. The fact that the association and causal eﬀects
can have diﬀerent signs is very confusing to many people.
The example makes it clear that, in general, we cannot use the association
to estimate the causal eﬀect θ. The reason that θ/negationslash=αis that ( C0,C1)w a s
not independent of X. That is, treatment assignment was not independent of
person type.
Can we ever estimate the causal eﬀect? The answer is: sometimes. In par-
ticular, random assignment to treatment makes it possible to estimate θ.
16.3 Theorem. Suppose we randomly assign subjects to treatment and that
P(X=0 )>0andP(X=1 )>0. Then α=θ. Hence, any consistent estima-
tor of αis a consistent estimator of θ. In particular, a consistent estimator
is
hatwideθ=hatwideE(Y|X=1 )−hatwideE(Y|X=0 )

<<<PAGE 271>>>

16.2 Beyond Binary Treatments 255
=Y1−Y0
is a consistent estimator of θ, where
Y1=1
n1nsummationdisplay
i=1YiXi,Y0=1
n0nsummationdisplay
i=1Yi(1−Xi),
n1=summationtextn
i=1Xi, and n0=summationtextn
i=1(1−Xi).
Proof. Since Xis randomly assigned, Xis independent of ( C0,C1). Hence,
θ=E(C1)−E(C0)
=E(C1|X=1 )−E(C0|X= 0) since X/coproduct(C0,C1)
=E(Y|X=1 )−E(Y|X= 0) since Y=CX
=α.
The consistency follows from the law of large numbers. /squaresolid
IfZis a covariate, we deﬁne the conditional causal eﬀect by
θz=E(C1|Z=z)−E(C0|Z=z).
For example, if Zdenotes gender with values Z= 0 (women) and Z=1
(men), then θ0is the causal eﬀect among women and θ1is the causal eﬀect
among men. In a randomized experiment, θz=E(Y|X=1,Z=z)−E(Y|X=
0,Z=z) and we can estimate the conditional causal eﬀect using appropriate
sample averages.
Summary of the Counterfactual Model
Random variables: ( C0,C1,X,Y ).
Consistency relationship: Y=CX.
Causal Eﬀect: θ=E(C1)−E(C0).
Association: α=E(Y|X=1 )−E(Y|X= 0).
Random assignment = ⇒(C0,C1)/coproductX=⇒θ=α.
16.2 Beyond Binary Treatments
Let us now generalize beyond the binary case. Suppose that X∈X.F o r
example, Xcould be the dose of a drug in which case X∈R. The counterfac-
tual vector ( C0,C1) now becomes the counterfactual function C(x) where

<<<PAGE 272>>>

256 16. Causal Inference
0123456789 1 0 1 101234567
xC(x)
XY=C(X)
FIGURE 16.1. A counterfactual function C(x). The outcome Yis the value of the
curve C(x) evaluated at the observed dose X.
C(x) is the outcome a subject would have if he received dose x. The observed
response is given by the consistency relation
Y≡C(X). (16.4)
See Figure 16.1. The causal regression function is
θ(x)=E(C(x)). (16.5)
The regression function, which measures association, is r(x)=E(Y|X=x).
16.4 Theorem. In general, θ(x)/negationslash=r(x). However, when Xis randomly as-
signed, θ(x)=r(x).
16.5 Example. An example in which θ(x) is constant but r(x) is not constant
is shown in Figure 16.2. The ﬁgure shows the counterfactual functions forfour subjects. The dots represent their Xvalues X
1,X2,X3,X4. Since Ci(x)
is constant over xfor all i, there is no causal eﬀect and hence
θ(x)=C1(x)+C2(x)+C3(x)+C4(x)
4

<<<PAGE 273>>>

16.3 Observational Studies and Confounding 257
is constant. Changing the dose xwill not change anyone’s outcome. The four
dots in the lower plot represent the observed data points Y1=C1(X1),Y2=
C2(X2),Y3=C3(X3),Y4=C4(X4). The dotted line represents the regression
r(x)=E(Y|X=x). Although there is no causal eﬀect, there is an association
since the regression curve r(x) is not constant. /squaresolid
16.3 Observational Studies and Confounding
A study in which treatment (or exposure) is not randomly assigned is called an
observational study. In these studies, subjects select their own value of the
exposure X. Many of the health studies you read about in the newspaper are
like this. As we saw, association and causation could in general be quite diﬀer-ent. This discrepancy occurs in non-randomized studies because the potentialoutcome Cis not independent of treatment X. However, suppose we could
ﬁnd groupings of subjects such that, within groups, Xand{C(x):x∈X }
are independent. This would happen if the subjects are very similar withingroups. For example, suppose we ﬁnd people who are very similar in age, gen-der, educational background, and ethnic background. Among these people wemight feel it is reasonable to assume that the choice of Xis essentially ran-
dom. These other variables are called confounding variables.
1If we denote
these other variables collectively as Z, then we can express this idea by saying
that
{C(x):x∈X} /coproductX|Z. (16.6)
Equation (16.6) means that, within groups of Z, the choice of treatment X
does not depend on type, as represented by {C(x):x∈X }. If (16.6) holds
and we observe Zthen we say that there is no unmeasured confounding .
16.6 Theorem. Suppose that (16.6) holds. Then,
θ(x)=integraldisplay
E(Y|X=x, Z=z)dFZ(z)dz. (16.7)
Ifhatwider(x, z)is a consistent estimate of the regression function E(Y|X=x, Z=
z), then a consistent estimate of θ(x)is
hatwideθ(x)=1
nnsummationdisplay
i=1hatwider(x, Zi).
1A more precise deﬁnition of confounding is given in the next chapter.

<<<PAGE 274>>>

258 16. Causal Inference
0123450123C1(x)
C2(x)
C3(x)
C4(x)
x/Bullet/Bullet/Bullet/Bullet
0123450123θ(x)
/Bullet/Bullet/Bullet/BulletY1
Y2
Y3
Y4
x
FIGURE 16.2. The top plot shows the counterfactual function C(x) for four sub-
jects. The dots represent their Xvalues. Since Ci(x) is constant over xfor all i, there
is no causal eﬀect. Changing the dose will not change anyone’s outcome. The lowerplot shows the causal regression function θ(x)=(C
1(x)+C2(x)+C3(x)+C4(x))/4.
The four dots represent the observed data points Y1=C1(X1),Y2=C2(X2),
Y3=C3(X3),Y4=C4(X4). The dotted line represents the regression
r(x)= E(Y|X=x). There is no causal eﬀect since Ci(x) is constant for all i.
But there is an association since the regression curve r(x) is not constant.

<<<PAGE 275>>>

16.4 Simpson’s Paradox 259
In particular, if r(x, z)=β0+β1x+β2zis linear, then a consistent estimate
ofθ(x)is
hatwideθ(x)=hatwideβ0+hatwideβ1x+hatwideβ2Zn (16.8)
where (hatwideβ0,hatwideβ1,hatwideβ2)are the least squares estimators.
16.7 Remark. It is useful to compare equation (16.7) to E(Y|X=x) which
can be written as E(Y|X=x)=integraltext
E(Y|X=x, Z=z)dFZ|X(z|x).
Epidemiologists call (16.7) the adjusted treatment eﬀect . The process of
computing adjusted treatment eﬀects is called adjusting (or controlling)
for confounding. The selection of what confounders Zto measure and con-
trol for requires scientiﬁc insight. Even after adjusting for confounders, wecannot be sure that there are not other confounding variables that we missed.This is why observational studies must be treated with healthy skepticism.Results from observational studies start to become believable when: (i) theresults are replicated in many studies, (ii) each of the studies controlled forplausible confounding variables, (iii) there is a plausible scientiﬁc explanationfor the existence of a causal relationship.
A good example is smoking and cancer. Numerous studies have shown a
relationship between smoking and cancer even after adjusting for many con-founding variables. Moreover, in laboratory studies, smoking has been shownto damage lung cells. Finally, a causal link between smoking and cancer hasbeen found in randomized animal studies. It is this collection of evidenceover many years that makes this a convincing case. One single observationalstudy is not, by itself, strong evidence. Remember that when you read thenewspaper.
16.4 Simpson’s Paradox
Simpson’s paradox is a puzzling phenomenon that is discussed in most statis-tics texts. Unfortunately, most explanations are confusing (and in some casesincorrect). The reason is that it is nearly impossible to explain the paradoxwithout using counterfactuals (or directed acyclic graphs).
LetXbe a binary treatment variable, Ya binary outcome, and Za third
binary variable such as gender. Suppose the joint distribution of X,Y,Z is

<<<PAGE 276>>>

260 16. Causal Inference
Y=1 Y=0 Y=1 Y=0
X=1 .1500 .2250 .1000 .0250
X=0 .0375 .0875 .2625 .1125
Z= 1 (men) Z= 0 (women)
The marginal distribution for ( X,Y)i s
Y=1 Y=0
X= 1 .25 .25 .50
X= 0 .30 .20 .50
.55 .45 1
From these tables we ﬁnd that,
P(Y=1|X=1 )−P(Y=1|X=0 ) = −0.1
P(Y=1|X=1,Z=1 )−P(Y=1|X=0,Z=1 ) = 0 .1
P(Y=1|X=1,Z=0 )−P(Y=1|X=0,Z=0 ) = 0 .1.
To summarize, we seemto have the following information:
Mathematical Statement English Statement?
P(Y=1|X=1 )<P(Y=1|X= 0) treatment is harmful
P(Y=1|X=1,Z=1 )>P(Y=1|X=0,Z= 1) treatment is beneﬁcial to men
P(Y=1|X=1,Z=0 )>P(Y=1|X=0,Z= 0) treatment is beneﬁcial to women
Clearly, something is amiss. There can’t be a treatment which is good for
men, good for women, but bad overall. This is nonsense. The problem is withthe set of English statements in the table. Our translation from math intoEnglish is specious.
The inequality P(Y=1|X=1 )<P(Y=1|X=0 )does not
mean that treatment is harmful.
The phrase “treatment is harmful” should be written mathematically as
P(C
1=1 )<P(C0= 1). The phrase “treatment is harmful for men” should
be written P(C1=1|Z=1 )<P(C0=1|Z= 1). The three mathematical
statements in the table are not at all contradictory. It is only the translationinto English that is wrong.
Let us now show that a real Simpson’s paradox cannot happen, that is,
there cannot be a treatment that is beneﬁcial for men and women but harmfuloverall. Suppose that treatment is beneﬁcial for both sexes. Then
P(C
1=1|Z=z)>P(C0=1|Z=z)

<<<PAGE 277>>>

16.5 Bibliographic Remarks 261
for all z. It then follows that
P(C1=1 ) =summationdisplay
zP(C1=1|Z=z)P(Z=z)
>summationdisplay
zP(C0=1|Z=z)P(Z=z)
=P(C0=1 ).
Hence, P(C1=1 )>P(C0= 1), so treatment is beneﬁcial overall. No paradox.
16.5 Bibliographic Remarks
The use of potential outcomes to clarify causation is due mainly to Jerzy Ney-
man and Donald Rubin. Later developments are due to Jamie Robins, PaulRosenbaum, and others. A parallel development took place in econometricsby various people including James Heckman and Charles Manski. Texts oncausation include Pearl (2000), Rosenbaum (2002), Spirtes et al. (2000), andvan der Laan and Robins (2003).
16.6 Exercises
1. Create an example like Example 16.2 in which α>0 and θ<0.
2. Prove Theorem 16.4.3. Suppose you are given data ( X
1,Y1),...,(Xn,Yn) from an observational
study, where Xi∈{0,1}andYi∈{0,1}. Although it is not possible
to estimate the causal eﬀect θ, it is possible to put bounds on θ. Find
upper and lower bounds on θthat can be consistently estimated from
the data. Show that the bounds have width 1.
Hint: Note that E(C1)=E(C1|X=1 )P(X=1 )+ E(C1|X=0 )P(X=
0).
4. Suppose that X∈Rand that, for each subject i,Ci(x)=β1ix. Each
subject has their own slope β1i. Construct a joint distribution on ( β1,X)
such that P(β1>0) = 1 but E(Y|X=x) is a decreasing function of x,
where Y=C(X). Interpret.
5. Let X∈{0,1}be a binary treatment variable and let ( C0,C1) denote
the corresponding potential outcomes. Let Y=CXdenote the observed

<<<PAGE 278>>>

262 16. Causal Inference
response. Let F0andF1be the cumulative distribution functions for
C0andC1. Assume that F0andF1are both continuous and strictly
increasing. Let θ=m1−m0where m0=F−1
0(1/2) is the median of C0
andm1=F−1
1(1/2) is the median of C1. Suppose that the treatment X
is assigned randomly. Find an expression for θinvolving only the joint
distribution of XandY.

<<<PAGE 279>>>

17
Directed Graphs and Conditional
Independence
17.1 Introduction
A directed graph consists of a set of nodes with arrows between some nodes.
An example is shown in Figure 17.1.
Graphs are useful for representing independence relations between variables.
They can also be used as an alternative to counterfactuals to represent causalrelationships. Some people use the phrase Bayesian network to refer to a
directed graph endowed with a probability distribution. This is a poor choiceof terminology. Statistical inference for directed graphs can be performed using
Y
XZ
FIGURE 17.1. A directed graph with vertices V={X,Y,Z }and edges
E={(Y,X),(Y,Z)}.

<<<PAGE 280>>>

264 17. Directed Graphs and Conditional Independence
frequentist or Bayesian methods, so it is misleading to call them Bayesian
networks.
Before getting into details about directed acyclic graphs (DAGs), we need
to discuss conditional independence.
17.2 Conditional Independence
17.1 Deﬁnition. LetX,YandZbe random variables. XandYare
conditionally independent given Z, written X/coproductY|Z,i f
fX,Y|Z(x, y|z)=fX|Z(x|z)fY|Z(y|z). (17.1)
for all x,yandz.
Intuitively, this means that, once you know Z,Yprovides no extra infor-
mation about X. An equivalent deﬁnition is that
f(x|y,z)=f(x|z). (17.2)
The conditional independence relation satisﬁes some basic properties.
17.2 Theorem. The following implications hold:1
X/coproductY|Z=⇒Y/coproductX|Z
X/coproductY|ZandU=h(X)=⇒U/coproductY|Z
X/coproductY|ZandU=h(X)=⇒X/coproductY|(Z,U)
X/coproductY|ZandX/coproductW|(Y,Z)=⇒X/coproduct(W, Y)|Z
X/coproductY|ZandX/coproductZ|Y=⇒X/coproduct(Y,Z).
17.3 DAGs
Adirected graph Gconsists of a set of vertices Vand an edge set Eof
ordered pairs of vertices. For our purposes, each vertex will correspond to arandom variable. If ( X,Y)∈Ethen there is an arrow pointing from XtoY.
See Figure 17.1.
1The last property requires the assumption that all events have positive probability; the ﬁrst
four do not.

<<<PAGE 281>>>

17.3 DAGs 265
heart disease coughoverweight smoking
FIGURE 17.2. DAG for Example 17.4.
If an arrow connects two variables XandY(in either direction) we say
thatXandYareadjacent. If there is an arrow from XtoYthenXis a
parent ofYandYis achild ofX. The set of all parents of Xis denoted
byπXorπ(X). Adirected path between two variables is a set of arrows
all pointing in the same direction linking one variable to the other such as:
X/Bullet/Bullet/BulletY
A sequence of adjacent vertices staring with Xand ending with Ybut
ignoring the direction of the arrows is called an undirected path. The se-
quence {X,Y,Z }in Figure 17.1 is an undirected path. Xis anancestor of
Yif there is a directed path from XtoY(orX=Y). We also say that Yis
adescendant ofX.
A conﬁguration of the form:
X Y Z
is called a collider atY. A conﬁguration not of that form is called a non-
collider , for example,
X Y Z

<<<PAGE 282>>>

266 17. Directed Graphs and Conditional Independence
or
X Y Z
The collider property is path dependent. In Figure 17.7, Yis a collider on
the path {X,Y,Z }but it is a non-collider on the path {X,Y,W }. When the
variables pointing into the collider are not adjacent, we say that the colliderisunshielded . A directed path that starts and ends at the same variable is
called a cycle. A directed graph is acyclic if it has no cycles. In this case we
say that the graph is a directed acyclic graph orDAG . From now on, we
only deal with acyclic graphs.
17.4 Probability and DAGs
LetGbe a DAG with vertices V=(X1,...,X k).
17.3 Deﬁnition. IfPis a distribution for Vwith probability function f,
we say that PisMarkov to G, or that Grepresents P,i f
f(v)=kproductdisplay
i=1f(xi|πi) (17.3)
where πiare the parents of Xi. The set of distributions represented by G
is denoted by M(G).
17.4 Example. Figure 17.2 shows a DAG with four variables. The probability
function for this example factors as
f(overweight ,smoking ,heart disease ,cough)
=f(overweight) ×f(smoking)
×f(heart disease |overweight ,smoking)
×f(cough |smoking) ./squaresolid
17.5 Example. For the DAG in Figure 17.3, P∈M(G) if and only if its
probability function fhas the form
f(x, y, z, w )=f(x)f(y)f(z|x, y)f(w|z)./squaresolid

<<<PAGE 283>>>

17.5 More Independence Relations 267
YX
ZW
FIGURE 17.3. Another DAG.
The following theorem says that P∈M(G) if and only if the Markov
Condition holds. Roughly speaking, the Markov Condition means that every
variable Wis independent of the “past” given its parents.
17.6 Theorem. A distribution P∈M(G)if and only if the following Markov
Condition holds: for every variable W,
W/coproducttildewiderW|πW (17.4)
wheretildewiderWdenotes all the other variables except the parents and descendants
ofW.
17.7 Example. In Figure 17.3, the Markov Condition implies that
X/coproductYandW/coproduct{X,Y}|Z./squaresolid
17.8 Example. Consider the DAG in Figure 17.4. In this case probability
function must factor like
f(a, b, c, d, e )=f(a)f(b|a)f(c|a)f(d|b, c)f(e|d).
The Markov Condition implies the following independence relations:
D/coproductA|{B,C},E /coproduct{A, B, C }|Dand B/coproductC|A/squaresolid
17.5 More Independence Relations
The Markov Condition allows us to list some independence relations implied
by a DAG. These relations might imply other independence relations. Con-

<<<PAGE 284>>>

268 17. Directed Graphs and Conditional Independence
A
CB
DE
FIGURE 17.4. Yet another DAG.
sider the DAG in Figure 17.5. The Markov Condition implies:
X1/coproductX2,X 2/coproduct{X1,X4},X 3/coproductX4|{X1,X2},
X4/coproduct{X2,X3}|X1,X 5/coproduct{X1,X2}|{X3,X4}
It turns out (but it is not obvious) that these conditions imply that
{X4,X5}/coproductX2|{X1,X3}.
How do we ﬁnd these extra independence relations? The answer is “d-
separation” which means “directed separation.” d-separation can be summa-rized by three rules. Consider the four DAG’s in Figure 17.6 and the DAG inFigure 17.7. The ﬁrst 3 DAG’s in Figure 17.6 have no colliders. The DAG inthe lower right of Figure 17.6 has a collider. The DAG in Figure 17.7 has acollider with a descendant.

<<<PAGE 285>>>

17.5 More Independence Relations 269
X1X2
X3
X4X5
FIGURE 17.5. And yet another DAG.
XY Z XY Z
XY Z XY Z
FIGURE 17.6. The ﬁrst three DAG’s have no colliders. The fourth DAG in the lower
right corner has a collider at Y.
XY Z
W
FIGURE 17.7. A collider with a descendant.

<<<PAGE 286>>>

270 17. Directed Graphs and Conditional Independence
XUVWY
S1 S2
FIGURE 17.8. d-separation explained.
The Rules of d-Separation
Consider the DAGs in Figures 17.6 and 17.7.
1. When Yis not a collider, XandZared-connected , but they are
d-separated given Y.
2. IfXandZcollide at Y, then XandZared-separated , but they
ared-connected given Y.
3. Conditioning on the descendant of a collider has the same eﬀect as
conditioning on the collider. Thus in Figure 17.7, XandZare
d-separated but they are d-connected given W.
Here is a more formal deﬁnition of d-separation. Let XandYbe distinct
vertices and let Wbe a set of vertices not containing XorY. Then Xand
Yared-separated given Wif there exists no undirected path Ubetween
XandYsuch that (i) every collider on Uhas a descendant in W, and (ii)
no other vertex on Uis inW.I fA, B, and Ware distinct sets of vertices and
AandBare not empty, then AandBare d-separated given Wif for every
X∈AandY∈B,XandYare d-separated given W. Sets of vertices that
are not d-separated are said to be d-connected.
17.9 Example. Consider the DAG in Figure 17.8. From the d-separation rules
we conclude that:
XandYare d-separated (given the empty set);
XandYare d-connected given {S1,S2};
XandYare d-separated given {S1,S2,V}.
17.10 Theorem.2LetA,B, andCbe disjoint sets of vertices. Then A/coproductB|C
if and only if AandBare d-separated by C.
2We implicitly assume that Pisfaithful toGwhich means that Phas no extra independence
relations other than those logically implied by the Markov Condition.

<<<PAGE 287>>>

17.5 More Independence Relations 271
latealiens watch
FIGURE 17.9. Jordan’s alien example (Example 17.11). Was your friend kidnapped
by aliens or did you forget to set your watch?
17.11 Example. The fact that conditioning on a collider creates dependence
might not seem intuitive. Here is a whimsical example from Jordan (2004) thatmakes this idea more palatable. Your friend appears to be late for a meetingwith you. There are two explanations: she was abducted by aliens or you forgotto set your watch ahead one hour for daylight savings time. (See Figure 17.9.)Aliens andWatch are blocked by a collider which implies they are marginally
independent. This seems reasonable since — before we know anything aboutyour friend being late — we would expect these variables to be independent.We would also expect that P(Aliens =yes|Late=yes)>P(Aliens =yes);
learning that your friend is late certainly increases the probability that shewas abducted. But when we learn that you forgot to set your watch properly,we would lower the chance that your friend was abducted. Hence, P(Aliens =
yes|Late=yes)/negationslash=P(Aliens =yes|Late=yes,Watch =no). Thus, Aliens and
Watch are dependent given Late.
/squaresolid
17.12 Example. Consider the DAG in Figure 17.2. In this example, over-
weight andsmoking are marginally independent but they are dependent given
heart disease ./squaresolid
Graphs that look diﬀerent may actually imply the same independence re-
lations. If Gi saD A G ,w el e t I(G) denote all the independence statements
implied by G. Two DAGs G1andG2for the same variables VareMarkov
equivalent ifI(G1)=I(G2). Given a DAG G, let skeleton( G) denote the
undirected graph obtained by replacing the arrows with undirected edges.
17.13 Theorem. Two DAGs G1andG2are Markov equivalent if and only if
(i)skeleton( G1) = skeleton( G2)and (ii) G1andG2have the same unshielded
colliders.
17.14 Example. The ﬁrst three DAGs in Figure 17.6 are Markov equivalent.
The DAG in the lower right of the Figure is not Markov equivalent to theothers.
/squaresolid

<<<PAGE 288>>>

272 17. Directed Graphs and Conditional Independence
17.6 Estimation for DAGs
Two estimation questions arise in the context of DAGs. First, given a DAG
Gand data V1,...,V nfrom a distribution fconsistent with G, how do we
estimate f? Second, given data V1,...,V nhow do we estimate G? The ﬁrst
question is pure estimation while the second involves model selection. Theseare very involved topics and are beyond the scope of this book. We will justbrieﬂy mention the main ideas.
Typically, one uses some parametric model f(x|π
x;θx) for each conditional
density. The likelihood function is then
L(θ)=nproductdisplay
i=1f(Vi;θ)=nproductdisplay
i=1mproductdisplay
j=1f(Xij|πj;θj),
where Xijis the value of Xjfor the ithdata point and θjare the parameters for
thejthconditional density. We can then estimate the parameters by maximum
likelihood.
To estimate the structure of the DAG itself, we could ﬁt every possible DAG
using maximum likelihood and use AIC (or some other method) to choose aDAG. However, there are many possible DAGs so you would need much datafor such a method to be reliable. Also, searching through all possible DAGsis a serious computational challenge. Producing a valid, accurate conﬁdenceset for the DAG structure would require astronomical sample sizes. If priorinformation is available about part of the DAG structure, the computationaland statistical problems are at least partly ameliorated.
17.7 Bibliographic Remarks
There are a number of texts on DAGs including Edwards (1995) and Jordan(2004). The ﬁrst use of DAGs for representing causal relationships was byWright (1934). Modern treatments are contained in Spirtes et al. (2000) andPearl (2000). Robins et al. (2003) discuss the problems with estimating causalstructure from data.
17.8 Appendix
Causation Revisited. We discussed causation in Chapter 16 using the idea
of counterfactual random variables. A diﬀerent approach to causation uses

<<<PAGE 289>>>

17.8 Appendix 273
XY Z
FIGURE 17.10. Conditioning versus intervening.
DAGs. The two approaches are mathematically equivalent though they appear
to be quite diﬀerent. In the DAG approach, the extra element is the idea ofintervention . Consider the DAG in Figure 17.10.
The probability function for a distribution consistent with this DAG has
the form f(x, y, z)=f(x)f(y|x)f(z|x, y). The following is pseudocode for
generating from this distribution.
Fori=1 ,...,n :
x
i<−pX(xi)
yi<−pY|X(yi|xi)
zi<−pZ|X,Y(zi|xi,yi)
Suppose we repeat this code many times, yielding data ( x1,y1,z1),...,(xn,yn,zn).
Among all the times that we observe Y=y, how often is Z=z? The answer
to this question is given by the conditional distribution of Z|Y. Speciﬁcally,
P(Z=z|Y=y)=P(Y=y,Z=z)
P(Y=y)=f(y,z)
f(y)
=summationtext
xf(x, y, z)
f(y)=summationtext
xf(x)f(y|x)f(z|x, y)
f(y)
=summationdisplay
xf(z|x, y)f(y|x)f(x)
f(y)=summationdisplay
xf(z|x, y)f(x, y)
f(y)
=summationdisplay
xf(z|x, y)f(x|y).
Now suppose we intervene by changing the computer code. Speciﬁcally, sup-
pose we ﬁx Yat the value y. The code now looks like this:
setY=y
fori=1 ,...,n
xi<−pX(xi)
zi<−pZ|X,Y(zi|xi,y)

<<<PAGE 290>>>

274 17. Directed Graphs and Conditional Independence
Having setY=y, how often was Z=z? To answer, note that the inter-
vention has changed the joint probability to be
f∗(x, z)=f(x)f(z|x, y).
The answer to our question is given by the marginal distribution
f∗(z)=summationdisplay
xf∗(x, z)=summationdisplay
xf(x)f(z|x, y).
We shall denote this as P(Z=z|Y:=y)o rf(z|Y:=y). We call P(Z=
z|Y=y)conditioning by observation orpassive conditioning. We call
P(Z=z|Y:=y)conditioning by intervention oractive conditioning.
Passive conditioning is used to answer a predictive question like:“Given that Joe smokes, what is the probability he will get lung cancer?”Active conditioning is used to answer a causal question like:“If Joe quits smoking, what is the probability he will get lung cancer?”Consider a pair ( G,P) where Gis a DAG and Pis a distribution for the
variables Vof the DAG. Let pdenote the probability function for P. Con-
sider intervening and ﬁxing a variable Xto be equal to x. We represent the
intervention by doing two things:
(1) Create a new DAG G
∗by removing all arrows pointing into X;
(2) Create a new distribution f∗(v)=P(V=v|X:=x) by removing the
termf(x|πX) from f(v).
The new pair ( G∗,f∗) represents the intervention “set X=x.”
17.15 Example. You may have noticed a correlation between rain and having
a wet lawn, that is, the variable “Rain” is not independent of the variable “WetLawn” and hence p
R,W(r, w)/negationslash=pR(r)pW(w) where Rdenotes Rain and W
denotes Wet Lawn. Consider the following two DAGs:
Rain−→Wet Lawn Rain ←−Wet Lawn .
The ﬁrst DAG implies that f(w,r)=f(r)f(w|r) while the second implies
thatf(w,r)=f(w)f(r|w) No matter what the joint distribution f(w,r) is,
both graphs are correct. Both imply that RandWare not independent. But,
intuitively, if we want a graph to indicate causation, the ﬁrst graph is rightand the second is wrong. Throwing water on your lawn doesn’t cause rain.The reason we feel the ﬁrst is correct while the second is wrong is because theinterventions implied by the ﬁrst graph are correct.
Look at the ﬁrst graph and form the intervention W= 1 where 1 denotes
“wet lawn.” Following the rules of intervention, we break the arrows into W

<<<PAGE 291>>>

17.8 Appendix 275
to get the modiﬁed graph:
Rain setW e tL a w n= 1
with distribution f∗(r)=f(r). Thus P(R=r|W:=w)=P(R=r) tells us
that “wet lawn” does not cause rain.
Suppose we (wrongly) assume that the second graph is the correct causal
graph and form the intervention W= 1 on the second graph. There are no
arrows into Wthat need to be broken so the intervention graph is the same
as the original graph. Thus f∗(r)=f(r|w) which would imply that changing
“wet” changes “rain.” Clearly, this is nonsense.
Both are correct probability graphs but only the ﬁrst is correct causally.
We know the correct causal graph by using background knowledge.
17.16 Remark. We could try to learn the correct causal graph from data but
this is dangerous. In fact it is impossible with two variables. With more thantwo variables there are methods that can ﬁnd the causal graph under certainassumptions but they are large sample methods and, furthermore, there is noway to ever know if the sample size you have is large enough to make themethods reliable.
We can use DAGs to represent confounding variables. If Xis a treatment
andYis an outcome, a confounding variable Zis a variable with arrows into
bothXandY; see Figure 17.11. It is easy to check, using the formalism of
interventions, that the following facts are true:
In a randomized study, the arrow between ZandXis broken. In this case,
even with Zunobserved (represented by enclosing Zin a circle), the causal
relationship between XandYis estimable because it can be shown that
E(Y|X:=x)=E(Y|X=x) which does not involve the unobserved Z.I n
an observational study, with all confounders observed, we get E(Y|X:=x)=integraltext
E(Y|X=x, Z=z)dF
Z(z) as in formula (16.7). If Zis unobserved then we
cannot estimate the causal eﬀect because E(Y|X:=x)=integraltext
E(Y|X=x, Z=
z)dFZ(z) involves the unobserved Z. We can’t just use XandYsince in this
case. P(Y=y|X=x)/negationslash=P(Y=y|X:=x) which is just another way of saying
that causation is not association.
In fact, we can make a precise connection between DAGs and counterfac-
tuals as follows. Suppose that XandYare binary. Deﬁne the confounding

<<<PAGE 292>>>

276 17. Directed Graphs and Conditional Independence
XYZ
XYZ
XYZ
FIGURE 17.11. Randomized study; Observational study with measured con-
founders; Observational study with unmeasured confounders. The circled variablesare unobserved.
variable Zby
Z=

1i f ( C
0,C1)=( 0 ,0)
2i f ( C0,C1)=( 0 ,1)
3i f ( C0,C1)=( 1 ,0)
4i f ( C0,C1)=( 1 ,1).
From this, you can make the correspondence between the DAG approach and
the counterfactual approach explicit. I leave this for the interested reader.
17.9 Exercises
1. Show that (17.1) and (17.2) are equivalent.
2. Prove Theorem 17.2.3. Let X,YandZhave the following joint distribution:
Y=0 Y=1 Y=0 Y=1
X= 0 .405 .045 X= 0 .125 .125
X= 1 .045 .005 X= 1 .125 .125
Z=0 Z=1
(a) Find the conditional distribution of XandYgiven Z= 0 and the
conditional distribution of XandYgiven Z=1 .
(b) Show that X
/coproductY|Z.
(c) Find the marginal distribution of XandY.
(d) Show that XandYare not marginally independent.
4. Consider the three DAGs in Figure 17.6 without a collider. Prove that
X/coproductZ|Y.

<<<PAGE 293>>>

17.9 Exercises 277
X Y1 Z1 Y3 Z3Y4Z4
Y2
Z2
FIGURE 17.12. DAG for exercise 7.
5. Consider the DAG in Figure 17.6 with a collider. Prove that X/coproductZand
thatXandZare dependent given Y.
6. Let X∈{0,1},Y∈{0,1},Z∈{0,1,2}. Suppose the distribution of
(X,Y,Z ) is Markov to:
X−→Y−→Z
Create a joint distribution f(x, y, z) that is Markov to this DAG. Gen-
erate 1000 random vectors from this distribution. Estimate the distribu-tion from the data using maximum likelihood. Compare the estimateddistribution to the true distribution. Let θ=(θ
000,θ001,...,θ 112) where
θrst=P(X=r, Y=s, Z=t). Use the bootstrap to get standard errors
and 95 percent conﬁdence intervals for these 12 parameters.
7. Consider the DAG in Figure 17.12.
(a) Write down the factorization of the joint density.(b) Prove that X
/coproductZj.
8. Let V=(X,Y,Z ) have the following joint distribution
X∼Bernoulliparenleftbigg1
2parenrightbigg

<<<PAGE 294>>>

278 17. Directed Graphs and Conditional Independence
Y|X=x∼Bernoulliparenleftbigge4x−2
1+e4x−2parenrightbigg
Z|X=x, Y=y∼Bernoulliparenleftbigge2(x+y)−2
1+e2(x+y)−2parenrightbigg
.
(a) Find an expression for P(Z=z|Y=y). In particular, ﬁnd P(Z=
1|Y= 1).
(b) Write a program to simulate the model. Conduct a simulation and
compute P(Z=1|Y= 1) empirically. Plot this as a function of
the simulation size N. It should converge to the theoretical value you
computed in (a).
(c) (Refers to material in the appendix.) Write down an expression for
P(Z=1|Y:=y). In particular, ﬁnd P(Z=1|Y:= 1).
(d) (Refers to material in the appendix.) Modify your program to sim-
ulate the intervention “set Y= 1.” Conduct a simulation and compute
P(Z=1|Y:= 1) empirically. Plot this as a function of the simulation
sizeN. It should converge to the theoretical value you computed in (c).
9. This is a continuous, Gaussian version of the last question. Let V=
(X,Y,Z ) have the following joint distribution
X∼Normal (0 ,1)
Y|X=x∼Normal ( αx,1)
Z|X=x, Y=y∼Normal ( βy+γx,1).
Here, α,βandγare ﬁxed parameters. economists refer to models like
this as structural equation models.
(a) Find an explicit expression for f(z|y) and E(Z|Y=y)=integraltext
zf(z|
y)dz.
(b) (Refers to material in the appendix.) Find an explicit expression
forf(z|Y:=y) and then ﬁnd E(Z|Y:=y)≡integraltext
zf(z|Y:=y)dy.
Compare to (b).
(c) Find the joint distribution of ( Y,Z). Find the correlation ρbetween
YandZ.
(d) (Refers to material in the appendix.) Suppose that Xis not observed
and we try to make causal conclusions from the marginal distribution of(Y,Z). (Think of Xas unobserved confounding variables.) In particular,

<<<PAGE 295>>>

17.9 Exercises 279
suppose we declare that Ycauses Zifρ/negationslash= 0 and we declare that Ydoes
not cause Zifρ= 0. Show that this will lead to erroneous conclusions.
(e) (Refers to material in the appendix.) Suppose we conduct a ran-
domized experiment in which Yis randomly assigned. To be concrete,
suppose that
X∼Normal(0 ,1)
Y∼Normal( α,1)
Z|X=x, Y=y∼Normal( βy+γx,1).
Show that the method in (d) now yields correct conclusions (i.e., ρ=0
if and only if f(z|Y:=y) does not depend on y).

<<<PAGE 296>>>



<<<PAGE 297>>>

18
Undirected Graphs
Undirected graphs are an alternative to directed graphs for representing in-
dependence relations. Since both directed and undirected graphs are used inpractice, it is a good idea to be facile with both. The main diﬀerence betweenthe two is that the rules for reading independence relations from the graphare diﬀerent.
18.1 Undirected Graphs
Anundirected graph G=(V,E) has a ﬁnite set Vofvertices (or nodes)
and a set Eofedges (or arcs) consisting of pairs of vertices. The vertices
correspond to random variables X,Y,Z,... and edges are written as unordered
pairs. For example, ( X,Y)∈Emeans that XandYare joined by an edge.
An example of a graph is in Figure 18.1.
Two vertices are adjacent , written X∼Y, if there is an edge between
them. In Figure 18.1, XandYare adjacent but XandZare not adjacent. A
sequence X0,...,X nis called a path ifXi−1∼Xifor each i. In Figure 18.1,
X,Y,Z is a path. A graph is complete if there is an edge between every pair
of vertices. A subset U⊂Vof vertices together with their edges is called a
subgraph.

<<<PAGE 298>>>

282 18. Undirected Graphs
/Bullet/Bullet
/BulletXY
Z
FIGURE 18.1. A graph with vertices V={X,Y,Z }. The edge set is
E={(X,Y),(Y,Z)}.
/Bullet/Bullet
/Bullet/Bullet
YWX
Z
FIGURE 18.2. {Y,W}and{Z}are separated by {X}. Also, WandZare separated
by{X,Y}.
IfA, BandCare three distinct subsets of V, we say that Cseparates
AandBif every path from a variable in Ato a variable in Bintersects a
variable in C. In Figure 18.2 {Y,W}and{Z}are separated by {X}. Also, W
andZare separated by {X,Y}.
18.2 Probability and Graphs
LetVbe a set of random variables with distribution P. Construct a graph
with one vertex for each random variable in V. Omit the edge between a pair
of variables if they are independent given the rest of the variables:
no edge between XandY⇐⇒ X/coproductY|rest

<<<PAGE 299>>>

18.2 Probability and Graphs 283
/Bullet/Bullet
/BulletXY
Z
FIGURE 18.3. X/coproductZ|Y.
/Bullet/Bullet
/BulletXY
Z
FIGURE 18.4. No implied independence relations.
where “rest” refers to all the other variables besides XandY. The resulting
graph is called a pairwise Markov graph . Some examples are shown in
Figures 18.3, 18.4, 18.5, and 18.6.
The graph encodes a set of pairwise conditional independence relations.
These relations imply other conditional independence relations. How can weﬁgure out what they are? Fortunately, we can read these other conditionalindependence relations directly from the graph as well, as is explained in thenext theorem.
18.1 Theorem. LetG=(V,E)be a pairwise Markov graph for a distribution
P.L e tA, BandCbe distinct subsets of Vsuch that Cseparates AandB.
Then A
/coproductB|C.
18.2 Remark. IfAandBare not connected (i.e., there is no path from Ato
B) then we may regard AandBas being separated by the empty set. Then
Theorem 18.1 implies that A/coproductB.

<<<PAGE 300>>>

284 18. Undirected Graphs
/Bullet/Bullet
/Bullet/Bullet
YXW
Z
FIGURE 18.5. X/coproductZ|{Y,W}andY/coproductW|{X,Z}.
/Bullet/Bullet/Bullet/Bullet
XY ZW
FIGURE 18.6. Pairwise independence implies that X/coproductZ|{Y,W}. But is X/coproductZ|Y?
The independence condition in Theorem 18.1 is called the global Markov
property. We thus see that the pairwise and global Markov properties are
equivalent. Let us state this more precisely. Given a graph G, letMpair(G)
be the set of distributions which satisfy the pairwise Markov property: thusP∈M
pair(G) if, under P,X/coproductY|rest if and only if there is no edge between
XandY. LetMglobal(G) be the set of distributions which satisfy the global
Markov property: thus P∈Mpair(G) if, under P,A/coproductB|Cif and only if C
separates AandB.
18.3 Theorem. LetGbe a graph. Then, Mpair(G)=Mglobal(G).
Theorem 18.3 allows us to construct graphs using the simpler pairwise prop-
erty and then we can deduce other independence relations using the globalMarkov property. Think how hard this would be to do algebraically. Returningto 18.6, we now see that X
/coproductZ|YandY/coproductW|Z.
18.4 Example. Figure 18.7 implies that X/coproductY,X/coproductZandX/coproduct(Y,Z)./squaresolid
18.5 Example. Figure 18.8 implies that X/coproductW|(Y,Z) and X/coproductZ|Y./squaresolid

<<<PAGE 301>>>

18.3 Cliques and Potentials 285
/Bullet/Bullet
/BulletXY
Z
FIGURE 18.7. X/coproductY,X/coproductZandX/coproduct(Y,Z).
/Bullet
/Bullet/Bullet
/BulletXY
ZW
FIGURE 18.8. X/coproductW|(Y,Z) and X/coproductZ|Y.
18.3 Cliques and Potentials
Aclique is a set of variables in a graph that are all adjacent to each other. A
set of variables is a maximal clique if it is a clique and if it is not possible
to include another variable and still be a clique. A potential is any positive
function. Under certain conditions, it can be shown that Pis Markov Gif and
only if its probability function fcan be written as
f(x)=producttext
C∈CψC(xC)
Z(18.1)
where Cis the set of maximal cliques and
Z=summationdisplay
xproductdisplay
C∈CψC(xC).
18.6 Example. The maximal cliques for the graph in Figure 18.1 are C1=
{X,Y}andC2={Y,Z}. Hence, if Pis Markov to the graph, then its proba-
bility function can be written
f(x, y, z)∝ψ1(x, y)ψ2(y,z)
for some positive functions ψ1andψ2./squaresolid

<<<PAGE 302>>>

286 18. Undirected Graphs
/Bullet/Bullet/Bullet /Bullet/Bullet/Bullet
X3 X5X1X2 X4
X6
FIGURE 18.9. The maximumly cliques of this graph are
{X1,X2},{X1,X3},{X2,X4},{X3,X5},{X2,X5,X6}.
18.7 Example. The maximal cliques for the graph in Figure 18.9 are
{X1,X2},{X1,X3},{X2,X4},{X3,X5},{X2,X5,X6}.
Thus we can write the probability function as
f(x1,x2,x3,x4,x5,x6)∝ψ12(x1,x2)ψ13(x1,x3)ψ24(x2,x4)
×ψ35(x3,x5)ψ256(x2,x5,x6)./squaresolid
18.4 Fitting Graphs to Data
Given a data set, how do we ﬁnd a graphical model that ﬁts the data? As
with directed graphs, this is a big topic that we will not treat here. However,in the discrete case, one way to ﬁt a graph to data is to use a log-linear
model , which is the subject of the next chapter.
18.5 Bibliographic Remarks
Thorough treatments of undirected graphs can be found in Whittaker (1990)
and Lauritzen (1996). Some of the exercises below are from Whittaker (1990).
18.6 Exercises
1. Consider random variables ( X1,X2,X3). In each of the following cases,
draw a graph that has the given independence relations.

<<<PAGE 303>>>

18.6 Exercises 287
/Bullet/Bullet/Bullet/BulletX1 X2 X4
X3
FIGURE 18.10.
/Bullet/Bullet/Bullet/BulletX1 X2 X3 X4
FIGURE 18.11.
(a)X1/coproductX3|X2.
(b)X1/coproductX2|X3andX1/coproductX3|X2.
(c)X1/coproductX2|X3andX1/coproductX3|X2andX2/coproductX3|X1.
2. Consider random variables ( X1,X2,X3,X4). In each of the following
cases, draw a graph that has the given independence relations.
(a)X1/coproductX3|X2,X4andX1/coproductX4|X2,X3andX2/coproductX4|X1,X3.
(b)X1/coproductX2|X3,X4andX1/coproductX3|X2,X4andX2/coproductX3|X1,X4.
(c)X1/coproductX3|X2,X4andX2/coproductX4|X1,X3.
3. A conditional independence between a pair of variables is minimal if it
is not possible to use the Separation Theorem to eliminate any variablefrom the conditioning set, i.e. from the right hand side of the bar Whit-taker (1990). Write down the minimal conditional independencies from:(a) Figure 18.10; (b) Figure 18.11; (c) Figure 18.12; (d) Figure 18.13.
4. Let X
1,X2,X3be binary random variables. Construct the likelihood
ratio test for
H0:X1/coproductX2|X3versus H1:X1is not independent of X2|X3.
5. Here are breast cancer data from Morrison et al. (1973) on diagnostic
center ( X1), nuclear grade ( X2), and survival ( X3):

<<<PAGE 304>>>

288 18. Undirected Graphs
/Bullet/Bullet
/Bullet/Bullet
X4X3 X2
X1
FIGURE 18.12.
/Bullet/Bullet
/Bullet/Bullet/Bullet
/Bullet
X4X2 X3X1
X6 X5
FIGURE 18.13.
X2malignant malignant benign benign
X3 died survived died survived
X1 Boston 35 59 47 112
Glamorgan 42 77 26 76
(a) Treat this as a multinomial and ﬁnd the maximum likelihood esti-
mator.
(b) If someone has a tumor classiﬁed as benign at the Glamorgan clinic,
what is the estimated probability that they will die? Find the standarderror for this estimate.

<<<PAGE 305>>>

18.6 Exercises 289
(c) Test the following hypotheses:
X1/coproductX2|X3versus X1/gluonbelement/gluonelement/gluonelement/gluonelement/gluonelement/gluoneelementX2|X3
X1/coproductX3|X2versus X1/gluonbelement/gluonelement/gluonelement/gluonelement/gluonelement/gluoneelementX3|X2
X2/coproductX3|X1versus X2/gluonbelement/gluonelement/gluonelement/gluonelement/gluonelement/gluoneelementX3|X1
Use the test from question 4. Based on the results of your tests, draw
and interpret the resulting graph.

<<<PAGE 306>>>



<<<PAGE 307>>>

19
Log-Linear Models
In this chapter we study log-linear models which are useful for modeling
multivariate discrete data. There is a strong connection between log-linear
models and undirected graphs.
19.1 The Log-Linear Model
LetX=(X1,...,X m) be a discrete random vector with probability function
f(x)=P(X=x)=P(X1=x1,...,X m=xm)
where x=(x1,...,x m). Let rjbe the number of values that Xjtakes. Without
loss of generality, we can assume that Xj∈{0,1,...,r j−1}. Suppose now
that we have nsuch random vectors. We can think of the data as a sample
from a Multinomial with N=r1×r2×···× rmcategories. The data can be
represented as counts in a r1×r2×···× rmtable. Let p=(p1,...,p N) denote
the multinomial parameter.
LetS={1,...,m }. Given a vector x=(x1,...,x m) and a subset A⊂S,
letxA=(xj:j∈A). For example, if A={1,3}thenxA=(x1,x3).

<<<PAGE 308>>>

292 19. Log-Linear Models
19.1 Theorem. The joint probability function f(x)of a single random vector
X=(X1,...,X m)can be written as
logf(x)=summationdisplay
A⊂SψA(x) (19.1)
where the sum is over all subsets AofS={1,...,m }and the ψ’s satisfy the
following conditions:
1.ψ∅(x)is a constant;
2. For every A⊂S,ψA(x)is only a function of xAand not the rest of the
x/prime
js.
3. Ifi∈Aandxi=0, then ψA(x)=0.
The formula in equation (19.1) is called the log-linear expansion off.
Each ψA(x) may depend on some unknown parameters βA. Let β=(βA:
A⊂S) be the set of all these parameters. We will write f(x)=f(x;β) when
we want to emphasize the dependence on the unknown parameters β.
In terms of the multinomial, the parameter space is
P=braceleftbigg
p=(p1,...,p N):pj≥0,Nsummationdisplay
j=1pj=1bracerightbigg
.
This is an N−1 dimensional space. In the log-linear representation, the pa-
rameter space is
Θ=braceleftbigg
β=(β1,...,β N):β=β(p),p∈Pbracerightbigg
where β(p) is the set of βvalues associated with p.T h es e tΘi sa N−1
dimensional surface in RN. We can always go back and forth between the two
parameterizations we can write β=β(p) and p=p(β).
19.2 Example. LetX∼Bernoulli( p) where 0 <p< 1. We can write the
probability mass function for Xas
f(x)=px(1−p)1−x=px
1p1−x
2
forx=0,1, where p1=pandp2=1−p. Hence,
logf(x)=ψ∅(x)+ψ1(x)

<<<PAGE 309>>>

19.1 The Log-Linear Model 293
where
ψ∅(x) = log( p2)
ψ1(x)= xlogparenleftbiggp1
p2parenrightbigg
.
Notice that ψ∅(x) is a constant (as a function of x) and ψ1(x) = 0 when x=0 .
Thus the three conditions of Theorem 19.1 hold. The log-linear parametersare
β
0= log( p2),β1= logparenleftbiggp1
p2parenrightbigg
.
The original, multinomial parameter space is P={(p1,p2):pj≥0,p1+p2=
1}. The log-linear parameter space is
Θ=braceleftbigg
(β0,β1)∈R2:eβ0+β1+eβ0=1.bracerightbigg
Given ( p1,p2) we can solve for ( β0,β1). Conversely, given ( β0,β1) we can solve
for (p1,p2)./squaresolid
19.3 Example. LetX=(X1,X2) where X1∈{0,1}andX2∈{0,1,2}. The
joint distribution of nsuch random vectors is a multinomial with 6 categories.
The multinomial parameters can be written as a 2-by-3 table as follows:
multinomial x2 012
x10 p00p01p02
1 p10p11p12
Thendata vectors can be summarized as counts:
data x2 012
x10 C00C01C02
1 C10C11C12
Forx=(x1,x2), the log-linear expansion takes the form
logf(x)=ψ∅(x)+ψ1(x)+ψ2(x)+ψ12(x)
where
ψ∅(x) = log p00
ψ1(x)= x1logparenleftbiggp10
p00parenrightbigg
ψ2(x)= I(x2= 1) logparenleftbiggp01
p00parenrightbigg
+I(x2= 2) logparenleftbiggp02
p00parenrightbigg
ψ12(x)= I(x1=1,x2= 1) logparenleftbiggp11p00
p01p10parenrightbigg
+I(x1=1,x2= 2) logparenleftbiggp12p00
p02p10parenrightbigg
.

<<<PAGE 310>>>

294 19. Log-Linear Models
Convince yourself that the three conditions on the ψ’s of the theorem are
satisﬁed. The six parameters of this model are:
β1= log p00 β2= logparenleftBig
p10
p00parenrightBig
β3= logparenleftBig
p01
p00parenrightBig
β4= logparenleftBig
p02
p00parenrightBig
β5= logparenleftBig
p11p00
p01p10parenrightBig
β6= logparenleftBig
p12p00
p02p10parenrightBig
./squaresolid
The next theorem gives an easy way to check for conditional independence
in a log-linear model.
19.4 Theorem. Let(Xa,Xb,Xc)be a partition of a vectors (X1,...,Xm).
Then Xb/coproductXc|Xaif and only if all the ψ-terms in the log-linear expansion
that have at least one coordinate in band one coordinate in care 0.
To prove this theorem, we will use the following lemma whose proof follows
easily from the deﬁnition of conditional independence.
19.5 Lemma. A partition (Xa,Xb,Xc)satisﬁes Xb/coproductXc|Xaif and only if
f(xa,xb,xc)=g(xa,xb)h(xa,xc)for some functions gandh
Proof. (Theorem 19.4.) Suppose that ψtis 0 whenever thas coordinates
inbandc. Hence, ψti s0i f t/negationslash⊂auniontextbort/negationslash⊂auniontextc. Therefore
logf(x)=summationdisplay
t⊂a/uniontextbψt(x)+summationdisplay
t⊂a/uniontextcψt(x)−summationdisplay
t⊂aψt(x).
Exponentiating, we see that the joint density is of the form g(xa,xb)h(xa,xc).
By Lemma 19.5, Xb/coproductXc|Xa. The converse follows by reversing the argument.
/squaresolid
19.2 Graphical Log-Linear Models
A log-linear model is graphical if missing terms correspond only to condi-
tional independence constraints.
19.6 Deﬁnition. Letlogf(x)=summationtext
A⊂SψA(x)be a log-linear model. Then
fisgraphical if allψ-terms are nonzero except for any pair of
coordinates not in the edge set for some graph G. In other words,
ψA(x)=0if and only if {i, j}⊂Aand(i, j)is not an edge.
Here is a way to think about the deﬁnition above:

<<<PAGE 311>>>

19.2 Graphical Log-Linear Models 295
/Bullet/Bullet/Bullet/Bullet/Bullet
X1 X2 X3X5 X4
FIGURE 19.1. Graph for Example 19.7.
If you can add a term to the model and the graph does not change,
then the model is not graphical.
19.7 Example. Consider the graph in Figure 19.1.
The graphical log-linear model that corresponds to this graph is
logf(x)= ψ∅+ψ1(x)+ψ2(x)+ψ3(x)+ψ4(x)+ψ5(x)
+ψ12(x)+ψ23(x)+ψ25(x)+ψ34(x)+ψ35(x)+ψ45(x)+ψ235(x)+ψ345(x).
Let’s see why this model is graphical. The edge (1 ,5) is missing in the graph.
Hence any term containing that pair of indices is omitted from the model. Forexample,
ψ
15,ψ125,ψ135,ψ145,ψ1235,ψ1245,ψ1345,ψ12345
are all omitted. Similarly, the edge (2 ,4) is missing and hence
ψ24,ψ124,ψ234,ψ245,ψ1234,ψ1245,ψ2345,ψ12345
are all omitted. There are other missing edges as well. You can check that the
model omits all the corresponding ψterms. Now consider the model
logf(x)= ψ∅(x)+ψ1(x)+ψ2(x)+ψ3(x)+ψ4(x)+ψ5(x)
+ψ12(x)+ψ23(x)+ψ25(x)+ψ34(x)+ψ35(x)+ψ45(x).
This is the same model except that the three way interactions were removed.
If we draw a graph for this model, we will get the same graph. For example,noψterms contain (1 ,5) so we omit the edge between X
1andX5. But this is
not graphical since it has extra terms omitted. The independencies and graphs

<<<PAGE 312>>>

296 19. Log-Linear Models
/Bullet/Bullet/Bullet
X2 X1 X3
FIGURE 19.2. Graph for Example 19.10.
for the two models are the same but the latter model has other constraints
besides conditional independence constraints. This is not a bad thing. It justmeans that if we are only concerned about presence or absence of conditionalindependences, then we need not consider such a model. The presence of thethree-way interaction ψ
235means that the strength of association between X2
andX3varies as a function of X5. Its absence indicates that this is not so. /squaresolid
19.3 Hierarchical Log-Linear Models
There is a set of log-linear models that is larger than the set of graphical
models and that are used quite a bit. These are the hierarchical log-linearmodels.
19.8 Deﬁnition. A log-linear model is hierarchical ifψA=0andA⊂B
implies that ψB=0.
19.9 Lemma. A graphical model is hierarchical but the reverse need not be
true.
19.10 Example. Let
logf(x)=ψ∅(x)+ψ1(x)+ψ2(x)+ψ3(x)+ψ12(x)+ψ13(x).
The model is hierarchical; its graph is given in Figure 19.2. The model is
graphical because all terms involving (2,3) are omitted. It is also hierarchical.
/squaresolid
19.11 Example. Let
logf(x)=ψ∅(x)+ψ1(x)+ψ2(x)+ψ3(x)+ψ12(x)+ψ13(x)+ψ23(x).

<<<PAGE 313>>>

19.4 Model Generators 297
/Bullet/Bullet /Bullet
X3X1 X2
FIGURE 19.3. The graph is complete. The model is hierarchical but not graphical.
/Bullet/Bullet/Bullet
X1 X2 X3
FIGURE 19.4. The model for this graph is not hierarchical.
The model is hierarchical. It is not graphical. The graph corresponding to this
model is complete; see Figure 19.3. It is not graphical because ψ123(x)=0
which does not correspond to any pairwise conditional independence. /squaresolid
19.12 Example. Let
logf(x)=ψ∅(x)+ψ3(x)+ψ12(x).
The graph corresponding is in Figure 19.4. This model is not hierarchical since
ψ2= 0 but ψ12is not. Since it is not hierarchical, it is not graphical either. /squaresolid
19.4 Model Generators
Hierarchical models can be written succinctly using generators . This is most
easily explained by example. Suppose that X=(X1,X2,X3). Then, M=
1.2+1.3 stands for
logf=ψ∅+ψ1+ψ2+ψ3+ψ12+ψ13.

<<<PAGE 314>>>

298 19. Log-Linear Models
The formula M=1.2+1.3 says: “include ψ12andψ13.” We have to also include
the lower order terms or it won’t be hierarchical. The generator M=1.2.3i s
thesaturated model
logf=ψ∅+ψ1+ψ2+ψ3+ψ12+ψ13+ψ23+ψ123.
The saturated models corresponds to ﬁtting an unconstrained multinomial.
Consider M=1+2+3 which means
logf=ψ∅+ψ1+ψ2+ψ3.
This is the mutual independence model. Finally, consider M=1.2 which has
log-linear expansion
logf=ψ∅+ψ1+ψ2+ψ12.
This model makes X3|X2=x2,X1=x1a uniform distribution.
19.5 Fitting Log-Linear Models to Data
Letβdenote all the parameters in a log-linear model M. The loglikelihood
forβis
/lscript(β)=nsummationdisplay
i=1logf(Xi;β)
where f(Xi;β) is the probability function for the ithrandom vector Xi=
(Xi1,...,X im) as give by equation (19.1). The mlehatwideβgenerally has to be
found numerically. The Fisher information matrix is also found numericallyand we can then get the estimated standard errors from the inverse Fisherinformation matrix.
When ﬁtting log-linear models, one has to address the following model
selection problem: which ψterms should we include in the model? This is
essentially the same as the model selection problem in linear regression.
One approach is is to use AIC. Let Mdenote some log-linear model. Diﬀer-
ent models correspond to setting diﬀerent ψterms to 0. Now we choose the
model Mwhich maximizes
AIC(M)=hatwide/lscript(M)−|M| (19.2)
where |M|is the number of parameters in model Mandhatwide/lscript(M) is the value
of the log-likelihood evaluated at the mlefor that model. Usually the model
search is restricted to hierarchical models. This reduces the search space. Some

<<<PAGE 315>>>

19.5 Fitting Log-Linear Models to Data 299
also claim that we should only search through the hierarchical models because
other models are less interpretable.
A diﬀerent approach is based on hypothesis testing. The model that includes
all possible ψ-terms is called the saturated model and we denote it by Msat.
Now for each Mwe test the hypothesis
H0: the true model is Mversus H1: the true model is Msat.
The likelihood ratio test for this hypothesis is called the deviance.
19.13 Deﬁnition. For any submodel M, deﬁne the deviance dev(M)by
dev(M)=2 (hatwide/lscriptsat−hatwide/lscriptM)
wherehatwide/lscriptsatis the log-likelihood of the saturated model evaluated at the mle
andhatwide/lscriptMis the log-likelihood of the model Mevaluated at its mle.
19.14 Theorem. The deviance is the likelihood ratio test statistic for
H0: the model is Mversus H1: the model is Msat.
Under H0,dev(M)d→χ2
νwithνdegrees of freedom equal to the diﬀerence in
the number of parameters between the saturated model and M.
One way to ﬁnd a good model is to use the deviance to test every sub-model.
Every model that is not rejected by this test is then considered a plausiblemodel. However, this is not a good strategy for two reasons. First, we will endup doing many tests which means that there is ample opportunity for makingType I and Type II errors. Second, we will end up using models where wefailed to reject H
0. But we might fail to reject H0due to low power. The
result is that we end up with a bad model just due to low power.
After ﬁnding a “best model” this way we can draw the corresponding graph.
19.15 Example. The following breast cancer data are from Morrison et al.
(1973). The data are on diagnostic center ( X1), nuclear grade ( X2), and sur-
vival ( X3):
X2malignant malignant benign benign
X3 died survived died survived
X1 Boston 35 59 47 112
Glamorgan 42 77 26 76
The saturated log-linear model is:

<<<PAGE 316>>>

300 19. Log-Linear Models
Center Grade Survival
FIGURE 19.5. The graph for Example 19.15.
Variable hatwideβjhatwidese Wjp-value
(Intercept) 3.56 0.17 21.03 0.00 ***
center 0.18 0.22 0.79 0.42grade 0.29 0.22 1.32 0.18survival 0.52 0.21 2.44 0.01 *center ×grade -0.77 0.33 -2.31 0.02 *
center ×survival 0.08 0.28 0.29 0.76
grade×survival 0.34 0.27 1.25 0.20
center ×grade×survival 0.12 0.40 0.29 0.76
The best sub-model, selected using AIC and backward searching is:
Variable hatwideβ
jhatwidese Wjp-value
(Intercept) 3.52 0.13 25.62 <0.00 ***
center 0.23 0.13 1.70 0.08grade 0.26 0.18 1.43 0.15survival 0.56 0.14 3.98 6.65e-05 ***center ×grade -0.67 0.18 -3.62 0.00 ***
grade×survival 0.37 0.19 1.90 0.05
The graph for this model Mis shown in Figure 19.5. To test the ﬁt of this
model, we compute the deviance of Mwhich is 0.6. The appropriate χ
2has
8−6 = 2 degrees of freedom. The p-value is P(χ2
2>.6) =.74. So we have no
evidence to suggest that the model is a poor ﬁt. /squaresolid
19.6 Bibliographic Remarks
For this chapter, I drew heavily on Whittaker (1990) which is an excellenttext on log-linear models and graphical models. Some of the exercises are fromWhittaker. A classic reference on log-linear models is Bishop et al. (1975).

<<<PAGE 317>>>

19.7 Exercises 301
19.7 Exercises
1. Solve for the p/prime
ijsin terms of the β’s in Example 19.3.
2. Prove Lemma 19.5.3. Prove Lemma 19.9.4. Consider random variables ( X
1,X2,X3,X4). Suppose the log-density is
logf(x)=ψ∅(x)+ψ12(x)+ψ13(x)+ψ24(x)+ψ34(x).
(a) Draw the graph Gfor these variables.
(b) Write down all independence and conditional independence relations
implied by the graph.
(c) Is this model graphical? Is it hierarchical?
5. Suppose that parameters p(x1,x2,x3) are proportional to the following
values:
x200 11
x301 01
x102 841 61 16 128 32 256
Find the ψ-terms for the log-linear expansion. Comment on the model.
6. Let X1,...,X 4be binary. Draw the independence graphs correspond-
ing to the following log-linear models. Also, identify whether each isgraphical and/or hierarchical (or neither).
(a) log f=7+1 1 x
1+2x2+1.5x3+1 7x4
(b) log f=7+1 1 x1+2x2+1.5x3+1 7x4+1 2x2x3+7 8x2x4+3x3x4+
32x2x3x4
(c) log f= 7+11 x1+2x2+1.5x3+17x4+12x2x3+3x3x4+x1x4+2x1x2
(d) log f= 7 + 5055 x1x2x3x4

<<<PAGE 318>>>



<<<PAGE 319>>>

20
Nonparametric Curve Estimation
In this Chapter we discuss nonparametric estimation of probability density
functions and regression functions which we refer to as curve estimation or
smoothing .
In Chapter 7 we saw that it is possible to consistently estimate a cumulative
distribution function Fwithout making any assumptions about F.I fw ew a n t
to estimate a probability density function f(x) or a regression function r(x)=
E(Y|X=x) the situation is diﬀerent. We cannot estimate these functions
consistently without making some smoothness assumptions. Correspondingly,we need to perform some sort of smoothing operation on the data.
An example of a density estimator is a histogram , which we discuss in
detail in Section 20.2. To form a histogram estimator of a density f, we divide
the real line to disjoint sets called bins. The histogram estimator is a piecewise
constant function where the height of the function is proportional to numberof observations in each bin; see Figure 20.3. The number of bins is an exampleof asmoothing parameter . If we smooth too much (large bins) we get a
highly biased estimator while if we smooth too little (small bins) we get ahighly variable estimator. Much of curve estimation is concerned with tryingto optimally balance variance and bias.

<<<PAGE 320>>>

304 20. Nonparametric Curve Estimation
hatwideg(x)
This is a function of the data This is the point at which we are
evaluating hatwideg(·)
FIGURE 20.1. A curve estimate /hatwidegis random because it is a function of the data.
The point xat which we evaluate /hatwidegis not a random variable.
20.1 The Bias-Variance Tradeoﬀ
Letgdenote an unknown function such as a density function or a regression
function. Let hatwidegndenote an estimator of g. Bear in mind that hatwidegn(x) is a random
function evaluated at a point x. The estimator is random because it depends
on the data. See Figure 20.1.
As a loss function, we will use the integrated squared error (ISE) :1
L(g,hatwidegn)=integraldisplay
(g(u)−hatwidegn(u))2du. (20.1)
Theriskormean integrated squared error (MISE) with respect to
squared error loss is
R(f,hatwidef)=Eparenleftbigg
L(g,hatwideg)parenrightbigg
. (20.2)
20.1 Lemma. The risk can be written as
R(g,hatwidegn)=integraldisplay
b2(x)dx+integraldisplay
v(x)dx (20.3)
where
b(x)=E(hatwidegn(x))−g(x) (20.4)
is the bias of hatwidegn(x)at a ﬁxed xand
v(x)=V(hatwidegn(x)) =Eparenleftbiggparenleftbig
hatwidegn(x)−Eparenleftbig
hatwidegn(x)parenrightbig2parenrightbigparenrightbigg
(20.5)
is the variance of hatwidegn(x)at a ﬁxed x.
1We could use other loss functions. The results are similar but the analysis is much more
complicated.

<<<PAGE 321>>>

20.2 Histograms 305
/Bullet
/BulletRisk
Bias squared
Variance
Optimal
SmoothingMore Smoothing Less Smoothing
FIGURE 20.2. The Bias-Variance trade-oﬀ. The bias increases and the variance de-
creases with the amount of smoothing. The optimal amount of smoothing, indicatedby the vertical line, minimizes the risk = bias
2+ variance.
In summary,
RISK = BIAS2+ VARIANCE . (20.6)
When the data are oversmoothed, the bias term is large and the variance
is small. When the data are undersmoothed the opposite is true; see Figure20.2. This is called the bias-variance tradeoﬀ . Minimizing risk corresponds
to balancing bias and variance.
20.2 Histograms
LetX1,...,X nbeiidon [0,1] with density f. The restriction to [0 ,1] is not
crucial; we can always rescale the data to be on this interval. Let mbe an

<<<PAGE 322>>>

306 20. Nonparametric Curve Estimation
integer and deﬁne bins
B1=bracketleftbigg
0,1
mparenrightbigg
,B2=bracketleftbigg1
m,2
mparenrightbigg
, ..., B m=bracketleftbiggm−1
m,1bracketrightbigg
. (20.7)
Deﬁne the binwidth h=1/m, letνjbe the number of observations in Bj,
lethatwidepj=νj/nand let pj=integraltext
Bjf(u)du.
Thehistogram estimator is deﬁned by
hatwidefn(x)=

hatwidep
1/h x ∈B1
hatwidep2/h x ∈B2
......
hatwidep
m/h x ∈Bm
which we can write more succinctly as
hatwidefn(x)=nsummationdisplay
j=1hatwidepj
hI(x∈Bj). (20.8)
To understand the motivation for this estimator, let pj=integraltext
Bjf(u)duand note
that, for x∈Bjandhsmall,
E(hatwidefn(x)) =E(hatwidepj)
h=pj
h=integraltext
Bjf(u)du
h≈f(x)h
f(x)=f(x).
20.2 Example. Figure 20.3 shows three diﬀerent histograms based on n=
1,266 data points from an astronomical sky survey. Each data point repre-
sents the distance from us to a galaxy. The galaxies lie on a “pencilbeam”pointing directly from the Earth out into space. Because of the ﬁnite speed oflight, looking at galaxies farther and farther awaycorresponds to looking back
in time. Choosing the right number of bins involves ﬁnding a good tradeoﬀbetween bias and variance. We shall see later that the top left histogram hastoo few bins resulting in oversmoothing and too much bias. The bottom lefthistogram has too many bins resulting in undersmoothing and too few bins.The top right histogram is just right. The histogram reveals the presence ofclusters of galaxies. Seeing how the size and number of galaxy clusters varieswith time, helps cosmologists understand the evolution of the universe.
/squaresolid
The mean and variance of hatwidefn(x) are given in the following Theorem.
20.3 Theorem. Consider ﬁxed xand ﬁxed m, and let Bjbe the bin containing
x. Then,
E(hatwidefn(x)) =pj
hand V(hatwidefn(x)) =pj(1−pj)
nh2. (20.9)

<<<PAGE 323>>>

20.2 Histograms 307
Oversmoothed0.00 0.05 0.10 0.15 0.2002468 1 0 1 2 1 4
Just Right0.00 0.05 0.10 0.15 0.200 1 02 03 04 05 06 0
Undersmoothed0.00 0.05 0.10 0.15 0.2002 0 4 0 6 0
0 200 400 600 800 1000−14 −12 −10 −8 −6
number of binscross validation score
FIGURE 20.3. Three versions of a histogram for the astronomy data. The top left
histogram has too few bins. The bottom left histogram has too many bins. The topright histogram is just right. The lower, right plot shows the estimated risk versusthe number of bins.

<<<PAGE 324>>>

308 20. Nonparametric Curve Estimation
Let’s take a closer look at the bias-variance tradeoﬀ using equation (20.9).
Consider some x∈Bj. For any other u∈Bj,
f(u)≈f(x)+(u−x)f/prime(x)
and so
pj=integraldisplay
Bjf(u)du≈integraldisplay
Bjparenleftbigg
f(x)+(u−x)f/prime(x)parenrightbigg
du
=f(x)h+hf/prime(x)parenleftbigg
hparenleftbigg
j−1
2parenrightbigg
−xparenrightbigg
.
Therefore, the bias b(x)i s
b(x)= E(hatwidefn(x))−f(x)=pj
h−f(x)
≈f(x)h+hf/prime(x)parenleftbig
hparenleftbig
j−1
2parenrightbig
−xparenrightbig
h−f(x)
=f/prime(x)parenleftbigg
hparenleftbigg
j−1
2parenrightbigg
−xparenrightbigg
.
Iftildewidexjis the center of the bin, then
integraldisplay
Bjb2(x)dx≈integraldisplay
Bj(f/prime(x))2parenleftbigg
hparenleftbigg
j−1
2parenrightbigg
−xparenrightbigg2
dx
≈(f/prime(tildewidexj))2integraldisplay
Bjparenleftbigg
hparenleftbigg
j−1
2parenrightbigg
−xparenrightbigg2
dx
=(f/prime(tildewidexj))2h3
12.
Therefore,
integraldisplay1
0b2(x)dx=msummationdisplay
j=1integraldisplay
Bjb2(x)dx≈msummationdisplay
j=1(f/prime(tildewidexj))2h3
12
=h2
12msummationdisplay
j=1h(f/prime(tildewidexj))2≈h2
12integraldisplay1
0(f/prime(x))2dx.
Note that this increases as a function of h. Now consider the variance. For h
small, 1 −pj≈1, so
v(x)≈pj
nh2
=f(x)h+hf/prime(x)parenleftbig
hparenleftbig
j−1
2parenrightbig
−xparenrightbig
nh2
≈f(x)
nh

<<<PAGE 325>>>

20.2 Histograms 309
where we have kept only the dominant term. So,
integraldisplay1
0v(x)dx≈1
nh.
Note that this decreases with h. Putting all this together, we get:
20.4 Theorem. Suppose thatintegraltext
(f/prime(u))2du <∞. Then
R(hatwidefn,f)≈h2
12integraldisplay
(f/prime(u))2du+1
nh. (20.10)
The value h∗that minimizes (20.10) is
h∗=1
n1/3parenleftbigg6integraltext
(f/prime(u))2duparenrightbigg1/3
. (20.11)
With this choice of binwidth,
R(hatwidefn,f)≈C
n2/3(20.12)
where C=( 3/4)2/3parenleftbiggintegraltext
(f/prime(u))2duparenrightbigg1/3
.
Theorem 20.4 is quite revealing. We see that with an optimally chosen bin-
width, the MISE decreases to 0 at rate n−2/3. By comparison, most parametric
estimators converge at rate n−1. The slower rate of convergence is the price
we pay for being nonparametric. The formula for the optimal binwidth h∗is
of theoretical interest but it is not useful in practice since it depends on theunknown function f.
A practical way to choose the binwidth is to estimate the risk function
and minimize over h. Recall that the loss function, which we now write as a
function of h,i s
L(h)=integraldisplay
(hatwidef
n(x)−f(x))2dx
=integraldisplay
hatwidef2
n(x)dx−2integraldisplay
hatwidefn(x)f(x)dx+integraldisplay
f2(x)dx.
The last term does not depend on the binwidth hso minimizing the risk is
equivalent to minimizing the expected value of
J(h)=integraldisplay
hatwidef2
n(x)dx−2integraldisplay
hatwidefn(x)f(x)dx.

<<<PAGE 326>>>

310 20. Nonparametric Curve Estimation
We shall refer to E(J(h)) as the risk, although it diﬀers from the true risk by
the constant termintegraltext
f2(x)dx.
20.5 Deﬁnition. Thecross-validation estimator of risk is
hatwideJ(h)=integraldisplayparenleftbigg
hatwidefn(x)parenrightbigg2
dx−2
nnsummationdisplay
i=1hatwidef(−i)(Xi) (20.13)
wherehatwidef(−i)is the histogram estimator obtained after removing the ith
observation. We refer to hatwideJ(h)as the cross-validation score or estimated
risk.
20.6 Theorem. The cross-validation estimator is nearly unbiased:
E(hatwideJ(x))≈E(J(x)).
In principle, we need to recompute the histogram ntimes to compute hatwideJ(h).
Moreover, this has to be done for all values of h. Fortunately, there is a
shortcut formula.
20.7 Theorem. The following identity holds:
hatwideJ(h)=2
(n−1)h−n+1
(n−1)msummationdisplay
j=1hatwidep2
j. (20.14)
20.8 Example. We used cross-validation in the astronomy example. The cross-
validation function is quite ﬂat near its minimum. Any min the range of 73 to
310 is an approximate minimizer but the resulting histogram does not changemuch over this range. The histogram in the top right plot in Figure 20.3 wasconstructed using m= 73 bins. The bottom right plot shows the estimated
risk, or more precisely, hatwideA, plotted versus the number of bins.
/squaresolid
Next we want a conﬁdence set for f. Suppose hatwidefnis a histogram with mbins
and binwidth h=1/m. We cannot realistically make conﬁdence statements
about the ﬁne details of the true density f. Instead, we shall make conﬁdence
statements about fat the resolution of the histogram. To this end, deﬁne
fn(x)=E(hatwidefn(x)) =pj
hforx∈Bj (20.15)
where pj=integraltext
Bjf(u)du. Think of f(x) as a “histogramized” version of f.

<<<PAGE 327>>>

20.2 Histograms 311
20.9 Deﬁnition. A pair of functions (/lscriptn(x),un(x))is a1−αconﬁdence
band (or conﬁdence envelope) if
Pparenleftbigg
/lscript(x)≤fn(x)≤u(x) for all xparenrightbigg
≥1−α. (20.16)
20.10 Theorem. Letm=m(n)be the number of bins in the histogram hatwidefn.
Assume that m(n)→∞andm(n) logn/n→0asn→∞. Deﬁne
/lscriptn(x)=parenleftbigg
maxbraceleftbiggradicalBig
hatwidefn(x)−c,0bracerightbiggparenrightbigg2
un(x)=parenleftbiggradicalBig
hatwidefn(x)+cparenrightbigg2
(20.17)
where
c=zα/(2m)
2radicalbiggm
n. (20.18)
Then, (/lscriptn(x),un(x))is an approximate 1−αconﬁdence band.
Proof. Here is an outline of the proof. From the central limit theorem, hatwidepj≈
N(pj,pj(1−pj)/n). By the delta method,radicalbig
hatwidepj≈N(√pj,1/(4n)). Moreover,
it can be shown that theradicalbig
hatwidepj’s are approximately independent. Therefore,
2√nparenleftbiggradicalbig
hatwidepj−√pjparenrightbigg
≈Zj (20.19)
where Z1,...,Z m∼N(0,1). Let
A=braceleftbigg
/lscriptn(x)≤fn(x)≤un(x) for all xbracerightbigg
=braceleftbigg
max
xvextendsinglevextendsinglevextendsinglevextendsingleradicalBig
hatwidefn(x)−radicalBig
f(x)vextendsinglevextendsinglevextendsinglevextendsingle≤cbracerightbigg
.
Then,
P(A
c)= Pparenleftbigg
max
xvextendsinglevextendsinglevextendsinglevextendsingleradicalBig
hatwidefn(x)−radicalBig
f(x)vextendsinglevextendsinglevextendsinglevextendsingle>cparenrightbigg
=PparenleftBigg
max
jvextendsinglevextendsinglevextendsinglevextendsinglevextendsingleradicalbigg
hatwidepj
h−radicalbiggpj
hvextendsinglevextendsinglevextendsinglevextendsinglevextendsingle>cparenrightBigg
=Pparenleftbigg
max
j2√nvextendsinglevextendsinglevextendsingleradicalbig
hatwidepj−√pjvextendsinglevextendsinglevextendsingle>z
α/(2m)parenrightbigg
≈Pparenleftbigg
max
j|Zj|>zα/(2m)parenrightbigg
≤msummationdisplay
j=1Pparenleftbig
|Zj|>zα/(2m)parenrightbig
=msummationdisplay
j=1α
m=α./squaresolid

<<<PAGE 328>>>

312 20. Nonparametric Curve Estimation
0.00 0.05 0.10 0.15 0.200 1 02 03 04 05 0
FIGURE 20.4. 95 percent conﬁdence envelope for astronomy data using m=7 3
bins.
20.11 Example. Figure 20.4 shows a 95 percent conﬁdence envelope for the
astronomy data. We see that even with over 1,000 data points, there is stillsubstantial uncertainty.
/squaresolid
20.3 Kernel Density Estimation
Histograms are discontinuous. Kernel density estimators are smoother and
they converge faster to the true density than histograms.
LetX1,...,X ndenote the observed data, a sample from f. In this chap-
ter, akernel is deﬁned to be any smooth function Ksuch that K(x)≥0,integraltext
K(x)dx=1 ,integraltext
xK(x)dx= 0 and σ2
K≡integraltext
x2K(x)dx >0. Two examples of
kernels are the Epanechnikov kernel
K(x)=braceleftbigg3
4(1−x2/5)/√
5|x|<√
5
0 otherwise(20.20)
and the Gaussian (Normal) kernel K(x)=( 2 π)−1/2e−x2/2.

<<<PAGE 329>>>

20.3 Kernel Density Estimation 313
−10 −5 0 5 10
FIGURE 20.5. A kernel density estimator /hatwidef. At each point x,/hatwidef(x) is the average
of the kernels centered over the data points Xi. The data points are indicated by
short vertical bars.
20.12 Deﬁnition. Given a kernel Kand a positive number h, called the
bandwidth , thekernel density estimator is deﬁned to be
hatwidef(x)=1
nnsummationdisplay
i=11
hKparenleftbiggx−Xi
hparenrightbigg
. (20.21)
An example of a kernel density estimator is show in Figure 20.5. The kernel
estimator eﬀectively puts a smoothed-out lump of mass of size 1 /nover each
data point Xi. The bandwidth hcontrols the amount of smoothing. When h
is close to 0, hatwidefnconsists of a set of spikes, one at each data point. The height
of the spikes tends to inﬁnity as h→0. When h→∞,hatwidefntends to a uniform
density.

<<<PAGE 330>>>

314 20. Nonparametric Curve Estimation
20.13 Example. Figure 20.6 shows kernel density estimators for the astron-
omy data using three diﬀerent bandwidths. In each case we used a Gaussiankernel. The properly smoothed kernel density estimator in the top right panelshows similar structure as the histogram. However, it is easier to see the clus-ters with the kernel estimator.
/squaresolid
To construct a kernel density estimator, we need to choose a kernel Kand
a bandwidth h. It can be shown theoretically and empirically that the choice
ofKis not crucial.2However, the choice of bandwidth his very important.
As with the histogram, we can make a theoretical statement about how therisk of the estimator depends on the bandwidth.
20.14 Theorem. Under weak assumptions on fandK,
R(f,hatwidef
n)≈1
4σ4
Kh4integraldisplay
(f/prime/prime(x))2+integraltext
K2(x)dx
nh(20.22)
where σ2
K=integraltext
x2K(x)dx. The optimal bandwidth is
h∗=c−2/5
1c1/5
2c−1/5
3
n1/5(20.23)
where c1=integraltext
x2K(x)dx,c2=integraltext
K(x)2dxandc3=integraltext
(f/prime/prime(x))2dx. With this
choice of bandwidth,
R(f,hatwidefn)≈c4
n4/5
for some constant c4>0.
Proof. Write Kh(x, X)=h−1K((x−X)/h) andhatwidefn(x)=n−1summationtext
iKh(x, X i).
Thus, E[hatwidefn(x)] =E[Kh(x, X)] and V[hatwidefn(x)] =n−1V[Kh(x, X)]. Now,
E[Kh(x, X)] =integraldisplay1
hKparenleftbiggx−t
hparenrightbigg
f(t)dt
=integraldisplay
K(u)f(x−hu)du
=integraldisplay
K(u)bracketleftbigg
f(x)−hf/prime(x)+1
2f/prime/prime(x)+···bracketrightbigg
du
=f(x)+1
2h2f/prime/prime(x)integraldisplay
u2K(u)du···
sinceintegraltext
K(x)dx= 1 andintegraltext
xK(x)dx= 0. The bias is
E[Kh(x, X)]−f(x)≈1
2σ2
kh2f/prime/prime(x).
2It can be shown that the Epanechnikov kernel is optimal in the sense of giving smallest
asymptotic mean squared error, but it is really the choice of bandwidth which is crucial.

<<<PAGE 331>>>

20.3 Kernel Density Estimation 315
0.0 0.1 0.2 0.0 0.1 0.2
0.0 0.1 0.2 0.002 0.004 0.006
FIGURE 20.6. Kernel density estimators and estimated risk for the astronomy data.
Top left: oversmoothed. Top right: just right (bandwidth chosen by cross-validation).Bottom left: undersmoothed. Bottom right: cross-validation curve as a function ofbandwidth h. The bandwidth was chosen to be the value of hwhere the curve is a
minimum.

<<<PAGE 332>>>

316 20. Nonparametric Curve Estimation
By a similar calculation,
V[hatwidefn(x)]≈f(x)integraltext
K2(x)dx
nhn.
The result follows from integrating the squared bias plus the variance. /squaresolid
We see that kernel estimators converge at rate n−4/5while histograms con-
verge at the slower rate n−2/3. It can be shown that, under weak assumptions,
there does not exist a nonparametric estimator that converges faster thann
−4/5.
The expression for h∗depends on the unknown density fwhich makes
the result of little practical use. As with the histograms, we shall use cross-validation to ﬁnd a bandwidth. Thus, we estimate the risk (up to a constant)by
hatwideJ(h)=integraldisplay
hatwidef
2(x)dz−2
nnsummationdisplay
i=1hatwidef−i(Xi) (20.24)
wherehatwidef−iis the kernel density estimator after omitting the ithobservation.
20.15 Theorem. For any h>0,
EbracketleftBig
hatwideJ(h)bracketrightBig
=E[J(h)].
Also,
hatwideJ(h)≈1
hn2summationdisplay
isummationdisplay
jK∗parenleftbiggXi−Xj
hparenrightbigg
+2
nhK(0) (20.25)
where K∗(x)=K(2)(x)−2K(x)andK(2)(z)=integraltext
K(z−y)K(y)dy.I np a r -
ticular, if Kis a N(0,1) Gaussian kernel then K(2)(z)is the N(0,2)density.
We then choose the bandwidth hnthat minimizes hatwideJ(h).3A justiﬁcation for
this method is given by the following remarkable theorem due to Stone.
20.16 Theorem (Stone’s Theorem) .Suppose that fis bounded. Let hatwidefhdenote
the kernel estimator with bandwidth hand let hndenote the bandwidth chosen
by cross-validation. Then,
integraltextparenleftBig
f(x)−hatwidefhn(x)parenrightBig2
dx
infhintegraltextparenleftBig
f(x)−hatwidefh(x)parenrightBig2
dxP−→1. (20.26)
3For large data sets, /hatwidefand (20.25) can be computed quickly using the fast Fourier transform.

<<<PAGE 333>>>

20.3 Kernel Density Estimation 317
20.17 Example. The top right panel of Figure 20.6 is based on cross-validation.
These data are rounded which problems for cross-validation. Speciﬁcally, itcauses the minimizer to be h= 0. To overcome this problem, we added a
small amount of random Normal noise to the data. The result is that hatwideJ(h)i s
very smooth with a well deﬁned minimum.
/squaresolid
20.18 Remark. Do not assume that, if the estimator hatwidefis wiggly, then cross-
validation has let you down. The eye is not a good judge of risk.
To construct conﬁdence bands, we use something similar to histograms.
Again, the conﬁdence band is for the smoothed version,
fn=E(hatwidefn(x)) =integraldisplay1
hKparenleftbiggx−u
hparenrightbigg
f(u)du,
of the true density f.4Assume the density is on an interval ( a, b). The band
is
/lscriptn(x)=hatwidefn(x)−qse(x),u n(x)=hatwidefn(x)+qse(x) (20.27)
where
se(x)=s(x)√n,
s2(x)=1
n−1nsummationdisplay
i=1(Yi(x)−Yn(x))2,
Yi(x)=1
hKparenleftbiggx−Xi
hparenrightbigg
,
q=Φ−1parenleftbigg1+( 1 −α)1/m
2parenrightbigg
,
m=b−a
ω
where ωis the width of the kernel. In case the kernel does not have ﬁnite
width then we take ωto be the eﬀective width, that is, the range over which
the kernel is non-negligible. In particular, we take ω=3hfor the Normal
kernel.
20.19 Example. Figure 20.7 shows approximate 95 percent conﬁdence bands
for the astronomy data. /squaresolid
4This is a modiﬁed version of the band described in Chaudhuri and Marron (1999).

<<<PAGE 334>>>

318 20. Nonparametric Curve Estimation
0.00 0.05 0.10 0.15 0.200 1 02 03 04 0
FIGURE 20.7. 95 percent conﬁdence bands for kernel density estimate for the as-
tronomy data.
Suppose now that the data Xi=(Xi1,...,X id) ared-dimensional. The ker-
nel estimator can easily be generalized to ddimensions. Let h=(h1,...,h d)
be a vector of bandwidths and deﬁne
hatwidefn(x)=1
nnsummationdisplay
i=1Kh(x−Xi) (20.28)
where
Kh(x−Xi)=1
nh1···hd

dproductdisplay
j=1Kparenleftbiggxi−Xij
hjparenrightbigg

(20.29)
where h1,...,h dare bandwidths. For simplicity, we might take hj=sjhwhere
sjis the standard deviation of the jthvariable. There is now only a single
bandwidth hto choose. Using calculations like those in the one-dimensional
case, the risk is given by
R(f,hatwidefn)≈1
4σ4
K
dsummationdisplay
j=1h4
jintegraldisplay
f2
jj(x)dx+summationdisplay
j/negationslash=kh2
jh2kintegraldisplay
fjjfkkdx

+parenleftbigintegraltext
K2(x)dxparenrightbigd
nh1···hd
where fjjis the second partial derivative of f. The optimal bandwidth satisﬁes
hi≈c1n−1/(4+d), leading to a risk of order n−4/(4+d). From this fact, we see

<<<PAGE 335>>>

20.4 Nonparametric Regression 319
that the risk increases quickly with dimension, a problem usually called the
curse of dimensionality . To get a sense of how serious this problem is,
consider the following table from Silverman (1986) which shows the samplesize required to ensure a relative mean squared error less than 0.1 at 0 whenthe density is multivariate normal and the optimal bandwidth is selected:
Dimension Sample Size
14
21 936 74 2235 7686 27907 10,7008 43,7009 187,000
10 842,000
This is bad news indeed. It says that having 842,000 observations in a ten-
dimensional problem is really like having 4 observations in a one-dimensionalproblem.
20.4 Nonparametric Regression
Consider pairs of points ( x1,Y1),...,(xn,Yn) related by
Yi=r(xi)+/epsilon1i (20.30)
where E(/epsilon1i) = 0. We have written the xi’s in lower case since we will treat
them as ﬁxed. We can do this since, in regression, it is only the mean of Y
conditional on xthat we are interested in. We want to estimate the regression
function r(x)=E(Y|X=x).
There are many nonparametric regression estimators. Most involve esti-
mating r(x) by taking some sort of weighted average of the Yi’s, giving higher
weight to those points near x. A popular version is the Nadaraya-Watson
kernel estimator.
20.20 Deﬁnition. TheNadaraya-Watson kernel estimator is deﬁned
by
hatwider(x)=nsummationdisplay
i=1wi(x)Yi (20.31)

<<<PAGE 336>>>

320 20. Nonparametric Curve Estimation
where Kis a kernel and the weights wi(x)are given by
wi(x)=Kparenleftbigx−xi
hparenrightbig
summationtextn
j=1KparenleftBig
x−xj
hparenrightBig. (20.32)
The form of this estimator comes from ﬁrst estimating the joint density
f(x, y) using kernel density estimation and then inserting the estimate into
the formula,
r(x)=E(Y|X=x)=integraldisplay
yf(y|x)dy=integraltext
yf(x, y)dyintegraltext
f(x, y)dy.
20.21 Theorem. Suppose that V(/epsilon1i)=σ2. The risk of the Nadaraya-Watson
kernel estimator is
R(hatwidern,r)≈h4
4parenleftbiggintegraldisplay
x2K2(x)dxparenrightbigg4integraldisplayparenleftbigg
r/prime/prime(x)+2r/prime(x)f/prime(x)
f(x)parenrightbigg2
dx
+integraldisplayσ2integraltext
K2(x)dx
nhf(x)dx. (20.33)
The optimal bandwidth decreases at rate n−1/5and with this choice the risk
decreases at rate n−4/5.
In practice, to choose the bandwidth hwe minimize the cross validation
score
hatwideJ(h)=nsummationdisplay
i=1(Yi−hatwider−i(xi))2(20.34)
wherehatwider−iis the estimator we get by omitting the ithvariable. Fortunately,
there is a shortcut formula for computing hatwideJ.
20.22 Theorem. hatwideJcan be written as
hatwideJ(h)=nsummationdisplay
i=1(Yi−hatwider(xi))2 1
parenleftBigg
1−K(0)
/summationtextn
j=1K/parenleftBigxi−xj
h/parenrightBigparenrightBigg2. (20.35)
20.23 Example. Figures 20.8 shows cosmic micr owave bac kground (CMB)
data from BOOMERaNG (Netterﬁeld et al. (2002)), Maxima (Lee et al.(2001)), and DASI (Halverson et al. (2002))). The data consist of npairs
(x
1,Y1),...,(xn,Yn) where xiis called the multipole moment and Yiis the

<<<PAGE 337>>>

20.4 Nonparametric Regression 321
estimated power spectrum of the temperature ﬂuctuations. What you are see-
ing are sound waves in the cosmic micr owave bac kground radiation which is
the heat, left over from the big bang. If r(x) denotes the true power spectrum,
then
Yi=r(xi)+/epsilon1i
where /epsilon1iis a random error with mean 0. The location and size of peaks in
r(x) provides valuable clues about the behavior of the early universe. Figure
20.8 shows the ﬁt based on cross-validation as well as an undersmoothed andoversmoothed ﬁt. The cross-validation ﬁt shows the presence of three well-deﬁned peaks, as predicted by the physics of the big bang.
/squaresolid
The procedure for ﬁnding conﬁdence bands is similar to that for density
estimation. However, we ﬁrst need to estimate σ2. Suppose that the xi’s are
ordered. Assuming r(x) is smooth, we have r(xi+1)−r(xi)≈0 and hence
Yi+1−Yi=bracketleftbigg
r(xi+1)+/epsilon1i+1bracketrightbigg
−bracketleftbigg
r(xi)+/epsilon1ibracketrightbigg
≈/epsilon1i+1−/epsilon1i
and hence
V(Yi+1−Yi)≈V(/epsilon1i+1−/epsilon1i)=V(/epsilon1i+1)+V(/epsilon1i)=2σ2.
We can thus use the average of the n−1 diﬀerences Yi+1−Yito estimate σ2.
Hence, deﬁne
hatwideσ2=1
2(n−1)n−1summationdisplay
i=1(Yi+1−Yi)2. (20.36)
As with density estimate, the conﬁdence band is for the smoothed version
rn(x)=E(hatwidern(x)) of the true regression function r.

<<<PAGE 338>>>

322 20. Nonparametric Curve Estimation
200 400 600 800 10000 1000 2000 3000 4000 5000 6000
Undersmoothed200 400 600 800 10000 1000 2000 3000 4000 5000 6000
Oversmoothed
200 400 600 800 10000 1000 2000 3000 4000 5000 6000
Just Right (Using cross−valdiation)20 40 60 80 100 1203e+05 4e+05 5e+05 6e+05 7e+05 8e+05
bandwidthestimated risk
FIGURE 20.8. Regression analysis of the CMB data. The ﬁrst ﬁt is undersmoothed,
the second is oversmoothed, and the third is based on cross-validation. The last
panel shows the estimated risk versus the bandwidth of the smoother. The data arefrom BOOMERaNG, Maxima, and DASI.

<<<PAGE 339>>>

20.4 Nonparametric Regression 323
Conﬁdence Bands for Kernel Regression
An approximate 1 −αconﬁdence band for rn(x)i s
/lscriptn(x)=hatwidern(x)−qhatwidese(x),u n(x)=hatwidern(x)+qhatwidese(x) (20.37)
where
hatwidese(x)=hatwideσradicaltpradicalvertexradicalvertexradicalbtnsummationdisplay
i=1w2
i(x),
q=Φ−1parenleftbigg1+( 1 −α)1/m
2parenrightbigg
,
m=b−a
ω,
hatwideσis deﬁned in (20.36) and ωis the width of the kernel. In case the kernel
does not have ﬁnite width then we take ωto be the eﬀective width, that
is, the range over which the kernel is non-negligible. In particular, we takeω=3hfor the Normal kernel.
20.24 Example. Figure 20.9 shows a 95 percent conﬁdence envelope for the
CMB data. We see that we are highly conﬁdent of the existence and positionof the ﬁrst peak. We are more uncertain about the second and third peak.At the time of this writing, more accurate data are becoming available thatapparently provide sharper estimates of the second and third peak.
/squaresolid
The extension to multiple regressors X=(X1,...,X p) is straightforward.
As with kernel density estimation we just replace the kernel with a multivari-ate kernel. However, the same caveats about the curse of dimensionality apply.In some cases, we might consider putting some restrictions on the regressionfunction which will then reduce the curse of dimensionality. For example,additive regression is based on the model
Y=
psummationdisplay
j=1rj(Xj)+/epsilon1. (20.38)
Now we only need to ﬁt pone-dimensional functions. The model can be en-
riched by adding various interactions, for example,
Y=psummationdisplay
j=1rj(Xj)+summationdisplay
j<krjk(XjXk)+/epsilon1. (20.39)
Additive models are usually ﬁt by an algorithm called backﬁtting .

<<<PAGE 340>>>

324 20. Nonparametric Curve Estimation
200 400 600 800 1000−1000 0 1000 2000 3000 4000 5000 6000
FIGURE 20.9. 95 percent conﬁdence envelope for the CMB data.
Backﬁtting
1. Initialize r1(x1) , ..., rp(xp).
2. For j=1,...,p :
(a) Let /epsilon1i=Yi−summationtext
s/negationslash=jrs(xi).
(b) Let rjbe the function estimate obtained by regressing the /epsilon1i’s
on the jthcovariate.
3. If converged STOP. Else, go back to step 2.
Additive models have the advantage that they avoid the curse of dimension-
ality and they can be ﬁt quickly, but they have one disadvantage: the modelis not fully nonparametric. In other words, the true regression function r(x)
may not be of the form (20.38).
20.5 Appendix
Confidence Sets and Bias . The conﬁdence bands we computed are not
for the density function or regression function but rather for the smoothed

<<<PAGE 341>>>

20.6 Bibliographic Remarks 325
function. For example, the conﬁdence band for a kernel density estimate with
bandwidth his a band for the function one gets by smoothing the true function
with a kernel with the same bandwidth. Getting a conﬁdence set for the truefunction is complicated for reasons we now explain.
Lethatwidef
n(x) denote an estimate of the function f(x). Denote the mean and
standard deviation of hatwidefn(x)b yfn(x) and sn(x). Then,
hatwidefn(x)−f(x)
sn(x)=hatwidefn(x)−fn(x)
sn(x)+fn(x)−f(x)
sn(x).
Typically, the ﬁrst term converges to a standard Normal from which one de-
rives conﬁdence bands. The second term is the bias divided by the standarddeviation. In parametric inference, the bias is usually smaller than the stan-dard deviation of the estimator so this term goes to 0 as the sample sizeincreases. In nonparametric inference, optimal smoothing leads us to balancethe bias and the standard deviation. Thus the second term does not vanisheven with large sample sizes. This means that the conﬁdence interval will notbe centered around the true function f.
20.6 Bibliographic Remarks
Two very good books on density estimation are Scott (1992) and Silverman
(1986). The literature on nonparametric regression is very large. Two goodstarting points are Hardle (1990) and Loader (1999). The latter emphasizes aclass of techniques called local likelihood methods.
20.7 Exercises
1. Let X1,...,X n∼fand lethatwidefnbe the kernel density estimator using the
boxcar kernel:
K(x)=braceleftbigg
1−1
2<x<1
2
0 otherwise .
(a) Show that
E(hatwidef(x)) =1
hintegraldisplayx+(h/2)
x−(h/2)f(y)dy
and
V(hatwidef(x)) =1
nh2
integraldisplayx+(h/2)
x−(h/2)f(y)dy−parenleftBiggintegraldisplayx+(h/2)
x−(h/2)f(y)dyparenrightBigg2
.

<<<PAGE 342>>>

326 20. Nonparametric Curve Estimation
(b) Show that if h→0 and nh→∞ asn→∞, thenhatwidefn(x)P−→f(x).
2. Get the data on fragments of glass collected in forensic work from the
book website. Estimate the density of the ﬁrst variable (refractive in-dex) using a histogram and use a kernel density estimator. Use cross-validation to choose the amount of smoothing. Experiment with diﬀerentbinwidths and bandwidths. Comment on the similarities and diﬀerences.Construct 95 percent conﬁdence bands for your estimators.
3. Consider the data from question 2. Let Ybe refractive index and let
xbe aluminum content (the fourth variable). Perform a nonparametric
regression to ﬁt the model Y=f(x)+/epsilon1. Use cross-validation to estimate
the bandwidth. Construct 95 percent conﬁdence bands for your estimate.
4. Prove Lemma 20.1.5. Prove Theorem 20.3.6. Prove Theorem 20.7.7. Prove Theorem 20.15.8. Consider regression data ( x
1,Y1),...,(xn,Yn). Suppose that 0 ≤xi≤1
for all i. Deﬁne bins Bjas in equation (20.7). For x∈Bjdeﬁne
hatwidern(x)=Yj
where Yjis the mean of all the Yi’s corresponding to those xi’s inBj.
Find the approximate risk of this estimator. From this expression forthe risk, ﬁnd the optimal bandwidth. At what rate does the risk go tozero?
9. Show that with suitable smoothness assumptions on r(x),hatwideσ
2in equation
(20.36) is a consistent estimator of σ2.
10. Prove Theorem 20.22.

<<<PAGE 343>>>

21
Smoothing Using Orthogonal Functions
In this chapter we will study an approach to nonparametric curve estima-
tion based on orthogonal functions . We begin with a brief introduction to
the theory of orthogonal functions, then we turn to density estimation andregression.
21.1 Orthogonal Functions and L2Spaces
Letv=(v1,v2,v3) denote a three-dimensional vector, that is, a list of three
real numbers. Let Vdenote the set of all such vectors. If ais a scalar (a
number) and vis a vector, we deﬁne av=(av1,a v2,a v3). The sum of vectors
vandwis deﬁned by v+w=(v1+w1,v2+w2,v3+w3). The inner product
between two vectors vandwis deﬁned by /angbracketleftv,w/angbracketright=summationtext3
i=1viwi. Thenorm
(or length) of a vector vis deﬁned by
||v||=radicalbig
/angbracketleftv,v/angbracketright=radicaltpradicalvertexradicalvertexradicalbt3summationdisplay
i=1v2
i. (21.1)
Two vectors are orthogonal (or perpendicular) if/angbracketleftv,w/angbracketright= 0. A set of
vectors are orthogonal if each pair in the set is orthogonal. A vector is normal
if||v||=1 .

<<<PAGE 344>>>

328 21. Smoothing Using Orthogonal Functions
Letφ1=( 1,0,0),φ2=( 0,1,0),φ3=( 0,0,1). These vectors are said to be
anorthonormal basis forVsince they have the following properties:
(i) they are orthogonal;(ii) they are normal;(iii) they form a basis for V, which means that any v∈Vcan be written as a
linear combination of φ
1,φ2,φ3:
v=3summationdisplay
j=1βjφjwhere βj=/angbracketleftφj,v/angbracketright. (21.2)
For example, if v= (12 ,3,4) then v=1 2φ1+3φ2+4φ3. There are other
orthonormal bases for V, for example,
ψ1=parenleftbigg1√
3,1√
3,1√
3parenrightbigg
,ψ2=parenleftbigg1√
2,−1√
2,0parenrightbigg
,ψ3=parenleftbigg1√
6,1√
6,−2√
6parenrightbigg
.
You can check that these three vectors also form an orthonormal basis for V.
Again, if vis any vector then we can write
v=3summationdisplay
j=1βjψjwhere βj=/angbracketleftψj,v/angbracketright.
For example, if v= (12,3,4) then
v=1 0.97ψ1+6.36ψ2+2.86ψ3.
Now we make the leap from vectors to functions. Basically, we just replace
vectors with functions and sums with integrals. Let L2(a, b) denote all func-
tions deﬁned on the interval [ a, b] such thatintegraltextb
af(x)2dx <∞:
L2(a, b)=braceleftBigg
f:[a, b]→R,integraldisplayb
af(x)2dx <∞bracerightBigg
. (21.3)
We sometimes write L2instead of L2(a, b). The inner product between two
functions f,g∈L2is deﬁned byintegraltext
f(x)g(x)dx. The norm of fis
||f||=radicalBiggintegraldisplay
f(x)2dx. (21.4)
Two functions are orthogonal ifintegraltext
f(x)g(x)dx= 0. A function is normal if
||f||=1 .
A sequence of functions φ1,φ2,φ3,...isorthonormal ifintegraltext
φ2
j(x)dx= 1 for
eachjandintegraltext
φi(x)φj(x)dx= 0 for i/negationslash=j. An orthonormal sequence is com-
plete if the only function that is orthogonal to each φjis the zero function.

<<<PAGE 345>>>

21.1 Orthogonal Functions and L2Spaces 329
In this case, the functions φ1,φ2,φ3,...form in basis, meaning that if f∈L2
thenfcan be written as1
f(x)=∞summationdisplay
j=1βjφj(x),where βj=integraldisplayb
af(x)φj(x)dx. (21.5)
A useful result is Parseval’s relation which says that
||f||2≡integraldisplay
f2(x)dx=∞summationdisplay
j=1β2
j≡| |β||2(21.6)
where β=(β1,β2,...).
21.1 Example. An example of an orthonormal basis for L2(0,1) is the cosine
basis deﬁned as follows. Let φ0(x) = 1 and for j≥1 deﬁne
φj(x)=√
2 cos(jπx). (21.7)
The ﬁrst six functions are plotted in Figure 21.1. /squaresolid
21.2 Example. Let
f(x)=radicalbig
x(1−x) sinparenleftbigg2.1π
(x+.05)parenrightbigg
which is called the “doppler function.” Figure 21.2 shows f(top left) and its
approximation
fJ(x)=Jsummationdisplay
j=1βjφj(x)
withJequal to 5 (top right), 20 (bottom left), and 200 (bottom right).
AsJincreases we see that fJ(x) gets closer to f(x). The coeﬃcients βj=integraltext1
0f(x)φj(x)dxwere computed numerically. /squaresolid
21.3 Example. TheLegendre polynomials on [−1,1] are deﬁned by
Pj(x)=1
2jj!dj
dxj(x2−1)j,j=0,1,2,... (21.8)
It can be shown that these functions are complete and orthogonal and that
integraldisplay1
−1P2
j(x)dx=2
2j+1. (21.9)
1The equality in the displayed equation means that/integraltext
(f(x)−fn(x))2dx→0where fn(x)=/summationtextn
j=1βjφj(x).

<<<PAGE 346>>>

330 21. Smoothing Using Orthogonal Functions
FIGURE 21.1. The ﬁrst six functions in the cosine basis.
FIGURE 21.2. Approximating the doppler function with its expansion
in the cosine basis. The function f(top left) and its approximation
fJ(x)=/summationtextJ
j=1βjφj(x) with Jequal to 5 (top right), 20 (bottom left),
and 200 (bottom right). The coeﬃcients βj=/integraltext1
0f(x)φj(x)dxwere
computed numerically.

<<<PAGE 347>>>

21.2 Density Estimation 331
It follows that the functions φj(x)=radicalbig
(2j+1 )/2Pj(x),j=0,1,...form an
orthonormal basis for L2(−1,1). The ﬁrst few Legendre polynomials are:
P0(x)=1 ,
P1(x)= x,
P2(x)=1
2parenleftbigg
3x2−1parenrightbigg
,and
P3(x)=1
2parenleftbigg
5x3−3xparenrightbigg
.
These polynomials may be constructed explicitly using the following recursive
relation:
Pj+1(x)=(2j+1 )xPj(x)−jPj−1(x)
j+1./squaresolid (21.10)
The coeﬃcients β1,β2,...are related to the smoothness of the function f.
To see why, note that if fis smooth, then its derivatives will be ﬁnite. Thus we
expect that, for some k,integraltext1
0(f(k)(x))2dx <∞where f(k)is the kthderivative
off. Now consider the cosine basis (21.7) and let f(x)=summationtext∞
j=0βjφj(x). Then,
integraldisplay1
0(f(k)(x))2dx=2∞summationdisplay
j=1β2
j(πj)2k.
The only way thatsummationtext∞
j=1β2
j(πj)2kcan be ﬁnite is if the βj’s get small when
jgets large. To summarize:
If the function fis smooth, then the coeﬃcients βjwill be small
whenjis large.
For the rest of this chapter, assume we are using the cosine basis unless
otherwise speciﬁed.
21.2 Density Estimation
LetX1,...,X nbeiidobservations from a distribution on [0 ,1] with density
f. Assuming f∈L2we can write
f(x)=∞summationdisplay
j=0βjφj(x)
where φ1,φ2,...is an orthonormal basis. Deﬁne
hatwideβj=1
nnsummationdisplay
i=1φj(Xi). (21.11)

<<<PAGE 348>>>

332 21. Smoothing Using Orthogonal Functions
21.4 Theorem. The mean and variance of hatwideβjare
EparenleftBig
hatwideβjparenrightBig
=βj,VparenleftBig
hatwideβjparenrightBig
=σ2
j
n(21.12)
where
σ2
j=V(φj(Xi)) =integraldisplay
(φj(x)−βj)2f(x)dx. (21.13)
Proof. The mean is
EparenleftBig
hatwideβjparenrightBig
=1
nnsummationdisplay
i=1E(φj(Xi))
=E(φj(X1))
=integraldisplay
φj(x)f(x)dx=βj.
The calculation for the variance is similar. /squaresolid
Hence,hatwideβjis an unbiased estimate of βj. It is tempting to estimate fbysummationtext∞
j=1hatwideβjφj(x) but this turns out to have a very high variance. Instead, consider
the estimator
hatwidef(x)=Jsummationdisplay
j=1hatwideβjφj(x). (21.14)
The number of terms Jis a smoothing parameter. Increasing Jwill decrease
bias while increasing variance. For technical reasons, we restrict Jto lie in
the range
1≤J≤p
where p=p(n)=√n. To emphasize the dependence of the risk function on
J, we write the risk function as R(J).
21.5 Theorem. The risk of hatwidefis
R(J)=Jsummationdisplay
j=1σ2
j
n+∞summationdisplay
j=J+1β2
j. (21.15)
An estimate of the risk is
hatwideR(J)=Jsummationdisplay
j=1hatwideσ2
j
n+psummationdisplay
j=J+1parenleftBigg
hatwideβ2
j−hatwideσ2
j
nparenrightBigg
+(21.16)
where a+= max {a,0}and
hatwideσ2
j=1
n−1nsummationdisplay
i=1parenleftBig
φj(Xi)−hatwideβjparenrightBig2
. (21.17)

<<<PAGE 349>>>

21.2 Density Estimation 333
To motivate this estimator, note that hatwideσ2
jis an unbiased estimate of σ2
jand
hatwideβ2
j−hatwideσ2
jis an unbiased estimator of β2
j. We take the positive part of the latter
term since we know that β2
jcannot be negative. We now choose 1 ≤hatwideJ≤pto
minimize hatwideR(hatwidef,f). Here is a summary:
Summary of Orthogonal Function Density Estimation
1. Let
hatwideβj=1
nnsummationdisplay
i=1φj(Xi).
2. Choose hatwideJto minimize hatwideR(J) over 1 ≤J≤p=√nwherehatwideRis given in
equation (21.16).
3. Let
hatwidef(x)=/hatwideJsummationdisplay
j=1hatwideβjφj(x).
The estimator hatwidefncan be negative. If we are interested in exploring the
shape of f, this is not a problem. However, if we need our estimate to be a
probability density function, we can truncate the estimate and then normalizeit. That is, we take hatwidef
∗= max {hatwidefn(x),0}/integraltext1
0max{hatwidefn(u),0}du.
Now let us construct a conﬁdence band for f. Suppose we estimate fusing
Jorthogonal functions. We are essentially estimating fJ(x)=summationtextJ
j=1βjφj(x)
not the true density f(x)=summationtext∞
j=1βjφj(x). Thus, the conﬁdence band should
be regarded as a band for fJ(x).
21.6 Theorem. An approximate 1−αconﬁdence band for fJis(/lscript(x),u(x))
where
/lscript(x)=hatwidefn(x)−c, u(x)=hatwidefn(x)+c (21.18)
where
c=K2radicalBigg
Jχ2
J,α
n(21.19)
and
K= max
1≤j≤Jmax
x|φj(x)|.
For the cosine basis, K=√
2.
Proof. Here is an outline of the proof. Let L=summationtextJ
j=1(hatwideβj−βj)2.B yt h e
central limit theorem, hatwideβj≈N(βj,σ2
j/n). Hence, hatwideβj≈βj+σj/epsilon1j/√nwhere

<<<PAGE 350>>>

334 21. Smoothing Using Orthogonal Functions
/epsilon1j∼N(0,1), and therefore
L≈1
nJsummationdisplay
j=1σ2
j/epsilon12j≤K2
nJsummationdisplay
j=1/epsilon12
jd=K2
nχ2
J. (21.20)
Thus we have, approximately, that
Pparenleftbigg
L>K2
nχ2
J,αparenrightbigg
≤PparenleftbiggK2
nχ2
J>K2
nχ2
J,αparenrightbigg
=α.
Also,
max
x|hatwidefJ(x)−fJ(x)|≤ max
xJsummationdisplay
j=1|φj(x)||hatwideβj−βj|
≤KJsummationdisplay
j=1|hatwideβj−βj|
≤√
JKradicaltpradicalvertexradicalvertexradicalbtJsummationdisplay
j=1(hatwideβj−βj)2
=√
JK√
L
where the third inequality is from the Cauchy-S chwartz inequality (Theorem
4.8). So,
P
max
x|hatwidefJ(x)−fJ(x)|>K2radicalBigg
Jχ2
J,α
n
≤P
√
JK√
L>K2radicalBigg
Jχ2
J,α
n

=P
√
L>KradicalBigg
χ2
J,α
n

=PparenleftBigg
L>K2χ2
J,α
nparenrightBigg
≤α./squaresolid
21.7 Example. Let
f(x)=5
6φ(x;0,1) +1
65summationdisplay
j=1φ(x;µj,.1)
where φ(x;µ, σ) denotes a Normal density with mean µand standard deviation
σ, and ( µ1,...,µ 5)=(−1,−1/2,0,1/2,1). Marron and Wand (1992) call this

<<<PAGE 351>>>

21.3 Regression 335
0.0 0.2 0.4 0.6 0.8 1.001234
0.0 0.2 0.4 0.6 0.8 1.001234
FIGURE 21.3. The top plot is the true density for the Bart Simpson distribution
(rescaled to have most of its mass between 0 and 1). The bottom plot is the orthog-onal function density estimate and 95 percent conﬁdence band.
“the claw” although the “Bart Simpson” might be more appropriate. Figure
21.3 shows the true density as well as the estimated density based on n=
5,000 observations and a 95 percent conﬁdence band. The density has been
rescaled to have most of its mass between 0 and 1 using the transformationy=(x+3 )/6.
/squaresolid
21.3 Regression
Consider the regression model
Yi=r(xi)+/epsilon1i,i=1,...,n (21.21)
where the /epsilon1iare independent with mean 0 and variance σ2. We will initially
focus on the special case where xi=i/n. We assume that r∈L2(0,1) and
hence we can write
r(x)=∞summationdisplay
j=1βjφj(x) where βj=integraldisplay1
0r(x)φj(x)dx (21.22)
where φ1,φ2,...where is an orthonormal basis for [0 ,1].

<<<PAGE 352>>>

336 21. Smoothing Using Orthogonal Functions
Deﬁne
hatwideβj=1
nnsummationdisplay
i=1Yiφj(xi),j=1,2,... (21.23)
Sincehatwideβjis an average, the central limit theorem tells us that hatwideβjwill be
approximately Normally distributed.
21.8 Theorem.
hatwideβj≈Nparenleftbigg
βj,σ2
nparenrightbigg
. (21.24)
Proof. The mean of hatwideβjis
E(hatwideβj)=1
nnsummationdisplay
i=1E(Yi)φj(xi)=1
nnsummationdisplay
i=1r(xi)φj(xi)
≈integraldisplay
r(x)φj(x)dx=βj
where the approximate equality follows from the deﬁnition of a Riemann in-
tegral:summationtext
i∆nh(xi)→integraltext1
0h(x)dxwhere ∆ n=1/n. The variance is
V(hatwideβj)=1
n2nsummationdisplay
i=1V(Yi)φ2
j(xi)
=σ2
n2nsummationdisplay
i=1φ2
j(xi)=σ2
n1
nnsummationdisplay
i=1φ2
j(xi)
≈σ2
nintegraldisplay
φ2
j(x)dx=σ2
n
sinceintegraltext
φ2
j(x)dx=1 . /squaresolid
Let
hatwider(x)=Jsummationdisplay
j=1hatwideβjφj(x),
and let
R(J)=Eintegraldisplay
(r(x)−hatwider(x))2dx
be the risk of the estimator.
21.9 Theorem. The risk R(J)of the estimator hatwidern(x)=summationtextJ
j=1hatwideβjφj(x)is
R(J)=Jσ2
n+∞summationdisplay
j=J+1β2
j. (21.25)

<<<PAGE 353>>>

21.3 Regression 337
To estimate for σ2=V(/epsilon1i)w eu s e
hatwideσ2=n
knsummationdisplay
i=n−k+1hatwideβ2
j (21.26)
where k=n/4. To motivate this estimator, recall that if fis smooth, then
βj≈0 for large j. So, for j≥k,hatwideβj≈N(0,σ2/n) and thus, hatwideβj≈σZj/√nfor
forj≥k, where Zj∼N(0,1). Therefore,
hatwideσ2=n
knsummationdisplay
i=n−k+1hatwideβ2
j≈n
knsummationdisplay
i=n−k+1parenleftbiggσ√nhatwideβjparenrightbigg2
=σ2
knsummationdisplay
i=n−k+1hatwideβ2
j=σ2
kχ2
k
since a sum of kNormals has a χ2
kdistribution. Now E(χ2
k)=kand hence
E(hatwideσ2)≈σ2. Also, V(χ2
k)=2kand hence V(hatwideσ2)≈(σ4/k2)(2k)=( 2 σ4/k)→0
asn→∞. Thus we expect hatwideσ2to be a consistent estimator of σ2. There is
nothing special about the choice k=n/4. Any kthat increases with nat an
appropriate rate will suﬃce.
We estimate the risk with
hatwideR(J)=Jhatwideσ2
n+nsummationdisplay
j=J+1parenleftbigg
hatwideβ2
j−hatwideσ2
nparenrightbigg
+. (21.27)
21.10 Example. Figure 21.4 shows the doppler function fandn=2,048
observations generated from the model
Yi=r(xi)+/epsilon1i
where xi=i/n,/epsilon1i∼N(0,(.1)2). The ﬁgure shows the data and the estimated
function. The estimate was based on hatwideJ= 234 terms. /squaresolid
We are now ready to give a complete description of the method.
Orthogonal Series Regression Estimator
1. Let
hatwideβj=1
nnsummationdisplay
i=1Yiφj(xi),j=1,...,n .
2. Let
hatwideσ2=n
knsummationdisplay
i=n−k+1hatwideβ2
j (21.28)

<<<PAGE 354>>>

338 21. Smoothing Using Orthogonal Functions
FIGURE 21.4. Data from the doppler test function and the estimated function. See
Example 21.10.
where k≈n/4.
3. For 1 ≤J≤n, compute the risk estimate
hatwideR(J)=Jhatwideσ2
n+nsummationdisplay
j=J+1parenleftbigg
hatwideβ2
j−hatwideσ2
nparenrightbigg
+.
4. Choose hatwideJ∈{1,...n}to minimize hatwideR(J).
5. Let
hatwider(x)=/hatwideJsummationdisplay
j=1hatwideβjφj(x).
Finally, we turn to conﬁdence bands. As before, these bands are not really
for the true function r(x) but rather for the smoothed version of the function
rJ(x)=summationtext/hatwideJ
j=1βjφj(x).
21.11 Theorem. Suppose the estimate hatwideris based on Jterms and hatwideσis deﬁned
as in equation (21.28). Assume that J<n −k+1. An approximate 1−α
conﬁdence band for rJis(/lscript, u)where
/lscript(x)=hatwidern(x)−c, u (x)=hatwidern(x)+c, (21.29)
where
c=a(x)hatwideσχJ,α√n,a(x)=radicaltpradicalvertexradicalvertexradicalbtJsummationdisplay
j=1φ2
j(x),

<<<PAGE 355>>>

21.3 Regression 339
andhatwideσis given in equation (21.28).
Proof. LetL=summationtextJ
j=1(hatwideβj−βj)2. By the central limit theorem, hatwideβj≈
N(βj,σ2/n). Hence, hatwideβj≈βj+σ/epsilon1j/√nwhere /epsilon1j∼N(0,1) and therefore
L≈σ2
nJsummationdisplay
j=1/epsilon12
jd=σ2
nχ2
J.
Thus,
Pparenleftbigg
L>σ2
nχ2
J,αparenrightbigg
=Pparenleftbiggσ2
nχ2
J>σ2
nχ2
J,αparenrightbigg
=α.
Also,
|hatwider(x)−rJ(x)|≤Jsummationdisplay
j=1|φj(x)||hatwideβj−βj|
≤radicaltpradicalvertexradicalvertexradicalbtJsummationdisplay
j=1φ2
j(x)radicaltpradicalvertexradicalvertexradicalbtJsummationdisplay
j=1(hatwideβj−β2
j)
≤a(x)√
L
by the Cauchy-Schwartz inequality (Theorem 4.8). So,
PparenleftBigg
max
x|hatwidefJ(x)−f(x)|
a(x)>hatwideσχJ,α√nparenrightBigg
≤Pparenleftbigg√
L>hatwideσχJ,α√nparenrightbigg
=α
and the result follows. /squaresolid
21.12 Example. Figure 21.5 shows the conﬁdence envelope for the doppler
signal. The ﬁrst plot is based on J= 234 (the value of Jthat minimizes the
estimated risk). The second is based on J=4 5≈√n. Larger Jyields a higher
resolution estimator at the cost of large conﬁdence bands. Smaller Jyields a
lower resolution estimator but has tighter conﬁdence bands. /squaresolid
So far, we have assumed that the xi’s are of the form {1/n,2/ n ,..., 1}.
If the xi’s are on interval [ a, b], then we can rescale them so that are in the
interval [0 ,1]. If the xi’s are not equally spaced, the methods we have discussed
still apply so long as the xi’s “ﬁll out” the interval [0,1] in such a way so as to
not be too clumped together. If we want to treat the xi’s as random instead
of ﬁxed, then the method needs signiﬁcant modiﬁcations which we shall notdeal with here.

<<<PAGE 356>>>

340 21. Smoothing Using Orthogonal Functions
FIGURE 21.5. Estimates and conﬁdence bands for the doppler test function using
n=2,048 observations. First plot: J= 234 terms. Second plot: J= 45 terms.
21.4 Wavelets
Suppose there is a sharp jump in a regression function fat some point x
but that fis otherwise very smooth. Such a function fis said to be spa-
tially inhomogeneous . The doppler function is an example of a spatially
inhomogeneous function; it is smooth for large xand unsmooth for small x.
It is hard to estimate fusing the methods we have discussed so far. If we
use a cosine basis and only keep low order terms, we will miss the peak; ifwe allow higher order terms we will ﬁnd the peak but we will make the restof the curve very wiggly. Similar comments apply to kernel regression. If weuse a large bandwidth, then we will smooth out the peak; if we use a smallbandwidth, then we will ﬁnd the peak but we will make the rest of the curvevery wiggly.
One way to estimate inhomogeneous functions is to use a more carefully
chosen basis that allows us to place a “blip” in some small region withoutadding wiggles elsewhere. In this section, we describe a special class of basescalled wavelets , that are aimed at ﬁxing this problem. Statistical inference
using wavelets is a large and active area. We will just discuss a few of the
main ideas to get a ﬂavor of this approach.
We start with a particular wavelet called the Haar wavelet. TheHaar
father wavelet orHaar scaling function is deﬁned by
φ(x)=braceleftbigg1i f 0 ≤x<1
0 otherwise .(21.30)

<<<PAGE 357>>>

21.4 Wavelets 341
Themother Haar wavelet is deﬁned by
ψ(x)=braceleftbigg
−1i f 0 ≤x≤1
2,
1i f1
2<x≤1.(21.31)
For any integers jandkdeﬁne
ψj,k(x)=2j/2ψ(2jx−k). (21.32)
The function ψj,khas the same shape as ψbut it has been rescaled by a factor
of 2j/2and shifted by a factor of k.
See Figure 21.6 for some examples of Haar wavelets. Notice that for large
j,ψj,kis a very localized function. This makes it possible to add a blip to a
function in one place without adding wiggles elsewhere. Increasing jis like
looking in a microscope at increasing degrees of resolution. In technical terms,we say that wavelets provide a multiresolution analysis ofL
2(0,1).
-2-1012
-2-1012
FIGURE 21.6. Some Haar wavelets. Left: the mother wavelet ψ(x); Right: ψ2,2(x).
Let
Wj={ψjk,k=0,1,...,2j−1}
be the set of rescaled and shifted mother wavelets at resolution j.
21.13 Theorem. The set of functions
braceleftbigg
φ, W0,W1,W2,...,bracerightbigg
is an orthonormal basis for L2(0,1).

<<<PAGE 358>>>

342 21. Smoothing Using Orthogonal Functions
It follows from this theorem that we can expand any function f∈L2(0,1) in
this basis. Because each Wjis itself a set of functions, we write the expansion
as a double sum:
f(x)=αφ(x)+∞summationdisplay
j=02j−1summationdisplay
k=0βj,kψj,k(x) (21.33)
where
α=integraldisplay1
0f(x)φ(x)dx, β j,k=integraldisplay1
0f(x)ψj,k(x)dx.
We call αthescaling coeﬃcient and the βj,k’s are called the detail
coeﬃcients . We call the ﬁnite sum
fJ(x)=αφ(x)+J−1summationdisplay
j=02j−1summationdisplay
k=0βj,kψj,k(x) (21.34)
theresolution Japproximation to f. The total number of terms in this sum
is
1+J−1summationdisplay
j=02j=1+2J−1=2J.
21.14 Example. Figure 21.7 shows the doppler signal, and its reconstruction
using J=3,5 and J=8 . /squaresolid
Haarwavelets are localized, meaning that they are zero outside an interval.
But they are not smooth. This raises the question of whether there existsmooth, localized wavelets that from an orthonormal basis. In 1988, Ingrid
Daubechie showed that such wavelets do exist. These smooth wavelets are
diﬃcult to describe. They can be constructed numerically but there is noclosed form formula for the smoother wavelets. To keep things simple, we will
continue to use Haar wavelets.
Consider the regression model Y
i=r(xi)+σ/epsilon1iwhere /epsilon1i∼N(0,1) and
xi=i/n. To simplify the discussion we assume that n=2Jfor some J.
There is one major diﬀerence between estimation using wavelets instead of
a cosine (or polynomial) basis. With the cosine basis, we used all the terms1≤j≤Jfor some J. The number of terms Jacted as a smoothing parameter.
With wavelets, we control smoothing using a method called thresholding
where we keep a term in the function approximation if its coeﬃcient is large,

<<<PAGE 359>>>

21.4 Wavelets 343
0.0 0.2 0.4 0.6 0.8 1.0−0.4 −0.2 0.0 0.2 0.4
xf
0.0 0.2 0.4 0.6 0.8 1.0−0.4 −0.2 0.0 0.2 0.4
0.0 0.2 0.4 0.6 0.8 1.0−0.4 −0.2 0.0 0.2 0.4
0.0 0.2 0.4 0.6 0.8 1.0−0.4 −0.2 0.0 0.2 0.4
FIGURE 21.7. The doppler signal and its reconstruction
fJ(x)=αφ(x)+/summationtextJ−1
j=0/summationtext
kβj,kψj,k(x) based on J=3 ,J= 5, and J=8 .
otherwise, we throw out that term. There are many versions of thresholding.
The simplest is called hard, universal thresholding. Let J= log2(n) and deﬁne
hatwideα=1
nsummationdisplay
iφk(xi)Yiand Dj,k=1
nsummationdisplay
iψj,k(xi)Yi (21.35)
for 0≤j≤J−1.
Haar Wavelet Regression
1. Compute hatwideαandDj,kas in (21.35), for 0 ≤j≤J−1.
2. Estimate σ; see (21.37).
3. Apply universal thresholding:
hatwideβj,k=braceleftBigg
Dj,kif|Dj,k|>hatwideσradicalBig
2 logn
n
0 otherwise .bracerightBigg
(21.36)
4. Sethatwidef(x)=hatwideαφ(x)+summationtextJ−1
j=j0summationtext2j−1
k=0hatwideβj,kψj,k(x).

<<<PAGE 360>>>

344 21. Smoothing Using Orthogonal Functions
In practice, we do not compute SkandDj,kusing (21.35). Instead, we use
thediscrete wavelet transform (DWT) which is very fast. The DWT for
Haarwavelets is described in the appendix. The estimate of σis
hatwideσ=√n×medianparenleftbig
|DJ−1,k|:k=0,...,2J−1−1parenrightbig
0.6745. (21.37)
The estimate for σmay look strange. It is similar to the estimate we used
for the cosine basis but it is designed to be insensitive to sharp peaks in thefunction.
To understand the intuition behind universal thresholding, consider what
happens when there is no signal, that is, when β
j,k= 0 for all jandk.
21.15 Theorem. Suppose that βj,k=0for all jandkand let hatwideβj,kbe the
universal threshold estimator. Then
P(hatwideβj,k= 0 for all j,k)→1
asn→∞.
Proof. To simplify the proof, assume that σis known. Now Dj,k≈
N(0,σ2/n). We will need Mill’s inequality (Theorem 4.7): if Z∼N(0,1)
then P(|Z|>t)≤(c/t)e−t2/2where c=radicalbig
2/πis a constant. Thus,
P(max|Dj,k|>λ)≤summationdisplay
j,kP(|Dj,k|>λ)=summationdisplay
j,kPparenleftbigg√n|Dj,k|
σ>√nλ
σparenrightbigg
≤summationdisplay
j,kcσ
λ√nexpbraceleftbigg
−1
2nλ2
σ2bracerightbigg
=c√2 logn→0./squaresolid
21.16 Example. Consider Yi=r(xi)+σ/epsilon1iwhere fis the doppler signal,
σ=.1 and n=2,048. Figure 21.8 shows the data and the estimated function
using universal thresholding. Of course, the estimate is not smooth since Haarwavelets are not smooth. Nonetheless, the estimate is quite accurate.
/squaresolid

<<<PAGE 361>>>

21.5 Appendix 345
0.0 0.2 0.4 0.6 0.8 1.0−0.5 0.0 0.5
0.0 0.2 0.4 0.6 0.8 1.0−0.4 −0.2 0.0 0.2 0.4
FIGURE 21.8. Estimate of the Doppler function using Haar wavelets and universal
thresholding.
21.5 Appendix
The DWT for Haar Wavelets . Letybe the vector of Yi’s (length n) and
letJ= log2(n). Create a list Dwith elements
D[[0]], ..., D [[J−1]].
Set:
temp←y/√n.
Then do:
for(ji n (J−1 ):0 ) {
m←2j
I←(1 :m)
D[[j]]←parenleftbigg
temp[2∗I]−temp[(2∗I)−1]parenrightbigg
/√
2
temp ←parenleftbigg
temp[2∗I]+temp[(2∗I)−1]parenrightbigg
/√
2
}

<<<PAGE 362>>>

346 21. Smoothing Using Orthogonal Functions
21.6 Bibliographic Remarks
Efromovich (1999) is a reference for orthogonal function methods. See also
Beran (2000) and Beran and D¨ umbgen (1998). An introduction to wavelets is
given in Ogden (1997). A more advanced treatment can be found in H¨ ardle
et al. (1998). The theory of statistical estimation using wavelets has been
developed by many authors, especially David Donoho and Ian Johnstone. SeeDonoho and Johnstone (1994), Donoho and Johnstone (1995), Donoho et al.(1995), and Donoho and Johnstone (1998).
21.7 Exercises
1. Prove Theorem 21.5.
2. Prove Theorem 21.9.3. Let
ψ
1=parenleftbigg1√
3,1√
3,1√
3parenrightbigg
,ψ2=parenleftbigg1√
2,−1√
2,0parenrightbigg
,ψ3=parenleftbigg1√
6,1√
6,−2√
6parenrightbigg
.
Show that these vectors have norm 1 and are orthogonal.
4. Prove Parseval’s relation equation (21.6).5. Plot the ﬁrst ﬁve Legendre polynomials. Verify, numerically, that they
are orthonormal.
6. Expand the following functions in the cosine basis on [0 ,1]. For (a)
and (b), ﬁnd the coeﬃcients β
janalytically. For (c) and (d), ﬁnd the
coeﬃcients βjnumerically, i.e.
βj=integraldisplay1
0f(x)φj(x)≈1
NNsummationdisplay
r=1fparenleftBigr
NparenrightBig
φjparenleftBigr
NparenrightBig
for some large integer N. Then plot the partial sumsummationtextn
j=1βjφj(x) for
increasing values of n.
(a)f(x)=√
2 cos(3 πx).
(b)f(x) = sin( πx).
(c)f(x)=summationtext11
j=1hjK(x−tj) where K(t) = (1+sign( t))/2, sign( x)=−1
ifx<0, sign( x)=0i f x= 0, sign( x)=1i f x>0,

<<<PAGE 363>>>

21.7 Exercises 347
(tj)=(.1,.13,.15,.23,.25,.40,.44,.65,.76,.78,.81),
(hj)=( 4 ,−5,3,−4,5,−4.2,2.1,4.3,−3.1,2.1,−4.2).
(d)f=radicalbig
x(1−x) sinparenleftBig
2.1π
(x+.05)parenrightBig
.
7. Consider the glass fragments data from the book’s website. Let Ybe
refractive index and let Xbe aluminum content (the fourth variable).
(a) Do a nonparametric regression to ﬁt the model Y=f(x)+/epsilon1using
the cosine basis method. The data are not on a regular grid. Ignore thiswhen estimating the function. (But do sort the data ﬁrst according tox.) Provide a function estimate, an estimate of the risk, and a conﬁdence
band.
(b) Use the wavelet method to estimate f.
8. Show that the Haar wavelets are orthonormal.
9. Consider again the doppler signal:
f(x)=radicalbig
x(1−x) sinparenleftbigg2.1π
x+0.05parenrightbigg
.
Letn=1,024,σ=0.1, and let ( x1,...,x n)=( 1 / n ,..., 1). Generate
data
Yi=f(xi)+σ/epsilon1i
where /epsilon1i∼N(0,1).
(a) Fit the curve using the cosine basis method. Plot the function esti-
mate and conﬁdence band for J=1 0,20,...,100.
(b) Use Haar wavelets to ﬁt the curve.
10. (Haar density Estimation.) Let X1,...,X n∼ffor some density fon
[0,1]. Let’s consider constructing a wavelet histogram. Let φandψbe
the Haar father and mother wavelet. Write
f(x)≈φ(x)+J−1summationdisplay
j=02j−1summationdisplay
k=0βj,kψj,k(x)
where J≈log2(n). Let
hatwideβj,k=1
nnsummationdisplay
i=1ψj,k(Xi).

<<<PAGE 364>>>

348 21. Smoothing Using Orthogonal Functions
(a) Show that hatwideβj,kis an unbiased estimate of βj,k.
(b) Deﬁne the Haar histogram
hatwidef(x)=φ(x)+Bsummationdisplay
j=02j−1summationdisplay
k=0hatwideβj,kψj,k(x)
for 0≤B≤J−1.
(c) Find an approximate expression for the MSE as a function of B.
(d) Generate n=1,000 observations from a Beta (15,4) density. Es-
timate the density using the Haar histogram. Use leave-one-out crossvalidation to choose B.
11. In this question, we will explore the motivation for equation (21.37). Let
X
1,...,X n∼N(0,σ2). Let
hatwideσ=√n×median ( |X1|,...,|Xn|)
0.6745.
(a) Show that E(hatwideσ)=σ.
(b) Simulate n= 100 observations from a N(0,1) distribution. Compute
hatwideσas well as the usual estimate of σ. Repeat 1,000 times and compare
the MSE.
(c) Repeat (b) but add some outliers to the data. To do this, simulate
each observation from a N(0,1) with probability .95 and simulate eachobservation from a N(0,10) with probability .95.
12. Repeat question 6 using the Haar basis.

<<<PAGE 365>>>

22
Classiﬁcation
22.1 Introduction
The problem of predicting a discrete random variable Yfrom another random
variable Xis called classiﬁcation ,supervised learning ,discrimination ,
orpattern recognition.
Consider iiddata ( X1,Y1),...,(Xn,Yn) where
Xi=(Xi1,...,X id)∈X⊂ Rd
is ad-dimensional vector and Yitakes values in some ﬁnite set Y.Aclassiﬁ-
cation rule is a function h:X→Y . When we observe a new X, we predict
Yto be h(X).
22.1 Example. Here is a an example with fake data. Figure 22.1 shows 100
data points. The covariate X=(X1,X2) is 2-dimensional and the outcome
Y∈Y={0,1}. The Yvalues are indicated on the plot with the triangles
representing Y= 1 and the squares representing Y= 0. Also shown is a linear
classiﬁcation rule represented by the solid line. This is a rule of the form
h(x)=braceleftbigg1i fa+b1x1+b2x2>0
0 otherwise .
Everything above the line is classiﬁed a s a 0 and everything below the line is
classiﬁed as a 1. /squaresolid

<<<PAGE 366>>>

350 22. Classiﬁcation
/SolidTriangle/Triangle/SolidTriangle/Triangle/SolidTriangle/Triangle
/SolidTriangle/Triangle/SolidTriangle/Triangle
/SolidTriangle/Triangle/SolidSquare/Square
/SolidSquare/Square /SolidSquare/Square/SolidSquare/Square /SolidSquare/Square/SolidSquare/Square
x1x2
FIGURE 22.1. Two covariates and a linear decision boundary. /trianglemeans Y=1 .
/Boxmeans Y= 0. These two groups are perfectly separated by the linear decision
boundary; you probably won’t see real data like this.
22.2 Example. Recall the the Coronary Risk-Factor Study (CORIS) data
from Example 13.17. There are 462 males between the ages of 15 and 64 fromthree rural areas in South Africa. The outcome Yis the presence ( Y=1 )o r
absence ( Y= 0) of coronary heart disease and there are 9 covariates: systolic
blood pressure, cumulative tobacco (kg), ldl (low density lipoprotein choles-terol), adiposity, famhist (family history of heart disease), typea (type-A be-havior), obesity, alcohol (current alcohol consumption), and age. I computeda linear decision boundary using the LDA method based on two of the co-variates, systolic blood pressure and tobacco consumption. The LDA methodwill be explained shortly. In this example, the groups are hard to tell apart.In fact, 141 of the 462 subjects are misclassiﬁed using this classiﬁcation rule.
/squaresolid
At this point, it is worth revisiting the Statistics/Data Mining dictionary:
Statistics Computer Science Meaning
classiﬁcation supervised learning predicting a discrete YfromX
data training sample ( X1,Y1),...,(Xn,Yn)
covariates features the Xi’s
classiﬁer hypothesis map h:X→Y
estimation learning ﬁnding a good classiﬁer
22.2 Error Rates and the Bayes Classiﬁer
Our goal is to ﬁnd a classiﬁcation rule hthat makes accurate predictions. We
start with the following deﬁnitions:

<<<PAGE 367>>>

22.2 Error Rates and the Bayes Classiﬁer 351
22.3 Deﬁnition. Thetrue error rate1of a classiﬁer his
L(h)=P({h(X)/negationslash=Y}) (22.1)
and the empirical error rate ortraining error rate is
hatwideLn(h)=1
nnsummationdisplay
i=1I(h(Xi)/negationslash=Yi). (22.2)
First we consider the special case where Y={0,1}. Let
r(x)=E(Y|X=x)=P(Y=1|X=x)
denote the regression function . From Bayes’ theorem we have that
r(x)= P(Y=1|X=x)
=f(x|Y=1 )P(Y=1 )
f(x|Y=1 )P(Y=1 )+ f(x|Y=0 )P(Y=0 )
=πf1(x)
πf1(x)+( 1 −π)f0(x)(22.3)
where
f0(x)= f(x|Y=0 )
f1(x)= f(x|Y=1 )
π=P(Y=1 ).
22.4 Deﬁnition. TheBayes classiﬁcation rule h∗is
h∗(x)=braceleftbigg
1i f r(x)>1
2
0 otherwise .(22.4)
The set D(h)={x:P(Y=1|X=x)=P(Y=0|X=x)}is called the
decision boundary.
Warning! The Bayes rule has nothing to do with Bayesian inference. We
could estimate the Bayes rule using either frequentist or Bayesian methods.
The Bayes rule may be written in several equivalent forms:
1One can use other loss functions. For simplicity we will use the error rate as our loss function.

<<<PAGE 368>>>

352 22. Classiﬁcation
h∗(x)=braceleftbigg
1i f P(Y=1|X=x)>P(Y=0|X=x)
0 otherwise(22.5)
and
h∗(x)=braceleftbigg
1i fπf1(x)>(1−π)f0(x)
0 otherwise .(22.6)
22.5 Theorem. The Bayes rule is optimal, that is, if his any other classiﬁ-
cation rule then L(h∗)≤L(h).
The Bayes rule depends on unknown quantities so we need to use the data
to ﬁnd some approximation to the Bayes rule. At the risk of oversimplifying,there are three main approaches:
1.Empirical Risk Minimization . Choose a set of classiﬁers Hand ﬁnd hatwideh∈H
that minimizes some estimate of L(h).
2.Regression . Find an estimate hatwiderof the regression function rand deﬁne
hatwideh(x)=braceleftbigg
1i fhatwider(x)>
1
2
0 otherwise .
3.Density Estimation. Estimate f0from the Xi’s for which Yi= 0, estimate
f1from the Xi’s for which Yi= 1 and let hatwideπ=n−1summationtextn
i=1Yi. Deﬁne
hatwider(x)=hatwideP(Y=1|X=x)=hatwideπhatwidef1(x)
hatwideπhatwidef1(x)+( 1 −hatwideπ)hatwidef0(x)
and
hatwideh(x)=braceleftbigg
1i fhatwider(x)>1
2
0 otherwise .
Now let us generalize to the case where Ytakes on more than two values
as follows.
22.6 Theorem. Suppose that Y∈Y={1,...,K }. The optimal rule is
h(x) = argmaxkP(Y=k|X=x) (22.7)
= argmaxkπkfk(x) (22.8)
where
P(Y=k|X=x)=fk(x)πksummationtext
rfr(x)πr, (22.9)
πr=P(Y=r),fr(x)=f(x|Y=r)andargmaxkmeans “the value of k
that maximizes that expression.”

<<<PAGE 369>>>

22.3 Gaussian and Linear Classiﬁers 353
22.3 Gaussian and Linear Classiﬁers
Perhaps the simplest approach to classiﬁcation is to use the density estima-
tion strategy and assume a parametric model for the densities. Suppose thatY={0,1}and that f
0(x)=f(x|Y= 0) and f1(x)=f(x|Y= 1) are both
multivariate Gaussians:
fk(x)=1
(2π)d/2|Σk|1/2expbraceleftbigg
−1
2(x−µk)TΣ−1
k(x−µk)bracerightbigg
,k=0,1.
Thus, X|Y=0∼N(µ0,Σ0) and X|Y=1∼N(µ1,Σ1).
22.7 Theorem. IfX|Y=0∼N(µ0,Σ0)andX|Y=1∼N(µ1,Σ1), then the
Bayes rule is
h∗(x)=braceleftBigg
1i f r2
1<r2
0+ 2 logparenleftBig
π1
π0parenrightBig
+ logparenleftBig
|Σ0|
|Σ1|parenrightBig
0 otherwise(22.10)
where
r2
i=(x−µi)TΣ−1
i(x−µi),i=1,2 (22.11)
is theManalahobis distance. An equivalent way of expressing the Bayes’
rule is
h∗(x) = argmaxkδk(x)
where
δk(x)=−1
2log|Σk|−1
2(x−µk)TΣ−1
k(x−µk) + log πk (22.12)
and|A|denotes the determinant of a matrix A.
The decision boundary of the above classiﬁer is quadratic so this procedure
is called quadratic discriminant analysis (QDA) . In practice, we use
sample estimates of π,µ1,µ2,Σ0,Σ1in place of the true value, namely:
hatwideπ0=1
nnsummationdisplay
i=1(1−Yi),hatwideπ1=1
nnsummationdisplay
i=1Yi
hatwideµ0=1
n0summationdisplay
i:Yi=0Xi,hatwideµ1=1
n1summationdisplay
i:Yi=1Xi
S0=1
n0summationdisplay
i:Yi=0(Xi−hatwideµ0)(Xi−hatwideµ0)T,S1=1
n1summationdisplay
i:Yi=1(Xi−hatwideµ1)(Xi−hatwideµ1)T
where n0=summationtext
i(1−Yi) and n1=summationtext
iYi.

<<<PAGE 370>>>

354 22. Classiﬁcation
A simpliﬁcation occurs if we assume that Σ 0=Σ0= Σ. In that case, the
Bayes rule is
h∗(x) = argmaxkδk(x) (22.13)
where now
δk(x)=xTΣ−1µk−1
2µT
kΣ−1+ logπk. (22.14)
The parameters are estimated as before, except that the mleo fΣi s
S=n0S0+n1S1
n0+n1.
The classiﬁcation rule is
h∗(x)=braceleftbigg1i fδ1(x)>δ0(x)
0 otherwise(22.15)
where
δj(x)=xTS−1hatwideµj−1
2hatwideµT
jS−1hatwideµj+ loghatwideπj
is called the discriminant function . The decision boundary {x:δ0(x)=
δ1(x)}is linear so this method is called linear discrimination analysis
(LDA).
22.8 Example. Let us return to the South African heart disease data. The
decision rule in in Example 22.2 was obtained by linear discrimination. Theoutcome was
classiﬁed as 0 classiﬁed as 1
y= 0 277 25
y= 1 116 44
The observed misclassiﬁcation rate is 141 /462 = .31. Including all the covari-
ates reduces the error rate to .27. The results from quadratic discriminationare
classiﬁed as 0 classiﬁed as 1
y= 0 272 30
y= 1 113 47
which has about the same error rate 143 /462 = .31. Including all the covariates
reduces the error rate to .26. In this example, there is little advantage to QDAover LDA.
/squaresolid
Now we generalize to the case where Ytakes on more than two values.

<<<PAGE 371>>>

22.3 Gaussian and Linear Classiﬁers 355
22.9 Theorem. Suppose that Y∈{1,...,K }.I ffk(x)=f(x|Y=k)is
Gaussian, the Bayes rule is
h(x) = argmaxkδk(x)
where
δk(x)=−1
2log|Σk|−1
2(x−µk)TΣ−1
k(x−µk) + log πk. (22.16)
If the variances of the Gaussians are equal, then
δk(x)=xTΣ−1µk−1
2µT
kΣ−1+ logπk. (22.17)
We estimate δk(x) by by inserting estimates of µk,Σkandπk. There is
another version of linear discriminant analysis due to Fisher. The idea isto ﬁrst reduce the dimension of covariates to one dimension by projectingthe data onto a line. Algebraically, this means replacing the covariate X=
(X
1,...,X d) with a linear combination U=wTX=summationtextd
j=1wjXj. The goal is
to choose the vector w=(w1,...,w d) that “best separates the data.” Then
we perform classiﬁcation with the one-dimensional covariate Zinstead of X.
We need deﬁne what we mean by separation of the groups. We would like
the two groups to have means that are far apart relative to their spread. Letµ
jdenote the mean of XforYjand let Σ be the variance matrix of X. Then
E(U|Y=j)=E(wTX|Y=j)=wTµjandV(U)=wTΣw.2Deﬁne the
separation by
J(w)=(E(U|Y=0 )−E(U|Y= 1))2
wTΣw
=(wTµ0−wTµ1)2
wTΣw
=wT(µ0−µ1)(µ0−µ1)Tw
wTΣw.
We estimate Jas follows. Let nj=summationtextn
i=1I(Yi=j) be the number of obser-
vations in group j, letXjbe the sample mean vector of the X’s for group j,
and let Sjbe the sample covariance matrix in group j. Deﬁne
hatwideJ(w)=wTSBw
wTSWw(22.18)
2The quantity Jarises in physics, where it is called the Rayleigh coeﬃcient.

<<<PAGE 372>>>

356 22. Classiﬁcation
where
SB=(X0−X1)(X0−X1)T
SW=(n0−1)S0+(n1−1)S1
(n0−1 )+( n1−1).
22.10 Theorem. The vector
w=S−1
W(X0−X1) (22.19)
is a minimizer of hatwideJ(w).W ec a l l
U=wTX=(X0−X1)TS−1
WX (22.20)
theFisher linear discriminant function . The midpoint mbetween X0and
X1is
m=1
2(X0+X1)=1
2(X0−X1)TS−1
B(X0+X1) (22.21)
Fisher’s classiﬁcation rule is
h(x)=braceleftbigg0i f wTX≥m
1i f wTX<m .
Fisher’s rule is the same as the Bayes linear classiﬁer in equation (22.14)
whenhatwideπ=1/2.
22.4 Linear Regression and Logistic Regression
A more direct approach to classiﬁcation is to estimate the regression function
r(x)=E(Y|X=x) without bothering to estimate the densities fk. For the
rest of this section, we will only consider the case where Y={0,1}. Thus,
r(x)=P(Y=1|X=x) and once we have an estimate hatwider, we will use the
classiﬁcation rule
hatwideh(x)=braceleftbigg
1i fhatwider(x)>1
2
0 otherwise .(22.22)
The simplest regression model is the linear regression model
Y=r(x)+/epsilon1=β0+dsummationdisplay
j=1βjXj+/epsilon1 (22.23)
where E(/epsilon1) = 0. This model can’t be correct since it does not force Y=0o r
1. Nonetheless, it can sometimes lead to a decent classiﬁer.

<<<PAGE 373>>>

22.4 Linear Regression and Logistic Regression 357
Recall that the least squares estimate of β=(β0,β1,...,β d)Tminimizes
the residual sums of squares
rss(β)=nsummationdisplay
i=1parenleftbigg
Yi−β0−dsummationdisplay
j=1Xijβjparenrightbigg2
.
LetXdenote the N×(d+ 1) matrix of the form
X=
1X
11... X 1d
1X21... X 2d
............
1X
n1... X nd
.
Also let Y=(Y
1,...,Y n)T. Then,
RSS(β)=(Y−Xβ)T(Y−Xβ)
and the model can be written as
Y=Xβ+/epsilon1
where /epsilon1=(/epsilon11,...,/epsilon1 n)T. From Theorem 13.13,
hatwideβ=(XTX)−1XTY.
The predicted values are
hatwideY=Xhatwideβ.
Now we use (22.22) to classify, where hatwider(x)=hatwideβ0+summationtext
jhatwideβjxj.
An alternative is to use logistic regression which was also discussed in Chap-
ter 13. The model is
r(x)=P(Y=1|X=x)=eβ0+/summationtext
jβjxj
1+eβ0+/summationtext
jβjxj(22.24)
and the mlehatwideβis obtained numerically.
22.11 Example. Let us return to the heart disease data. The mleis given in
Example 13.17. The error rate, using this model for classiﬁcation, is .27. Theerror rate from a linear regression is .26.
We can get a better classiﬁer by ﬁtting a richer model. For example, we
could ﬁt
logit P(Y=1|X=x)=β
0+summationdisplay
jβjxj+summationdisplay
j,kβjkxjxk. (22.25)

<<<PAGE 374>>>

358 22. Classiﬁcation
More generally, we could add terms of up to order rfor some integer r. Large
values of rgive a more complicated model which should ﬁt the data better.
But there is a bias–variance tradeoﬀ which we’ll discuss later.
22.12 Example. If we use model (22.25) for the heart disease data with r=2 ,
the error rate is reduced to .22. /squaresolid
22.5 Relationship Between Logistic Regression and
LDA
LDA and logistic regression are almost the same thing. If we assume that each
group is Gaussian with the same covariance matrix, then we saw earlier that
logparenleftbiggP(Y=1|X=x)
P(Y=0|X=x)parenrightbigg
= logparenleftbiggπ0
π1parenrightbigg
−1
2(µ0+µ1)TΣ−1(µ1−µ0)
+xTΣ−1(µ1−µ0)
≡α0+αTx.
On the other hand, the logistic model is, by assumption,
logparenleftbiggP(Y=1|X=x)
P(Y=0|X=x)parenrightbigg
=β0+βTx.
These are the same model since they both lead to classiﬁcation rules that are
linear in x. The diﬀerence is in how we estimate the parameters.
The joint density of a single observation is f(x, y)=f(x|y)f(y)=f(y|x)f(x).
In LDA we estimated the whole joint distribution by maximizing the likeli-hoodproductdisplay
if(xi,yi)=productdisplay
if(xi|yi)
bracehtipupleftbracehtipdownrightbracehtipdownleftbracehtipupright
Gaussianproductdisplay
if(yi)
bracehtipupleftbracehtipdownrightbracehtipdownleftbracehtipupright
Bernoulli. (22.26)
In logistic regression we maximized the conditional likelihoodproducttext
if(yi|xi) but
we ignored the second term f(xi):
productdisplay
if(xi,yi)=productdisplay
if(yi|xi)
bracehtipupleftbracehtipdownrightbracehtipdownleftbracehtipupright
logisticproductdisplay
if(xi)
bracehtipupleftbracehtipdownrightbracehtipdownleftbracehtipupright
ignored. (22.27)
Since classiﬁcation only requires knowing f(y|x), we don’t really need to es-
timate the whole joint distribution. Logistic regression leaves the marginal

<<<PAGE 375>>>

22.6 Density Estimation and Naive Bayes 359
distribution f(x) unspeciﬁed so it is more nonparametric than LDA. This is
an advantage of the logistic regression approach over LDA.
To summarize: LDA and logistic regression both lead to a linear classi-
ﬁcation rule. In LDA we estimate the entire joint distribution f(x, y)=
f(x|y)f(y). In logistic regression we only estimate f(y|x) and we don’t bother
estimating f(x).
22.6 Density Estimation and Naive Bayes
The Bayes rule is h(x) = argmaxkπkfk(x).If we can estimate πkandfk
then we can estimate the Bayes classiﬁcation rule. Estimating πkis easy but
what about fk? We did this previously by assuming fkwas Gaussian. An-
other strategy is to estimate fkwith some nonparametric density estimator
hatwidefksuch as a kernel estimator. But if x=(x1,...,x d) is high-dimensional,
nonparametric density estimation is not very reliable. This problem is amelio-rated if we assume that X
1,...,X dare independent, for then, fk(x1,...,x d)=producttextd
j=1fkj(xj). This reduces the problem to done-dimensional density estima-
tion problems, within each of the kgroups. The resulting classiﬁer is called
the naive Bayes classiﬁer. The assumption that the components of Xare
independent is usually wrong yet the resulting classiﬁer might still be accu-rate. Here is a summary of the steps in the naive Bayes classiﬁer:
The Naive Bayes Classiﬁer
1. For each group k, compute an estimate hatwidefkjof the density fkjforXj,
using the data for which Yi=k.
2. Let
hatwidefk(x)=hatwidefk(x1,...,x d)=dproductdisplay
j=1hatwidefkj(xj).
3. Let
hatwideπk=1
nnsummationdisplay
i=1I(Yi=k)
where I(Yi=k)=1i f Yi=kandI(Yi=k)=0i f Yi/negationslash=k.
4. Let
h(x) = argmaxkhatwideπkhatwidefk(x).

<<<PAGE 376>>>

360 22. Classiﬁcation
01Blood Pressure 1Age
<100 ≥100<50 ≥50
FIGURE 22.2. A simple classiﬁcation tree.
The naive Bayes classiﬁer is popular when xis high-dimensional and dis-
crete. In that case, hatwidefkj(xj) is especially simple.
22.7 Trees
Trees are classiﬁcation methods that partition the covariate space Xinto
disjoint pieces and then classify the observations according to which partitionelement they fall in. As the name implies, the classiﬁer can be represented asa tree.
For illustration, suppose there are two covariates, X
1= age and X2= blood
pressure. Figure 22.2 shows a classiﬁcation tree using these variables.
The tree is used in the following way. If a subject has Age ≥50 then we
classify him as Y= 1. If a subject has Age <50 then we check his blood
pressure. If systolic blood pressure is <100 then we classify him as Y=1 ,
otherwise we classify him as Y= 0. Figure 22.3 shows the same classiﬁer as
a partition of the covariate space.
Here is how a tree is constructed. First, suppose that y∈Y={0,1}and
that there is only a single covariate X. We choose a split point tthat divides
the real line into two sets A1=(−∞,t] and A2=(t,∞). Lethatwideps(j)b et h e
proportion of observations in Assuch that Yi=j:
hatwideps(j)=summationtextn
i=1I(Yi=j,Xi∈As)summationtextn
i=1I(Xi∈As)(22.28)
fors=1,2 and j=0,1. The impurity of the split tis deﬁned to be
I(t)=2summationdisplay
s=1γs (22.29)

<<<PAGE 377>>>

22.7 Trees 361
1
01
AgeBlood Pressure
50110
FIGURE 22.3. Partition representation of classiﬁcation tree.
where
γs=1−1summationdisplay
j=0hatwideps(j)2. (22.30)
This particular measure of impurity is known as the Gini index . If a partition
element Ascontains all 0’s or all 1’s, then γs= 0. Otherwise, γs>0. We
choose the split point tto minimize the impurity. (Other indices of impurity
besides can be used besides the Gini index.)
When there are several covariates, we choose whichever covariate and split
that leads to the lowest impurity. This process is continued until some stoppingcriterion is met. For example, we might stop when every partition element hasfewer than n
0data points, where n0is some ﬁxed number. The bottom nodes
of the tree are called the leaves . Each leaf is assigne da0o r1d e p ending on
whether there are more data points with Y=0o r Y= 1 in that partition
element.
This procedure is easily generalized to the case where Y∈{1,...,K }.We
simply deﬁne the impurity by
γs=1−ksummationdisplay
j=1hatwideps(j)2(22.31)
wherehatwidepi(j) is the proportion of observations in the partition element for which
Y=j.

<<<PAGE 378>>>

362 22. Classiﬁcation
tobacco
<0.51 ≥0.51age
<31.5 ≥31.5
age
<50.5 ≥50.5
tobacco
<7.47 ≥7.47000
01
FIGURE 22.4. A classiﬁcation tree for the heart disease data using two covariates.
22.13 Example. A classiﬁcation tree for the heart disease data yields a mis-
classiﬁcation rate of .21. If we build a tree using only tobacco and age, themisclassiﬁcation rate is then .29. The tree is shown in Figure 22.4.
/squaresolid
Our description of how to build trees is incomplete. If we keep splitting
until there are few cases in each leaf of the tree, we are likely to overﬁt thedata. We should choose the complexity of the tree in such a way that theestimated true error rate is low. In the next section, we discuss estimation ofthe error rate.
22.8 Assessing Error Rates and Choosing a Good
Classiﬁer
How do we choose a good classiﬁer? We would like to have a classiﬁer hwith
a low true error rate L(h). Usually, we can’t use the training error rate hatwideLn(h)
as an estimate of the true error rate because it is biased downward.
22.14 Example. Consider the heart disease data again. Suppose we ﬁt a se-
quence of logistic regression models. In the ﬁrst model we include one co-variate. In the second model we include two covariates, and so on. The ninthmodel includes all the covariates. We can go even further. Let’s also ﬁt a tenthmodel that includes all nine covariates plus the ﬁrst covariate squared. Then

<<<PAGE 379>>>

22.8 Assessing Error Rates and Choosing a Good Classiﬁer 363
we ﬁt an eleventh model that includes all nine covariates plus the ﬁrst covari-
ate squared and the second covariate squared. Continuing this way we will geta sequence of 18 classiﬁers of increasing complexity. The solid line in Figure22.5 shows the observed classiﬁcation error which steadily decreases as wemake the model more complex. If we keep going, we can make a model withzero observed classiﬁcation error. The dotted line shows the 10-fold cross-
validation estimate of the error rate (to be explained shortly) which is a
better estimate of the true error rate than the observed classiﬁcation error.The estimated error decreases for a while then increases. This is essentiallythe bias–variance tradeoﬀ phenomenon we have seen in Chapter 20.
/squaresolid
number of terms in modelerror rate
0.260.300.34
51 5
FIGURE 22.5. The solid line is the observed error rate and dashed line is the
cross-validation estimate of true error rate.
There are many ways to estimate the error rate. We’ll consider two: cross-
validation andprobability inequalities.
Cross-Validation. The basic idea of cross-validation, which we have al-
ready encountered in curve estimation, is to leave out some of the data whenﬁtting a model. The simplest version of cross-validation involves randomlysplitting the data into two pieces: the training set Tand the validation
setV. Often, about 10 per cent of the data might be set aside as the validation
set. The classiﬁer his constructed from the training set. We then estimate

<<<PAGE 380>>>

364 22. Classiﬁcation
Training Data T Validation Data V
bracehtipupleft bracehtipdownrightbracehtipdownleft bracehtipupright
hatwidehbracehtipupleftbracehtipdownrightbracehtipdownleft bracehtipupright
hatwideL
FIGURE 22.6. Cross-validation. The data are divided into two groups: the training
data and the validation data. The training data are used to produce an estimatedclassiﬁer /hatwideh. Then, /hatwidehis applied to the validation data to obtain an estimate /hatwideLof the
error rate of /hatwideh.
the error by
hatwideL(h)=1
msummationdisplay
Xi∈VI(h(Xi)/negationslash=YI). (22.32)
where mis the size of the validation set. See Figure 22.6.
Another approach to cross-validation is K-fold cross-validation which is
obtained from the following algorithm.
K-fold cross-validation.
1. Randomly divide the data into Kchunks of approximately equal size.
A common choice is K= 10.
2 . F o rk=1t oK ,d ot h e following:
(a) Delete chunk kfrom the data.
(b) Compute the classiﬁer hatwideh(k)from the rest of the data.
(c) Use hatwideh(k)to the predict the data in chunk k. LethatwideL(k)denote
the observed error rate.
3. Let
hatwideL(h)=1
KKsummationdisplay
k=1hatwideL(k). (22.33)
22.15 Example. We applied 10-fold cross-validation to the heart disease data.
The minimum cross-validation error as a function of the number of leavesoccurred at six. Figure 22.7 shows the tree with six leaves.
/squaresolid

<<<PAGE 381>>>

22.8 Assessing Error Rates and Choosing a Good Classiﬁer 365
age
<31.5 ≥31.5
age
<50.5 ≥50.5
type A
<68.5 ≥68.5family history
no yes
tobacco
<7.605 ≥7.6050
01
011
FIGURE 22.7. Smaller classiﬁcation tree with size chosen by cross-validation.
Probability Inequalities . Another approach to estimating the error rate
is to ﬁnd a conﬁdence interval for hatwideLn(h) using probability inequalities. This
method is useful in the context of empirical risk minimization .
LetHbe a set of classiﬁers, for example, all linear classiﬁers. Empirical risk
minimization means choosing the classiﬁer hatwideh∈Hto minimize the training
errorhatwideLn(h), also called the empirical risk. Thus,
hatwideh= argminh∈HhatwideLn(h) = argminh∈HparenleftBigg
1
nsummationdisplay
iI(h(Xi)/negationslash=Yi)parenrightBigg
.(22.34)
Typically, hatwideLn(hatwideh) underestimates the true error rate L(hatwideh) because hatwidehwas chosen
to make hatwideLn(hatwideh) small. Our goal is to assess how much underestimation is taking
place. Our main tool for this analysis is Hoeﬀding’s inequality (Theorem
4.5). Recall that if X1,...,X n∼Bernoulli( p), then, for any /epsilon1>0,
P(|hatwidep−p|>/epsilon1)≤2e−2n/epsilon12(22.35)
wherehatwidep=n−1summationtextn
i=1Xi.

<<<PAGE 382>>>

366 22. Classiﬁcation
First, suppose that H={h1,...,h m}consists of ﬁnitely many classiﬁers.
For any ﬁxed h,hatwideLn(h) converges in almost surely to L(h) by the law of large
numbers. We will now establish a stronger result.
22.16 Theorem (Uniform Convergence) .Assume His ﬁnite and has mele-
ments. Then,
Pparenleftbigg
max
h∈H|hatwideLn(h)−L(h)|>/epsilon1parenrightbigg
≤2me−2n/epsilon12.
Proof. We will use Hoeﬀding’s inequality and we will also use the fact
that if A1,...,A mis a set of events then P(uniontextm
i=1Ai)≤summationtextm
i=1P(Ai). Now,
Pparenleftbigg
max
h∈H|hatwideLn(h)−L(h)|>/epsilon1parenrightbigg
=PparenleftBigguniondisplay
h∈H|hatwideLn(h)−L(h)|>/epsilon1parenrightBigg
≤summationdisplay
H∈HPparenleftBig
|hatwideLn(h)−L(h)|>/epsilon1parenrightBig
≤summationdisplay
H∈H2e−2n/epsilon12=2me−2n/epsilon12./squaresolid
22.17 Theorem. Let
/epsilon1=radicalBigg
2
nlogparenleftbigg2m
αparenrightbigg
.
ThenhatwideLn(hatwideh)±/epsilon1is a1−αconﬁdence interval for L(hatwideh).
Proof. This follows from the fact that
P(|hatwideLn(hatwideh)−L(hatwideh)|>/epsilon1)≤Pparenleftbigg
max
h∈H|hatwideLn(hatwideh)−L(hatwideh)|>/epsilon1parenrightbigg
≤2me−2n/epsilon12=α./squaresolid
When His large the conﬁdence interval for L(hatwideh) is large. The more functions
there are in Hthe more likely it is we have “overﬁt” which we compensate
for by having a larger conﬁdence interval.
In practice we usually use sets Hthat are inﬁnite, such as the set of linear
classiﬁers. To extend our analysis to these cases we want to be able to saysomething like
Pparenleftbigg
sup
h∈H|hatwideLn(h)−L(h)|>/epsilon1parenrightbigg
≤something not too big .
One way to develop such a generalization is by way of the Vapnik-Chervonenkis
orVC dimension.

<<<PAGE 383>>>

22.8 Assessing Error Rates and Choosing a Good Classiﬁer 367
LetAbe a class of sets. Give a ﬁnite set F={x1,...,x n}let
NA(F)=#braceleftbigg
Fintersectiondisplay
A:A∈Abracerightbigg
(22.36)
be the number of subsets of F“picked out” by A. Here #( B) denotes the
number of elements of a set B. Theshatter coeﬃcient is deﬁned by
s(A,n) = max
F∈FnNA(F) (22.37)
where Fnconsists of all ﬁnite sets of size n. Now let X1,...,X n∼Pand let
Pn(A)=1
nsummationdisplay
iI(Xi∈A)
denote the empirical probability measure . The following remarkable the-
orem bounds the distance between PandPn.
22.18 Theorem (Vapnik and Chervonenkis (1971)) .For any P,nand/epsilon1>0,
Pbraceleftbigg
sup
A∈A|Pn(A)−P(A)|>/epsilon1bracerightbigg
≤8s(A,n)e−n/epsilon12/32. (22.38)
The proof, though very elegant, is long and we omit it. If His a set of
classiﬁers, deﬁne Ato be the class of sets of the form {x:h(x)=1}.W e
then deﬁne s(H,n)=s(A,n).
22.19 Theorem.
Pbraceleftbigg
sup
h∈H|hatwideLn(h)−L(h)|>/epsilon1bracerightbigg
≤8s(H,n)e−n/epsilon12/32.
A1−αconﬁdence interval for L(hatwideh)ishatwideLn(hatwideh)±/epsilon1nwhere
/epsilon12
n=32
nlogparenleftbigg8s(H,n)
αparenrightbigg
.
These theorems are only useful if the shatter coeﬃcients do not grow too
quickly with n. This is where VC dimension enters.
22.20 Deﬁnition. The VC (Vapnik-Chervonenkis) dimension of a class of
setsAis deﬁned as follows. If s(A,n)=2nfor all n, setVC(A)=∞.
Otherwise, deﬁne VC(A)to be the largest kfor which s(A,n)=2k.
Thus, the VC-dimension is the size of the largest ﬁnite set Fthat can be
shattered byAmeaning that Apicks out each subset of F.I fHis a set of
classiﬁers we deﬁne VC(H)=VC(A) where Ais the class of sets of the form
{x:h(x)=1}ashvaries in H. The following theorem shows that if Ahas
ﬁnite VC-dimension, then the shatter coeﬃcients grow as a polynomial in n.

<<<PAGE 384>>>

368 22. Classiﬁcation
22.21 Theorem. IfAhas ﬁnite VC-dimension v, then
s(A,n)≤nv+1.
22.22 Example. LetA={(−∞,a];a∈R }. The Ashatters every 1-point
set{x}but it shatters no set of the form {x, y}. Therefore, VC(A)=1 . /squaresolid
22.23 Example. LetAbe the set of closed intervals on the real line. Then
Ashatters S={x, y}but it cannot shatter sets with 3 points. Consider
S={x, y, z}where x<y<z . One cannot ﬁnd an interval Asuch that
AintersectiontextS={x, z}. So,VC(A)=2 . /squaresolid
22.24 Example. LetAbe all linear half-spaces on the plane. Any 3-point
set (not all on a line) can be shattered. No 4 point set can be shattered.Consider, for example, 4 points forming a diamond. Let Tbe the left and
rightmost points. This can’t be picked out. Other conﬁgurations can also beseen to be unshatterable. So VC(A) = 3. In general, halfspaces in R
dhave
VC dimension d+1 . /squaresolid
22.25 Example. LetAbe all rectangles on the plane with sides parallel to
the axes. Any 4 point set can be shattered. Let Sb ea5p o i n t set. There is
one point that is not leftmost, rightmost, uppermost, or lowermost. Let Tbe
all points in Sexcept this point. Then Tcan’t be picked out. So VC(A)=4 .
/squaresolid
22.26 Theorem. Letxhave dimension dand let Hbe th set of linear classi-
ﬁers. The VC-dimension of Hisd+1. Hence, a 1−αconﬁdence interval for
the true error rate is hatwideL(hatwideh)±/epsilon1where
/epsilon12
n=32
nlogparenleftbigg8(nd+1+1 )
αparenrightbigg
.
22.9 Support Vector Machines
In this section we consider a class of linear classiﬁers called support vector
machines . Throughout this section, we assume that Yis binary. It will be
convenient to label the outcomes as −1 and +1 instead of 0 and 1. A linear
classiﬁer can then be written as
h(x) = signparenleftbigg
H(x)parenrightbigg

<<<PAGE 385>>>

22.9 Support Vector Machines 369
where x=(x1,...,x d),
H(x)=a0+dsummationdisplay
i=1aixi
and
sign(z)=

−1i fz<0
0i fz=0
1i fz>0.
First, suppose that the data are linearly separable , that is, there exists
a hyperplane that perfectly separates the two classes.
22.27 Lemma. The data can be separated by some hyperplane if and only if
there exists a hyperplane H(x)=a0+summationtextd
i=1aixisuch that
YiH(xi)≥1,i=1,...,n . (22.39)
Proof. Suppose the data can be separated by a hyperplane W(x)=b0+summationtextd
i=1bixi. It follows that there exists some constant csuch that Yi= 1 implies
W(Xi)≥candYi=−1 implies W(Xi)≤−c. Therefore, YiW(Xi)≥cfor
alli. LetH(x)=a0+summationtextd
i=1aixiwhere aj=bj/c. Then YiH(Xi)≥1 for all
i. The reverse direction is straightforward. /squaresolid
In the separable case, there will be many separating hyperplanes. How
should we choose one? Intuitively, it seems reasonable to choose the hyper-plane “furthest” from the data in the sense that it separates the +1s and -1sand maximizes the distance to the closest point. This hyperplane is called themaximum margin hyperplane. The margin is the distance to from the
hyperplane to the nearest point. Points on the boundary of the margin arecalledsupport vectors. See Figure 22.8.
22.28 Theorem. The hyperplane hatwideH(x)=hatwidea
0+summationtextd
i=1hatwideaixithat separates the
data and maximizes the margin is given by minimizing (1/2)summationtextd
j=1a2
jsubject
to (22.39).
It turns out that this problem can be recast as a quadratic programming
problem. Let /angbracketleftXi,Xk/angbracketright=XT
iXkdenote the inner product of XiandXk.
22.29 Theorem. LethatwideH(x)=hatwidea0+summationtextd
i=1hatwideaixidenote the optimal (largest mar-
gin) hyperplane. Then, for j=1,...,d ,
hatwideaj=nsummationdisplay
i=1hatwideαiYiXj(i)

<<<PAGE 386>>>

370 22. Classiﬁcation
/Bullet/Circle/Bullet/Circle
/Bullet/Circle/Bullet/Circle
/Bullet/Circle/Bullet/Circle
/Bullet/Circle
/Bullet/Circle/Bullet/Circle
/Bullet/Circle/Bullet/Circle/Bullet/Circle
/Bullet/Circle
/Bullet/Circle
H(x)=a0+aTx=0
FIGURE 22.8. The hyperplane H(x) has the largest margin of all hyperplanes that
separate the two classes.
where Xj(i)is the value of the covariate Xjfor the ithdata point, and hatwideα=
(hatwideα1,...,hatwideαn)is the vector that maximizes
nsummationdisplay
i=1αi−1
2nsummationdisplay
i=1nsummationdisplay
k=1αiαkYiYk/angbracketleftXi,Xk/angbracketright (22.40)
subject to
αi≥0
and
0=summationdisplay
iαiYi.
The points Xifor which hatwideα/negationslash=0are called support vectors .hatwidea0can be found
by solving
hatwideαiparenleftbigg
Yi(XT
ihatwidea+hatwideβ0parenrightbigg
=0
for any support point Xi.hatwideHmay be written as
hatwideH(x)=hatwideα0+nsummationdisplay
i=1hatwideαiYi/angbracketleftx, X i/angbracketright.
There are many software packages that will solve this problem quickly. If
there is no perfect linear classiﬁer, then one allows overlap between the groups

<<<PAGE 387>>>

22.10 Kernelization 371
by replacing the condition (22.39) with
YiH(xi)≥1−ξi,ξ i≥0,i=1,...,n . (22.41)
The variables ξ1,...,ξ nare called slack variables .
We now maximize (22.40) subject to
0≤ξi≤c, i =1,...,n
andnsummationdisplay
i=1αiYi=0.
The constant cis a tuning parameter that controls the amount of overlap.
22.10 Kernelization
There is a trick called kernelization for improving a computationally simple
classiﬁer h. The idea is to map the covariate X— which takes values in X—
into a higher dimensional space Zand apply the classiﬁer in the bigger space
Z. This can yield a more ﬂexible classiﬁer while retaining computationally
simplicity.
The standard example of this idea is illustrated in Figure 22.9. The covariate
x=(x1,x2). The Yis can be separated into two groups using an ellipse. Deﬁne
a mapping φby
z=(z1,z2,z3)=φ(x)=(x2
1,√
2x1x2,x2
2).
Thus, φmaps X=R2intoZ=R3. In the higher-dimensional space Z, the
Yi’s are separable by a linear decision boundary. In other words,
a linear classiﬁer in a higher-dimensional space corresponds to a non-
linear classiﬁer in the original space.
The point is that to get a richer set of classiﬁers we do not need to give up the
convenience of linear classiﬁers. We simply map the covariates to a higher-dimensional space. This is akin to making linear regression more ﬂexible byusing polynomials.
There is a potential drawback. If we signiﬁcantly expand the dimension
of the problem, we might increase the computational burden. For example,ifxhas dimension d= 256 and we wanted to use all fourth-order terms,
thenz=φ(x) has dimension 183,181,376. We are spared this computational

<<<PAGE 388>>>

372 22. Classiﬁcation
x1x2
++
++
+
+++++++
+++
z1z2
z3+
+
+++
+++φ
FIGURE 22.9. Kernelization. Mapping the covariates into a higher-dimensional
space can make a complicated decision boundary into a simpler decision bound-ary.
nightmare by the following two facts. First, many classiﬁers do not require
that we know the values of the individual points but, rather, just the innerproduct between pairs of points. Second, notice in our example that the innerproduct in Zcan be written
/angbracketleftz,tildewidez/angbracketright=/angbracketleftφ(x),φ(tildewidex)/angbracketright
=x
2
1tildewidex2
1+2x1tildewidex1x2tildewidex2+x2
2tildewidex2
2
=(/angbracketleftx,tildewidex/angbracketright)2≡K(x,tildewidex).
Thus, we can compute /angbracketleftz,tildewidez/angbracketrightwithout ever computing Zi=φ(Xi).
To summarize, kernelization involves ﬁnding a mapping φ:X→Z and a
classiﬁer such that:
1.Zhas higher dimension than Xand so leads a richer set of classiﬁers.
2. The classiﬁer only requires computing inner products.3. There is a function K, called a kernel, such that /angbracketleftφ(x),φ(tildewidex)/angbracketright=K(x,tildewidex).
4. Everywhere the term /angbracketleftx,tildewidex/angbracketrightappears in the algorithm, replace it with
K(x,tildewidex).

<<<PAGE 389>>>

22.10 Kernelization 373
In fact, we never need to construct the mapping φat all. We only need
to specify a kernel K(x,tildewidex) that corresponds to /angbracketleftφ(x),φ(tildewidex)/angbracketrightfor some φ. This
raises an interesting question: given a function of two variables K(x, y), does
there exist a function φ(x) such that K(x, y)=/angbracketleftφ(x),φ(y)/angbracketright? The answer is
provided by Mercer’s theorem which says, roughly, that if Kis positive
deﬁnite — meaning that
integraldisplayintegraldisplay
K(x, y)f(x)f(y)dxdy≥0
for square integrable functions f— then such a φexists. Examples of com-
monly used kernels are:
polynomial K(x,tildewidex)=parenleftbigg
/angbracketleftx,tildewidex/angbracketright+aparenrightbiggr
sigmoid K(x,tildewidex) = tanh( a/angbracketleftx,tildewidex/angbracketright+b)
Gaussian K(x,tildewidex) = expparenleftbigg
−||x−tildewidex||2/(2σ2)parenrightbigg
Let us now see how we can use this trick in LDA and in support vector
machines.
Recall that the Fisher linear discriminant method replaces XwithU=
wTXwhere wis chosen to maximize the Rayleigh coeﬃcient
J(w)=wTSBw
wTSWw,
SB=(X0−X1)(X0−X1)T
and
SW=parenleftbigg(n0−1)S0
(n0−1 )+( n1−1)parenrightbigg
+parenleftbigg(n1−1)S1
(n0−1 )+( n1−1)parenrightbigg
.
In the kernelized version, we replace XiwithZi=φ(Xi) and we ﬁnd wto
maximize
J(w)=wTtildewideSBw
wTtildewideSWw(22.42)
where
tildewideSB=(Z0−Z1)(Z0−Z1)T
and
SW=parenleftBigg
(n0−1)tildewideS0
(n0−1 )+( n1−1)parenrightBigg
+parenleftBigg
(n1−1)tildewideS1
(n0−1 )+( n1−1)parenrightBigg
.
Here,tildewideSjis the sample of covariance of the Zi’s for which Y=j. However, to
take advantage of kernelization, we need to re-express this in terms of innerproducts and then replace the inner products with kernels.

<<<PAGE 390>>>

374 22. Classiﬁcation
It can be shown that the maximizing vector wis a linear combination of
theZi’s. Hence we can write
w=nsummationdisplay
i=1αiZi.
Also,
Zj=1
njnsummationdisplay
i=1φ(Xi)I(Yi=j).
Therefore,
wTZj=parenleftbiggnsummationdisplay
i=1αiZiparenrightbiggTparenleftbigg1
njnsummationdisplay
i=1φ(Xi)I(Yi=j)parenrightbigg
=1
njnsummationdisplay
i=1nsummationdisplay
s=1αiI(Ys=j)ZT
iφ(Xs)
=1
njnsummationdisplay
i=1αinsummationdisplay
s=1I(Ys=j)φ(Xi)Tφ(Xs)
=1
njnsummationdisplay
i=1αinsummationdisplay
s=1I(Ys=j)K(Xi,Xs)
=αTMj
where Mjis a vector whose ithcomponent is
Mj(i)=1
njnsummationdisplay
s=1K(Xi,Xs)I(Yi=j).
It follows that
wTtildewideSBw=αTMα
where M=(M0−M1)(M0−M1)T. By similar calculations, we can write
wTtildewideSWw=αTNα
where
N=K0parenleftbigg
I−1
n01parenrightbigg
KT
0+K1parenleftbigg
I−1
n11parenrightbigg
KT
1,
Iis the identity matrix, 1is a matrix of all one’s, and Kjis the n×nj
matrix with entries ( Kj)rs=K(xr,xs) with xsvarying over the observations
in group j. Hence, we now ﬁnd αto maximize
J(α)=αTMα
αTNα.

<<<PAGE 391>>>

22.11 Other Classiﬁers 375
All the quantities are expressed in terms of the kernel. Formally, the solution
isα=N−1(M0−M1). However, Nmight be non-invertible. In this case one
replaces NbyN+bI, for some constant b. Finally, the projection onto the
new subspace can be written as
U=wTφ(x)=nsummationdisplay
i=1αiK(xi,x).
The support vector machine can similarly be kernelized. We simply replace
/angbracketleftXi,Xj/angbracketrightwithK(Xi,Xj). For example, instead of maximizing (22.40), we now
maximize
nsummationdisplay
i=1αi−1
2nsummationdisplay
i=1nsummationdisplay
k=1αiαkYiYkK(Xi,Xj). (22.43)
The hyperplane can be written as hatwideH(x)=hatwidea0+summationtextn
i=1hatwideαiYiK(X,X i).
22.11 Other Classiﬁers
There are many other classiﬁers and space precludes a full discussion of all of
them. Let us brieﬂy mention a few.
Thek-nearest-neighbors classiﬁer is very simple. Given a point x, ﬁnd
thekdata points closest to x. Classify xusing the majority vote of these k
neighbors. Ties can be broken randomly. The parameter kcan be chosen by
cross-validation.
Bagging is a method for reducing the variability of a classiﬁer. It is most
helpful for highly nonlinear classiﬁers such as trees. We draw Bbootstrap
samples from the data. The bthbootstrap sample yields a classiﬁer hb. The
ﬁnal classiﬁer is
hatwideh(x)=braceleftbigg
1i f1
BsummationtextB
b=1hb(x)≥1
2
0 otherwise .
Boosting is a method for starting with a simple classiﬁer and gradually
improving it by reﬁtting the data giving higher weight to misclassiﬁed samples.Suppose that His a collection of classiﬁers, for example, trees with only
one split. Assume that Y
i∈{ −1,1}and that each his such that h(x)∈
{−1,1}. We usually give equal weight to all data points in the methods we
have discussed. But one can incorporate unequal weights quite easily in mostalgorithms. For example, in constructing a tree, we could replace the impuritymeasure with a weighted impurity measure. The original version of boosting,called AdaBoost, is as follows.

<<<PAGE 392>>>

376 22. Classiﬁcation
1. Set the weights wi=1/n,i=1,...,n .
2. For j=1,...,J , do the following steps:
(a) Constructing a classiﬁer hjfrom the data using the weights w1,...,w n.
(b) Compute the weighted error estimate:
hatwideLj=summationtextn
i=1wiI(Yi/negationslash=hj(Xi))summationtextn
i=1wi.
(c) Let αj= log((1 −hatwideLj)/hatwideLj).
(d) Update the weights:
wi←−wieαjI(Yi/negationslash=hj(Xi))
3. The ﬁnal classiﬁer is
hatwideh(x) = signparenleftbiggJsummationdisplay
j=1αjhj(x)parenrightbigg
.
There is now an enormous literature trying to explain and improve on
boosting. Whereas bagging is a variance reduction technique, boosting canbe thought of as a bias reduction technique. We starting with a simple —and hence highly-biased — classiﬁer, and we gradually reduce the bias. Thedisadvantage of boosting is that the ﬁnal classiﬁer is quite complicated.
Neural Networks are regression models of the form
3
Y=β0+psummationdisplay
j=1βjσ(α0+αTX)
where σis a smooth function, often taken to be σ(v)=ev/(1 +ev). This
is really nothing more than a nonlinear regression model. Neural nets werefashionable for some time but they pose great computational diﬃculties. Inparticular, one often encounters multiple minima when trying to ﬁnd the leastsquares estimates of the parameters. Also, the number of terms pis essentially
a smoothing parameter and there is the usual problem of trying to choose p
to ﬁnd a good balance between bias and variance.
3This is the simplest version of a neural net. There are more complex versions of the model.

<<<PAGE 393>>>

22.12 Bibliographic Remarks 377
22.12 Bibliographic Remarks
The literature on classiﬁcation is vast and is growing quickly. An excellent
reference is Hastie et al. (2001). For more on the theory, see Devroye et al.(1996) and Vapnik (1998). Two recent books on kernels are Scholkopf andSmola (2002) and Herbich (2002).
22.13 Exercises
1. Prove Theorem 22.5.
2. Prove Theorem 22.7.3. Download the spam data from:
http://www-stat.stanford.edu/ ∼tibs/ElemStatLearn/index.html
The data ﬁle can also be found on the course web page. The data con-
tain 57 covariates relating to email messages. Each email message wasclassiﬁed as spam (Y=1) or not spam (Y=0). The outcome Y is the lastcolumn in the ﬁle. The goal is to predict whether an email is spam ornot.
(a) Construct classiﬁcation rules using (i) LDA, (ii) QDA, (iii) logistic
regression, and (iv) a classiﬁcation tree. For each, report the observedmisclassiﬁcation error rate and construct a 2-by-2 table of the form
hatwideh(x)=0hatwideh(x)=1
Y=0 ?? ??
Y=1 ?? ??
(b) Use 5-fold cross-validation to estimate the prediction accuracy of
LDA and logistic regression.
(c) Sometimes it helps to reduce the number of covariates. One strategy
is to compare Xifor the spam and email group. For each of the 57
covariates, test whether the mean of the covariate is the same or diﬀerentbetween the two groups. Keep the 10 covariates with the smallest p-values. Try LDA and logistic regression using only these 10 variables.

<<<PAGE 394>>>

378 22. Classiﬁcation
4. Let Abe the set of two-dimensional spheres. That is, A∈AifA=
{(x, y): (x−a)2+(y−b)2≤c2}for some a, b, c. Find the VC-dimension
ofA.
5. Classify the spam data using support vector machines. Free software for
the support vector machine is at http://svmlight.joachims.org/
6. Use VC theory to get a conﬁdence interval on the true error rate of the
LDA classiﬁer for the iris data (from the book web site).
7. Suppose that Xi∈Rand that Yi= 1 whenever |Xi|≤1 and Yi=0
whenever |Xi|>1. Show that no linear classiﬁer can perfectly classify
these data. Show that the kernelized data Zi=(Xi,X2
i) can be linearly
separated.
8. Repeat question 5 using the kernel K(x,tildewidex)=( 1+ xTtildewidex)p. Choose pby
cross-validation.
9. Apply the knearest neighbors classiﬁer to the “iris data.” Choose kby
cross-validation.
10. (Curse of Dimensionality.) Suppose that Xhas a uniform distribution
on the d-dimensional cube [ −1/2,1/2]d. LetRbe the distance from the
origin to the closest neighbor. Show that the median of Ris

parenleftBig
1−parenleftbig1
2parenrightbig1/nparenrightBig
vd(1)
1/d
where
vd(r)=rdπd/2
Γ((d/2 )+1 )
is the volume of a sphere of radius r. For what dimension ddoes the
median of Rexceed the edge of the cube when n= 100, n=1,000,
n=1 0,000? (Hastie et al. (2001), p. 22–27.)
11. Fit a tree to the data in question 3. Now apply bagging and report your
results.
12. Fit a tree that uses only one split on one variable to the data in question
3. Now apply boosting.

<<<PAGE 395>>>

22.13 Exercises 379
13. Let r(x)=P(Y=1|X=x) and let hatwider(x) be an estimate of r(x). Consider
the classiﬁer
h(x)=braceleftbigg
1i fhatwider(x)≥1/2
0 otherwise .
Assume that hatwider(x)≈N(r(x),σ2(x)) for some functions r(x) and σ2(x).
Show that, for ﬁxed x,
P(Y/negationslash=h(x))≈P(Y/negationslash=h∗(x))
+vextendsinglevextendsinglevextendsinglevextendsinglevextendsingle2r(x)−1vextendsinglevextendsinglevextendsinglevextendsinglevextendsingle×
1−Φ
signparenleftbigg
r(x)−(1/2)parenrightbigg
(
r(x)−(1/2))
σ(x)


where Φ is the standard Normal cdfandh∗is the Bayes rule. Regard
signparenleftbigg
(r(x)−(1/2))(r(x)−(1/2))parenrightbigg
as a type of bias term. Explain the
implications for the bias–variance tradeoﬀ in classiﬁcation (Friedman
(1997)).
Hint: ﬁrst show that
P(Y/negationslash=h(x)) =|2r(x)−1|P(h(x)/negationslash=h∗(x)) +P(Y/negationslash=h∗(x)).

<<<PAGE 396>>>



<<<PAGE 397>>>

23
Probability Redux: Stochastic Processes
23.1 Introduction
Most of this book has focused on iidsequences of random variables. Now we
consider sequences of dependent random variables. For example, daily tem-
peratures will form a sequence of time-ordered random variables and clearlythe temperature on one day is not independent of the temperature on theprevious day.
Astochastic process {X
t:t∈T}is a collection of random variables.
We shall sometimes write X(t) instead of Xt. The variables Xttake values in
some set Xcalled the state space . The set Tis called the index set and
for our purposes can be thought of as time. The index set can be discreteT={0,1,2,...}or continuous T=[ 0,∞) depending on the application.
23.1 Example (iidobservations) .A sequence of iidrandom variables can be
written as {X
t:t∈T}where T={1,2,3,...,}. Thus, a sequence of iid
random variables is an example of a stochastic process. /squaresolid
23.2 Example (The Weather) .LetX={sunny ,cloudy }. A typical sequence
(depending on where you live) might be
sunny ,sunny ,cloudy ,sunny ,cloudy ,cloudy ,···
This process has a discrete state space and a discrete index set. /squaresolid

<<<PAGE 398>>>

382 23. Probability Redux: Stochastic Processes
timeprice
FIGURE 23.1. Stock price over ten week period.
23.3 Example (Stock Prices) .Figure 23.1 shows the price of a ﬁctitious stock
over time. The price is monitored continuously so the index set Tis continuous.
Price is discrete but for all practical purposes we can treat it as a continuousvariable.
/squaresolid
23.4 Example (Empirical Distribution Function) .LetX1,...,X n∼Fwhere
Fis some cdfon [0,1]. Let
hatwideFn(t)=1
nnsummationdisplay
i=1I(Xi≤t)
be the empirical cdf. For any ﬁxed value t,hatwideFn(t) is a random variable. But
the whole empirical cdf
braceleftbigg
hatwideFn(t):t∈[0,1]bracerightbigg
is a stochastic process with a continuous state space and a continuous index
set./squaresolid
We end this section by recalling a basic fact. If X1,...,X nare random
variables, then we can write the joint density as
f(x1,...,x n)= f(x1)f(x2|x1)···f(xn|x1,...,x n−1)
=nproductdisplay
i=1f(xi|pasti) (23.1)

<<<PAGE 399>>>

23.2 Markov Chains 383
where pasti=(X1,...,X i−1).
23.2 Markov Chains
A Markov chain is a stochastic process for which the distribution of Xtde-
pends only on Xt−1. In this section we assume that the state space is dis-
crete, either X={1,...,N }orX={1,2,...,}and that the index set is
T={0,1,2,...}. Typically, most authors write Xninstead of Xtwhen dis-
cussing Markov chains and I will do so as well.
23.5 Deﬁnition. The process {Xn:n∈T}is aMarkov chain if
P(Xn=x|X0,...,X n−1)=P(Xn=x|Xn−1) (23.2)
for all nand for all x∈X.
For a Markov chain, equation (23.1) simpliﬁes to
f(x1,...,x n)=f(x1)f(x2|x1)f(x3|x2)···f(xn|xn−1).
A Markov chain can be represented by the following DAG:
X0 X1 X2 ··· Xn ···
Each variable has a single parent, namely, the previous observation.
The theory of Markov chains is a very rich and complex. We have to get
through many deﬁnitions before we can do anything interesting. Our goal isto answer the following questions:
1. When does a Markov chain “settle down” into some sort of equilibrium?
2. How do we estimate the parameters of a Markov chain?3. How can we construct Markov chains that converge to a given equilib-
rium distribution and why would we want to do that?
We will answer questions 1 and 2 in this chapter. We will answer question
3 in the next chapter. To understand question 1, look at the two chains inFigure 23.2. The ﬁrst chain oscillates all over the place and will continue todo so forever. The second chain eventually settles into an equilibrium. If weconstructed a histogram of the ﬁrst process, it would keep changing as we got

<<<PAGE 400>>>

384 23. Probability Redux: Stochastic Processes
more and more observations. But a histogram from the second chain would
eventually converge to some ﬁxed distribution.
time time
FIGURE 23.2. Two Markov chains. The ﬁrst chain does not settle down into an
equilibrium. The second does.
Transition Probabilities. The key quantities of a Markov chain are the
probabilities of jumping from one state into another state. A Markov chain ishomogeneous ifP(X
n+1=j|Xn=i) does not change with time. Thus, for
a homogeneous Markov chain, P(Xn+1=j|Xn=i)=P(X1=j|X0=i). We
shall only deal with homogeneous Markov chains.
23.6 Deﬁnition. We call
pij≡P(Xn+1=j|Xn=i) (23.3)
thetransition probabilities. The matrix Pwhose (i, j)element is pij
is called the transition matrix.
We will only consider homogeneous chains. Notice that Phas two proper-
ties: (i) pij≥0 and (ii)summationtext
ipij= 1. Each row can be regarded as a probability
mass function.
23.7 Example (Random Walk With Absorbing Barriers) .LetX={1,...,N }.
Suppose you are standing at one of these points. Flip a coin with P(Heads) = p
andP(Tails) = q=1−p. If it is heads, take one step to the right. If it is
tails, take one step to the left. If you hit one of the endpoints, stay there. The

<<<PAGE 401>>>

23.2 Markov Chains 385
transition matrix is
P=
1000 ···00
q0p0···00
0q0p···00
··· ··· ··· ··· ··· ··· ···0000 q0p
0000001
.
/squaresolid
23.8 Example. Suppose the state space is X={sunny ,cloudy }. Then X1,
X2,...represents the weather for a sequence of days. The weather today
clearly depends on yesterday’s weather. It might also depend on the weathertwo days ago but as a ﬁrst approximation we might assume that the depen-dence is only one day back. In that case the weather is a Markov chain and atypical transition matrix might be
Sunny Cloudy
Sunny 0.4 0.6Cloudy 0.8 0.2
For example, if it is sunny today, there is a 60 per cent chance it will be cloudy
tomorrow.
/squaresolid
Let
pij(n)=P(Xm+n=j|Xm=i) (23.4)
be the probability of of going from state i to state j in nsteps. Let Pnbe the
matrix whose ( i, j) element is pij(n). These are called the n-step transition
probabilities.
23.9 Theorem (The Chapman-Kolmogorov equations) .Then-step probabilities
satisfy
pij(m+n)=summationdisplay
kpik(m)pkj(n). (23.5)
Proof. Recall that, in general,
P(X=x, Y=y)=P(X=x)P(Y=y|X=x).
This fact is true in the more general form
P(X=x, Y=y|Z=z)=P(X=x|Z=z)P(Y=y|X=x, Z=z).
Also, recall the law of total probability:
P(X=x)=summationdisplay
yP(X=x, Y=y).

<<<PAGE 402>>>

386 23. Probability Redux: Stochastic Processes
Using these facts and the Markov property we have
pij(m+n)= P(Xm+n=j|X0=i)
=summationdisplay
kP(Xm+n=j,Xm=k|X0=i)
=summationdisplay
kP(Xm+n=j|Xm=k,X0=i)P(Xm=k|X0=i)
=summationdisplay
kP(Xm+n=j|Xm=k)P(Xm=k|X0=i)
=summationdisplay
kpik(m)pkj(n)./squaresolid
Look closely at equation (23.5). This is nothing more than the equation for
matrix multiplication. Hence we have shown that
Pm+n=PmPn. (23.6)
By deﬁnition, P1=P. Using the above theorem, P2=P1+1=P1P1=
PP=P2. Continuing this way, we see that
Pn=Pn≡P×P×···× Pbracehtipupleftbracehtipdownrightbracehtipdownleft bracehtipupright
multiply the matrix n times. (23.7)
Letµn=(µn(1),...,µ n(N)) be a row vector where
µn(i)=P(Xn=i) (23.8)
is the marginal probability that the chain is in state iat time n. In particular,
µ0is called the initial distribution. To simulate a Markov chain, all you
need to know is µ0andP. The simulation would look like this:
Step 1: Draw X0∼µ0. Thus, P(X0=i)=µ0(i).
Step 2: Denote the outcome of step 1 by i. Draw X1∼P. In other words,
P(X1=j|X0=i)=pij.
Step 3: Suppose the outcome of step 2 is j. Draw X2∼P. In other words,
P(X2=k|X1=j)=pjk.
And so on.
It might be diﬃcult to understand the meaning of µn. Imagine simulating
the chain many times. Collect all the outcomes at time nfrom all the chains.
This histogram would look approximately like µn. A consequence of theorem
23.9 is the following:

<<<PAGE 403>>>

23.2 Markov Chains 387
23.10 Lemma. The marginal probabilities are given by
µn=µ0Pn.
Proof.
µn(j)= P(Xn=j)
=summationdisplay
iP(Xn=j|X0=i)P(X0=i)
=summationdisplay
iµ0(i)pij(n)=µ0Pn./squaresolid
Summary of Terminology
1. Transition matrix: P(i, j)=P(Xn+1=j|Xn=i)=pij.
2.n-step matrix: Pn(i, j)=P(Xn+m=j|Xm=i).
3.Pn=Pn.
4. Marginal: µn(i)=P(Xn=i).
5.µn=µ0Pn.
States. The states of a Markov chain can be classiﬁed according to various
properties.
23.11 Deﬁnition. We say that ireaches j(orjisaccessible fromi)i f
pij(n)>0for some n, and we write i→j.I fi→jandj→ithen we
write i↔jand we say that iandjcommunicate.
23.12 Theorem. The communication relation satisﬁes the following proper-
ties:
1.i↔i.
2. Ifi↔jthenj↔i.
3. Ifi↔jandj↔ktheni↔k.
4. The set of states Xcan be written as a disjoint union of classes X=
X1uniontextX2uniontext···where two states iandjcommunicate with each other if
and only if they are in the same class.

<<<PAGE 404>>>

388 23. Probability Redux: Stochastic Processes
If all states communicate with each other, then the chain is called irre-
ducible . A set of states is closed if, once you enter that set of states you
never leave. A closed set consisting of a single state is called an absorbing
state.
23.13 Example. LetX={1,2,3,4}and
P=
1
32
300
2
31
300
1
41
41
41
4
0001

The classes are {1,2},{3}and{4}. State 4 is an absorbing state.
/squaresolid
Suppose we start a chain in state i. Will the chain ever return to state i?
If so, that state is called persistent or recurrent.
23.14 Deﬁnition. State iisrecurrent orpersistent if
P(Xn=ifor some n≥1|X0=i)=1.
Otherwise, state iistransient .
23.15 Theorem. A state iis recurrent if and only if
summationdisplay
npii(n)=∞. (23.9)
A state iis transient if and only if
summationdisplay
npii(n)<∞. (23.10)
Proof. Deﬁne
In=braceleftbigg1i fXn=i
0i fXn/negationslash=i.
The number of times that the chain is in state iisY=summationtext∞
n=0In. The mean
ofY, given that the chain starts in state i,i s
E(Y|X0=i)=∞summationdisplay
n=0E(In|X0=i)=∞summationdisplay
n=0P(Xn=i|X0=i)=∞summationdisplay
n=0pii(n).
Deﬁne ai=P(Xn=ifor some n≥1|X0=i). Ifiis recurrent, ai= 1. Thus,
the chain will eventually return to i. Once it does return to i, we argue again

<<<PAGE 405>>>

23.2 Markov Chains 389
that since ai= 1, the chain will return to state iagain. By repeating this
argument, we conclude that E(Y|X0=i)=∞.I fiis transient, then ai<1.
When the chain is in state i, there is a probability 1 −ai>0 that it will never
return to state i. Thus, the probability that the chain is in state iexactly n
times is an−1
i(1−ai). This is a geometric distribution which has ﬁnite mean.
/squaresolid
23.16 Theorem. Facts about recurrence.
1. If state iis recurrent and i↔j, then jis recurrent.
2. If state iis transient and i↔j, then jis transient.
3. A ﬁnite Markov chain must have at least one recurrent state.4. The states of a ﬁnite, irreducible Markov chain are all recurrent.
23.17 Theorem (Decomposition Theorem) .The state space Xcan be written
as the disjoint union
X=X
Tuniondisplay
X1uniondisplay
X2···
where XTare the transient states and each Xiis a closed, irreducible set of
recurrent states.
23.18 Example (Random Walk) .LetX={...,−2,−1,0,1,2,...,}and sup-
pose that pi,i+1=p,pi,i−1=q=1−p. All states communicate, hence either
all the states are recurrent or all are transient. To see which, suppose we startatX
0= 0. Note that
p00(2n)=parenleftbigg2n
nparenrightbigg
pnqn(23.11)
since the only way to get back to 0 is to have nheads (steps to the right) and
ntails (steps to the left). We can approximate this expression using Stirling’s
formula which says that
n!∼nn√ne−n√
2π.
Inserting this approximation into (23.11) shows that
p00(2n)∼(4pq)n
√nπ.
It is easy to check thatsummationtext
np00(n)<∞if and only ifsummationtext
np00(2n)<∞.
Moreover,summationtext
np00(2n)=∞if and only if p=q=1/2. By Theorem (23.15),
the chain is recurrent if p=1/2 otherwise it is transient. /squaresolid

<<<PAGE 406>>>

390 23. Probability Redux: Stochastic Processes
Convergence of Markov Chains. To discuss the convergence of chains,
we need a few more deﬁnitions. Suppose that X0=i. Deﬁne the recurrence
time
Tij= min {n>0:Xn=j} (23.12)
assuming Xnever returns to state i, otherwise deﬁne Tij=∞. Themean
recurrence time of a recurrent state iis
mi=E(Tii)=summationdisplay
nnfii(n) (23.13)
where
fij(n)=P(X1/negationslash=j,X2/negationslash=j,...,X n−1/negationslash=j,Xn=j|X0=i).
A recurrent state is nullifmi=∞otherwise it is called non-null orposi-
tive.
23.19 Lemma. If a state is null and recurrent, then pn
ii→0.
23.20 Lemma. In a ﬁnite state Markov chain, all recurrent states are positive.
Consider a three-state chain with transition matrix

010
001100
.
Suppose we start the chain in state 1. Then we will be in state 3 at times 3, 6,
9 ,.... This is an example of a periodic chain. Formally, the period of state i
isdifp
ii(n) = 0 whenever nis not divisible by danddis the largest integer
with this property. Thus, d= gcd {n:pii(n)>0}where gcd means “greater
common divisor.” State iisperiodic ifd(i)>1 andaperiodic ifd(i)=1 .
A state with period 1 is called aperiodic .
23.21 Lemma. If state ihas period dandi↔jthenjhas period d.
23.22 Deﬁnition. A state is ergodic if it is recurrent, non-null and
aperiodic. A chain is ergodic if all its states are ergodic.
Letπ=(πi:i∈X) be a vector of non-negative numbers that sum to one.
Thus πcan be thought of as a probability mass function.
23.23 Deﬁnition. We say that πis astationary (orinvariant )
distribution if π=πP.

<<<PAGE 407>>>

23.2 Markov Chains 391
Here is the intuition. Draw X0from distribution πand suppose that πis a
stationary distribution. Now draw X1according to the transition probability
of the chain. The distribution of X1is then µ1=µ0P=πP=π. The
distribution of X2isπP2=(πP)P=πP=π. Continuing this way, we see
that the distribution of XnisπPn=π. In other words:
If at any time the chain has distribution π, then it will continue to
have distribution πforever.
23.24 Deﬁnition. We say that a chain has limiting distribution if
Pn→
π
π
...
π

for some π, that is, π
j= lim n→∞Pn
ijexists and is independent of i.
Here is the main theorem about convergence. The theorem says that an
ergodic chain converges to its stationary distribution. Also, sample averagesconverge to their theoretical expectations under the stationary distribution.
23.25 Theorem. An irreducible, ergodic Markov chain has a unique
stationary distribution π. The limiting distribution exists and is equal to
π.I fgis any bounded function, then, with probability 1,
lim
N→∞1
NNsummationdisplay
n=1g(Xn)→Eπ(g)≡summationdisplay
jg(j)πj. (23.14)
Finally, there is another deﬁnition that will be useful later. We say that π
satisﬁes detailed balance if
πipij=pjiπj. (23.15)
Detailed balance guarantees that πis a stationary distribution.
23.26 Theorem. Ifπsatisﬁes detailed balance, then πis a stationary distri-
bution.
Proof. We need to show that πP=π. The jthelement of πPissummationtext
iπipij=summationtext
iπjpji=πjsummationtext
ipji=πj./squaresolid

<<<PAGE 408>>>

392 23. Probability Redux: Stochastic Processes
The importance of detailed balance will become clear when we discuss
Markov chain Monte Carlo methods in Chapter 24.
Warning! Just because a chain has a stationary distribution does not mean
it converges.
23.27 Example. Let
P=
010
001100
.
Letπ=( 1/3,1/3,1/3). Then πP=πsoπis a stationary distribution. If
the chain is started with the distribution πit will stay in that distribution.
Imagine simulating many chains and checking the marginal distribution ateach time n. It will always be the uniform distribution π. But this chain does
not have a limit. It continues to cycle around forever.
/squaresolid
Examples of Markov Chains.
23.28 Example. LetX={1,2,3,4,5,6}. Let
P=
1
21
20000
1
43
40000
1
41
41
41
400
1
401
41
401
4
00001
21
2
00001
21
2

Then C
1={1,2}andC2={5,6}are irreducible closed sets. States 3 and
4 are transient because of the path 3 →4→6 and once you hit state 6
you cannot return to 3 or 4. Since pii(1)>0, all the states are aperiodic. In
summary, 3 and 4 are transient while 1, 2, 5, and 6 are ergodic. /squaresolid
23.29 Example (Hardy-Weinberg) .Here is a famous example from genetics.
Suppose a gene can be type Aor type a. There are three types of people (called
genotypes): AA, Aa, and aa. Let ( p, q, r) denote the fraction of people of each
genotype. We assume that everyone contributes one of their two copies of thegene at random to their children. We also assume that mates are selected atrandom. The latter is not realistic however, it is often reasonable to assumethat you do not choose your mate based on whether they are AA, Aa, oraa. (This would be false if the gene was for eye color and if people chosemates based on eye color.) Imagine if we pooled everyone’s genes together.The proportion of Agenes is P=p+(q/2) and the proportion of agenes is

<<<PAGE 409>>>

23.2 Markov Chains 393
Q=r+(q/2). A child is AA with probability P2, aA with probability 2 PQ,
and aa with probability Q2. Thus, the fraction of Agenes in this generation
is
P2+PQ=parenleftBig
p+q
2parenrightBig2
+parenleftBig
p+q
2parenrightBigparenleftBig
r+q
2parenrightBig
.
However, r=1−p−q. Substitute this in the above equation and you get
P2+PQ=P. A similar calculation shows that the fraction of “a” genes is
Q. We have shown that the proportion of type A and type a is PandQand
this remains stable after the ﬁrst generation. The proportion of people of typeAA, Aa, aa is thus ( P
2,2PQ,Q2) from the second generation and on. This is
called the Hardy-Weinberg law.
Assume everyone has exactly one child. Now consider a ﬁxed person and
letXnbe the genotype of their nthdescendant. This is a Markov chain with
state space X={AA, Aa, aa }. Some basic calculations will show you that the
transition matrix is
PQ 0
P
2P+Q
2Q
2
0PQ
.
The stationary distribution is π=(P2,2PQ,Q2)./squaresolid
23.30 Example (Markov chain Monte Carlo) .In Chapter 24 we will present a
simulation method called Markov chain Monte Carlo (MCMC). Here is a briefdescription of the idea. Let f(x) be a probability density on the real line and
suppose that f(x)=cg(x) where g(x) is a known function and c>0i s
unknown. In principle, we can compute csinceintegraltext
f(x)dx= 1 implies that
c=1/integraltext
g(x)dx. However, it may not be feasible to perform this integral, nor
is it necessary to know cin the following algorithm. Let X
0be an arbitrary
starting value. Given X0,..., X i, draw Xi+1as follows. First, draw W∼
N(Xi,b2) where b>0 is some ﬁxed constant. Let
r= minbraceleftbiggg(W)
g(Xi),1bracerightbigg
.
Draw U∼Uniform(0 ,1) and set
Xi+1=braceleftbiggWifU<r
XiifU≥r.
We will see in Chapter 24 that, under weak conditions, X0,X1,..., is an
ergodic Markov chain with stationary distribution f. Hence, we can regard
the draws as a sample from f./squaresolid

<<<PAGE 410>>>

394 23. Probability Redux: Stochastic Processes
Inference for Markov Chains. Consider a chain with ﬁnite state space
X={1,2,...,N }. Suppose we observe nobservations X1,...,X nfrom this
chain. The unknown parameters of a Markov chain are the initial probabilitiesµ
0=(µ0(1),µ0(2),...,) and the elements of the transition matrix P. Each
row of Pis a multinomial distribution. So we are essentially estimating N
distributions (plus the initial probabilities). Let nijbe the observed number
of transitions from state ito state j. The likelihood function is
L(µ0,P)=µ0(x0)nproductdisplay
r=1pXr−1,Xr=µ0(x0)Nproductdisplay
i=1Nproductdisplay
j=1pnij
ij.
There is only one observation on µ0so we can’t estimate that. Rather, we
focus on estimating P. The mleis obtained by maximizing L(µ0,P) subject
to the constraint that the elements are non-negative and the rows sum to 1.The solution is
hatwidep
ij=nij
ni
where ni=summationtextN
j=1nij. Here we are assuming that ni>0. If not, then we set
hatwidepij= 0 by convention.
23.31 Theorem (Consistency and Asymptotic Normality of the mle).Assume that
the chain is ergodic. Let hatwidepij(n)denote the mleafternobservations. Then
hatwidepij(n)P−→pij. Also,
bracketleftBigradicalbig
Ni(n)(hatwidepij−pij)bracketrightBig
/squigglerightN(0,Σ)
where the left-hand side is a matrix, Ni(n)=summationtextn
r=1I(Xr=i)and
Σij,k/lscript=

pij(1−pij)(i, j)=(k,/lscript)
−pijpi/lscript i=k,j/negationslash=/lscript
0 otherwise .
23.3 Poisson Processes
The Poisson process arises when we count occurrences of events over time, for
example, traﬃc accidents, radioactive decay, arrival of email messages, etc.As the name suggests, the Poisson process is intimately related to the Poissondistribution. Let’s ﬁrst review the Poisson distribution.
Recall that Xhas a Poisson distribution with parameter λ— written X∼
Poisson( λ)—i f
P(X=x)≡p(x;λ)=e
−λλx
x!,x=0,1,2,...

<<<PAGE 411>>>

23.3 Poisson Processes 395
Also recall that E(X)=λandV(X)=λ.I fX∼Poisson( λ),Y∼Poisson( ν)
andX/coproductY, then X+Y∼Poisson( λ+ν). Finally, if N∼Poisson( λ) andY|N=
n∼Binomial( n, p), then the marginal distribution of YisY∼Poisson( λp).
Now we describe the Poisson process. Imagine that you are at your com-
puter. Each time a new email message arrives you record the time. Let Xtbe
the number of messages you have received up to and including time t. Then,
{Xt:t∈[0,∞)}is a stochastic process with state space X={0,1,2,...}.
A process of this form is called a counting process . A Poisson process is
a counting process that satisﬁes certain conditions. In what follows, we willsometimes write X(t) instead of X
t. Also, we need the following notation.
Write f(h)=o(h)i ff(h)/h→0a sh→0. This means that f(h) is smaller
thanhwhen his close to 0. For example, h2=o(h).
23.32 Deﬁnition. APoisson process is a stochastic p rocess
{Xt:t∈[0,∞)}with state space X={0,1,2,...}such that
1.X( 0 )=0 .
2. For any 0=t0<t1<t2<···<tn, the increments
X(t1)−X(t0),X(t2)−X(t1),···,X(tn)−X(tn−1)
are independent.
3. There is a function λ(t)such that
P(X(t+h)−X(t)=1 ) = λ(t)h+o(h) (23.16)
P(X(t+h)−X(t)≥2) = o(h). (23.17)
We call λ(t)theintensity function .
The last condition means that the probability of an event in [ t, t+h]i s
approximately hλ(t) while the probability of more than one event is small.
23.33 Theorem. IfXtis a Poisson p rocess with intensity function λ(t), then
X(s+t)−X(s)∼Poisson( m(s+t)−m(s))
where
m(t)=integraldisplayt
0λ(s)ds.
In particular, X(t)∼Poisson( m(t)). Hence, E(X(t)) =m(t)andV(X(t)) =
m(t).

<<<PAGE 412>>>

396 23. Probability Redux: Stochastic Processes
23.34 Deﬁnition. A Poisson p rocess with intensity function λ(t)≡λfor
some λ>0is called a homogeneous Poisson process with rate λ.I n
this case,
X(t)∼Poisson( λt).
LetX(t) be a homogeneous Poisson process with rate λ. Let Wnbe the
time at which the nthevent occurs and set W0= 0. The random variables
W0,W1,...,are called waiting times . LetSn=Wn+1−Wn. Then S0,S1,...,
are called sojourn times orinterarrival times .
23.35 Theorem. The sojourn times S0,S1,...areiidrandom variables. Their
distribution is exponential with mean 1/λ, that is, they have density
f(s)=λe−λs,s≥0.
The waiting time Wn∼Gamma( n,1/λ)i.e., it has density
f(w)=1
Γ(n)λnwn−1e−λt.
Hence, E(Wn)=n/λandV(Wn)=n/λ2.
Proof. First, we have
P(S1>t)=P(X(t)=0 )= e−λt
with shows that the cdfforS1is 1−e−λt. This shows the result for S1.N o w ,
P(S2>t|S1=s)= P(no events in ( s, s+t]|S1=s)
=P(no events in ( s, s+t]) (increments are independent)
=e−λt.
Hence, S2has an exponential distribution and is independent of S1. The result
follows by repeating the argument. The result for Wnfollows since a sum of
exponentials has a Gamma distribution. /squaresolid
23.36 Example. Figure 23.3 shows requests to a WWW server in Calgary.1
Assuming that this is a homogeneous Poisson process, N≡X(T)∼Poisson( λT).
The likelihood is
L(λ)∝e−λT(λT)N
1See http://ita.ee.lbl.gov/html/contrib/Calgary-HTTP.html for more information.

<<<PAGE 413>>>

23.4 Bibliographic Remarks 397
0 400 800 1200time
FIGURE 23.3. Hits on a web server. Each vertical line represents one event.
which is maximized at
hatwideλ=N
T=4 8.0077
in units per minute. Let’s now test the assumption that the data follow a ho-
mogeneous Poisson process using a goodness-of-ﬁt test. We divide the interval[0,T] into 4 equal length intervals I
1,I2,I3,I4. If the process is a homogeneous
Poisson process then, given the total number of events, the probability that anevent falls into any of these intervals must be equal. Let p
ibe the probability
of a point being in Ii. The null hypothesis is that p1=p2=p3=p4=1/4.
We can test this hypothesis using either a likelihood ratio test or a χ2test.
The latter is
4summationdisplay
i=1(Oi−Ei)2
Ei
where Oiis the number of observations in IiandEi=n/4 is the expected
number under the null. This yields χ2= 252 with a p-value near 0. This is
strong evidence against the null so we reject the hypothesis that the data arefrom a homogeneous Poisson process. This is hardly surprising since we wouldexpect the intensity to vary as a function of time.
/squaresolid
23.4 Bibliographic Remarks
This is standard material and there are many good references including Grim-mett and Stirzaker (1982), Taylor and Karlin (1994), Guttorp (1995), andRoss (2002). The following exercises are from those texts.

<<<PAGE 414>>>

398 23. Probability Redux: Stochastic Processes
23.5 Exercises
1. Let X0,X1,...be a Markov chain with states {0,1,2}and transition
matrix
P=
0.10.20.7
0.90.10.0
0.10.80.1

Assume that µ0=( 0.3,0.4,0.3). Find P(X0=0,X1=1,X2= 2) and
P(X0=0,X1=1,X2= 1).
2. Let Y1,Y2,...be a sequence of iid observations such that P(Y=0 )=
0.1,P(Y=1 )=0 .3,P(Y=2 )=0 .2,P(Y=3 )=0 .4. Let X0= 0 and
let
Xn= max {Y1,...,Y n}.
Show that X0,X1,...is a Markov chain and ﬁnd the transition matrix.
3. Consider a two-state Markov chain with states X={1,2}and transition
matrix
P=bracketleftbigg1−aa
b1−bbracketrightbigg
where 0 <a< 1 and 0 <b< 1. Prove that
lim
n→∞Pn=bracketleftbiggb
a+ba
a+b
b
a+ba
a+bbracketrightbigg
.
4. Consider the chain from question 3 and set a=.1 and b=.3. Simulate
the chain. Let
hatwidepn(1) =1
nnsummationdisplay
i=1I(Xi=1 )
hatwidepn(2) =1
nnsummationdisplay
i=1I(Xi=2 )
be the proportion of times the chain is in state 1 and state 2. Plot hatwidepn(1)
andhatwidepn(2) versus nand verify that they converge to the values predicted
from the answer in the previous question.
5. An important Markov chain is the branching process which is used in
biology, genetics, nuclear physics, and many other ﬁelds. Suppose thatan animal has Ychildren. Let p
k=P(Y=k). Hence, pk≥0 for all
kandsummationtext∞
k=0pk= 1. Assume each animal has the same lifespan and

<<<PAGE 415>>>

23.5 Exercises 399
that they produce oﬀspring according to the distribution pk. LetXnbe
the number of animals in the nthgeneration. Let Y(n)
1,...,Y(n)
Xnbe the
oﬀspring produced in the nthgeneration. Note that
Xn+1=Y(n)
1+···+Y(n)
Xn.
Letµ=E(Y) and σ2=V(Y). Assume throughout this question that
X0= 1. Let M(n)=E(Xn) and V(n)=V(Xn).
(a) Show that M(n+1 )= µM(n) and V(n+1 )= σ2M(n)+µ2V(n).
(b) Show that M(n)=µnand that V(n)=σ2µn−1(1+µ+···+µn−1).
(c) What happens to the variance if µ>1? What happens to the vari-
ance if µ= 1? What happens to the variance if µ<1?
(d) The population goes extinct if Xn= 0 for some n. Let us thus deﬁne
the extinction time Nby
N= min {n:Xn=0}.
LetF(n)=P(N≤n)b et h e cdfof the random variable N. Show that
F(n)=∞summationdisplay
k=0pk(F(n−1))k,n=1,2,...
Hint: Note that the event {N≤n}is the same as event {Xn=0}.
Thus, P({N≤n})=P({Xn=0}). Let kbe the number of oﬀspring
of the original parent. The population becomes extinct at time nif and
only if each of the ksub-populations generated from the koﬀspring goes
extinct in n−1 generations.
(e) Suppose that p0=1/4,p1=1/2,p2=1/4. Use the formula from
(5d) to compute the cdfF(n).
6. Let
P=
0.40 0 .50 0 .10
0.05 0 .70 0 .25
0.05 0 .50 0 .45

Find the stationary distribution π.
7. Show that if iis a recurrent state and i↔j, then jis a recurrent state.

<<<PAGE 416>>>

400 23. Probability Redux: Stochastic Processes
8. Let
P=
1
301
3001
31
21
41
4000
000010
1
41
41
4001
4
001000
000001

Which states are transient? Which states are recurrent?
9. Let
P=bracketleftbigg
01
10bracketrightbigg
Show that π=( 1/2,1/2) is a stationary distribution. Does this chain
converge? Why/why not?
10. Let 0 <p< 1 and q=1−p. Let
P=
qp 000
q0p00
q00 p0
q000 p
10000

Find the limiting distribution of the chain.
11. Let X(t) be an inhomogeneous Poisson process with intensity function
λ(t)>0. Let Λ( t)=integraltext
t
0λ(u)du. Deﬁne Y(s)=X(t) where s=Λ (t).
Show that Y(s) is a homogeneous Poisson process with intensity λ=1 .
12. Let X(t) be a Poisson process with intensity λ. Find the conditional
distribution of X(t) given that X(t+s)=n.
13. Let X(t) be a Poisson process with intensity λ. Find the probability
thatX(t) is odd, i.e. P(X(t)=1,3,5,...).
14. Suppose that people logging in to the University computer system is
described by a Poisson process X(t) with intensity λ. Assume that a
person stays logged in for some random time with cdfG. Assume these
times are all independent. Let Y(t) be the number of people on the
system at time t. Find the distribution of Y(t).
15. Let X(t) be a Poisson process with intensity λ. LetW1,W2,...,be the
waiting times. Let fbe an arbitrary function. Show that
E
X(t)summationdisplay
i=1f(Wi)
=λintegraldisplayt
0f(w)dw.

<<<PAGE 417>>>

23.5 Exercises 401
16. A two-dimensional Poisson point process is a process of random points
on the plane such that (i) for any set A, the number of points falling
inAis Poisson with mean λµ(A) where µ(A) is the area of A, (ii) the
number of events in non-overlapping regions is independent. Consideran arbitrary point x
0in the plane. Let Xdenote the distance from x0
to the nearest random point. Show that
P(X>t )=e−λπt2
and
E(X)=1
2√
λ.

<<<PAGE 418>>>



<<<PAGE 419>>>

24
Simulation Methods
In this chapter we will show how simulation can be used to approximate inte-
grals. Our leading example is the problem of computing integrals in Bayesianinference but the techniques are widely applicable. We will look at three inte-gration methods: (i) basic Monte Carlo integration, (ii) importance sampling,and (iii) Markov chain Monte Carlo (MCMC).
24.1 Bayesian Inference Revisited
Simulation methods are especially useful in Bayesian inference so let us brieﬂyreview the main ideas in Bayesian inference. See Chapter 11 for more details.
Given a prior f(θ) and data X
n=(X1,...,X n) the posterior density is
f(θ|Xn)=L(θ)f(θ)
c
where L(θ) is the likelihood function and
c=integraldisplay
L(θ)f(θ)dθ
is thenormalizing constant . The posterior mean is
θ=integraldisplay
θf(θ|Xn)dθ=integraltext
θL(θ)f(θ)dθ
c.

<<<PAGE 420>>>

404 24. Simulation Methods
Ifθ=(θ1,...,θ k) is multidimensional, then we might be interested in the
posterior for one of the components, θ1, say. This marginal posterior density
is
f(θ1|Xn)=integraldisplayintegraldisplay
···integraldisplay
f(θ1,...,θ k|Xn)dθ2···dθk
which involves high-dimensional integration.
When θis high-dimensional, it may not be feasible to calculate these inte-
grals analytically. Simulation methods will often be helpful.
24.2 Basic Monte Carlo Integration
Suppose we want to evaluate the integral
I=integraldisplayb
ah(x)dx
for some function h.I fhis an “easy” function like a polynomial or trigono-
metric function, then we can do the integral in closed form. If his complicated
there may be no known closed form expression for I. There are many numer-
ical techniques for evaluating Isuch as Simpson’s rule, the trapezoidal rule
and Gaussian quadrature. Monte Carlo integration is another approach forapproximating Iwhich is notable for its simplicity, generality and scalability.
Let us begin by writing
I=integraldisplay
b
ah(x)dx=integraldisplayb
aw(x)f(x)dx (24.1)
where w(x)=h(x)(b−a) andf(x)=1/(b−a). Notice that fis the probability
density for a uniform random variable over ( a, b). Hence,
I=Ef(w(X))
where X∼Unif(a, b). If we generate X1,...,X N∼Unif(a, b), then by the
law of large numbers
hatwideI≡1
NNsummationdisplay
i=1w(Xi)P−→E(w(X)) =I. (24.2)
This is the basic Monte Carlo integration method . We can also compute
the standard error of the estimate
hatwidese=s√
N

<<<PAGE 421>>>

24.2 Basic Monte Carlo Integration 405
where
s2=summationtextN
i=1(Yi−hatwideI)2
N−1
where Yi=w(Xi). A 1 −αconﬁdence interval for IishatwideI±zα/2hatwidese. We can take
Nas large as we want and hence make the length of the conﬁdence interval
very small.
24.1 Example. Leth(x)=x3. Then, I=integraltext1
0x3dx=1/4. Based on N=
10,000 observations from a Uniform(0 ,1) we get hatwideI=.248 with a standard
error of .0028. /squaresolid
A generalization of the basic method is to consider integrals of the form
I=integraldisplay
h(x)f(x)dx (24.3)
where f(x) is a probability density function. Taking fto be a Uniform (a,b)
gives us the special case above. Now we draw X1,...,X N∼fand take
hatwideI≡1
NNsummationdisplay
i=1h(Xi)
as before.
24.2 Example. Let
f(x)=1√
2πe−x2/2
be the standard Normal pdf. Suppose we want to compute the cdfat some
point x:
I=integraldisplayx
−∞f(s)ds=Φ (x).
Write
I=integraldisplay
h(s)f(s)ds
where
h(s)=braceleftbigg1s<x
0s≥x.
Now we generate X1,...,X N∼N(0,1) and set
hatwideI=1
Nsummationdisplay
ih(Xi)=number of observations ≤x
N.
For example, with x= 2, the true answer is Φ(2) = .9772 and the Monte
Carlo estimate with N=1 0,000 yields .9751. Using N= 100 ,000 we get
.9771. /squaresolid

<<<PAGE 422>>>

406 24. Simulation Methods
24.3 Example (Bayesian Inference for Two Binomials) .LetX∼Binomial( n, p1)
andY∼Binomial( m, p2). We would like to estimate δ=p2−p1. The mle
ishatwideδ=hatwidep2−hatwidep1=(Y/m)−(X/n). We can get the standard error hatwideseusing the
delta method which yields
hatwidese=radicalbigg
hatwidep1(1−hatwidep1)
n+hatwidep2(1−hatwidep2)
m
and then construct a 95 percent conﬁdence interval hatwideδ±2hatwidese. Now consider a
Bayesian analysis. Suppose we use the prior f(p1,p2)=f(p1)f(p2) = 1, that
is, a ﬂat prior on ( p1,p2). The posterior is
f(p1,p2|X,Y)∝pX
1(1−p1)n−XpY
2(1−p2)m−Y.
The posterior mean of δis
δ=integraldisplay1
0integraldisplay1
0δ(p1,p2)f(p1,p2|X,Y)=integraldisplay1
0integraldisplay1
0(p2−p1)f(p1,p2|X,Y).
If we want the posterior density of δwe can ﬁrst get the posterior cdf
F(c|X,Y)=P(δ≤c|X,Y)=integraldisplay
Af(p1,p2|X,Y)
where A={(p1,p2):p2−p1≤c}. The density can then be obtained by
diﬀerentiating F.
To avoid all these integrals, let’s use simulation. Note that f(p1,p2|X,Y)=
f(p1|X)f(p2|Y) which implies that p1andp2are independent under the pos-
terior distribution. Also, we see that p1|X∼Beta(X+1,n−X+1) and p2|Y∼
Beta(Y+1,m−Y+1). Hence, we can simulate ( P(1)
1,P(1)
2),...,(P(N)
1,P(N)
2)
from the posterior by drawing
P(i)
1∼Beta(X+1,n−X+1 )
P(i)
2∼Beta(Y+1,m−Y+1 )
fori=1,...,N . Now let δ(i)=P(i)
2−P(i)
1. Then,
δ≈1
Nsummationdisplay
iδ(i).
We can also get a 95 percent posterior interval for δby sorting the simulated
values, and ﬁnding the .025 and .975 quantile. The posterior density f(δ|X,Y)
can be obtained by applying density estimation techniques to δ(1),...,δ(N)
or, simply by plotting a histogram. For example, suppose that n=m= 10,

<<<PAGE 423>>>

24.2 Basic Monte Carlo Integration 407
−0.6 0.0 0.6
FIGURE 24.1. Posterior of δfrom simulation.
X= 8 and Y= 6. From a posterior sample of size 1000 we get a 95 percent
posterior interval of (-0.52,0.20). The posterior density can be estimated froma histogram of the simulated values as shown in Figure 24.1.
/squaresolid
24.4 Example (Bayesian Inference for Dose Response) .Suppose we conduct an
experiment by giving rats one of ten possible doses of a drug, denoted byx
1<x2< ... < x 10. For each dose level xiwe use nrats and we observe
Yi, the number that survive. Thus we have ten independent binomials Yi∼
Binomial( n, pi). Suppose we know from biological considerations that higher
doses should have higher probability of death. Thus, p1≤p2≤···≤ p10.W e
want to estimate the dose at which the animals have a 50 percent chance ofdying. This is called the LD50. Formally, δ=x
jwhere
j= min {i:pi≥.50}.
Notice that δis implicitly a (complicated) function of p1,...,p 10so we can
writeδ=g(p1,...,p 10) for some g. This just means that if we know ( p1,...,p 10)
then we can ﬁnd δ. The posterior mean of δis
integraldisplayintegraldisplay
···integraldisplay
Ag(p1,...,p 10)f(p1,...,p 10|Y1,...,Y 10)dp1dp2...d p 10.
The integral is over the region
A={(p1,...,p 10):p1≤···≤ p10}.
The posterior cdfofδis
F(c|Y1,...,Y 10)= P(δ≤c|Y1,...,Y 10)
=integraldisplayintegraldisplay
···integraldisplay
Bf(p1,...,p 10|Y1,...,Y 10)dp1dp2...d p 10

<<<PAGE 424>>>

408 24. Simulation Methods
where
B=AintersectiondisplaybraceleftBigg
(p1,...,p 10):g(p1,...,p 10)≤cbracerightBigg
.
We need to do a 10-dimensional integral over a restricted region A. Instead,
we will use simulation. Let us take a ﬂat prior truncated over A. Except for
the truncation, each Pihas once again a Beta distribution. To draw from the
posterior we do the following steps:
(1) Draw Pi∼Beta(Yi+1,n−Yi+1 ),i=1,...,10.
(2) If P1≤P2≤ ··· ≤ P10keep this draw. Otherwise, throw it away and
draw again until you get one you can keep.
(3) Let δ=xjwhere
j= min {i:Pi>.50}.
We repeat this Ntimes to get δ(1),...,δ(N)and take
E(δ|Y1,...,Y 10)≈1
Nsummationdisplay
iδ(i).
δis a discrete variable. We can estimate its probability mass function by
P(δ=xj|Y1,...,Y 10)≈1
NNsummationdisplay
i=1I(δ(i)=j).
For example, consider the following data:
Dose 123456789 1 0
Number of animals ni15 15 15 15 15 15 15 15 15 15
Number of survivors Yi00228 1 0 1 2 1 4 1 5 1 4
The posterior draws for p1,...,p 10are shown in the second panel in the
ﬁgure. We ﬁnd that that δ=4.04 with a 95 percent interval of (3,5). /squaresolid
24.3 Importance Sampling
Consider again the integral I=integraltext
h(x)f(x)dxwhere fis a probability density.
The basic Monte Carlo method involves sampling from f. However, there
are cases where we may not know how to sample from f. For example, in
Bayesian inference, the posterior density density is is obtained by multiplyingthe likelihood L(θ) times the prior f(θ). There is no guarantee that f(θ|x)
will be a known distribution like a Normal or Gamma or whatever.

<<<PAGE 425>>>

24.3 Importance Sampling 409
Importance sampling is a generalization of basic Monte Carlo which over-
comes this problem. Let gbe a probability density that we know how to
simulate from. Then
I=integraldisplay
h(x)f(x)dx=integraldisplayh(x)f(x)
g(x)g(x)dx=Eg(Y) (24.4)
where Y=h(X)f(X)/g(X) and the expectation Eg(Y) is with respect to g.
We can simulate X1,...,X N∼gand estimate Iby
hatwideI=1
Nsummationdisplay
iYi=1
Nsummationdisplay
ih(Xi)f(Xi)
g(Xi). (24.5)
This is called importance sampling. By the law of large numbers, hatwideIP−→I.
However, there is a catch. It’s possible that hatwideImight have an inﬁnite standard
error. To see why, recall that Iis the mean of w(x)=h(x)f(x)/g(x). The
second moment of this quantity is
Eg(w2(X)) =integraldisplayparenleftbiggh(x)f(x)
g(x)parenrightbigg2
g(x)dx=integraldisplayh2(x)f2(x)
g(x)dx. (24.6)
Ifghas thinner tails than f, then this integral might be inﬁnite. To avoid this,
a basic rule in importance sampling is to sample from a density gwith thicker
tails than f. Also, suppose that g(x) is small over some set Awhere f(x)i s
large. Again, the ratio of f/gcould be large leading to a large variance. This
implies that we should choose gto be similar in shape to f. In summary, a
good choice for an importance sampling density gshould be similar to fbut
with thicker tails. In fact, we can say what the optimal choice of gis.
24.5 Theorem. The choice of gthat minimizes the variance of hatwideIis
g∗(x)=|h(x)|f(x)integraltext
|h(s)|f(s)ds.
Proof. The variance of w=fh/g is
Eg(w2)−(E(w2))2=integraldisplay
w2(x)g(x)dx−parenleftbiggintegraldisplay
w(x)g(x)dxparenrightbigg2
=integraldisplayh2(x)f2(x)
g2(x)g(x)dx−parenleftbiggintegraldisplayh(x)f(x)
g(x)g(x)dxparenrightbigg2
=integraldisplayh2(x)f2(x)
g2(x)g(x)dx−parenleftbiggintegraldisplay
h(x)f(x)dxparenrightbigg2
.

<<<PAGE 426>>>

410 24. Simulation Methods
The second integral does not depend on g, so we only need to minimize the
ﬁrst integral. From Jensen’s inequality (Theorem 4.9) we have
Eg(W2)≥(Eg(|W|))2=parenleftbiggintegraldisplay
|h(x)|f(x)dxparenrightbigg2
.
This establishes a lower bound on Eg(W2). However, Eg∗(W2) equals this
lower bound which proves the claim. /squaresolid
This theorem is interesting but it is only of theoretical interest. If we did
not know how to sample from fthen it is unlikely that we could sample from
|h(x)|f(x)/integraltext
|h(s)|f(s)ds. In practice, we simply try to ﬁnd a thick-tailed
distribution gwhich is similar to f|h|.
24.6 Example (Tail Probability) .Let’s estimate I=P(Z>3) =.0013 where
Z∼N(0,1). Write I=integraltext
h(x)f(x)dxwhere f(x) is the standard Normal
density and h(x)=1i f x>3, and 0 otherwise. The basic Monte Carlo
estimator is hatwideI=N−1summationtext
ih(Xi) where X1,...,X N∼N(0,1). Using N= 100
we ﬁnd (from simulating many times) that E(hatwideI)=.0015 and V(hatwideI)=.0039.
Notice that most observations are wasted in the sense that most are not nearthe right tail. Now we will estimate this with importance sampling taking g
to be a Normal(4,1) density. We draw values from gand the estimate is now
hatwideI=N
−1summationtext
if(Xi)h(Xi)/g(Xi). In this case we ﬁnd that E(hatwideI)=.0011 and
V(hatwideI)=.0002.We have reduced the standard deviation by a factor of 20. /squaresolid
24.7 Example (Measurement Model With Outliers) .Suppose we have measure-
ments X1,...,X nof some physical quantity θ. A reasonable model is
Xi=θ+/epsilon1i.
If we assume that /epsilon1i∼N(0,1) then Xi∼N(θi,1). However, when taking
measurements, it is often the case that we get the occasional wild observation,or outlier. This suggests that a Normal might be a poor model since Normalshave thin tails which implies that extreme observations are rare. One way toimprove the model is to use a density for /epsilon1
iwith a thicker tail, for example,
at-distribution with νdegrees of freedom which has the form
t(x)=Γparenleftbigν+1
2parenrightbig
Γparenleftbigν
2parenrightbig1
νπparenleftbigg
1+x2
νparenrightbigg−(ν+1)/2
.
Smaller values of νcorrespond to thicker tails. For the sake of illustration we
will take ν= 3. Suppose we observe nX i=θ+/epsilon1i,i=1,...,n where /epsilon1ihas

<<<PAGE 427>>>

24.4 MCMC Part I: The Metropolis–Hastings Algorithm 411
atdistribution with ν= 3. We will take a ﬂat prior on θ. The likelihood is
L(θ)=producttextn
i=1t(Xi−θ) and the posterior mean of θis
θ=integraltext
θL(θ)dθintegraltext
L(θ)dθ.
We can estimate the top and bottom integral using importance sampling. We
drawθ1,...,θ N∼gand then
θ≈1
NsummationtextN
j=1θjL(θj)
g(θj)
1
NsummationtextN
j=1L(θj)
g(θj).
To illustrate the idea, we drew n= 2 observations. The posterior mean (com-
puted numerically) is -0.54. Using a Normal importance sampler gyields an
estimate of -0.74. Using a Cauchy (t-distribution with 1 degree of freedom)importance sampler yields an estimate of -0.53.
/squaresolid
24.4 MCMC Part I: The Metropolis–Hastings
Algorithm
Consider once more the problem of estimating the integral I=integraltext
h(x)f(x)dx.
Now we introduce Markov chain Monte Carlo (MCMC) methods. The idea isto construct a Markov chain X
1,X2,...,whose stationary distribution is f.
Under certain conditions it will then follow that
1
NNsummationdisplay
i=1h(Xi)P−→Ef(h(X)) =I.
This works because there is a law of large numbers for Markov chains; see
Theorem 23.25.
TheMetropolis–Hastings algorithm is a speciﬁc MCMC method that
works as follows. Let q(y|x) be an arbitrary, friendly distribution (i.e., we
know how to sample from q(y|x)). The conditional density q(y|x) is called
theproposal distribution. The Metropolis–Hastings algorithm creates a
sequence of observations X0,X1,...,as follows.
Metropolis–Hastings Algorithm
Choose X0arbitrarily. Suppose we have generated X0,X1,...,X i.T o
generate Xi+1do the following:
(1) Generate a proposal orcandidate value Y∼q(y|Xi).

<<<PAGE 428>>>

412 24. Simulation Methods
(2) Evaluate r≡r(Xi,Y) where
r(x, y) = minbraceleftbiggf(y)
f(x)q(x|y)
q(y|x),1bracerightbigg
.
(3) Set
Xi+1=braceleftbigg
Ywith probability r
Xiwith probability 1 −r.
24.8 Remark. A simple way to execute step (3) is to generate U∼(0,1). If
U<r setXi+1=Yotherwise set Xi+1=Xi.
24.9 Remark. A common choice for q(y|x)i sN(x, b2) for some b>0. This
means that the proposal is draw from a Normal, centered at the currentvalue. In this case, the proposal density qis symmetric, q(y|x)=q(x|y), and
rsimpliﬁes to
r= minbraceleftbiggf(Y)
f(Xi),1bracerightbigg
.
By construction, X0,X1,...is a Markov chain. But why does this Markov
chain have fas its stationary distribution? Before we explain why, let us ﬁrst
do an example.
24.10 Example. The Cauchy distribution has density
f(x)=1
π1
1+x2.
Our goal is to simulate a Markov chain whose stationary distribution is f.
As suggested in the remark above, we take q(y|x)t ob ea N(x, b2). So in this
case,
r(x, y) = minbraceleftbiggf(y)
f(x),1bracerightbigg
= minbraceleftbigg1+x2
1+y2,1bracerightbigg
.
So the algorithm is to draw Y∼N(Xi,b2) and set
Xi+1=braceleftbiggYwith probability r(Xi,Y)
Xiwith probability 1 −r(Xi,Y).
The simulator requires a choice of b. Figure 24.2 shows three chains of length
N=1,000 using b=.1,b= 1 and b= 10. Setting b=.1 forces the chain
to take small steps. As a result, the chain doesn’t “explore” much of thesample space. The histogram from the sample does not approximate the truedensity very well. Setting b= 10 causes the proposals to often be far in the

<<<PAGE 429>>>

24.4 MCMC Part I: The Metropolis–Hastings Algorithm 413
FIGURE 24.2. Three Metropolis chains corresponding to b=.1,b=1 ,b= 10.
tails, making rsmall and hence we reject the proposal and keep the chain
at its current position. The result is that the chain “gets stuck” at the sameplace quite often. Again, this means that the histogram from the sample doesnot approximate the true density very well. The middle choice avoids theseextremes and results in a Markov chain sample that better represents thedensity sooner. In summary, there are tuning parameters and the eﬃciencyof the chain depends on these parameters. We’ll discuss this in more detaillater.
/squaresolid
If the sample from the Markov chain starts to “look like” the target distri-
bution fquickly, then we say that the chain is “mixing well.” Constructing a
chain that mixes well is somewhat of an art.
Why It Works. Recall from Chapter 23 that a distribution πsatisﬁes
detailed balance for a Markov chain if
pijπi=pjiπj.
We showed that if πsatisﬁes detailed balance, then it is a stationary distri-
bution for the chain.
Because we are now dealing with continuous state Markov chains, we will
change notation a little and write p(x, y) for the probability of making a
transition from xtoy. Also, let’s use f(x) instead of πfor a distribution. In

<<<PAGE 430>>>

414 24. Simulation Methods
this new notation, fis a stationary distribution if f(x)=integraltext
f(y)p(y,x)dyand
detailed balance holds for fif
f(x)p(x, y)=f(y)p(y,x). (24.7)
Detailed balance implies that fis a stationary distribution since, if detailed
balance holds, then
integraldisplay
f(y)p(y,x)dy=integraldisplay
f(x)p(x, y)dy=f(x)integraldisplay
p(x, y)dy=f(x)
which shows that f(x)=integraltext
f(y)p(y,x)dyas required. Our goal is to show that
fsatisﬁes detailed balance which will imply that fis a stationary distribution
for the chain.
Consider two points xandy. Either
f(x)q(y|x)<f(y)q(x|y)o r f(x)q(y|x)>f(y)q(x|y).
We will ignore ties (which occur with probability zero for continuous distribu-
tions). Without loss of generality, assume that f(x)q(y|x)>f(y)q(x|y). This
implies that
r(x, y)=f(y)
f(x)q(x|y)
q(y|x)
and that r(y,x)=1 .N o w p(x, y) is the probability of jumping from xtoy.
This requires two things: (i) the proposal distribution must generate y, and
(ii) you must accept y. Thus,
p(x, y)=q(y|x)r(x, y)=q(y|x)f(y)
f(x)q(x|y)
q(y|x)=f(y)
f(x)q(x|y).
Therefore,
f(x)p(x, y)=f(y)q(x|y). (24.8)
On the other hand, p(y,x) is the probability of jumping from ytox. This
requires two things: (i) the proposal distribution must generate x, and (ii) you
must accept x. This occurs with probability p(y,x)=q(x|y)r(y,x)=q(x|y).
Hence,
f(y)p(y,x)=f(y)q(x|y). (24.9)
Comparing (24.8) and (24.9), we see that we have shown that detailed balance
holds.

<<<PAGE 431>>>

24.5 MCMC Part II: Diﬀerent Flavors 415
24.5 MCMC Part II: Diﬀerent Flavors
There are diﬀerent types of MCMC algorithm. Here we will consider a few of
the most popular versions.
Random-Walk-Metropolis–Hastings. In the previous section we con-
sidered drawing a proposal Yof the form
Y=Xi+/epsilon1i
where /epsilon1icomes from some distribution with density g. In other words, q(y|x)=
g(y−x). We saw that in this case,
r(x, y) = minbraceleftbigg
1,f(y)
f(x)bracerightbigg
.
This is called a random-walk-Metropolis–Hastings method. The reason
for the name is that, if we did not do the accept–reject step, we would besimulating a random walk. The most common choice for gis aN(0,b
2). The
hard part is choosing bso that the chain mixes well. A good rule of thumb is:
choose bso that you accept the proposals about 50 percent of the time.
Warning! This method doesn’t make sense unless Xtakes values on the
whole real line. If Xis restricted to some interval then it is best to transform
X. For example, if X∈(0,∞) then you might take Y= log Xand then
simulate the distribution for Yinstead of X.
Independence-Metropolis–Hastings. This is an importance-sampling
version of MCMC. We draw the proposal from a ﬁxed distribution g. Gen-
erally, gis chosen to be an approximation to f. The acceptance probability
becomes
r(x, y) = minbraceleftbigg
1,f(y)
f(x)g(x)
g(y)bracerightbigg
.
Gibbs Sampling. The two previous methods can be easily adapted, in
principle, to work in higher dimensions. In practice, tuning the chains to makethem mix well is hard. Gibbs sampling is a way to turn a high-dimensionalproblem into several one-dimensional problems.
Here’s how it works for a bivariate problem. Suppose that ( X,Y) has den-
sityf
X,Y(x, y). First, suppose that it is possible to simulate from the condi-
tional distributions fX|Y(x|y) and fY|X(y|x). Let ( X0,Y0) be starting values.
Assume we have drawn ( X0,Y0),...,(Xn,Yn). Then the Gibbs sampling al-
gorithm for getting ( Xn+1,Yn+1) is:

<<<PAGE 432>>>

416 24. Simulation Methods
Gibbs Sampling
Xn+1∼fX|Y(x|Yn)
Yn+1∼fY|X(y|Xn+1)
repeat
This generalizes in the obvious way to higher dimensions.
24.11 Example (Normal Hierarchical Model) .Gibbs sampling is very useful
for a class of models called hierarchical models . Here is a simple case.
Suppose we draw a sample of kcities. From each city we draw nipeople and
observe how many people Yihave a disease. Thus, Yi∼Binomial( ni,pi). We
are allowing for diﬀerent disease rates in diﬀerent cities. We can also think ofthep
/prime
isas random draws from some distribution F. We can write this model
in the following way:
Pi∼F
Yi|Pi=pi∼Binomial( ni,pi).
We are interested in estimating the p/prime
isand the overall disease rateintegraltext
pd F(p).
To proceed, it will simplify matters if we make some transformations that
allow us to use some Normal approximations. Let hatwidepi=Yi/ni. Recall that
hatwidepi≈N(pi,si) where si=radicalbig
hatwidepi(1−hatwidepi)/ni. Let ψi= log( pi/(1−pi)) and
deﬁne Zi≡hatwideψi= log(hatwidepi/(1−hatwidepi)). By the delta method,
hatwideψi≈N(ψi,σ2
i)
where σ2
i=1/(nhatwidepi(1−hatwidepi)). Experience shows that the Normal approximation
forψis more accurate than the Normal approximation for pso we shall work
withψ. We shall treat σias known. Furthermore, we shall take the distribution
of the ψ/prime
isto be Normal. The hierarchical model is now
ψi∼N(µ, τ2)
Zi|ψi∼N(ψi,σ2
i).
As yet another simpliﬁcation we take τ= 1. The unknown parameter are
θ=(µ, ψ1,...,ψ k). The likelihood function is
L(θ)∝productdisplay
if(ψi|µ)productdisplay
if(Zi|ψ)
∝productdisplay
iexpbraceleftbigg
−1
2(ψi−µ)2bracerightbigg
expbraceleftbigg
−1
2σ2
i(Zi−ψi)2bracerightbigg
.

<<<PAGE 433>>>

24.5 MCMC Part II: Diﬀerent Flavors 417
If we use the prior f(µ)∝1 then the posterior is proportional to the likelihood.
To use Gibbs sampling, we need to ﬁnd the conditional distribution of eachparameter conditional on all the others. Let us begin by ﬁnding f(µ|rest)
where “rest” refers to all the other variables. We can throw away any terms
that don’t involve µ. Thus,
f(µ|rest) ∝productdisplay
iexpbraceleftbigg
−1
2(ψi−µ)2bracerightbigg
∝expbraceleftbigg
−k
2(µ−b)2bracerightbigg
where
b=1
ksummationdisplay
iψi.
Hence we see that µ|rest∼N(b,1/k). Next we will ﬁnd f(ψ|rest). Again, we
can throw away any terms not involving ψileaving us with
f(ψi|rest) ∝expbraceleftbigg
−1
2(ψi−µ)2bracerightbigg
expbraceleftbigg
−1
2σ2
i(Zi−ψi)2bracerightbigg
∝expbraceleftbigg
−1
2d2
i(ψi−ei)2bracerightbigg
where
ei=Zi
σ2
i+µ
1+1
σ2
iand d2
i=1
1+1
σ2
i
and so ψi|rest∼N(ei,d2
i). The Gibbs sampling algorithm then involves iter-
ating the following steps Ntimes:
drawµ∼N(b, v2)
drawψ1∼N(e1,d2
1)
......
drawψ
k∼N(ek,d2
k).
It is understood that at each step, the most recently drawn version of each
variable is used.
We generated a numerical example with k= 20 cities and n= 20 people
from each city. After running the chain, we can convert each ψiback into pi
by way of pi=eψi/(1 +eψi). The raw proportions are shown in Figure 24.4.
Figure 24.3 shows “trace plots” of the Markov chain for p1andµ. Figure
24.4 shows the posterior for µbased on the simulated values. The second

<<<PAGE 434>>>

418 24. Simulation Methods
panel of Figure 24.4 shows the raw proportions and the Bayes estimates. Note
that the Bayes estimates are “shrunk” together. The parameter τcontrols
the amount of shrinkage. We set τ= 1 but, in practice, we should treat τas
another unknown parameter and let the data determine how much shrinkageis needed.
/squaresolid
0 500 10000.0 0.5 1.0
0 500 1000−0.5 0.0 0.5
FIGURE 24.3. Posterior simulation for Example 24.11. The top panel shows simu-
lated values of p1. The top panel shows simulated values of µ.
So far we assumed that we know how to draw samples from the conditionals
fX|Y(x|y) and fY|X(y|x). If we don’t know how, we can still use the Gibbs
sampling algorithm by drawing each observation using a Metropolis–Hastingsstep. Let qbe a proposal distribution for xand lettildewideqbe a proposal distribution
fory. When we do a Metropolis step for X, we treat Yas ﬁxed. Similarly,
when we do a Metropolis step for Y, we treat Xas ﬁxed. Here are the steps:

<<<PAGE 435>>>

24.5 MCMC Part II: Diﬀerent Flavors 419
Metropolis within Gibbs
(1a) Draw a proposal Z∼q(z|Xn).
(1b) Evaluate
r= minbraceleftbiggf(Z,Yn)
f(Xn,Yn)q(Xn|Z)
q(Z|Xn),1bracerightbigg
.
(1c) Set
Xn+1=braceleftbigg
Zwith probability r
Xnwith probability 1 −r.
(2a) Draw a proposal Z∼tildewideq(z|Yn).
(2b) Evaluate
r= minbraceleftbiggf(Xn+1,Z)
f(Xn+1,Yn)tildewideq(Yn|Z)
tildewideq(Z|Yn),1bracerightbigg
.
(2c) Set
Yn+1=braceleftbiggZwith probability r
Ynwith probability 1 −r.
Again, this generalizes to more than two dimensions.
−0.6 0.0 0.6
0.0 0.5 1.0
FIGURE 24.4. Example 24.11. Top panel: posterior histogram of µ. Lower panel:
raw proportions and the Bayes posterior estimates. The Bayes estimates have beenshrunk closer together than the raw proportions.

<<<PAGE 436>>>

420 24. Simulation Methods
24.6 Bibliographic Remarks
MCMC methods go back to the eﬀort to build the atomic bomb in World War
II. They were used in various places after that, especially in spatial statistics.There was a new surge of interest in the 1990s that still continues. My mainreference for this chapter was Robert and Casella (1999). See also Gelmanet al. (1995) and Gilks et al. (1998).
24.7 Exercises
1. Let
I=integraldisplay2
1e−x2/2
√
2πdx.
(a) Estimate Iusing the basic Monte Carlo method. Use N= 100 ,000.
Also, ﬁnd the estimated standard error.
(b) Find an (analytical) expression for the standard error of your esti-
mate in (a). Compare to the estimated standard error.
(c) Estimate Iusing importance sampling. Take gto be N(1.5,v2) with
v=.1,v= 1 and v= 10. Compute the (true) standard errors in each
case. Also, plot a histogram of the values you are averaging to see ifthere are any extreme values.
(d) Find the optimal importance sampling function g
∗. What is the
standard error using g∗?
2. Here is a way to use importance sampling to estimate a marginal density.
LetfX,Y(x, y) be a bivariate density and let ( X1,X2),...,(XN,YN)∼
fX,Y.
(a) Let w(x) be an arbitrary probability density function. Let
hatwidefX(x)=1
NNsummationdisplay
i=1fX,Y(x, Yi)w(Xi)
fX,Y(Xi,Yi).
Show that, for each x,
hatwidefX(x)p→fX(x).
Find an expression for the variance of this estimator.
(b) Let Y∼N(0,1) and X|Y=y∼N(y,1+y2). Use the method in
(a) to estimate fX(x).

<<<PAGE 437>>>

24.7 Exercises 421
3. Here is a method called accept–reject sampling for drawing observa-
tions from a distribution.
(a) Suppose that fis some probability density function. Let gbe any
other density and suppose that f(x)≤Mg(x) for all x, where Mis a
known constant. Consider the following algorithm:
(step 1): Draw X∼gandU∼Unif(0 ,1);
(step 2): If U≤f(X)/(Mg(X)) set Y=X, otherwise go back to step
1. (Keep repeating until you ﬁnally get an observation.)
Show that the distribution of Yisf.
(b) Let fbe a standard Normal density and let g(x)=1/(1 +x2)b e
the Cauchy density. Apply the method in (a) to draw 1,000 observationsfrom the Normal distribution. Draw a histogram of the sample to verifythat the sample appears to be Normal.
4. A random variable Zhas ainverse Gaussian distribution if it has
density
f(z)∝z
−3/2expbraceleftbigg
−θ1z−θ2
z+2radicalbig
θ1θ2+ logparenleftBigradicalbig
2θ2parenrightBigbracerightbigg
,z > 0
where θ1>0 and θ2>0 are parameters. It can be shown that
E(Z)=radicalbigg
θ2
θ1and Eparenleftbigg1
Zparenrightbigg
=radicalbigg
θ1
θ2+1
2θ2.
(a) Let θ1=1.5 and θ2= 2. Draw a sample of size 1,000 using the
independence-Metropolis–Hastings method. Use a Gamma distributionas the proposal density. To assess the accuracy, compare the mean of Z
and 1/Zfrom the sample to the theoretical means Try diﬀerent Gamma
distributions to see if you can get an accurate sample.
(b) Draw a sample of size 1,000 using the random-walk-Metropolis–
Hastings method. Since z>0 we cannot just use a Normal density.
One strategy is this. Let W= log Z. Find the density of W. Use the
random-walk-Metropolis–Hastings method to get a sample W
1,...,W N
and let Zi=eWi. Assess the accuracy of the simulation as in part (a).
5. Get the heart disease data from the book web site. Consider a Bayesian
analysis of the logistic regression model
P(Y=1|X=x)=eβ0+/summationtextk
j=1βjxj
1+eβ0+/summationtextk
j=1βjxj.

<<<PAGE 438>>>

422 24. Simulation Methods
Use the ﬂat prior f(β0,...,β k)∝1. Use the Gibbs–Metropolis algorithm
to draw a sample of size 10,000 from the posterior f(β0,β1|data). Plot
histograms of the posteriors for the βj’s. Get the posterior mean and a
95 percent posterior interval for each βj.
(b) Compare your analysis to a frequentist approach using maximum
likelihood.

<<<PAGE 439>>>

Bibliography
Agresti, A. (1990). Categorical Data Analysis . Wiley.
Akaike, H. (1973). Information theory and an extension of the maximum
likelihood principle. Second International Symposium on Information The-
ory267–281.
Anderson, T. W. (1984). An Introduction to Multivariate Statistical Anal-
ysis (Second Edition) . Wiley.
Barron, A. ,Schervish, M. J. and Wasserman, L. (1999). The consis-
tency of posterior distributions in nonparametric problems. The Annals of
Statistics 27536–561.
Beecher, H. (1959). Measurement of Subjective Responses . Oxford Univer-
sity Press.
Benjamini, Y. and Hochberg, Y. (1995). Controlling the false discovery
rate: A practical and powerful approach to multiple testing. Journal of the
Royal Statistical Society, Series B, Methodological 57289–300.
Beran, R. (2000). REACT scatterplot smoothers: Supereﬃciency through
basis economy. Journal of the American Statistical Association 95155–171.
Beran, R. and D¨umbgen, L. (1998). Modulation of estimators and conﬁ-
dence sets. The Annals of Statistics 261826–1856.

<<<PAGE 440>>>

424 Bibliography
Berger, J. and Wolpert, R. (1984). The Likelihood Principle . Institute
of Mathematical Statistics.
Berger, J. O. (1985). Statistical Decision Theory and Bayesian Analysis
(Second Edition) . Springer-Verlag.
Berger, J. O. andDelampady, M. (1987). Testing precise hypotheses (c/r:
P335-352). Statistical Science 2317–335.
Berliner, L. M. (1983). Improving on inadmissible estimators in the control
problem. The Annals of Statistics 11814–826.
Bickel, P. J. and Doksum, K. A. (2000). Mathematical Statistics: Basic
Ideas and Selected Topics, Vol. I (Second Edition) . Prentice Hall.
Billingsley, P. (1979). Probability and Measure . Wiley.
Bishop, Y. M. M. ,Fienberg, S. E. andHolland, P. W. (1975). Discrete
Multivariate Analyses: Theory and Practice . MIT Press.
Breiman, L. (1992). Probability . Society for Industrial and Applied Mathe-
matics.
Brinegar, C. S. (1963). Mark Twain and the Quintus Curtius Snodgrass
letters: A statistical test of authorship. Journal of the American Statistical
Association 5885–96.
Carlin, B. P. andLouis, T. A. (1996). Bayes and Empirical Bayes Methods
for Data Analysis . Chapman & Hall.
Casella, G. and Berger, R. L. (2002). Statistical Inference . Duxbury
Press.
Chaudhuri, P. andMarron, J. S. (1999). Sizer for exploration of structures
in curves. Journal of the American Statistical Association 94807–823.
Cox, D. and Lewis, P. (1966). The Statistical Analysis of Series of Events .
Chapman & Hall.
Cox, D. D. (1993). An analysis of Bayesian inference for nonparametric
regression. The Annals of Statistics 21903–923.
Cox, D. R. andHinkley, D. V. (2000). Theoretical statistics . Chapman &
Hall.

<<<PAGE 441>>>

425
Davison, A. C. and Hinkley, D. V. (1997). Bootstrap Methods and Their
Application . Cambridge University Press.
DeGroot, M. and Schervish, M. (2002). Probability and Statistics (Third
Edition) . Addison-Wesley.
Devroye, L. ,Gy¨orfi, L. and Lugosi, G. (1996). A Probabilistic Theory
of Pattern Recognition . Springer-Verlag.
Diaconis, P. and Freedman, D. (1986). On inconsistent Bayes estimates
of location. The Annals of Statistics 1468–87.
Dobson, A. J. (2001). An introduction to generalized linear models . Chap-
man & Hall.
Donoho, D. L. and Johnstone, I. M. (1994). Ideal spatial adaptation by
wavelet shrinkage. Biometrika 81425–455.
Donoho, D. L. and Johnstone, I. M. (1995). Adapting to unknown
smoothness via wavelet shrinkage. Journal of the American Statistical As-
sociation 901200–1224.
Donoho, D. L. and Johnstone, I. M. (1998). Minimax estimation via
wavelet shrinkage. The Annals of Statistics 26879–921.
Donoho, D. L. ,Johnstone, I. M. ,Kerkyacharian, G. andPicard, D.
(1995). Wavelet shrinkage: Asymptopia? (Disc: p 337–369). Journal of the
Royal Statistical Society, Series B, Methodological 57301–337.
Dunsmore, I. ,D a l y ,F .e ta l . (1987). M345 Statistical Methods, Unit 9:
Categorical Data . The Open University.
Edwards, D. (1995). Introduction to graphical modelling . Springer-Verlag.
Efromovich, S. (1999). Nonparametric Curve Estimation: Methods, Theory
and Applications . Springer-Verlag.
Efron, B. (1979). Bootstrap methods: Another look at the jackknife. The
Annals of Statistics 71–26.
Efron, B. ,Tibshirani, R. ,Storey, J. D. and Tusher, V. (2001). Em-
pirical Bayes analysis of a microarray experiment. Journal of the American
Statistical Association 961151–1160.

<<<PAGE 442>>>

426 Bibliography
Efron, B. andTibshirani, R. J. (1993). An Introduction to the Bootstrap .
Chapman & Hall.
Ferguson, T. (1967). Mathematical Statistic s:aD e cision Theoretic Ap-
proach . Academic Press.
Fisher, R. (1921). On the probable error of a coeﬃcient of correlation de-
duced from a small sample. Metron 11–32.
Freedman, D. (1999). Wald lecture: On the Bernstein-von Mises theorem
with inﬁnite-dimensional parameters. The Annals of Statistics 271119–
1141.
Friedman, J. H. (1997). On bias, variance, 0/1-loss, and the curse-of-
dimensionality. Data Mining and Knowledge Discovery 155–77.
Gelman, A. ,Carlin, J. B. ,Stern, H. S. and Rubin, D. B. (1995).
Bayesian Data Analysis . Chapman & Hall.
Ghosal, S. ,Ghosh, J. K. and Van Der Vaart, A. W. (2000). Conver-
gence rates of posterior distributions. The Annals of Statistics 28500–531.
Gilks, W. R. ,Richardson, S. andSpiegelhalter, D. J. (1998). Markov
Chain Monte Carlo in Practice . Chapman & Hall.
Grimmett, G. and Stirzaker, D. (1982). Probability and Random Pro-
cesses . Oxford University Press.
Guttorp, P. (1995). Stochastic Modeling of Scientiﬁc Data . Chapman &
Hall.
Hall, P. (1992). The Bootstrap and Edgeworth Expansion . Springer-Verlag.
Halverson, N. ,Leitch, E. ,Pryke, C. ,Kovac, J. ,Carlstrom, J. ,
Holzapfel, W. ,Dragovan, M. ,Cartwright, J. ,Mason, B. ,Padin,
S.,Pearson, T. ,Shepherd, M. and Readhead, A. (2002). DASI ﬁrst
results: A measurement of the cosmic micr owave bac kground angular power
spectrum. Astrophysics Journal 56838–45.
Hardle, W. (1990). Applied nonparametric regression . Cambridge Univer-
sity Press.
H¨ardle, W. ,Kerkyacharian, G. ,Picard, D. andTsybakov, A. (1998).
Wavelets, Approximation, and Statistical Applications . Springer-Verlag.

<<<PAGE 443>>>

427
Hastie, T. ,Tibshirani, R. and Friedman, J. H. (2001). The Elements
of Statistical Learning: Data Mining, Inference, and Prediction . Springer-
Verlag.
Herbich, R. (2002). Learning Kernel Classiﬁers: Theory and Algorithms .
MIT Press.
Johnson, R. A. and Wichern, D. W. (1982). Applied Multivariate Statis-
tical Analysis . Prentice-Hall.
Johnson, S. and Johnson, R. (1972). New England Journal of Medicine
2871122–1125.
Jordan, M. (2004). Graphical models . In Preparation.
Karr, A. (1993). Probability . Springer-Verlag.
Kass, R. E. and Raftery, A. E. (1995). Bayes factors. Journal of the
American Statistical Association 90773–795.
Kass, R. E. andWasserman, L. (1996). The selection of prior distributions
by formal rules (corr: 1998 v93 p 412). Journal of the American Statistical
Association 911343–1370.
Larsen, R. J. and Marx, M. L. (1986). An Introduction to Mathematical
Statistics and Its Applications (Second Edition) . Prentice Hall.
Lauritzen, S. L. (1996). Graphical Models . Oxford University Press.
Lee, A. T. et al. (2001). A high spatial resolution analysis of the maxima-1
cosmic micr owave bac kground anisotropy data. Astrophys. J. 561L1–L6.
Lee, P. M. (1997). Bayesian Statistics: An Introduction . Edward Arnold.
Lehmann, E. L. (1986). Testing Statistical Hypotheses (Second Edition) .
Wiley.
Lehmann, E. L. and Casella, G. (1998). Theory of Point Estimation.
Springer-Verlag.
Loader, C. (1999). Local regr ession and likelihood . Springer-Verlag.
Marron, J. S. and Wand, M. P. (1992). Exact mean integrated squared
error. The Annals of Statistics 20712–736.

<<<PAGE 444>>>

428 Bibliography
Morrison, A. ,Black, M. ,Lowe, C. ,Macmahon, B. and Yusa, S.
(1973). Some international diﬀerences in histology and survival in breastcancer. International Journal of Cancer 11261–267.
Netterfield, C. B. et al. (2002). A measurement by boomerang of mul-
tiple peaks in the angular power spectrum of the cosmic micr owave back-
ground. Astrophys. J. 571604–614.
Ogden, R. T. (1997). Essential Wavelets for Statistical Applications and
Data Analysis . Birkh¨ auser.
Pearl, J. (2000). Casuality: models, reasoning, and inference . Cambridge
University Press.
Phillips, D. and King, E. (1988). Death takes a holiday: Mortality sur-
rounding major social occasions. Lancet 2728–732.
Phillips, D. and Smith, D. (1990). Postponement of death until symbol-
ically meaningful occasions. Journal of the American Medical Association
2631947–1961.
Quenouille, M. (1949). Approximate tests of correlation in time series.
Journal of the Royal Statistical Society B 1118–84.
Rice, J. A. (1995). Mathematical Statistics and Data Analysis (Second Edi-
tion). Duxbury Press.
Robert, C. P. (1994). The Bayesian Choice: A Decision-theoretic Motiva-
tion. Springer-Verlag.
Robert, C. P. and Casella, G. (1999). Monte Carlo Statistical Methods .
Springer-Verlag.
Robins, J. ,Scheines, R. ,Spirtes, P. andWasserman, L. (2003). Uniform
convergence in causal inference. Biometrika (to appear).
Robins, J. M. and Ritov, Y. (1997). Toward a curse of dimensionality ap-
propriate (CODA) asymptotic theory for semi-parametric models. Statistics
in Medicine 16285–319.
Rosenbaum, P. (2002). Observational Studies . Springer-Verlag.
Ross, S. (2002). Probability Models for Computer Science . Academic Press.

<<<PAGE 445>>>

429
Rousseauw, J. ,du Plessis, J. ,Benade, A. ,Jordaan, P. ,Kotze, J. ,
Jooste, P. and Ferreira, J. (1983). Coronary risk factor screening in
three rural communities. South African Medical Journal 64430–436.
Schervish, M. J. (1995). Theory of Statistics . Springer-Verlag.
Scholkopf, B. and Smola, A. (2002). Learning with Kernels: Support
Vector Machines, Regularization, Optimization, and Beyond . MIT Press.
Schwarz, G. (1978). Estimating the dimension of a model. The Annals of
Statistics 6461–464.
Scott, D. ,Gotto, A. ,Cole, J. and Gorry, G. (1978). Plasma lipids as
collateral risk factors in coronary artery disease: a study of 371 males withchest pain. Journal of Chronic Diseases 31337–345.
Scott, D. W. (1992). Multivariate Density Estimation: Theory, Practice,
and Visualization . Wiley.
Shao, J. and Tu, D. (1995). The Jackknife and Bootstrap (German) .
Springer-Verlag.
Shen, X. and Wasserman, L. (2001). Rates of convergence of posterior
distributions. The Annals of Statistics 29687–714.
Shorack, G. R. and Wellner, J. A. (1986). Empirical P rocesses With
Applications to Statistics . Wiley.
Silverman, B. W. (1986). Density Estimation for Statistics and Data Anal-
ysis. Chapman & Hall.
Spirtes, P. ,Glymour, C. N. andScheines, R. (2000). Causation, predic-
tion, and search. MIT Press.
Taylor, H. M. and Karlin, S. (1994). An Introduction to Stochastic Mod-
eling. Academic Press.
van der Laan, M. and Robins, J. (2003). Uniﬁed Methods for Censored
Longitudinal Data and Causality . Springer Verlag.
van der Vaart, A. W. (1998). Asymptotic Statistics . Cambridge University
Press.
van der Vaart, A. W. and Wellner, J. A. (1996). Weak Convergence
and Empirical P rocesses: With Applications to Statistics . Springer-Verlag.

<<<PAGE 446>>>

430 Bibliography
Vapnik, V. N. (1998). Statistical Learning Theory . Wiley.
Weisberg, S. (1985). Applied Linear Regression . Wiley.
Whittaker, J. (1990). Graphical Models in Applied Multivariate Statistics .
Wiley.
Wright, S. (1934). The method of path coeﬃcients. The Annals of Mathe-
matical Statistics 5161–215.
Zhao, L. H. (2000). Bayesian aspects of some nonparametric problems. The
Annals of Statistics 28532–552.
Zheng, X. and Loh, W.-Y. (1995). Consistent variable selection in linear
models. Journal of the American Statistical Association 90151–156.

<<<PAGE 447>>>

List of Symbols
General Symbols
R real numbers
infx∈Af(x) inﬁmum: the largest number ysuch that
y≤f(x) for all x∈A
think of this as the minimum of f
supx∈Af(x) supremum: the smallest number ysuch that
y≥f(x) for all x∈A
think of this as the maximum of f
n! n×(n−1)×(n−2)×···× 3×2×1parenleftbign
kparenrightbign!
k!(n−k)!
Γ(α) Gamma functionintegraltext∞
0yα−1e−ydy
Ω sample space (set of outcomes)
ω outcome, element, point
A event (subset of Ω)
IA(ω) indicator function; 1 if ω∈Aand 0 otherwise
|A| number of points in set A
Probability Symbols
P(A) probability of event A
A/coproductBA andBare independent
A/gluonbelement/gluonelement/gluonelement/gluonelement/gluonelement/gluoneelementBA andBare dependent
FX cumulative distribution functionF
X(x)=P(X≤x)
fX probability density (or mass) function
X∼FX has distribution F
X∼fX has density f
Xd=YX andYhave the same distribution
iid independent and identically distributed
X1,...,X n∼F iidsample of size nfromF
φ standard Normal probability density
Φ standard Normal distribution functionz
α upper αquantile of N(0,1):zα=Φ−1(1−α)
E(X)=integraltext
xd F(x) expected value (mean) of random variable X
E(r(X)) =integraltext
r(x)dF(x) expected value (mean) of r(X)
V(X) variance of random variable X
Cov(X,Y) covariance between XandY
X1,...,X n data
n sample size

<<<PAGE 448>>>

432 List of Symbols
Convergence Symbols
P−→ convergence in probability
/squiggleright convergence in distribution
qm−→ convergence in quadratic mean
Xn≈N(µ, σ2
n)(Xn−µ)/σn/squigglerightN(0,1)
xn=o(an) xn/an→0
xn=O(an) |xn/an|is bounded for large n
Xn=oP(an)Xn/anP−→0
Xn=OP(an)|Xn/an|is bounded in probability for large n
Statistical Models
F statistical model; a set of distribution functions,
density functions or regression functions
θ parameter
hatwideθ estimate of parameter
T(F) statistical functional (the mean, for example)
Ln(θ) likelihood function
Useful Math Facts
ex=summationtext∞
k=0xk
k!=1+ x+x2
2!+···
summationtext∞
j=krj=rk
1−rfor 0<r< 1
limn→∞parenleftbig
1+a
nparenrightbign=ea
Stirling’s approximation: n!≈nne−n√
2πn
The Gamma function . The Gamma function is deﬁned by
Γ(α)=integraldisplay∞
0yα−1e−ydy
forα≥0. Ifα>1 then Γ( α)=(α−1)Γ(α−1). Ifnis a positive integer then
Γ(n)=(n−1)!. Some special values are: Γ(1) = 1 and Γ(1 /2) =√π.

<<<PAGE 449>>>

Table of DistributionsList of Symbols 433Distribution
 pdf or probability function
 mean
 variance
 mgf
Point mass at a
 I(x=a)
 a
 0
 eat
Bernoulli( p)
 px(1−p)1−x
p
 p(1−p)
 pet+( 1−p)
Binomial( n,p)
/parenleftbign
x/parenrightbig
px(1−p)n−x
np
 np(1−p)
 (pet+( 1−p))n
Geometric( p)
 p(1−p)x−1I(x≥1)
 1/p
1−p
p2
pet
1−(1−p)et/parenleftbig
t<−log(1−p)/parenrightbig
Poisson( λ)
λxe−λ
x!
λ
 λ
 eλ(et−1)
Uniform( a, b)
 I(a<x<b )/(b−a)
a+b
2
(b−a)2
12
ebt−eat
(b−a)t
Normal( µ, σ2)
1
σ√
2πe−(x−µ)2/(2σ2)
µ
 σ2
exp/braceleftBig
µt+σ2t2
2/bracerightBig
Exponential( β)
e−x/β
β
β
 β2
 1
1−βt/parenleftbig
t<1/β/parenrightbig
Gamma( α, β)
xα−1e−x/β
Γ(α)βα
 αβ
 αβ2
/parenleftBig
1
1−βt/parenrightBigα
(t<1/β)
Beta(α, β)
Γ(α+β)
Γ(α)Γ(β)xα−1(1−x)β−1
 α
α+β
αβ
(α+β)2(α+β+1)
1+/summationtext∞
k=1/parenleftbigg/producttextk−1
r=0α+r
α+β+r/parenrightbigg
tk
k!
tν
Γ(ν+1
2)
Γ(ν
2)1
/parenleftBig
1+x2
ν/parenrightBig(ν+1)/2.
 0( i fν>1)
ν
ν−2(ifν>2)
 does not exist
χ2
p
1
Γ(p/2)2p/2x(p/2)−1e−x/2
p
 2p
/parenleftBig
1
1−2t/parenrightBigp/2/parenleftbig
t<1/2/parenrightbig

<<<PAGE 450>>>

Index
χ2distribution, 30
accept-reject sampling, 421
accessible, 387
actions, 193acyclic, 266additive regression, 323
adjacent, 281adjusted treatment eﬀect, 259admissibility
Bayes rules, 202
admissible, 202
AIC (Akaike Information Criterion),
220
Aliens, 271alternative hypothesis, 95, 149ancestor, 265aperiodic, 390arcs, 281associated, 239association, 253association is not causation, 16.1,
253
assume, 8
asymptotic Normality, 128
asymptotic theory, 71asymptotically Normal, 92
, 126
asymptotically optimal, 126
asymptotically uniformly integrable,
81
average causal eﬀect, 252average treatment eﬀect, 252Axiom 1, 5Axiom 2, 5Axiom 3, 5axioms of probability, 5
backﬁtting, 324
bagging, 375
bandwidth, 313
Bayes classiﬁcation rule, 351
Bayes Estimators, 197
Bayes risk, 195

<<<PAGE 451>>>

Index 435
Bayes rules, 197
admissibility, 202
Bayes’ Theorem, 12, 1.17,1 2
Bayesian inference, 89, 175
strengths and weaknesses, 185
Bayesian network, 263Bayesian philosophy, 175Bayesian testing, 184Benjamini and Hochberg, 10.26, 167
Benjamini-Hochberg (BH) method,
167
Bernoulli distribution, 26
,2 9
Beta distribution, 30
bias-variance tradeoﬀ, 305
Bibliographic Remarks, 13Binomial distribution, 26
bins, 303 , 306
binwidth, 306
bivariate distribution, 31Bonferroni method, 166boosting, 375
bootstrap, 107
parametric, 134
Bootstrap Conﬁdence Intervals, 110bootstrap percentile interval, 111bootstrap pivotal conﬁdence, 111Bootstrap variance estimation, 109branching process, 398
candidate, 411
Cauchy distribution, 30
Cauchy-Schwartz inequality, 4.8,6 6
causal odds ratio, 252causal regression function, 256causal relative risk, 253Central Limit Theorem (CLT), 5.8,
77
Chapman-Kolmogorov equations, 23.9,
385Chebyshev’s inequality, 4.2,6 4
checking assumptions, 135
child, 265classes, 387classiﬁcation, 349
classiﬁcation rule, 349
classiﬁcation trees, 360classiﬁer
assessing error rate, 362
clique, 285closed, 388CLT, 77collider, 265comparing risk functions, 194complete, 281, 328composite hypothesis, 151Computer Experiment, 16, 17concave, 66conditional causal eﬀect, 255conditional distribution, 36conditional expectation, 54conditional independence, 264
minimal, 287
conditional likelihood, 213Conditional Probability, 10conditional probability, 10, 10
conditional probability density func-
tion, 37
conditional probability mass func-
tion, 36
conditioning by intervention, 274
conditioning by observation, 274conﬁdence band, 99conﬁdence bands, 323conﬁdence interval, 65, 92conﬁdence set, 92confounding variables, 257conjugate, 179

<<<PAGE 452>>>

436 Index
consistency relationship, 252
consistent, 90 , 126
continuity of probabilities, 1.8,7
continuous, 23
converges in distribution, 72converges in probability, 72convex, 66correlation, 52
conﬁdence interval, 234
cosine basis, 329counterfactual, 251, 252counting process, 395covariance, 52
covariance matrix, 232covariate, 209coverage, 92critical value, 150cross-validation, 363
cross-validation estimator of risk,
310
cumulative distribution function, 20
curse of dimensionality, 319
curve estimation, 89, 303
d-connected, 270d-separated, 270DAG, 266data mining, viidecision rule, 193decision theory, 193decomposition theorem, 23.17, 389
delta method, 5.13, 79, 131
density estimation, 312
kernel approach, 312orthogonal function approach,
331
dependent, 34
, 239
dependent variable, 89derive, 8descendant, 265
detail coeﬃcients, 342detailed balance, 391, 413deviance, 299
directed acyclic graph, 266directed graph, 264directed path, 265discrete, 22
discrete uniform distribution, 26
discrete wavelet transform (DWT),
344
discriminant function, 354
discrimination, 349
disjoint, 5distribution
χ
2,3 0
Bernoulli, 26 ,2 9
Beta, 30
Binomial, 26
Cauchy, 30
conditional, 36discrete uniform, 26
Gaussian, 28
Geometric, 26
Multinomial, 39
multivariate Normal, 39
Normal, 28
point mass, 26
Poisson, 27
t, 30
Uniform, 27
Dvoretzky-Kiefer-Wolfowitz (DKW)
inequality, 7.5,9 8
edges, 281
eﬃcient, 126 , 131
elements, 3EM algorithm, 144
empirical distribution function, 97

<<<PAGE 453>>>

Index 437
empirical error rate, 351
empirical probability measure, 367
empirical risk minimization, 352, 365
Epanechnikov kernel, 312
equal in distribution, 25
equivariant, 126
ergodic, 390
Events, 3events, 3evidence, 157Exercises, 13expectation, 47
conditional, 54
expected value, 47
exponential families, 140
faithful, 270false discovery proportion, 166false discovery rate, 166FDP, 166FDR, 166feature, 89, 209ﬁrst moment, 47
ﬁrst quartile, 25Fisher information, 128
Fisher information matrix, 133
Fisher linear discriminant function,
356
ﬁtted line, 210
ﬁtted values, 210frequentist (or classical), 175frequentist inference, 89
Gamma function, 29
Gaussian classiﬁer, 353Gaussian distribution, 28
Geometric distribution, 26
Gibbs sampling, 416Gini index, 361Glivenko-Cantelli theorem, 7.4,9 8
goodness-of-ﬁt tests, 168graphical, 294
graphical log-linear models, 294
Haar father wavelet, 340
Haar scaling function, 340Haarwavelet regression, 343
hierarchical log-linear model, 296
hierarchical model, 56hierarchical models, 416histogram, 303, 305
histogram estimator, 306
Hoeﬀding’s inequality, 4.4, 64, 365
homogeneous, 384
homogeneous Poisson process, 396
Horwitz-Thompson, 188hypothesis testing, 94
identiﬁable, 126
importance sampling, 408
impurity, 360
inadmissible, 202
independent, 8, 8 ,3 4
Independent Events, 8independent random variables, 34independent variable, 89index set, 381indicator function, 5inequalities, 63inner product, 327integrated squared error (ISE), 304
intensity function, 395
interarrival times, 396intervene, 273intervention, 273Introduction, 3invariant, 390
inverse Gaussian distribution, 421

<<<PAGE 454>>>

438 Index
irreducible, 388
iterated expectations, 3.24,5 5
jackknife, 115
James-Stein estimator, 204Jeﬀreys-Lindley paradox, 192Jensen’s inequality, 4.9,6 6
joint mass function, 31
K-fold cross-validation, 364
k-nearest-neighbors, 375
kernel, 312
kernel density estimator, 312 ,313
kernelization, 371
Kolmogorov-Smirnov test, 245Kullback-Leibler distance, 126
Laplace transform, 56
large sample theory, 71law of large numbers, 72law of total probability, 1.16,1 2
lazy,3.6,4 8
least favorable prior, 198least squares estimates, 211
leave-one-out cross-validation, 220leaves, 361
Legendre polynomials, 329length, 327level, 150
likelihood function, 122
likelihood ratio statistic, 164
likelihood ratio test, 164limit theory, 71limiting distribution, 391
linear algebra notation, 231linear classiﬁer, 353linearly separable, 369
log odds ratio, 240
log-likelihood function, 122log-linear expansion, 292
log-linear model, 286log-linear models, 291logistic regression, 223loss function, 193
machine learning, vii
Manalahobis distance, 353marginal Distribution, 33marginal distribution, 197Markov chain, 383, 383
Markov condition, 267Markov equivalent, 271Markov’s inequality, 4.1,6 3
maximal clique, 285maximum likelihood, 122
maximum likelihood estimates
computing, 142
maximum likelihood estimator
consistent, 126
maximum risk, 195
mean, 47
mean integrated squared error (MISE),
304
mean recurrence time, 390mean squared error, 91measurable, 13, 43median, 25
bootstrap, 109
Mercer’s theorem, 373
method of moments estimator, 121
Metropolis within Gibbs, 419Metropolis–Hastings algorithm, 411Mill’s inequality, 4.7,6 5
minimal conditional independence,
287
minimal suﬃcient, 138
minimax rule, 197 , 198
missing data, 187

<<<PAGE 455>>>

Index 439
mixture of Normals, 143
model generator, 297model selection, 218moment generating function, 56
moments, 49monotone decreasing, 5monotone increasing, 5Monte Carlo integration, 404Monte Carlo integration method,
404
Monty Hall, 14most powerful, 152mother Haar wavelet, 341
MSE, 91Multinomial, 235Multinomial distribution, 39
multiparameter models, 133
multiple regression, 216multiple testing, 165multiresolution analysis, 341Multivariate central limit theorem,
5.12,7 8
Multivariate Delta Method, 5.15,
79
multivariate Normal, 234multivariate Normal distribution, 39
mutually exclusive, 5
Nadaraya-Watson kernel estimator,
319
naive Bayes classiﬁer, 359
natural parameter, 141
natural suﬃcient statistic, 140
neural networks, 376
Newton-Raphson, 143Neyman-Pearson, 10.30, 170
nodes, 281non-collider, 265non-null, 390nonparametric model, 88
nonparametric regression, 319
kernel approach, 319orthogonal function approach,
337
norm, 327normal, 327Normal distribution, 28
Normal-based conﬁdence interval,
6.16,9 4
normalizing constant, 177, 403not, 10nuisance parameter, 120
nuisance parameters, 88null, 390null hypothesis, 94, 149
observational studies, 257
odds ratio, 240
olive statistics, ione-parameter exponential family,
140
one-sided test, 151
optimality, 130
orthogonal, 327orthogonal functions, 327orthonormal, 328orthonormal basis, 328outcome, 89overﬁtting, 218
p-value, 156, 157
pairwise Markov graph, 283parameter of interest, 120
parameter space, 88parameters, 26parametric bootstrap, 134
parametric model, 87parent, 265

<<<PAGE 456>>>

440 Index
Parseval’s relation, 329
partition, 5path, 281Pearson’s χ
2test, 241
period, 390periodic, 390permutation distribution, 162permutation test, 161permutation test:algorithm, 163perpendicular, 327persistent, 388
pivot, 110plug-in estimator, 99
point estimation, 90point mass distribution, 26
pointwise asymptotic, 95Poisson distribution, 27
Poisson process, 394, 395
positive deﬁnite, 231posterior, 176
large sample properties, 181
posterior risk, 197potential, 285potential outcomes, 251power function, 150
precision matrix, 232predicted values, 210prediction, 89, 215prediction interval, 13.11, 215
prediction risk, 219predictor, 89predictor variable, 209prior distribution, 176Probability, 5probability, 5probability distribution, 5, 5
probability function, 22
probability inequalities, 63probability mass function, 22
probability measure, 5, 5
Probability on Finite Sample Spaces,
7
proposal, 411
quadratic discriminant analysis (QDA),
353
quantile function, 25
quantiles, 102
random variable, 19
independent, 34
random vector, 38, 232random walk, 59random-walk-Metropolis-Hastings,
415
realizations, 3recurrence time, 390recurrent, 388
regression, 89, 209, 335
nonparametric, 319
regression function, 89, 209, 351regression through the origin, 226regressor, 89rejection region, 150relative risk, 248represents, 266
residual sums of squares, 211residuals, 210response variable, 89, 209reweighted least squares, 224risk, 194
, 304
rule of the lazy statistician, 3.6,4 8
Rules of d-separation, 270
sample correlation, 102
sample mean, 51sample outcomes, 3

<<<PAGE 457>>>

Index 441
sample quantile, 102
sample space, 3Sample Spaces and Events, 3sample variance, 51sampling distribution, 90saturated model, 298, 299scaling coeﬃcient, 342score function, 128
se, 90shatter coeﬃcient, 367
shattered, 367
simple hypothesis, 151simple linear regression, 210Simpson’s paradox, 259simulation, 108, 180size, 150
slack variables, 371
Slutzky’s theorem, 75smoothing, 303
smoothing parameter, 303
Sobolev space, 88sojourn times, 396spatially inhomogeneous, 340standard deviation, 51
standard error, 90standard Normal distribution, 28state space, 381stationary, 390
statistic, 61, 107, 137
statistical functional, 89, 99statistical model, 87Stein’s paradox, 204stochastic process, 381Stone’s theorem, 20.16, 316
strong law of large numbers, 5.18,
81
strongly inadmissible, 204subjectivism, 181suﬃciency, 137
suﬃcient statistic, 137
Summary of Terminology, 4supervised learning, 349
support vector machines, 368
support vectors, 370
t distribution, 30
t-test, 170test statistic, 150third quartile, 25thresholding, 342training error, 219training error rate, 351
training set, 363
transformations of random variables,
41
transient, 388
true error rate, 351
two-sided test, 151type I error, 150type II error, 150types of convergence, 72
unbiased, 90
underﬁtting, 218undirected graph, 281uniform asymptotic, 95Uniform distribution, 27
unshielded collider, 266
validation set, 363
Vapnik-Chervonenkis, 366
variance, 51
conditional, 55
variance-covariance matrix, 53
vertices, 281
waiting times, 396
Wald test, 153

<<<PAGE 458>>>

442 Index
wavelets, 340
weak law of large numbers (WLLN),
5.6,7 6
Zheng-Loh method, 222