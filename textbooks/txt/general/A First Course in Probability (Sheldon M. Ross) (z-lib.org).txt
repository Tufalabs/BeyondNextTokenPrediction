
<<<PAGE 1>>>



<<<PAGE 2>>>

July 14, 2014 A01_ROSSS4772_09_SE_FM page i
AF IRST COURSE IN PROBABILITY

<<<PAGE 3>>>

July 14, 2014 A01_ROSSS4772_09_SE_FM page ii
This page intentionally left blank 

<<<PAGE 4>>>

July 14, 2014 A01_ROSSS4772_09_SE_FM page iii
AF IRST COURSE IN PROBABILITY
Ninth Edition
Sheldon Ross
University of Southern California
Boston Columbus Indianapolis New York San Francisco
Upper Saddle River Amsterdam Cape Town DubaiLondon Madrid Milan Munich Paris Montreal TorontoDelhi Mexico City Sao Paulo Sydney Hong Kong Seoul
Singapore Taipei Tokyo

<<<PAGE 5>>>

July 14, 2014 A01_ROSSS4772_09_SE_FM page iv
Editor in Chief: Deirdre Lynch Procurement Specialist: Linda Cox
Acquisitions Editor: Christopher Cummings Design Manager Andrea Nix
Sponsoring Editor: Christina Lepre Senior Text and Cover Designer: Beth Paquin
Editorial Assistant: Sonia Ashraf Cover Image: Konstantin Sutyagin/Fotolia
Marketing Manager: Erin Lane Full-Service Project Management: Integra
Marketing Coordinator: Kathleen DeChavez Software Services
Production Project Manager: Beth Houston Composition: Integra Software Services
Credits and acknowledgments borrowed from other sources and reproduced, with permission, in this
textbook appear on the appropriate page within text.
Copyright ©2014, 2010, 2006 by Pearson Education, Inc. All rights reserved. No part of this publication
may be reproduced, stored in a retrieval system, or transmitted, in any form or by any means, electronic,mechanical, photocopying, recording, or otherwise, without the prior written permission of the publisher.Printed in the United States of America. For information on obtaining permission for use of materialin this work, please submit a written request to Pearson Education, Inc., Rights and Contracts Depart-ment, 501 Boylston Street, Suite 900, Boston, MA 02116, fax your request to 617-671-3447, or e-mail athttp://www.pearsoned.com/legal/permissions.htm
Many of the designations by manufacturers and sellers to distinguish their products are claimed as trade-
marks. Where those designations appear in this book, and the publisher was aware of a trademark claim,the designations have been printed in initial caps or all caps.
Library of Congress Cataloging-in-Publication Data
Ross, Sheldon M.
A ﬁrst course in probability / Sheldon Ross, University of Southern California. — Ninth edition.
pages cm
Includes bibliographical references and index.
ISBN 978-0-321-79477-2
1. Probabilities—Textbooks. I. Title.
QA273.R83 2013519.2—dc23
2012023212
10 9 8 7 6 5 4 3 2 1 EBM 16 15 14 13 12
ISBN-10: 0-321-79477-XISBN-13: 978-0-321-79477-2

<<<PAGE 6>>>

July 14, 2014 A01_ROSSS4772_09_SE_FM page v
For Rebecca

<<<PAGE 7>>>

July 14, 2014 A01_ROSSS4772_09_SE_FM page vi
This page intentionally left blank 

<<<PAGE 8>>>

July 14, 2014 A01_ROSSS4772_09_SE_FM page vii
Contents
Preface ix
1Combinatorial Analysis 1
1.1 Introduction 1
1.2 The Basic Principle of Counting 2
1.3 Permutations 3
1.4 Combinations 5
1.5 Multinomial Coefﬁcients 9
1.6 The Number of Integer Solutions of
Equations 12
Summary 15
Problems 15
Theoretical Exercises 17
Self-Test Problems and Exercises 19
2Axioms of Probability 21
2.1 Introduction 21
2.2 Sample Space and Events 21
2.3 Axioms of Probability 25
2.4 Some Simple Propositions 28
2.5 Sample Spaces Having Equally Likely
Outcomes 32
2.6 Probability as a Continuous Set Function 42
2.7 Probability as a Measure of Belief 46
Summary 47
Problems 48
Theoretical Exercises 52
Self-Test Problems and Exercises 54
3Conditional Probability
and Independence 56
3.1 Introduction 56
3.2 Conditional Probabilities 56
3.3 Bayes’s Formula 62
3.4 Independent Events 75
3.5 P(· |F) Is a Probability
Summary 97
Problems 97Theoretical Exercises 106
Self-Test Problems and Exercises 109
4Random Variables 112
4.1 Random Variables 112
4.2 Discrete Random Variables 116
4.3 Expected Value 119
4.4 Expectation of a Function of a Random
Variable 121
4.5 Variance 125
4.6 The Bernoulli and Binomial Random
Variables 127
4.7 The Poisson Random Variable 135
4.8 Other Discrete Probability Distributions 147
4.9 Expected Value of Sums of Random
Variables 155
4.10 Properties of the Cumulative DistributionFunction
159
Summary 162
Problems 163
Theoretical Exercises 169
Self-Test Problems and Exercises 173
5Continuous Random
Variables 176
5.1 Introduction 176
5.2 Expectation and Variance of Continuous
Random Variables 179
5.3 The Uniform Random Variable 184
5.4 Normal Random Variables 187
5.5 Exponential Random Variables 197
5.6 Other Continuous Distributions 203
5.7 The Distribution of a Functionof a Random Variable
208
Summary 210
Problems 212
Theoretical Exercises 214
Self-Test Problems and Exercises 217
vii89

<<<PAGE 9>>>

July 14, 2014 A01_ROSSS4772_09_SE_FM page viii
viiiContents
6Jointly Distributed Random
Variables 220
6.1 Joint Distribution Functions 220
6.2 Independent Random Variables 228
6.3 Sums of Independent Random
Variables 239
6.4 Conditional Distributions: DiscreteCase
248
6.5 Conditional Distributions: ContinuousCase
250
6.6 Order Statistics 256
6.7 Joint Probability Distribution of Functionsof Random Variables
260
6.8 Exchangeable Random Variables 267
Summary 270
Problems 271
Theoretical Exercises 275
Self-Test Problems and Exercises 277
7Properties of Expectation 280
7.1 Introduction 280
7.2 Expectation of Sums of RandomVariables
281
7.3 Moments of the Number of Events that
Occur 298
7.4 Covariance, Variance of Sums, and
Correlations 304
7.5 Conditional Expectation 313
7.6 Conditional Expectation and
Prediction 330
7.7 Moment Generating Functions 334
7.8 Additional Properties of Normal Random
Variables 345
7.9 General Deﬁnition of Expectation 349
Summary 351
Problems 352
Theoretical Exercises 359
Self-Test Problems and Exercises 363
8Limit Theorems 367
8.1 Introduction 367
8.2 Chebyshev’s Inequality and the WeakLaw of Large Numbers
3678.3 The Central Limit Theorem 370
8.4 The Strong Law of Large Numbers 378
8.5 Other Inequalities 382
8.6 Bounding the Error Probability WhenApproximating a Sum of IndependentBernoulli Random Variables by a Poisson
Random Variable
388
Summary 390
Problems 390
Theoretical Exercises 392
Self-Test Problems and Exercises 393
9Additional Topics
in Probability 395
9.1 The Poisson Process 395
9.2 Markov Chains 397
9.3 Surprise, Uncertainty, and Entropy 402
9.4 Coding Theory and Entropy 405
Summary 411
Problems and Theoretical Exercises 412
Self-Test Problems and Exercises 413
10Simulation 415
10.1 Introduction 415
10.2 General Techniques for Simulating
Continuous Random Variables 417
10.3 Simulating from Discrete Distributions 424
10.4 Variance Reduction Techniques 426
Summary 430
Problems 430
Self-Test Problems and Exercises 431
Answers to Selected Problems 433
Solutions to Self-Test Problems
and Exercises 435
Index 465
Common Discrete Distributions inside
front cover
Common Continuous Distributions inside
back cover

<<<PAGE 10>>>

July 14, 2014 A01_ROSSS4772_09_SE_FM page ix
Preface
“We see that the theory of probability is at bottom only common sense reduced
to calculation; it makes us appreciate with exactitude what reasonable minds feel
by a sort of instinct, often without being able to account for it. ...It is remark-
able that this science, which originated in the consideration of games of chance,
should have become the most important object of human knowledge. ...The most
important questions of life are, for the most part, really only problems of proba-bility.” So said the famous French mathematician and astronomer (the “Newton ofFrance”) Pierre-Simon, Marquis de Laplace. Although many people believe that the
famous marquis, who was also one of the great contributors to the development of
probability, might have exaggerated somewhat, it is nevertheless true that proba-bility theory has become a tool of fundamental importance to nearly all scientists,
engineers, medical practitioners, jurists, and industrialists. In fact, the enlightened
individual had learned to ask not “Is it so?” but rather “What is the probability thatit is so?”
General Approach and Mathematical Level
This book is intended as an elementary introduction to the theory of probabilityfor students in mathematics, statistics, engineering, and the sciences (including com-
puter science, biology, the social sciences, and management science) who possess the
prerequisite knowledge of elementary calculus. It attempts to present not only themathematics of probability theory, but also, through numerous examples, the many
diverse possible applications of this subject.
Content and Course Planning
Chapter 1 presents the basic principles of combinatorial analysis, which are mostuseful in computing probabilities.
Chapter 2 handles the axioms of probability theory and shows how they can be
applied to compute various probabilities of interest.
Chapter 3 deals with the extremely important subjects of conditional probability
and independence of events. By a series of examples, we illustrate how conditional
probabilities come into play not only when some partial information is available,
but also as a tool to enable us to compute probabilities more easily, even when
no partial information is present. This extremely important technique of obtainingprobabilities by “conditioning” reappears in Chapter 7, where we use it to obtain
expectations.
The concept of random variables is introduced in Chapters 4, 5, and 6. Discrete
random variables are dealt with in Chapter 4, continuous random variables inChapter 5, and jointly distributed random variables in Chapter 6. The important
concepts of the expected value and the variance of a random variable are intro-
duced in Chapters 4 and 5, and these quantities are then determined for many of the
common types of random variables.
ix

<<<PAGE 11>>>

July 14, 2014 A01_ROSSS4772_09_SE_FM page x
xPreface
Additional properties of the expected value are considered in Chapter 7. Many
examples illustrating the usefulness of the result that the expected value of a sum
of random variables is equal to the sum of their expected values are presented.
Sections on conditional expectation, including its use in prediction, and on moment-generating functions are contained in this chapter. In addition, the ﬁnal section
introduces the multivariate normal distribution and presents a simple proof con-
cerning the joint distribution of the sample mean and sample variance of a samplefrom a normal distribution.
Chapter 8 presents the major theoretical results of probability theory. In par-
ticular, we prove the strong law of large numbers and the central limit theorem.
Our proof of the strong law is a relatively simple one that assumes that the random
variables have a ﬁnite fourth moment, and our proof of the central limit theoremassumes Levy’s continuity theorem. This chapter also presents such probability
inequalities as Markov’s inequality, Chebyshev’s inequality, and Chernoff bounds.
The ﬁnal section of Chapter 8 gives a bound on the error involved when a probabilityconcerning a sum of independent Bernoulli random variables is approximated by the
corresponding probability of a Poisson random variable having the same expected
value.
Chapter 9 presents some additional topics, such as Markov chains, the Poisson
process, and an introduction to information and coding theory, and Chapter 10 con-siders simulation.
As in the previous edition, three sets of exercises are given at the end of each
chapter. They are designated as Problems, Theoretical Exercises , and Self-Test Prob-
lems and Exercises. This last set of exercises, for which complete solutions appear inSolutions to Self-Test Problems and Exercises, is designed to help students test their
comprehension and study for exams.
Changes for the Ninth Edition
The ninth edition continues the evolution and ﬁne tuning of the text. Aside from amultitude of small changes made to increase the clarity of the text, the new edition
includes many new and updated problems, exercises, and text material chosen bothfor inherent interest and for their use in building student intuition about probability.
Illustrative of these goals are Examples 3h and 4k of Chapter 3, which deal with
estimating the fraction of twin pairs that are identical and with analyzing serve andrally games.
Acknowledgments
I would like to thank the following people who have graciously taken the time tocontact me with comments for improving the text: Amir Ardestani, Polytechnic
University of Teheran; Joe Blitzstein, Harvard University; Peter Nuesch, Univer-
sity of Lausaunne; Joseph Mitchell, SUNY, Stony Brook; Alan Chambless, actuary;Robert Kriner; Israel David, Ben-Gurion University; T. Lim, George Mason Univer-
sity; Wei Chen, Rutgers; D. Monrad, University of Illinois; W. Rosenberger, George
Mason University; E. Ionides, University of Michigan; J. Corvino, Lafayette College;T. Seppalainen, University of Wisconsin; Jack Goldberg; University of Michigan;
Sunil Dhar, New Jersey Institute of Technology; Vladislav Kargin, Stanford Univer-
sity; Marlene Miller; Ahmad Parsian; and Fritz Scholz, University of Washington.
I would also like to especially thank the reviewers of the ninth edition: Richard
Laugesen, University of Illinois; Stacey Hancock, Clark University; Stefan Heinz,University of Wyoming; and Brian Thelen, University of Michigan. I would like to

<<<PAGE 12>>>

July 14, 2014 A01_ROSSS4772_09_SE_FM page xi
Preface xi
thank the accuracy checkers, Keith Friedman (University of Texas at Austin) and
Stacey Hancock (Clark University), for their careful review.
Finally, I would like to thank the following reviewers for their many helpful
comments. Reviewers of the ninth edition are marked with an asterisk.
K. B. Athreya, Iowa State University
Richard Bass, University of Connecticut
Robert Bauer, University of Illinois at
Urbana-Champaign
Phillip Beckwith, Michigan Tech
Arthur Benjamin, Harvey Mudd College
Geoffrey Berresford, Long Island University
Baidurya Bhattacharya, University of Delaware
Howard Bird, St. Cloud State University
Shahar Boneh, Metropolitan State College of
Denver
Jean Cadet, State University of New York at Stony
Brook
Steven Chiappari, Santa Clara University
Nicolas Christou, University of California, Los
Angeles
James Clay, University of Arizona at Tucson
Francis Conlan, University of Santa Clara
Justin Corvino, Lafayette College
Jay DeVore, California Polytechnic University,
San Luis Obispo
Scott Emerson, University of Washington
Thomas R. Fischer, Texas A & M University
Anant Godbole, Michigan Technical
University
Zakkula Govindarajulu, University of Kentucky
Richard Groeneveld, Iowa State University
*Stacey Hancock, Clark University
Mike Hardy, Massachusetts Institute of
Technology
Bernard Harris, University of Wisconsin
Larry Harris, University of Kentucky
David Heath, Cornell University
*Stefan Heinz, University of Wyoming
Stephen Herschkorn, Rutgers University
Julia L. Higle, University of Arizona
Mark Huber, Duke UniversityEdward Ionides, University of Michigan
Anastasia Ivanova, University of North
Carolina
Hamid Jafarkhani, University of California,
Irvine
Chuanshu Ji, University of North Carolina,
Chapel Hill
Robert Keener, University of Michigan
*Richard Laugesen, University of Illinois
Fred Leysieffer, Florida State University
Thomas Liggett, University of California, Los
Angeles
Helmut Mayer, University of Georgia
Bill McCormick, University of Georgia
Ian McKeague, Florida State University
R. Miller, Stanford University
Ditlev Monrad, University of Illinois
Robb J. Muirhead, University of Michigan
Joe Naus, Rutgers University
Nhu Nguyen, New Mexico State University
Ellen O’Brien, George Mason University
N. U. Prabhu, Cornell University
Kathryn Prewitt, Arizona State University
Jim Propp, University of Wisconsin
William F. Rosenberger, George Mason University
Myra Samuels, Purdue University
I. R. Savage, Yale University
Art Schwartz, University of Michigan at Ann Arbor
Therese Shelton, Southwestern University
Malcolm Sherman, State University of New York at
Albany
Murad Taqqu, Boston University
*Brian Thelen, University of Michigan
Eli Upfal, Brown University
Ed Wheeler, University of Tennessee
Allen Webster, Bradley University
S. R.
smross@usc.edu

<<<PAGE 13>>>

July 14, 2014 A01_ROSSS4772_09_SE_FM page xii
This page intentionally left blank 

<<<PAGE 14>>>

Chapter
Combinatorial Analysis1
Contents
1.1 Introduction
1.2 The Basic Principle of Counting
1.3 Permutations
1.4 Combinations1.5 Multinomial Coefﬁcients
1.6 The Number of Integer Solutions of
Equations
1.1 Introduction
Here is a typical problem of interest involving probability: A communication system
is to consist of nseemingly identical antennas that are to be lined up in a linear order.
The resulting system will then be able to receive all incoming signals—and will becalled functional —as long as no two consecutive antennas are defective. If it turns
out that exactly mof the nantennas are defective, what is the probability that the
resulting system will be functional? For instance, in the special case where n=4 and
m=2, there are 6 possible system conﬁgurations, namely,
011001011010
0011
10011100
where 1 means that the antenna is working and 0 that it is defective. Because theresulting system will be functional in the ﬁrst 3 arrangements and not functional in
the remaining 3, it seems reasonable to take
3
6=1
2as the desired probability. In
the case of general nand m, we could compute the probability that the system is
functional in a similar fashion. That is, we could count the number of conﬁgurations
that result in the system’s being functional and then divide by the total number of all
possible conﬁgurations.
From the preceding discussion, we see that it would be useful to have an effec-
tive method for counting the number of ways that things can occur. In fact, many
problems in probability theory can be solved simply by counting the number of dif-ferent ways that a certain event can occur. The mathematical theory of counting is
formally known as combinatorial analysis.
1

<<<PAGE 15>>>

2Chapter 1 Combinatorial Analysis
1.2 The Basic Principle of Counting
The basic principle of counting will be fundamental to all our work. Loosely put, it
states that if one experiment can result in any of mpossible outcomes and if another
experiment can result in any of npossible outcomes, then there are mnpossible
outcomes of the two experiments.
The basic principle of counting
Suppose that two experiments are to be performed. Then if experiment 1 canresult in any one of mpossible outcomes and if, for each outcome of experiment
1, there are npossible outcomes of experiment 2, then together there are mn
possible outcomes of the two experiments.
Proof of the Basic Principle: The basic principle may be proven by enumerating
all the possible outcomes of the two experiments; that is,
(1, 1), (1, 2), ...,(1,n)
(2, 1), (2, 2), ...,(2,n)
#
#
#
(m,1 ),(m,2 ),...,(m,n)
where we say that the outcome is (i, j) if experiment 1 results in its ith possible
outcome and experiment 2 then results in its jth possible outcome. Hence, the set of
possible outcomes consists of mrows, each containing nelements. This proves the
result.
Example
2aA small community consists of 10 women, each of whom has 3 children. If one
woman and one of her children are to be chosen as mother and child of the year,how many different choices are possible?
Solution By regarding the choice of the woman as the outcome of the ﬁrst experi-
ment and the subsequent choice of one of her children as the outcome of the second
experiment, we see from the basic principle that there are 10 *3=30 possible
choices. .
When there are more than two experiments to be performed, the basic principle
can be generalized.
The generalized basic principle of counting
Ifrexperiments that are to be performed are such that the ﬁrst one may result
in any of n1possible outcomes; and if, for each of these n1possible outcomes,
there are n2possible outcomes of the second experiment; and if, for each of the
possible outcomes of the ﬁrst two experiments, there are n3possible outcomes
of the third experiment; and if ..., then there is a total of n1·n2···nrpossible
outcomes of the rexperiments.
Example
2bA college planning committee consists of 3 freshmen, 4 sophomores, 5 juniors, and2 seniors. A subcommittee of 4, consisting of 1 person from each class, is to be cho-
sen. How many different subcommittees are possible?

<<<PAGE 16>>>

A First Course in Probability 3
Solution We may regard the choice of a subcommittee as the combined outcome of
the four separate experiments of choosing a single representative from each of the
classes. It then follows from the generalized version of the basic principle that there
are 3 *4*5*2=120 possible subcommittees. .
Example
2cHow many different 7-place license plates are possible if the ﬁrst 3 places are to beoccupied by letters and the ﬁnal 4 by numbers?
Solution By the generalized version of the basic principle, the answer is 26 ·26·
26·10·10·10·10=175,760,000. .
Example
2dHow many functions deﬁned on npoints are possible if each functional value is
either 0 or 1?
Solution Let the points be 1, 2, ...,n. Since f(i)must be either 0 or 1 for each
i=1, 2,...,n, it follows that there are 2npossible functions. .
Example
2eIn Example 2c, how many license plates would be possible if repetition among lettersor numbers were prohibited?
Solution In this case, there would be 26 ·25·24·10·9·8·7=78,624,000
possible license plates. .
1.3 Permutations
How many different ordered arrangements of the letters a,b, and care possible?
By direct enumeration we see that there are 6, namely, abc, acb, bac, bca, cab ,
and cba. Each arrangement is known as a permutation. Thus, there are 6 possible
permutations of a set of 3 objects. This result could also have been obtainedfrom the basic principle, since the ﬁrst object in the permutation can be any ofthe 3, the second object in the permutation can then be chosen from any of the
remaining 2, and the third object in the permutation is then the remaining 1.
Thus, there are 3 ·2·1=6 possible permutations.
Suppose now that we have nobjects. Reasoning similar to that we have just used
for the 3 letters then shows that there are
n(n−1)(n−2)···3·2·1=n!
different permutations of the nobjects.
Whereas n! (read as “n factorial”) is deﬁned to equal 1 ·2···nwhen nis a
positive integer, it is convenient to deﬁne 0! to equal 1.
Example
3aHow many different batting orders are possible for a baseball team consisting of 9
players?
Solution There are 9! =362,880 possible batting orders. .
Example
3bA class in probability theory consists of 6 men and 4 women. An examination is
given, and the students are ranked according to their performance. Assume that notwo students obtain the same score.
(a) How many different rankings are possible?

<<<PAGE 17>>>

4Chapter 1 Combinatorial Analysis
(b) If the men are ranked just among themselves and the women just among them-
selves, how many different rankings are possible?
Solution (a) Because each ranking corresponds to a particular ordered arrangement
of the 10 people, the answer to this part is 10! =3,628,800.
(b) Since there are 6! possible rankings of the men among themselves and 4!
possible rankings of the women among themselves, it follows from the basic principlethat there are (6!)(4!)=(720)(24) =17,280 possible rankings in this case. .
Example
3cMs. Jones has 10 books that she is going to put on her bookshelf. Of these, 4 are math-ematics books, 3 are chemistry books, 2 are history books, and 1 is a language book.
Ms. Jones wants to arrange her books so that all the books dealing with the same
subject are together on the shelf. How many different arrangements are possible?
Solution There are 4! 3! 2! 1! arrangements such that the mathematics books are
ﬁrst in line, then the chemistry books, then the history books, and then the languagebook. Similarly, for each possible ordering of the subjects, there are 4! 3! 2! 1! pos-sible arrangements. Hence, as there are 4! possible orderings of the subjects, the
d e s i r e da n s w e ri s4 !4 !3 !2 !1 ! =6912. .
We shall now determine the number of permutations of a set of nobjects when
certain of the objects are indistinguishable from one another. To set this situationstraight in our minds, consider the following example.
Example
3dHow many different letter arrangements can be formed from the letters PEPPER?
Solution We ﬁrst note that there are 6! permutations of the letters P1E1P2P3E2R
when the 3P ’s and the 2E ’s are distinguished from one another. However, consider
any one of these permutations—for instance, P1P2E1P3E2R. If we now permute the
P’s among themselves and the E’s among themselves, then the resultant arrange-
ment would still be of the form PPEPER. That is, all 3! 2! permutations
P1P2E1P3E2RP 1P2E2P3E1R
P1P3E1P2E2RP 1P3E2P2E1R
P2P1E1P3E2RP 2P1E2P3E1R
P2P3E1P1E2RP 2P3E2P1E1R
P3P1E1P2E2RP 3P1E2P2E1R
P3P2E1P1E2RP 3P2E2P1E1R
are of the form PPEPER. Hence, there are 6!/(3! 2! )=60 possible letter arrange-
ments of the letters PEPPER. .
In general, the same reasoning as that used in Example 3d shows that there are
n!
n1!n2!···nr!
different permutations of nobjects, of which n1are alike, n2are alike, ...,nrare
alike.
Example
3eA chess tournament has 10 competitors, of which 4 are Russian, 3 are from theUnited States, 2 are from Great Britain, and 1 is from Brazil. If the tournament
result lists just the nationalities of the players in the order in which they placed, how
many outcomes are possible?

<<<PAGE 18>>>

A First Course in Probability 5
Solution There are10!
4! 3! 2! 1!=12,600
possible outcomes. .
Example
3fHow many different signals, each consisting of 9 ﬂags hung in a line, can be made
from a set of 4 white ﬂags, 3 red ﬂags, and 2 blue ﬂags if all ﬂags of the same color
are identical?
Solution There are9!
4! 3! 2!=1260
different signals. .
1.4 Combinations
We are often interested in determining the number of different groups of robjects
that could be formed from a total of nobjects. For instance, how many different
groups of 3 could be selected from the 5 items A,B,C,D, and E? To answer this
question, reason as follows: Since there are 5 ways to select the initial item, 4 ways to
then select the next item, and 3 ways to select the ﬁnal item, there are thus 5 ·4·3
ways of selecting the group of 3 when the order in which the items are selected is
relevant. However, since every group of 3—say, the group consisting of items A,B,
and C—will be counted 6 times (that is, all of the permutations ABC, ACB, BAC,
BCA, CAB, and CBA will be counted when the order of selection is relevant), it
follows that the total number of groups that can be formed is
5·4·3
3·2·1=10
In general, as n(n−1)···(n−r+1)represents the number of different ways that
a group of ritems could be selected from nitems when the order of selection is
relevant, and as each group of ritems will be counted r! times in this count, it follows
that the number of different groups of ritems that could be formed from a set of n
items is
n(n−1)···(n−r+1)
r!=n!
(n−r)!r!
Notation and terminology
We deﬁne/parenleftBigg
nr/parenrightBigg
,f o r r…n,b y
/parenleftBigg
n
r/parenrightBigg
=n!
(n−r)!r!
and say that/parenleftBigg
n
r/parenrightBigg
(read as “n choose r”) represents the number of possible
combinations of nobjects taken rat a time.†
†By convention, 0! is deﬁned to be 1. Thus,/parenleftBigg
n
0/parenrightBigg
=/parenleftBigg
nn/parenrightBigg
=1. We also take/parenleftBigg
n
i/parenrightBigg
to be equal to 0 when
either i<0o r i>n.

<<<PAGE 19>>>

6Chapter 1 Combinatorial Analysis
Thus,/parenleftBigg
n
r/parenrightBigg
represents the number of different groups of size rthat could be
selected from a set of nobjects when the order of selection is not considered relevant.
Equivalently,/parenleftBigg
n
r/parenrightBigg
is the number of subsets of size rthat can be chosen from
a set of size n. Using that 0! =1, note that/parenleftBigg
n
n/parenrightBigg
=/parenleftBigg
n
0/parenrightBigg
=n!
0!n!=1, which is
consistent with the preceding interpretation because in a set of size nthere is exactly
1 subset of size n(namely, the entire set), and exactly one subset of size 0 (namely
the empty set). A useful convention is to deﬁne/parenleftBigg
n
r/parenrightBigg
equal to 0 when either r>n
orr<0.
Example
4aA committee of 3 is to be formed from a group of 20 people. How many different
committees are possible?
Solution There are/parenleftBigg
20
3/parenrightBigg
=20·19·18
3·2·1=1140 possible committees. .
Example
4bFrom a group of 5 women and 7 men, how many different committees consisting of
2 women and 3 men can be formed? What if 2 of the men are feuding and refuse toserve on the committee together?
Solution As there are/parenleftBigg
52/parenrightBigg
possible groups of 2 women, and/parenleftBigg
73/parenrightBigg
possible
groups of 3 men, it follows from the basic principle that there are/parenleftBigg
52/parenrightBigg/parenleftBigg
73/parenrightBigg
=
/parenleftBigg
5·4
2·1/parenrightBigg
7·6·5
3·2·1=350 possible committees consisting of 2 women and 3 men.
Now suppose that 2 of the men refuse to serve together. Because a total of/parenleftBigg
22/parenrightBigg/parenleftBigg
51/parenrightBigg
=5 out of the/parenleftBigg
73/parenrightBigg
=35 possible groups of 3 men contain both of
the feuding men, it follows that there are 35 −5=30 groups that do not contain
both of the feuding men. Because there are still/parenleftBigg
52/parenrightBigg
=10 ways to choose the 2
women, there are 30 ·10=300 possible committees in this case. .
Example
4cConsider a set of nantennas of which mare defective and n−mare functional
and assume that all of the defectives and all of the functionals are considered indis-
tinguishable. How many linear orderings are there in which no two defectives are
consecutive?
Solution Imagine that the n−mfunctional antennas are lined up among them-
selves. Now, if no two defectives are to be consecutive, then the spaces between thefunctional antennas must each contain at most one defective antenna. That is, in the
n−m+1 possible positions—represented in Figure 1.1 by carets—between the
n−mfunctional antennas, we must select mof these in which to put the defective
antennas. Hence, there are/parenleftBigg
n−m+1
m/parenrightBigg
possible orderings in which there is at
least one functional antenna between any two defective ones. .

<<<PAGE 20>>>

A First Course in Probability 7
^ 1 ^ 1 ^ 1 . . . ^ 1 ^ 1 ^
1 /H11005 functional
^ /H11005 place for at most one defective
Figure 1.1 No consecutive defectives.
A useful combinatorial identity is
/parenleftBigg
n
r/parenrightBigg
=/parenleftBigg
n−1
r−1/parenrightBigg
+/parenleftBigg
n−1
r/parenrightBigg
1…r…n (4.1)
Equation (4.1) may be proved analytically or by the following combinatorial argu-
ment: Consider a group of nobjects, and ﬁx attention on some particular one of
these objects—call it object 1. Now, there are/parenleftBigg
n−1
r−1/parenrightBigg
groups of size rthat con-
tain object 1 (since each such group is formed by selecting r−1 from the remaining
n−1 objects). Also, there are/parenleftBigg
n−1
r/parenrightBigg
groups of size rthat do not contain object
1. As there is a total of/parenleftBigg
n
r/parenrightBigg
groups of size r, Equation (4.1) follows.
The values/parenleftBigg
n
r/parenrightBigg
are often referred to as binomial coefﬁcients because of their
prominence in the binomial theorem.
The binomial theorem
(x+y)n=n/summationdisplay
k=0/parenleftBigg
nk/parenrightBigg
x
kyn−k(4.2)
We shall present two proofs of the binomial theorem. The ﬁrst is a proof by
mathematical induction, and the second is a proof based on combinatorial consider-ations.
Proof of the Binomial Theorem by Induction: When n=1, Equation (4.2) reduces to
x+y=/parenleftBigg
1
0/parenrightBigg
x
0y1+/parenleftBigg
11/parenrightBigg
x
1y0=y+x
Assume Equation (4.2) for n−1. Now,
(x+y)n=(x+y)(x+y)n−1
=(x+y)n−1/summationdisplay
k=0/parenleftBigg
n−1
k/parenrightBigg
xkyn−1−k
=n−1/summationdisplay
k=0/parenleftBigg
n−1
k/parenrightBigg
xk+1yn−1−k+n−1/summationdisplay
k=0/parenleftBigg
n−1
k/parenrightBigg
xkyn−k

<<<PAGE 21>>>

8Chapter 1 Combinatorial Analysis
Letting i=k+1 in the ﬁrst sum and i=kin the second sum, we ﬁnd that
(x+y)n=n/summationdisplay
i=1/parenleftBigg
n−1
i−1/parenrightBigg
xiyn−i+n−1/summationdisplay
i=0/parenleftBigg
n−1
i/parenrightBigg
xiyn−i
=xn+n−1/summationdisplay
i=1⎡
⎣/parenleftBigg
n−1
i−1/parenrightBigg
+/parenleftBigg
n−1
i/parenrightBigg⎤⎦x
iyn−i+yn
=xn+n−1/summationdisplay
i=1/parenleftBigg
n
i/parenrightBigg
xiyn−i+yn
=n/summationdisplay
i=0/parenleftBigg
n
i/parenrightBigg
xiyn−i
where the next-to-last equality follows by Equation (4.1). By induction, the theorem
is now proved.
Combinatorial Proof of the Binomial Theorem: Consider the product
(x1+y1)(x2+y2)···(xn+yn)
Its expansion consists of the sum of 2nterms, each term being the product of nfac-
tors. Furthermore, each of the 2nterms in the sum will contain as a factor either xi
oryifor each i=1, 2,...,n. For example,
(x1+y1)(x2+y2)=x1x2+x1y2+y1x2+y1y2
Now, how many of the 2nterms in the sum will have kof the xi’s and (n−k)of
the yi’s as factors? As each term consisting of kof the xi’s and (n−k)of the yi’s
corresponds to a choice of a group of kfrom the nvalues x1,x2,...,xn, there are/parenleftBigg
n
k/parenrightBigg
such terms. Thus, letting xi=x,yi=y,i=1,...,n, we see that
(x+y)n=n/summationdisplay
k=0/parenleftBigg
n
k/parenrightBigg
xkyn−k
Example
4dExpand (x+y)3.
Solution
(x+y)3=/parenleftBigg
3
0/parenrightBigg
x0y3+/parenleftBigg
31/parenrightBigg
x
1y2+/parenleftBigg
32/parenrightBigg
x
2y1+/parenleftBigg
33/parenrightBigg
x
3y0
=y3+3xy2+3x2y+x3.
Example
4eHow many subsets are there of a set consisting of nelements?
Solution Since there are/parenleftBigg
nk/parenrightBigg
subsets of size k, the desired answer is
n/summationdisplay
k=0/parenleftBigg
n
k/parenrightBigg
=(1+1)n=2n

<<<PAGE 22>>>

A First Course in Probability 9
This result could also have been obtained by assigning either the number 0 or the
number 1 to each element in the set. To each assignment of numbers, there cor-
responds, in a one-to-one fashion, a subset, namely, that subset consisting of all
elements that were assigned the value 1. As there are 2npossible assignments, the
result follows.
Note that we have included the set consisting of 0 elements (that is, the null set)
as a subset of the original set. Hence, the number of subsets that contain at least 1element is 2
n−1. .
1.5 Multinomial Coefﬁcients
In this section, we consider the following problem: A set of ndistinct items is to be
divided into rdistinct groups of respective sizes n1,n2,...,nr, where/summationtextr
i=1ni=n.
How many different divisions are possible? To answer this question, we note that
there are/parenleftBigg
n
n1/parenrightBigg
possible choices for the ﬁrst group; for each choice of the ﬁrst group,
there are/parenleftBigg
n−n1
n2/parenrightBigg
possible choices for the second group; for each choice of the
ﬁrst two groups, there are/parenleftBigg
n−n1−n2
n3/parenrightBigg
possible choices for the third group; and
so on. It then follows from the generalized version of the basic counting principle
that there are
/parenleftBigg
n
n1/parenrightBigg/parenleftBigg
n−n1
n2/parenrightBigg
···/parenleftBigg
n−n1−n2− ··· − nr−1
nr/parenrightBigg
=n!
(n−n1)!n1!(n−n1)!
(n−n1−n2)!n2!···(n−n1−n2− ··· − nr−1)!
0!nr!
=n!
n1!n2!···n r!
possible divisions.
Another way to see this result is to consider the nvalues 1, 1, ...,1 ,2 ,... ,2 ,... ,
r,...,r, where iappears nitimes, for i=1,...,r.Every permutation of these values
corresponds to a division of the nitems into the rgroups in the following manner:
Let the permutation i1,i2,...,incorrespond to assigning item 1 to group i1, item 2 to
group i2, and so on. For instance, if n=8 and if n1=4,n2=3, and n3=1, then
the permutation 1, 1, 2, 3, 2, 1, 2, 1 corresponds to assigning items 1, 2, 6, 8 to the ﬁrst
group, items 3, 5, 7 to the second group, and item 4 to the third group. Because every
permutation yields a division of the items and every possible division results from
some permutation, it follows that the number of divisions of nitems into rdistinct
groups of sizes n1,n2,...,nris the same as the number of permutations of nitems of
which n1are alike, and n2are alike, ..., and nrare alike, which was shown in Section
1.3 to equaln!
n1!n2!···nr!.

<<<PAGE 23>>>

10Chapter 1 Combinatorial Analysis
Notation
Ifn1+n2+ ··· + nr=n, we deﬁne/parenleftBigg
n
n1,n2,...,nr/parenrightBigg
by
/parenleftBigg
n
n1,n2,...,nr/parenrightBigg
=n!
n1!n2!···nr!
Thus,/parenleftBigg
n
n1,n2,...,nr/parenrightBigg
represents the number of possible divisions of ndistinct
objects into rdistinct groups of respective sizes n1,n2,...,nr.
Example
5aA police department in a small city consists of 10 ofﬁcers. If the department policy is
to have 5 of the ofﬁcers patrolling the streets, 2 of the ofﬁcers working full time at the
station, and 3 of the ofﬁcers on reserve at the station, how many different divisionsof the 10 ofﬁcers into the 3 groups are possible?
Solution There are10!
5! 2! 3!=2520 possible divisions. .
Example
5bTen children are to be divided into an Ateam and a Bteam of 5 each. The Ateam
will play in one league and the Bteam in another. How many different divisions are
possible?
Solution There are10!
5! 5!=252 possible divisions. .
Example
5cIn order to play a game of basketball, 10 children at a playground divide themselves
into two teams of 5 each. How many different divisions are possible?
Solution Note that this example is different from Example 5b because now the
order of the two teams is irrelevant. That is, there is no AorBteam, but just a
division consisting of 2 groups of 5 each. Hence, the desired answer is
10!/(5! 5! )
2!=126 .
The proof of the following theorem, which generalizes the binomial theorem, is
left as an exercise.
The multinomial theorem
(x1+x2+ ··· + xr)n=
/summationdisplay
(n1,...,nr):
n1+ ··· + nr=n/parenleftBigg
n
n1,n2,...,nr/parenrightBigg
xn1
1xn2
2···xnr
r
That is, the sum is over all nonnegative integer-valued vectors (n1,n2,...,nr)
such that n1+n2+ ··· + nr=n.
The numbers/parenleftBigg
n
n1,n2,...,nr/parenrightBigg
are known as multinomial coefﬁcients .

<<<PAGE 24>>>

A First Course in Probability 11
Example
5dIn the ﬁrst round of a knockout tournament involving n=2mplayers, the nplayers
are divided into n/2 pairs, with each of these pairs then playing a game. The losers
of the games are eliminated while the winners go on to the next round, where the
process is repeated until only a single player remains. Suppose we have a knockout
tournament of 8 players.
(a) How many possible outcomes are there for the initial round? (For instance,one outcome is that 1 beats 2, 3 beats 4, 5 beats 6, and 7 beats 8.)
(b) How many outcomes of the tournament are possible, where an outcome gives
complete information for all rounds?
Solution One way to determine the number of possible outcomes for the initial
round is to ﬁrst determine the number of possible pairings for that round. To do so,note that the number of ways to divide the 8 players into a ﬁrst pair, a second pair, a
third pair, and a fourth pair is/parenleftbigg8
2, 2, 2, 2/parenrightbigg
=8!
24.Thus, the number of possible pair-
ings when there is no ordering of the 4 pairs is8!
244!.For each such pairing, there are
2 possible choices from each pair as to the winner of that game, showing that there
are8!24
244!=8!
4!possible results of round 1. [Another way to see this is to note that
there are/parenleftbigg8
4/parenrightbigg
possible choices of the 4 winners and, for each such choice, there are
4! ways to pair the 4 winners with the 4 losers, showing that there are 4!/parenleftbigg84/parenrightbigg
=8!
4!
possible results for the ﬁrst round.]
Similarly, for each result of round 1, there are4!
2!possible outcomes of round 2,
and for each of the outcomes of the ﬁrst two rounds, there are2!
1!possible outcomes
of round 3. Consequently, by the generalized basic principle of counting, there are
8!
4!4!
2!2!
1!=8! possible outcomes of the tournament. Indeed, the same argument
can be used to show that a knockout tournament of n=2mplayers has n! possible
outcomes.
Knowing the preceding result, it is not difﬁcult to come up with a more direct
argument by showing that there is a one-to-one correspondence between the set of
possible tournament results and the set of permutations of 1, ...,n. To obtain such
a correspondence, rank the players as follows for any tournament result: Give the
tournament winner rank 1, and give the ﬁnal-round loser rank 2. For the two play-
ers who lost in the next-to-last round, give rank 3 to the one who lost to the player
ranked 1 and give rank 4 to the one who lost to the player ranked 2. For the four play-ers who lost in the second-to-last round, give rank 5 to the one who lost to player
ranked 1, rank 6 to the one who lost to the player ranked 2, rank 7 to the one who
lost to the player ranked 3, and rank 8 to the one who lost to the player ranked 4.Continuing on in this manner gives a rank to each player. (A more succinct descrip-
tion is to give the winner of the tournament rank 1 and let the rank of a player who
lost in a round having 2
kmatches be 2kplus the rank of the player who beat him, for
k=0,...,m−1.) In this manner, the result of the tournament can be represented
by a permutation i1,i2,...,in, where ijis the player who was given rank j. Because
different tournament results give rise to different permutations, and because there isa tournament result for each permutation, it follows that there are the same number
of possible tournament results as there are permutations of 1, ...,n. .

<<<PAGE 25>>>

12Chapter 1 Combinatorial Analysis
Example
5e(x1+x2+x3)2=/parenleftBigg
2
2, 0, 0/parenrightBigg
x2
1x0
2x03+/parenleftBigg
2
0, 2, 0/parenrightBigg
x0
1x2
2x0
3
+/parenleftBigg
2
0, 0, 2/parenrightBigg
x0
1x0
2x2
3+/parenleftBigg
2
1, 1, 0/parenrightBigg
x1
1x1
2x0
3
+/parenleftBigg
2
1, 0, 1/parenrightBigg
x1
1x0
2x1
3+/parenleftBigg
2
0, 1, 1/parenrightBigg
x0
1x1
2x13
=x2
1+x2
2+x23+2x1x2+2x1x3+2x2x3 .
*1.6 The Number of Integer Solutions of Equations
An individual has gone ﬁshing at Lake Ticonderoga, which contains four types of
ﬁsh: lake trout, catﬁsh, bass, and blueﬁsh. If we take the result of the ﬁshing trip tobe the numbers of each type of ﬁsh caught, let us determine the number of possible
outcomes when a total of 10 ﬁsh are caught. To do so, note that we can denote the
outcome of the ﬁshing trip by the vector (x
1,x2,x3,x4)where x1is the number of
trout that are caught, x2is the number of catﬁsh, x3is the number of bass, and x4is
the number of blueﬁsh. Thus, the number of possible outcomes when a total of 10ﬁsh are caught is the number of nonnegative vectors (x
1,x2,x3,x4)that sum to 10.
More generally, if we supposed there were rtypes of ﬁsh and that a total of n
were caught then the number of possible outcomes would be the number of nonneg-ative integer-valued vectors x
1,...,xrsuch that
x1+x2+...+xr=n (6.1)
To compute this number, let us start by considering the number of positive integer-valued vectors x
1,...,xrthat satisfy the preceding. To determine this number, sup-
pose that we have nconsecutive values zero lined up in a row:
000 ...00
Note that any selection of r−1o ft h e n−1 spaces between adjacent zeroes (see
Figure 1.2) corresponds to a positive solution of (6.1) by letting x1be the number
of zeroes before the ﬁrst chosen space, x2be the number of zeroes between the ﬁrst
and second chosen space, ..., and xnbeing the number of zeroes following the last
chosen space.
0 ^ 0 ^ 0 ^ . . . ^ 0 ^ 0
n objects 0
Choose r /H11002 1 of the spaces ^.
Figure 1.2 Number of positive solutions.
∗Asterisks denote material that is optional.

<<<PAGE 26>>>

A First Course in Probability 13
For instance, if we have n=8 and r=3, then (with the choices represented by dots)
the choice
0.0000 .000
corresponds to the solution x1=1,x2=4,x3=3.As positive solutions of (6.1)
correspond, in a one-to-one fashion, to choices of r−1 of the adjacent spaces, it
follows that the number of differerent positive solutions is equal to the number of
different selections of r−1o ft h e n−1 adjacent spaces. Consequently, we have
the following proposition.
Proposition
6.1There are/parenleftBigg
n−1
r−1/parenrightBigg
distinct positive integer-valued vectors (x1,x2,...,xr)satisfy-
ing the equation
x1+x2+ ··· + xr=nx i>0, i=1,...,r
To obtain the number of nonnegative (as opposed to positive) solutions, note
that the number of nonnegative solutions of x1+x2+ ··· + xr=nis the same
as the number of positive solutions of y1+ ··· + yr=n+r(seen by letting
yi=xi+1,i=1,...,r). Hence, from Proposition 6.1, we obtain the following
proposition.
Proposition
6.2There are/parenleftBigg
n+r−1
r−1/parenrightBigg
distinct nonnegative integer-valued vectors (x1,x2,...,xr)
satisfying the equation
x1+x2+ ··· + xr=n
Thus, using Proposition 6.2, we see that there are/parenleftBigg
13
3/parenrightBigg
=286 possible outcomes
when a total of 10 Lake Ticonderoga ﬁsh are caught.
Example
6aHow many distinct nonnegative integer-valued solutions of x1+x2=3 are possible?
Solution There are/parenleftBigg
3+2−1
2−1/parenrightBigg
=4 such solutions: (0, 3), (1, 2), (2, 1), (3, 0). .
Example
6bAn investor has $20,000 to invest among 4 possible investments. Each investmentmust be in units of $1000. If the total $20,000 is to be invested, how many different
investment strategies are possible? What if not all the money needs to be invested?
Solution If we let xi,i=1, 2, 3, 4, denote the number of thousands invested in
investment i, then, when all is to be invested, x1,x2,x3,x4are integers satisfying the
equation
x1+x2+x3+x4=20 xiÚ0
Hence, by Proposition 6.2, there are/parenleftBigg
23
3/parenrightBigg
=1771 possible investment strategies. If
not all of the money needs to be invested, then if we let x5denote the amount kept in
reserve, a strategy is a nonnegative integer-valued vector (x1,x2,x3,x4,x5)satisfying
the equation
x1+x2+x3+x4+x5=20
Hence, by Proposition 6.2, there are now/parenleftBigg
24
4/parenrightBigg
=10,626 possible strategies. .

<<<PAGE 27>>>

14Chapter 1 Combinatorial Analysis
Example
6cHow many terms are there in the multinomial expansion of (x1+x2+ ··· + xr)n?
Solution
(x1+x2+ ··· + xr)n=/summationdisplay/parenleftBigg
n
n1,...,nr/parenrightBigg
xn1
1···xnrr
where the sum is over all nonnegative integer-valued (n1,...,nr)such that n1+···+
nr=n. Hence, by Proposition 6.2, there are/parenleftBigg
n+r−1
r−1/parenrightBigg
such terms. .
Example
6dLet us consider again Example 4c, in which we have a set of nitems, of which mare
(indistinguishable and) defective and the remaining n−mare (also indistinguish-
able and) functional. Our objective is to determine the number of linear orderings
in which no two defectives are next to each other. To determine this number, let us
imagine that the defective items are lined up among themselves and the functionalones are now to be put in position. Let us denote x
1as the number of functional
items to the left of the ﬁrst defective, x2as the number of functional items between
the ﬁrst two defectives, and so on. That is, schematically, we have
x10x20···xm0xm+1
Now, there will be at least one functional item between any pair of defectives as long
asxi>0,i=2,...,m. Hence, the number of outcomes satisfying the condition is
the number of vectors x1,...,xm+1 that satisfy the equation
x1+ ··· + xm+1=n−mx 1Ú0,xm+1 Ú0,xi>0,i=2,...,m
But, on letting y1=x1+1,yi=xi,i=2,...,m,ym+1=xm+1+1, we see that
this number is equal to the number of positive vectors (y1,...,ym+1)that satisfy the
equation
y1+y2+ ··· + ym+1=n−m+2
Hence, by Proposition 6.1, there are/parenleftBigg
n−m+1
m/parenrightBigg
such outcomes, in agreement
with the results of Example 4c.
Suppose now that we are interested in the number of outcomes in which each
pair of defective items is separated by at least 2 functional items. By the same rea-soning as that applied previously, this would equal the number of vectors satisfying
the equation
x
1+ ··· + xm+1=n−mx 1Ú0,xm+1 Ú0,xiÚ2,i=2,...,m
Upon letting y1=x1+1,yi=xi−1,i=2,...,m,ym+1=xm+1+1, we see that
this is the same as the number of positive solutions of the equation
y1+ ··· + ym+1=n−2m+3
Hence, from Proposition 6.1, there are/parenleftBigg
n−2m+2
m/parenrightBigg
such outcomes. .

<<<PAGE 28>>>

A First Course in Probability 15
Summary
The basic principle of counting states that if an experiment
consisting of two phases is such that there are npossible
outcomes of phase 1 and, for each of these noutcomes,
there are mpossible outcomes of phase 2, then there are
nmpossible outcomes of the experiment.
There are n!=n(n−1)···3·2·1 possible linear
orderings of nitems. The quantity 0! is deﬁned to equal 1.
Let
/parenleftbigg
n
i/parenrightbigg
=n!
(n−i)!i!
when 0 …i…n, and let it equal 0 otherwise. This quan-
tity represents the number of different subgroups of size i
that can be chosen from a set of size n. It is often called abinomial coefﬁcient because of its prominence in the bino-
mial theorem, which states that
(x+y)n=n/summationdisplay
i=0/parenleftbigg
n
i/parenrightbigg
xiyn−i
For nonnegative integers n1,...,nrsumming to n,
/parenleftbigg
n
n1,n2,...,nr/parenrightbigg
=n!
n1!n2!···nr!
is the number of divisions of nitems into rdistinct
nonoverlapping subgroups of sizes n1,n2...,nr.These
quantities are called multinomial coefﬁcients.
Problems
1.(a)How many different 7-place license plates are possi-
ble if the ﬁrst 2 places are for letters and the other 5 for
numbers?
(b)Repeat part (a) under the assumption that no let-
ter or number can be repeated in a single license
plate.
2.How many outcome sequences are possible when a die
is rolled four times, where we say, for instance, that the
outcome is 3, 4, 3, 1 if the ﬁrst roll landed on 3, the second
on 4, the third on 3, and the fourth on 1?
3.Twenty workers are to be assigned to 20 different jobs,
one to each job. How many different assignments are pos-
sible?
4.John, Jim, Jay, and Jack have formed a band consist-
ing of 4 instruments. If each of the boys can play all 4
instruments, how many different arrangements are possi-
ble? What if John and Jim can play all 4 instruments, butJay and Jack can each play only piano and drums?
5.For years, telephone area codes in the United States and
Canada consisted of a sequence of three digits. The ﬁrst
digit was an integer between 2 and 9, the second digit was
either 0 or 1, and the third digit was any integer from 1 to9. How many area codes were possible? How many area
codes starting with a 4 were possible?
6.A well-known nursery rhyme starts as follows:
“As I was going to St. Ives
I met a man with 7 wives.Each wife had 7 sacks.Each sack had 7 cats.Each cat had 7 kittens ...”
How many kittens did the traveler meet?7.(a)In how many ways can 3 boys and 3 girls sit in a row?
(b)In how many ways can 3 boys and 3 girls sit in a row if
the boys and the girls are each to sit together?(c)In how many ways if only the boys must sit together?
(d)In how many ways if no two people of the same sex are
allowed to sit together?
8.How many different letter arrangements can be made
from the letters
(a)Fluke?
(b)Propose?
(c)Mississippi?
(d)Arrange?
9.A child has 12 blocks, of which 6 are black, 4 are red, 1
is white, and 1 is blue. If the child puts the blocks in a line,
how many arrangements are possible?
10.In how many ways can 8 people be seated in a row if
(a)there are no restrictions on the seating arrangement?
(b)persons Aand Bmust sit next to each other?
(c)there are 4 men and 4 women and no 2 men or 2 women
can sit next to each other?
(d)there are 5 men and they must sit next to one another?
(e)there are 4 married couples and each couple must sit
together?
11.In how many ways can 3 novels, 2 mathematics books,
and 1 chemistry book be arranged on a bookshelf if
(a)the books can be arranged in any order?
(b)the mathematics books must be together and the nov-
els must be together?
(c)the novels must be together, but the other books can
be arranged in any order?

<<<PAGE 29>>>

16Chapter 1 Combinatorial Analysis
12.Five separate awards (best scholarship, best leadership
qualities, and so on) are to be presented to selected stu-
dents from a class of 30. How many different outcomesare possible if
(a)a student can receive any number of awards?
(b)each student can receive at most 1 award?
13.Consider a group of 20 people. If everyone shakes
hands with everyone else, how many handshakes take
place?
14.How many 5-card poker hands are there?
15.A dance class consists of 22 students, of which 10 are
women and 12 are men. If 5 men and 5 women are to be
chosen and then paired off, how many results are possible?
16.A student has to sell 2 books from a collection of
6 math, 7 science, and 4 economics books. How many
choices are possible if
(a)both books are to be on the same subject?
(b)the books are to be on different subjects?
17.Seven different gifts are to be distributed among 10
children. How many distinct results are possible if no child
is to receive more than one gift?
18.A committee of 7, consisting of 2 Republicans, 2
Democrats, and 3 Independents, is to be chosen from a
group of 5 Republicans, 6 Democrats, and 4 Independents.How many committees are possible?
19.From a group of 8 women and 6 men, a committee con-
sisting of 3 men and 3 women is to be formed. How many
different committees are possible if
(a)2 of the men refuse to serve together?
(b)2 of the women refuse to serve together?
(c)1 man and 1 woman refuse to serve together?
20.A person has 8 friends, of whom 5 will be invited to a
party.(a)How many choices are there if 2 of the friends are feud-
ing and will not attend together?
(b)How many choices if 2 of the friends will only attend
together?
21.Consider the grid of points shown at the top of the
next column. Suppose that, starting at the point labeled
A, you can go one step up or one step to the right at each
move. This procedure is continued until the point labeled
Bis reached. How many different paths from AtoBare
possible?
Hint : Note that to reach Bfrom A, you must take 4 steps
to the right and 3 steps upward.B
A
22.In Problem 21, how many different paths are there
from AtoBthat go through the point circled in the fol-
lowing lattice?
B
A
23.A psychology laboratory conducting dream research
contains 3 rooms, with 2 beds in each room. If 3 sets of
identical twins are to be assigned to these 6 beds so that
each set of twins sleeps in different beds in the same room,
how many assignments are possible?
24.Expand (3x2+y)5.
25.The game of bridge is played by 4 players, each of
whom is dealt 13 cards. How many bridge deals are pos-
sible?
26.Expand (x1+2x2+3x3)4.
27.If 12 people are to be divided into 3 committees of
respective sizes 3, 4, and 5, how many divisions are pos-
sible?
28.If 8 new teachers are to be divided among 4 schools,
how many divisions are possible? What if each school must
receive 2 teachers?
29.Ten weight lifters are competing in a team weight-
lifting contest. Of the lifters, 3 are from the United States,
4 are from Russia, 2 are from China, and 1 is from Canada.
If the scoring takes account of the countries that the liftersrepresent, but not their individual identities, how many
different outcomes are possible from the point of view
of scores? How many different outcomes correspond to

<<<PAGE 30>>>

A First Course in Probability 17
results in which the United States has 1 competitor in the
top three and 2 in the bottom three?
30.Delegates from 10 countries, including Russia, France,
England, and the United States, are to be seated in a row.
How many different seating arrangements are possible if
the French and English delegates are to be seated next to
each other and the Russian and U.S. delegates are not tobe next to each other?
*31.If 8 identical blackboards are to be divided among 4
schools, how many divisions are possible? How many if
each school must receive at least 1 blackboard?
*32.An elevator starts at the basement with 8 people (not
including the elevator operator) and discharges them all
by the time it reaches the top ﬂoor, number 6. In how manyways could the operator have perceived the people leaving
the elevator if all people look alike to him? What if the 8
people consisted of 5 men and 3 women and the operatorcould tell a man from a woman?*33.We have $20,000 that must be invested among 4 pos-
sible opportunities. Each investment must be integral in
units of $1000, and there are minimal investments that
need to be made if one is to invest in these opportuni-ties. The minimal investments are $2000, $2000, $3000, and
$4000. How many different investment strategies are avail-
able if
(a)an investment must be made in each opportunity?
(b)investments must be made in at least 3 of the 4 oppor-
tunities?
*34.Suppose that 10 ﬁsh are caught at a lake that contains
5 distinct types of ﬁsh.
(a)How many different outcomes are possible, where an
outcome speciﬁes the numbers of caught ﬁsh of each of the
5 types?
(b)How many outcomes are possible when 3 of the 10 ﬁsh
caught are trout?(c)How many when at least 2 of the 10 are trout?
Theoretical Exercises
1.Prove the generalized version of the basic counting
principle.
2.Two experiments are to be performed. The ﬁrst can
result in any one of mpossible outcomes. If the ﬁrst exper-
iment results in outcome i, then the second experiment
can result in any of nipossible outcomes, i=1, 2,...,m.
What is the number of possible outcomes of the two exper-
iments?
3.In how many ways can robjects be selected from a set of
nobjects if the order of selection is considered relevant?
4.There are/parenleftbigg
n
r/parenrightbigg
different linear arrangements of nballs
of which rare black and n−rare white. Give a combina-
torial explanation of this fact.5.Determine the number of vectors (x
1,...,xn), such that
each xiis either 0 or 1 and
n/summationdisplay
i=1xiÚk
6.How many vectors x1,...,xkare there for which each xi
is a positive integer such that 1 …xi…nand x1<x2<
···<xk?
7.Give an analytic proof of Equation (4.1).
8.Prove that
/parenleftbigg
n+m
r/parenrightbigg
=/parenleftbigg
n
0/parenrightbigg/parenleftbigg
m
r/parenrightbigg
+/parenleftbigg
n
1/parenrightbigg/parenleftbigg
m
r−1/parenrightbigg
+··· +/parenleftbigg
n
r/parenrightbigg/parenleftbigg
m
0/parenrightbiggHint : Consider a group of nmen and mwomen. How
many groups of size rare possible?
9.Use Theoretical Exercise 8 to prove that
/parenleftbigg
2n
n/parenrightbigg
=n/summationdisplay
k=0/parenleftbigg
n
k/parenrightbigg2
10.From a group of npeople, suppose that we want to
choose a committee of k,k…n, one of whom is to be des-
ignated as chairperson.
(a)By focusing ﬁrst on the choice of the committee and
then on the choice of the chair, argue that there are/parenleftbigg
n
k/parenrightbigg
k
possible choices.
(b)By focusing ﬁrst on the choice of the nonchair
committee members and then on the choice of the chair,
argue that there are/parenleftbigg
n
k−1/parenrightbigg
(n−k+1)possible
choices.
(c)By focusing ﬁrst on the choice of the chair and then
on the choice of the other committee members, argue that
there are n/parenleftbigg
n−1
k−1/parenrightbigg
possible choices.
(d)Conclude from parts (a), (b), and (c) that
k/parenleftBigg
n
k/parenrightBigg
=(n−k+1)/parenleftBigg
n
k−1/parenrightBigg
=n/parenleftBigg
n−1
k−1/parenrightBigg
(e)Use the factorial deﬁnition of/parenleftbigg
m
r/parenrightbigg
to verify the iden-
tity in part (d).

<<<PAGE 31>>>

18Chapter 1 Combinatorial Analysis
11.The following identity is known as Fermat’s combina-
torial identity:
/parenleftbigg
n
k/parenrightbigg
=n/summationdisplay
i=k/parenleftbigg
i−1
k−1/parenrightbigg
nÚk
Give a combinatorial argument (no computations are
needed) to establish this identity.
Hint : Consider the set of numbers 1 through n. How many
subsets of size khave ias their highest numbered member?
12.Consider the following combinatorial identity:
n/summationdisplay
k=1k/parenleftbigg
nk/parenrightbigg
=n·2
n−1
(a)Present a combinatorial argument for this identity by
considering a set of npeople and determining, in two ways,
the number of possible selections of a committee of any
size and a chairperson for the committee.
Hint :
(i) How many possible selections are there of a commit-
tee of size kand its chairperson?
(ii) How many possible selections are there of a chair-
person and the other committee members?
(b)Verify the following identity for n=1, 2, 3, 4, 5:
n/summationdisplay
k=1/parenleftbigg
n
k/parenrightbigg
k2=2n−2n(n+1)
For a combinatorial proof of the preceding, consider a set
ofnpeople and argue that both sides of the identity rep-
resent the number of different selections of a committee,its chairperson, and its secretary (possibly the same as thechairperson).
Hint :
(i) How many different selections result in the commit-
tee containing exactly kpeople?
(ii) How many different selections are there in which
the chairperson and the secretary are the same?
(answer: n2
n−1.)
(iii) How many different selections result in the chairper-
son and the secretary being different?
(c)Now argue that
n/summationdisplay
k=1/parenleftbigg
n
k/parenrightbigg
k3=2n−3n2(n+3)
13.Show that, for n>0,
n/summationdisplay
i=0(−1)i/parenleftbigg
n
i/parenrightbigg
=0
Hint : Use the binomial theorem.14.From a set of npeople, a committee of size jis to be
chosen, and from this committee, a subcommittee of size
i,i…j, is also to be chosen.
(a)Derive a combinatorial identity by computing, in two
ways, the number of possible choices of the committee and
subcommittee—ﬁrst by supposing that the committee is
chosen ﬁrst and then the subcommittee is chosen, and sec-ond by supposing that the subcommittee is chosen ﬁrst and
then the remaining members of the committee are chosen.
(b)Use part (a) to prove the following combinatorial
identity:
n/summationdisplay
j=i/parenleftbigg
n
j/parenrightbigg/parenleftbigg
j
i/parenrightbigg
=/parenleftbigg
n
i/parenrightbigg
2n−ii…n
(c)Use part (a) and Theoretical Exercise 13 to show that
n/summationdisplay
j=i/parenleftbigg
n
j/parenrightbigg/parenleftbigg
j
i/parenrightbigg
(−1)n−j=0i<n
15.Let Hk(n)be the number of vectors x1,...,xkfor
which each xiis a positive integer satisfying 1 …xi…n
and x1…x2…···…xk.
(a)Without any computations, argue that
H1(n)=n
Hk(n)=n/summationdisplay
j=1Hk−1(j)k>1
Hint : How many vectors are there in which xk=j?
(b)Use the preceding recursion to compute H3(5).
Hint : First compute H2(n)forn=1, 2, 3, 4, 5.
16.Consider a tournament of ncontestants in which the
outcome is an ordering of these contestants, with ties
allowed. That is, the outcome partitions the players into
groups, with the ﬁrst group consisting of the players who
tied for ﬁrst place, the next group being those who tiedfor the next-best position, and so on. Let N(n)denote
the number of different possible outcomes. For instance,N(2)=3, since, in a tournament with 2 contestants, player
1 could be uniquely ﬁrst, player 2 could be uniquely ﬁrst,
or they could tie for ﬁrst.
(a)List all the possible outcomes when n=3.
(b)With N(0) deﬁned to equal 1, argue, without any com-
putations, that
N(n)=
n/summationdisplay
i=1/parenleftbigg
n
i/parenrightbigg
N(n−i)
Hint : How many outcomes are there in which iplayers tie
for last place?

<<<PAGE 32>>>

A First Course in Probability 19
(c)Show that the formula of part (b) is equivalent to the
following:
N(n)=n−1/summationdisplay
i=0/parenleftbigg
n
i/parenrightbigg
N(i)
(d)Use the recursion to ﬁnd N(3) and N(4).
17.Present a combinatorial explanation of why/parenleftbigg
n
r/parenrightbigg
=/parenleftbigg
n
r,n−r/parenrightbigg
.
18.Argue that
/parenleftbigg
n
n1,n2,...,nr/parenrightbigg
=/parenleftbigg
n−1
n1−1,n2,...,nr/parenrightbigg
+/parenleftbigg
n−1
n1,n2−1,...,nr/parenrightbigg
+ ···
+/parenleftbigg
n−1
n1,n2,...,nr−1/parenrightbigg
Hint : Use an argument similar to the one used to establish
Equation (4.1).19.Prove the multinomial theorem.
*20.In how many ways can nidentical balls be distributed
into rurns so that the ith urn contains at least miballs, for
each i=1,...,r? Assume that nÚ/summationtextr
i=1mi.
*21.Argue that there are exactly/parenleftbigg
r
k/parenrightbigg/parenleftbigg
n−1
n−r+k/parenrightbigg
solutions of
x1+x2+ ··· + xr=n
for which exactly kof the xiare equal to 0.
*22.Consider a function f(x1,...,xn)ofnvariables. How
many different partial derivatives of order rdoes f
possess?
*23.Determine the number of vectors (x1,...,xn)such that
each xiis a nonnegative integer and
n/summationdisplay
i=1xi…k
Self-Test Problems and Exercises
1.How many different linear arrangements are there of
the letters A, B, C, D, E, F for which
(a)A and B are next to each other?
(b)Ai sb e f o r eB ?
(c)A is before B and B is before C?
(d)A is before B and C is before D?
(e)A and B are next to each other and C and D are also
next to each other?
(f)E is not last in line?
2.If 4 Americans, 3 French people, and 3 British people
are to be seated in a row, how many seating arrangements
are possible when people of the same nationality must sit
next to each other?
3.A president, treasurer, and secretary, all different, are to
be chosen from a club consisting of 10 people. How many
different choices of ofﬁcers are possible if
(a)there are no restrictions?
(b)Aand Bwill not serve together?
(c)Cand Dwill serve together or not at all?
(d)Emust be an ofﬁcer?
(e)Fwill serve only if he is president?
4.A student is to answer 7 out of 10 questions in an exami-
nation. How many choices has she? How many if she must
answer at least 3 of the ﬁrst 5 questions?
5.In how many ways can a man divide 7 gifts among his 3
children if the eldest is to receive 3 gifts and the others 2
each?6.How many different 7-place license plates are possible
when 3 of the entries are letters and 4 are digits? Assume
that repetition of letters and numbers is allowed and thatthere is no restriction on where the letters or numbers can
be placed.
7.Give a combinatorial explanation of the identity
/parenleftbigg
n
r/parenrightbigg
=/parenleftbigg
n
n−r/parenrightbigg
8.Consider n-digit numbers where each digit is one of the
10 integers 0, 1, ..., 9. How many such numbers are there
for which
(a)no two consecutive digits are equal?
(b)0 appears as a digit a total of itimes, i=0,...,n?
9.Consider three classes, each consisting of nstudents.
From this group of 3n students, a group of 3 students is
to be chosen.(a)How many choices are possible?
(b)How many choices are there in which all 3 students are
in the same class?
(c)H o wm a n yc h o i c e sa r et h e r ei nw h i c h2o ft h e3s t u -
dents are in the same class and the other student is in a
different class?
(d)How many choices are there in which all 3 students are
in different classes?(e)Using the results of parts (a) through (d), write a com-
binatorial identity.

<<<PAGE 33>>>

20Chapter 1 Combinatorial Analysis
10.How many 5-digit numbers can be formed from the
integers 1, 2, ..., 9 if no digit can appear more than twice?
(For instance, 41434 is not allowed.)
11.From 10 married couples, we want to select a group of
6 people that is not allowed to contain a married couple.
(a)How many choices are there?
(b)How many choices are there if the group must also
consist of 3 men and 3 women?
12.A committee of 6 people is to be chosen from a group
consisting of 7 men and 8 women. If the committee must
consist of at least 3 women and at least 2 men, how many
different committees are possible?
*13.An art collection on auction consisted of 4 Dalis, 5 van
Goghs, and 6 Picassos. At the auction were 5 art collectors.If a reporter noted only the number of Dalis, van Goghs,
and Picassos acquired by each collector, how many differ-
ent results could have been recorded if all of the workswere sold?
*14.Determine the number of vectors (x1,...,xn)such that
each xiis a positive integer and
n/summationdisplay
i=1xi…k
where kÚn.
15.A total of nstudents are enrolled in a review course
for the actuarial examination in probability. The postedresults of the examination will list the names of those who
passed, in decreasing order of their scores. For instance,
the posted result will be “Brown, Cho” if Brown and Choare the only ones to pass, with Brown receiving the higher
score. Assuming that all scores are distinct (no ties), how
many posted results are possible?16.How many subsets of size 4 of the set S={1, 2,...,2 0}
contain at least one of the elements 1, 2, 3, 4, 5?
17.Give an analytic veriﬁcation of
/parenleftbiggn
2/parenrightbigg
=/parenleftbiggk
2/parenrightbigg
+k(n−k)+/parenleftbiggn−k
2/parenrightbigg
,1 …k…n
Now, give a combinatorial argument for this identity.18.In a certain community, there are 3 families consisting
of a single parent and 1 child, 3 families consisting of a sin-
gle parent and 2 children, 5 families consisting of 2 parentsand a single child, 7 families consisting of 2 parents and 2
children, and 6 families consisting of 2 parents and 3 chil-
dren. If a parent and child from the same family are to bechosen, how many possible choices are there?
19.If there are no restrictions on where the digits and let-
ters are placed, how many 8-place license plates consisting
of 5 letters and 3 digits are possible if no repetitions of
letters or digits are allowed? What if the 3 digits must beconsecutive?
20.Verify that the equality
/summationdisplay
x1+...+xr=n,xiÚ0n!
x1!x2!···xr!=rn
when n=3,r=2, and then show that it always valid.
(The sum is over all vectors of rnonnegative integer val-
ues whose sum is n.)
Hint: How many different nletter sequences can be
formed from the ﬁrst rletters of the alphabet? How many
of them use letter iof the alphabet a total of xitimes for
each i=1,...,r?

<<<PAGE 34>>>

Chapter
Axioms of Probability2
Contents
2.1 Introduction
2.2 Sample Space and Events
2.3 Axioms of Probability
2.4 Some Simple Propositions2.5 Sample Spaces Having Equally Likely
Outcomes
2.6 Probability as a Continuous Set Function
2.7 Probability as a Measure of Belief
2.1 Introduction
In this chapter, we introduce the concept of the probability of an event and thenshow how probabilities can be computed in certain situations. As a preliminary,
however, we need to discuss the concept of the sample space and the events of anexperiment.
2.2 Sample Space and Events
Consider an experiment whose outcome is not predictable with certainty. However,although the outcome of the experiment will not be known in advance, let us suppose
that the set of all possible outcomes is known. This set of all possible outcomes of
an experiment is known as the sample space of the experiment and is denoted by S.
Following are some examples:
1. If the outcome of an experiment consists of the determination of the sex of a
newborn child, then
S={g,b}
where the outcome gmeans that the child is a girl and bthat it is a boy.
2. If the outcome of an experiment is the order of ﬁnish in a race among the 7
horses having post positions 1, 2, 3, 4, 5, 6, and 7, then
S={all 7! permutations of (1, 2, 3, 4, 5, 6, 7)}
The outcome (2, 3, 1, 6, 5, 4, 7) means, for instance, that the number 2 horsecomes in ﬁrst, then the number 3 horse, then the number 1 horse, and so on.
3. If the experiment consists of ﬂipping two coins, then the sample space consists
of the following four points:
S={(H,H),(H,T),(T,H),(T,T)}
21

<<<PAGE 35>>>

22Chapter 2 Axioms of Probability
The outcome will be (H ,H) if both coins are heads, ( H,T) if the ﬁrst coin is
heads and the second tails, (T ,H) if the ﬁrst is tails and the second heads, and
(T,T) if both coins are tails.
4. If the experiment consists of tossing two dice, then the sample space consists
of the 36 points
S={(i,j):i,j=1, 2, 3, 4, 5, 6}
where the outcome (i, j) is said to occur if iappears on the leftmost die and j
on the other die.
5. If the experiment consists of measuring (in hours) the lifetime of a transistor,
then the sample space consists of all nonnegative real numbers; that is,
S={x:0…x<q}
Any subset Eof the sample space is known as an event. In other words, an event
is a set consisting of possible outcomes of the experiment. If the outcome of the
experiment is contained in E, then we say that Ehas occurred. Following are some
examples of events.
In the preceding Example 1, if E={g}, then Eis the event that the child is a
girl. Similarly, if F={b}, then Fis the event that the child is a boy.
In Example 2, if
E={all outcomes in Sstarting with a 3}
then Eis the event that horse 3 wins the race.
In Example 3, if E={(H,H),(H,T)}, then Eis the event that a head appears
on the ﬁrst coin.
In Example 4, if E={(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)}, then Eis the event
that the sum of the dice equals 7.
In Example 5, if E={x:0…x…5}, then Eis the event that the transistor does
not last longer than 5 hours.
For any two events EandFof a sample space S, we deﬁne the new event E∪F
to consist of all outcomes that are either in Eor in For in both EandF. That is, the
event E∪Fwill occur if either E orFoccurs. For instance, in Example 1, if E={g}
is the event that the child is a girl and F={b}is the event that the child is a boy,
then
E∪F={g,b}
is the whole sample space S. In Example 3, if E={(H,H),(H,T)}is the event that
the ﬁrst coin lands heads, and F={(T,H),(H,H)}is the event that the second coin
lands heads, then
E∪F={(H,H),(H,T),(T,H)}
is the event that at least one of the coins lands heads and thus will occur providedthat both coins do not land tails.
The event E∪Fis called the union of the event Eand the event F.
Similarly, for any two events EandF, we may also deﬁne the new event EF,
called the intersection ofEandF, to consist of all outcomes that are both in Eand
inF. That is, the event EF(sometimes written E∩F) will occur only if both Eand
Foccur. For instance, in Example 3, if E={(H,H),(H,T),(T,H)}is the event that
at least 1 head occurs and F={(H,T),(T,H),(T,T)}is the event that at least 1 tail
occurs, then
EF={(H,T),(T,H)}

<<<PAGE 36>>>

A First Course in Probability 23
is the event that exactly 1 head and 1 tail occur. In Example 4, if E={(1, 6), (2, 5),
(3, 4), (4, 3), (5, 2), (6, 1)} is the event that the sum of the dice is 7 and F={(1, 5), (2, 4),
(3, 3), (4, 2), (5, 1)} is the event that the sum is 6, then the event EFdoes not contain
any outcomes and hence could not occur. To give such an event a name, we shall refer
to it as the null event and denote it by Ø. (That is, Ø refers to the event consisting of
no outcomes.) If EF=Ø, then EandFa r es a i dt ob e mutually exclusive .
We deﬁne unions and intersections of more than two events in a similar manner.
IfE1,E2,...are events, then the union of these events, denoted byq/uniontext
n=1En, is deﬁned
to be that event that consists of all outcomes that are in Enfor at least one value
ofn=1, 2,.... Similarly, the intersection of the events En, denoted byq/intersectiontext
n=1En,i s
deﬁned to be the event consisting of those outcomes that are in all of the eventsE
n,n=1, 2,....
Finally, for any event E, we deﬁne the new event Ec, referred to as the com-
plement ofE, to consist of all outcomes in the sample space Sthat are not in E.
That is, Ecwill occur if and only if Edoes not occur. In Example 4, if event E=
{(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)}, then Ecwill occur when the sum of the dice
does not equal 7. Note that because the experiment must result in some outcome, it
follows that Sc=Ø.
For any two events EandF, if all of the outcomes in Ea r ea l s oi nF , then we
say that Eiscontained inF,o rEis asubset ofF, and write E(F(or equivalently,
F)E, which we sometimes say as Fis asuperset ofE). Thus, if E(F, then the
occurrence of Eimplies the occurrence of F.I fE(FandF(E, we say that E
andFare equal and write E=F.
A graphical representation that is useful for illustrating logical relations among
events is the Venn diagram. The sample space Sis represented as consisting of
all the outcomes in a large rectangle, and the events E,F,G,...are represented
as consisting of all the outcomes in given circles within the rectangle. Events ofinterest can then be indicated by shading appropriate regions of the diagram. For
instance, in the three Venn diagrams shown in Figure 2.1, the shaded areas represent,
EF EFSS
(a) Shaded region: E /H20668 F. (b) Shaded region: EF.
S
(c) Shaded region: Ec.E
Figure 2.1 Venn diagrams.

<<<PAGE 37>>>

24Chapter 2 Axioms of Probability
S
F
E
Figure 2.2 E(F.
respectively, the events E∪F,EF, and Ec. The Venn diagram in Figure 2.2 indicates
thatE(F.
The operations of forming unions, intersections, and complements of events
obey certain rules similar to the rules of algebra. We list a few of these rules:
Commutative laws E∪F=F∪EE F=FE
Associative laws (E∪F)∪G=E∪(F∪G)( EF)G=E(FG)
Distributive laws (E∪F)G=EG∪FG EF ∪G=(E∪G)(F∪G)
These relations are veriﬁed by showing that any outcome that is contained in the
event on the left side of the equality sign is also contained in the event on the
right side, and vice versa. One way of showing this is by means of Venn diagrams.For instance, the distributive law may be veriﬁed by the sequence of diagrams in
Figure 2.3.
EF
(a) Shaded region: EG.GE F
(b) Shaded region: FG.G
EF
(c) Shaded region: (E /H20668 F )G.G
Figure 2.3 (E∪F)G=EG∪FG.

<<<PAGE 38>>>

A First Course in Probability 25
The following useful relationships among the three basic operations of forming
unions, intersections, and complements are known as DeMorgan’s laws:
⎛
⎝n/uniondisplay
i=1Ei⎞⎠c
=n/intersectiondisplay
i=1Ec
i
⎛⎝n/intersectiondisplay
i=1Ei⎞⎠c
=n/uniondisplay
i=1Ec
i
For instance, for two events EandF, DeMorgan’s laws state that
(E∪F)c=EcFcand (EF)c=Ec∪Fc
which can be easily proven by using Venn diagrams (see Theoretical Exercise 7).
To prove DeMorgan’s laws for general n, suppose ﬁrst that xis an outcome of/parenleftBigg
n/uniontext
i=1Ei/parenrightBiggc
. Then xis not contained inn/uniontext
i=1Ei, which means that xis not contained
in any of the events Ei,i=1, 2,...,n, implying that xis contained in Ec
ifor all
i=1, 2,...,nand thus is contained inn/intersectiontext
i=1Ec
i. To go the other way, suppose that xis
an outcome ofn/intersectiontext
i=1Ec
i. Then xis contained in Ec
ifor all i=1, 2,...,n, which means
thatxis not contained in Eifor any i=1, 2,...,n, implying that xis not contained
inn/uniontext
iEi, in turn implying that xis contained in/parenleftBigg
n/uniontext
1Ei/parenrightBiggc
. This proves the ﬁrst of
DeMorgan’s laws.
To prove the second of DeMorgan’s laws, we use the ﬁrst law to obtain
⎛⎝n/uniondisplay
i=1Ec
i⎞⎠c
=n/intersectiondisplay
i=1(Ec
i)c
which, since (Ec)c=E, is equivalent to
⎛
⎝n/uniondisplay
1Ec
i⎞⎠c
=n/intersectiondisplay
1Ei
Taking complements of both sides of the preceding equation yields the result we
seek, namely,
n/uniondisplay
1Ec
i=⎛
⎝n/intersectiondisplay
1Ei⎞⎠c
2.3 Axioms of Probability
One way of deﬁning the probability of an event is in terms of its relative frequency.
Such a deﬁnition usually goes as follows: We suppose that an experiment, whose
sample space is S, is repeatedly performed under exactly the same conditions. For
each event Eof the sample space S, we deﬁne n(E)to be the number of times in

<<<PAGE 39>>>

26Chapter 2 Axioms of Probability
the ﬁrst nrepetitions of the experiment that the event Eoccurs. Then P(E),t h e
probability of the event E, is deﬁned as
P(E)=limn→qn(E)
n
That is, P(E)is deﬁned as the (limiting) proportion of time that Eoccurs. It is thus
the limiting relative frequency of E.
Although the preceding deﬁnition is certainly intuitively pleasing and should
always be kept in mind by the reader, it possesses a serious drawback: How do we
know that n(E)/nwill converge to some constant limiting value that will be the same
for each possible sequence of repetitions of the experiment? For example, supposethat the experiment to be repeatedly performed consists of ﬂipping a coin. How do
we know that the proportion of heads obtained in the ﬁrst nﬂips will converge to
some value as ngets large? Also, even if it does converge to some value, how do we
know that, if the experiment is repeatedly performed a second time, we shall obtainthe same limiting proportion of heads?
Proponents of the relative frequency deﬁnition of probability usually answer
this objection by stating that the convergence of n(E)/nto a constant limiting value
is an assumption, or an axiom, of the system. However, to assume that n(E)/nwill
necessarily converge to some constant value seems to be an extraordinarily compli-
cated assumption. For, although we might indeed hope that such a constant limiting
frequency exists, it does not at all seem to be a priori evident that this need be thecase. In fact, would it not be more reasonable to assume a set of simpler and more
self-evident axioms about probability and then attempt to prove that such a con-
stant limiting frequency does in some sense exist? The latter approach is the modernaxiomatic approach to probability theory that we shall adopt in this text. In partic-
ular, we shall assume that, for each event Ein the sample space S, there exists a
value P(E), referred to as the probability of E. We shall then assume that all these
probabilities satisfy a certain set of axioms, which, we hope the reader will agree, isin accordance with our intuitive notion of probability.
Consider an experiment whose sample space is S. For each event Eof the sample
space S, we assume that a number P(E)is deﬁned and satisﬁes the following three
axioms:
The three axioms of probability
Axiom 1
0…P(E)…1
Axiom 2
P(S)=1
Axiom 3For any sequence of mutually exclusive events E
1,E2,...(that is, events for
which EiEj=Ø when iZj),
P⎛
⎝q/uniondisplay
i=1Ei⎞⎠=q/summationdisplay
i=1P(Ei)
We refer to P(E)as the probability of the event E.

<<<PAGE 40>>>

A First Course in Probability 27
Thus, Axiom 1 states that the probability that the outcome of the experiment
is an outcome in Eis some number between 0 and 1. Axiom 2 states that, with
probability 1, the outcome will be a point in the sample space S. Axiom 3 states
that, for any sequence of mutually exclusive events, the probability of at least one of
these events occurring is just the sum of their respective probabilities.
If we consider a sequence of events E1,E2,..., where E1=SandEi=Øf o r
i>1, then, because the events are mutually exclusive and because S=q/uniontext
i=1Ei,w e
have, from Axiom 3,
P(S)=q/summationdisplay
i=1P(Ei)=P(S)+q/summationdisplay
i=2P(Ø)
implying that
P(Ø)=0
That is, the null event has probability 0 of occurring.
Note that it follows that, for any ﬁnite sequence of mutually exclusive events E1,
E2,...,En,
P⎛
⎝n/uniondisplay
1Ei⎞⎠=n/summationdisplay
i=1P(Ei) (3.1)
This equation follows from Axiom 3 by deﬁning Eias the null event for all values
ofigreater than n. Axiom 3 is equivalent to Equation (3.1) when the sample space
is ﬁnite. (Why?) However, the added generality of Axiom 3 is necessary when the
sample space consists of an inﬁnite number of points.
Example
3aIf our experiment consists of tossing a coin and if we assume that a head is as likelyto appear as a tail, then we would have
P({H})=P({T})=1
2
On the other hand, if the coin were biased and we believed that a head were twiceas likely to appear as a tail, then we would have
P({H})=2
3P({T})=1
3.
Example
3bIf a die is rolled and we suppose that all six sides are equally likely to appear, then
we would have P({1})=P({2})=P({3})=P({4})=P({5})=P({6})=1
6.F r o m
Axiom 3, it would thus follow that the probability of rolling an even number would
equal
P({2, 4, 6}) =P({2})+P({4})+P({6})=1
2.
The assumption of the existence of a set function P, deﬁned on the events of
a sample space Sand satisfying Axioms 1, 2, and 3, constitutes the modern math-
ematical approach to probability theory. It is hoped that the reader will agree thatthe axioms are natural and in accordance with our intuitive concept of probability as
related to chance and randomness. Furthermore, using these axioms, we shall be able
to prove that if an experiment is repeated over and over again, then, with probability

<<<PAGE 41>>>

28Chapter 2 Axioms of Probability
1, the proportion of time during which any speciﬁc event Eoccurs will equal P(E).
This result, known as the strong law of large numbers, is presented in Chapter 8.
In addition, we present another possible interpretation of probability—as being a
measure of belief—in Section 2.7.
Technical Remark. We have supposed that P(E)is deﬁned for all the events E
of the sample space. Actually, when the sample space is an uncountably inﬁnite set,P(E)is deﬁned only for a class of events called measurable . However, this restriction
need not concern us, as all events of any practical interest are measurable.
2.4 Some Simple Propositions
In this section, we prove some simple propositions regarding probabilities. We ﬁrstnote that since EandE
care always mutually exclusive and since E∪Ec=S,w e
have, by Axioms 2 and 3,
1=P(S)=P(E∪Ec)=P(E)+P(Ec)
Or, equivalently, we have Proposition 4.1.
Proposition
4.1P(Ec)=1−P(E)
In words, Proposition 4.1 states that the probability that an event does not occur
is 1 minus the probability that it does occur. For instance, if the probability of obtain-
ing a head on the toss of a coin is3
8, then the probability of obtaining a tail must be5
8.
Our second proposition states that if the event Eis contained in the event F,
then the probability of Eis no greater than the probability of F.
Proposition
4.2IfE(F, then P(E)…P(F).
Proof Since E(F, it follows that we can express Fas
F=E∪EcF
Hence, because EandEcFare mutually exclusive, we obtain, from Axiom 3,
P(F)=P(E)+P(EcF)
which proves the result, since P(EcF)Ú0.
Proposition 4.2 tells us, for instance, that the probability of rollin ga1w i t had i e
is less than or equal to the probability of rolling an odd value with the die.
The next proposition gives the relationship between the probability of the union
of two events, expressed in terms of the individual probabilities, and the probability
of the intersection of the events.
Proposition
4.3P(E∪F)=P(E)+P(F)−P(EF)
Proof To derive a formula for P(E∪F), we ﬁrst note that E∪Fcan be written as
the union of the two disjoint events EandEcF. Thus, from Axiom 3, we obtain
P(E∪F)=P(E∪EcF)
=P(E)+P(EcF)

<<<PAGE 42>>>

A First Course in Probability 29
EF
Figure 2.4 Venn diagram.
EF
I III II
Figure 2.5 Venn diagram in sections.
Furthermore, since F=EF∪EcF, we again obtain from Axiom 3
P(F)=P(EF)+P(EcF)
or, equivalently,
P(EcF)=P(F)−P(EF)
thereby completing the proof.
Proposition 4.3 could also have been proved by making use of the Venn diagram
in Figure 2.4.
Let us divide E∪Finto three mutually exclusive sections, as shown in Figure 2.5.
In words, section I represents all the points in Ethat are not in F(that is, EFc),
section II represents all points both in Eand in F(that is, EF), and section III rep-
resents all points in Fthat are not in E(that is, EcF).
From Figure 2.5, we see that
E∪F=I∪II∪III
E=I∪II
F=II∪III
As I, II, and III are mutually exclusive, it follows from Axiom 3 that
P(E∪F)=P(I)+P(II)+P(III)
P(E)=P(I)+P(II)
P(F)=P(II)+P(III)
which shows that
P(E∪F)=P(E)+P(F)−P(II)
and Proposition 4.3 is proved, since II =EF.
Example
4aJ is taking two books along on her holiday vacation. With probability .5, she will like
the ﬁrst book; with probability .4, she will like the second book; and with probabil-
ity .3, she will like both books. What is the probability that she likes neither book?

<<<PAGE 43>>>

30Chapter 2 Axioms of Probability
Solution LetBidenote the event that J likes book i,i=1, 2. Then the probability
that she likes at least one of the books is
P(B1∪B2)=P(B1)+P(B2)−P(B1B2)=.5+.4−.3=.6
Because the event that J likes neither book is the complement of the event that she
likes at least one of them, we obtain the result
P(Bc
1Bc
2)=P/parenleftbig
(B1∪B2)c/parenrightbig
=1−P(B1∪B2)=.4 .
We may also calculate the probability that any one of the three events E,F, and
Goccurs, namely,
P(E∪F∪G)=P[(E∪F)∪G]
which, by Proposition 4.3, equals
P(E∪F)+P(G)−P[(E∪F)G]
Now, it follows from the distributive law that the events (E∪F)GandEG∪FG
are equivalent; hence, from the preceding equations, we obtain
P(E∪F∪G)
=P(E)+P(F)−P(EF)+P(G)−P(EG∪FG)
=P(E)+P(F)−P(EF)+P(G)−P(EG) −P(FG) +P(EGFG)
=P(E)+P(F)+P(G)−P(EF)−P(EG) −P(FG) +P(EFG)
In fact, the following proposition, known as the inclusion–exclusion identity , can
be proved by mathematical induction:
Proposition
4.4 P(E1∪E2∪ ··· ∪ En)=n/summationdisplay
i=1P(Ei)−/summationdisplay
i1<i2P(Ei1Ei2)+ ···
+(−1)r+1/summationdisplay
i1<i2<···<i rP(Ei1Ei2···E ir)
+ ··· + (−1)n+1P(E1E2···En)
The summation/summationtext
i1<i2<···<i rP(Ei1Ei2···E ir)is taken over all of the/parenleftBigg
n
r/parenrightBigg
possible sub-
sets of size rof the set {1, 2,...,n}.
In words, Proposition 4.4 states that the probability of the union of nevents
equals the sum of the probabilities of these events taken one at a time, minus the
sum of the probabilities of these events taken two at a time, plus the sum of the
probabilities of these events taken three at a time, and so on.
Remarks 1. For a noninductive argument for Proposition 4.4, note ﬁrst that if an
outcome of the sample space is not a member of any of the sets Ei, then its probabil-
ity does not contribute anything to either side of the equality. Now, suppose that an
outcome is in exactly mof the events Ei, where m>0. Then, since it is in/uniontext
iEi,i t s
probability is counted once in P/parenleftBigg
/uniontext
iEi/parenrightBigg
; also, as this outcome is contained in/parenleftBigg
m
k/parenrightBigg

<<<PAGE 44>>>

A First Course in Probability 31
subsets of the type Ei1Ei2···E ik, its probability is counted
/parenleftBigg
m
1/parenrightBigg
−/parenleftBigg
m
2/parenrightBigg
+/parenleftBigg
m
3/parenrightBigg
− ··· ;/parenleftBigg
m
m/parenrightBigg
times on the right of the equality sign in Proposition 4.4. Thus, for m>0, we must
show that
1=/parenleftBigg
m
1/parenrightBigg
−/parenleftBigg
m
2/parenrightBigg
+/parenleftBigg
m
3/parenrightBigg
− ··· ;/parenleftBigg
mm/parenrightBigg
However, since 1 =/parenleftBigg
m
0/parenrightBigg
, the preceding equation is equivalent to
m/summationdisplay
i=0/parenleftBigg
m
i/parenrightBigg
(−1)i=0
and the latter equation follows from the binomial theorem, since
0=(−1+1)m=m/summationdisplay
i=0/parenleftBigg
m
i/parenrightBigg
(−1)i(1)m−i
2. The following is a succinct way of writing the inclusion–exclusion identity:
P(∪n
i=1Ei)=n/summationdisplay
r=1(−1)r+1/summationdisplay
i1<···<i rP(Ei1···Eir)
3. In the inclusion–exclusion identity, going out one term results in an upper
bound on the probability of the union, going out two terms results in a lower bound
on the probability, going out three terms results in an upper bound on the proba-
bility, going out four terms results in a lower bound, and so on. That is, for eventsE
1,...,En, we have
P(∪n
i=1Ei)…n/summationdisplay
i=1P(Ei) (4.1)
P(∪ni=1Ei)Ún/summationdisplay
i=1P(Ei)−/summationdisplay
j<iP(EiEj) (4.2)
P(∪ni=1Ei)…n/summationdisplay
i=1P(Ei)−/summationdisplay
j<iP(EiEj)+/summationdisplay
k<j<iP(EiEjEk) (4.3)
and so on. To prove the validity of these bounds, note the identity
∪ni=1Ei=E1∪Ec
1E2∪Ec
1Ec
2E3∪ ··· ∪ Ec
1···Ec
n−1En
That is, at least one of the events Eioccurs if E1occurs, or if E1does not occur but
E2does, or if E1andE2do not occur but E3does, and so on. Because the right-hand
side is the union of disjoint events, we obtain
P(∪ni=1Ei)=P(E1)+P(Ec
1E2)+P(Ec
1Ec
2E3)+...+P(Ec
1···Ec
n−1En)
=P(E1)+n/summationdisplay
i=2P(Ec
1···Ec
i−1Ei) (4.4)

<<<PAGE 45>>>

32Chapter 2 Axioms of Probability
Now, let Bi=Ec
1···Ec
i−1=(∪j<iEj)cbe the event that none of the ﬁrst i−1
events occurs. Applying the identity
P(Ei)=P(BiEi)+P(Bc
iEi)
shows that
P(Ei)=P(Ec
1···Ec
i−1Ei)+P(Ei∪j<iEj)
or, equivalently,
P(Ec
1···Ec
i−1Ei)=P(Ei)−P(∪j<iEiEj)
Substituting this equation into (4.4) yields
P(∪n
i=1Ei)=/summationdisplay
iP(Ei)−/summationdisplay
iP(∪j<iEiEj) (4.5)
Because probabilities are always nonnegative, Inequality (4.1) follows directly from
Equation (4.5). Now, ﬁxing iand applying Inequality (4.1) to P(∪j<iEiEj)yields
P(∪j<iEiEj)…/summationdisplay
j<iP(EiEj)
which, by Equation (4.5), gives Inequality (4.2). Similarly, ﬁxing iand applying
Inequality (4.2) to P(∪j<iEiEj)yields
P(∪j<iEiEj)Ú/summationdisplay
j<iP(EiEj)−/summationdisplay
k<j<iP(EiEjEiEk)
=/summationdisplay
j<iP(EiEj)−/summationdisplay
k<j<iP(EiEjEk)
which, by Equation (4.5), gives Inequality (4.3). The next inclusion–exclusioninequality is now obtained by ﬁxing iand applying Inequality (4.3) to P(∪
j<iEiEj),
and so on.
2.5 Sample Spaces Having Equally Likely Outcomes
In many experiments, it is natural to assume that all outcomes in the sample spaceare equally likely to occur. That is, consider an experiment whose sample space Sis
a ﬁnite set, say, S={1, 2,...,N}. Then, it is often natural to assume that
P({1})=P({2})=···= P({N})
which implies, from Axioms 2 and 3 (why?), that
P({i})=1
Ni=1, 2,...,N
From this equation, it follows from Axiom 3 that, for any event E,
P(E)=number of outcomes in E
number of outcomes in S
In words, if we assume that all outcomes of an experiment are equally likely to occur,
then the probability of any event Eequals the proportion of outcomes in the sample
space that are contained in E.

<<<PAGE 46>>>

A First Course in Probability 33
Example
5aIf two dice are rolled, what is the probability that the sum of the upturned faces will
equal 7?
Solution We shall solve this problem under the assumption that all of the 36 possible
outcomes are equally likely. Since there are 6 possible outcomes—namely, (1, 6),(2, 5), (3, 4), (4, 3), (5, 2), and (6, 1)—that result in the sum of the dice being equal
to 7, the desired probability is
6
36=1
6. .
Example
5bIf 3 balls are “randomly drawn” from a bowl containing 6 white and 5 black balls,
what is the probability that one of the balls is white and the other two black?
Solution If we regard the balls as being distinguishable and the order in which they
are selected as being relevant, then the sample space consists of 11 ·10·9=990
outcomes. Furthermore, there are 6 ·5·4=120 outcomes in which the ﬁrst ball
selected is white and the other two are black; 5 ·6·4=120 outcomes in which
the ﬁrst is black, the second is white, and the third is black; and 5 ·4·6=120 in
which the ﬁrst two are black and the third is white. Hence, assuming that “randomly
drawn” means that each outcome in the sample space is equally likely to occur, wesee that the desired probability is
120+120+120
990=4
11
This problem could also have been solved by regarding the outcome of the
experiment as the unordered set of drawn balls. From this point of view, there are/parenleftBigg
11
3/parenrightBigg
=165 outcomes in the sample space. Now, each set of 3 balls corresponds
to 3! outcomes when the order of selection is noted. As a result, if all outcomes
are assumed equally likely when the order of selection is noted, then it follows thatthey remain equally likely when the outcome is taken to be the unordered set of
selected balls. Hence, using the latter representation of the experiment, we see that
the desired probability is/parenleftBigg
61/parenrightBigg/parenleftBigg
52/parenrightBigg
/parenleftBigg
11
3/parenrightBigg=4
11
which, of course, agrees with the answer obtained previously. .
When the experiment consists of a random selection of kitems from a set of n
items, we have the ﬂexibility of either letting the outcome of the experiment be theordered selection of the kitems or letting it be the unordered set of items selected.
In the former case, we would assume that each new selection is equally likely to beany of the so far unselected items of the set, and in the latter case, we would assumethat all/parenleftbig
n
k/parenrightbig
possible subsets of kitems are equally likely to be the set selected. For
instance, suppose 5 people are to be randomly selected from a group of 20 individu-
als consisting of 10 married couples, and we want to determine P(N), the probability
that the 5 chosen are all unrelated. (That is, no two are married to each other.) If
we regard the sample space as the set of 5 people chosen, then there are/parenleftbig20
5/parenrightbig
equally
likely outcomes. An outcome that does not contain a married couple can be thought
of as being the result of a six-stage experiment: In the ﬁrst stage, 5 of the 10 couples
to have a member in the group are chosen; in the next 5 stages, 1 of the 2 members of

<<<PAGE 47>>>

34Chapter 2 Axioms of Probability
each of these couples is selected. Thus, there are/parenleftbig10
5/parenrightbig
25possible outcomes in which
the 5 members selected are unrelated, yielding the desired probability of
P(N)=/parenleftbigg10
5/parenrightbigg
25
/parenleftbigg20
5/parenrightbigg
In contrast, we could let the outcome of the experiment be the ordered selection
of the 5 individuals. In this setting, there are 20 ·19·18·17·16 equally likely
outcomes, of which 20 ·18·16·14·12 outcomes result in a group of 5 unrelated
individuals, yielding the result
P(N)=20·18·16·14·12
20·19·18·17·16
We leave it for the reader to verify that the two answers are identical.
Example
5cA committee of 5 is to be selected from a group of 6 men and 9 women. If the
selection is made randomly, what is the probability that the committee consists of 3
men and 2 women?
Solution Because each of the/parenleftbig15
5/parenrightbig
possible committees is equally likely to be selected,
the desired probability is/parenleftBigg
63/parenrightBigg/parenleftBigg
92/parenrightBigg
/parenleftBigg
15
5/parenrightBigg=240
1001.
Example
5dAn urn contains nballs, one of which is special. If kof these balls are withdrawn one
at a time, with each selection being equally likely to be any of the balls that remainat the time, what is the probability that the special ball is chosen?
Solution Since all of the balls are treated in an identical manner, it follows that the
set of kballs selected is equally likely to be any of the/parenleftBigg
nk/parenrightBigg
sets of kballs. Therefore,
P{special ball is selected }=/parenleftBigg
11/parenrightBigg/parenleftBigg
n−1
k−1/parenrightBigg
/parenleftBigg
nk/parenrightBigg =k
n
We could also have obtained this result by letting Aidenote the event that the special
ball is the ith ball to be chosen, i=1,...,k. Then, since each one of the nballs is
equally likely to be the ith ball chosen, it follows that P(Ai)=1/n. Hence, because
these events are clearly mutually exclusive, we have
P{special ball is selected }=P⎛
⎝k/uniondisplay
i=1Ai⎞⎠=k/summationdisplay
i=1P(Ai)=k
n
We could also have argued that P(Ai)=1/n, by noting that there are n(n−1)···
(n−k+1)=n!/(n −k)! equally likely outcomes of the experiment, of which

<<<PAGE 48>>>

A First Course in Probability 35
(n−1)(n−2)···(n−i+1)(1)(n −i)···(n−k+1)=(n−1)!/(n −k)! result
in the special ball being the ith one chosen. From this reasoning, it follows that
P(Ai)=(n−1)!
n!=1
n.
Example
5eSuppose that n+mballs, of which nare red and mare blue, are arranged in a linear
order in such a way that all (n+m)! possible orderings are equally likely. If we
record the result of this experiment by listing only the colors of the successive balls,
show that all the possible results remain equally likely.
Solution Consider any one of the (n+m)! possible orderings, and note that any per-
mutation of the red balls among themselves and of the blue balls among themselvesdoes not change the sequence of colors. As a result, every ordering of colorings cor-
responds to n!m! different orderings of the n+mballs, so every ordering of the
colors has probability
n!m!
(n+m)!of occurring.
For example, suppose that there are 2 red balls, numbered r1,r2, and 2 blue balls,
numbered b1,b2. Then, of the 4! possible orderings, there will be 2! 2! orderings that
result in any speciﬁed color combination. For instance, the following orderings result
in the successive balls alternating in color, with a red ball ﬁrst:
r1,b1,r2,b2r1,b2,r2,b1r2,b1,r1,b2r2,b2,r1,b1
Therefore, each of the possible orderings of the colors has probability4
24=1
6of
occurring. .
Example
5fA poker hand consists of 5 cards. If the cards have distinct consecutive values andare not all of the same suit, we say that the hand is a straight. For instance, a handconsisting of the ﬁve of spades, six of spades, seven of spades, eight of spades, and
nine of hearts is a straight. What is the probability that one is dealt a straight?
Solution We start by assuming that all/parenleftBigg
52
5/parenrightBigg
possible poker hands are equally
likely. To determine the number of outcomes that are straights, let us ﬁrst deter-
mine the number of possible outcomes for which the poker hand consists of an ace,
two, three, four, and ﬁve (the suits being irrelevant). Since the ace can be any 1 of the
4 possible aces, and similarly for the two, three, four, and ﬁve, it follows that thereare 4
5outcomes leading to exactly one ace, two, three, four, and ﬁve. Hence, since
in 4 of these outcomes all the cards will be of the same suit (such a hand is called astraight ﬂush), it follows that there are 4
5−4 hands that make up a straight of the
form ace, two, three, four, and ﬁve. Similarly, there are 45−4 hands that make up a
straight of the form ten, jack, queen, king, and ace. Thus, there are 10 (45−4)hands
that are straights, and it follows that the desired probability is
10(45−4)/parenleftBigg
52
5/parenrightBiggL.0039 .
Example
5gA 5-card poker hand is said to be a full house if it consists of 3 cards of the samedenomination and 2 other cards of the same denomination (of course, different from
the ﬁrst denomination). Thus, a full house is three of a kind plus a pair. What is the
probability that one is dealt a full house?

<<<PAGE 49>>>

36Chapter 2 Axioms of Probability
Solution Again, we assume that all/parenleftBigg
52
5/parenrightBigg
possible hands are equally likely. To
determine the number of possible full houses, we ﬁrst note that there are/parenleftBigg
4
2/parenrightBigg/parenleftBigg
43/parenrightBigg
different combinations of, say, 2 tens and 3 jacks. Because there are 13 differentchoices for the kind of pair and, after a pair has been chosen, there are 12 other
choices for the denomination of the remaining 3 cards, it follows that the probability
of a full house is
13·12·/parenleftBigg
4
2/parenrightBigg/parenleftBigg
43/parenrightBigg
/parenleftBigg
52
5/parenrightBigg L.0014 .
Example
5hIn the game of bridge, the entire deck of 52 cards is dealt out to 4 players. What isthe probability that
(a) one of the players receives all 13 spades;
(b) each player receives 1 ace?
Solution (a) Letting Eibe the event that hand ihas all 13 spades, then
P(Ei)=1
/parenleftbig52
13/parenrightbig,i=1, 2, 3, 4
Because the events Ei,i=1, 2, 3, 4, are mutually exclusive, the probability that one
of the hands is dealt all 13 spades is
P(∪4
i=1Ei)=4/summationdisplay
i=1P(Ei)=4//parenleftbigg52
13/parenrightbigg
L6.3*10−12
(b) To determine the number of outcomes in which each of the distinct players
receives exactly 1 ace, put aside the aces and note that there are/parenleftBigg
48
12, 12, 12, 12/parenrightBigg
possible divisions of the other 48 cards when each player is to receive 12. Because
there are 4! ways of dividing the 4 aces so that each player receives 1, we see that
the number of possible outcomes in which each player receives exactly 1 ace is
4!/parenleftBigg
48
12, 12, 12, 12/parenrightBigg
.
As there are/parenleftbig52
13,13,13,13/parenrightbig
possible hands, the desired probability is thus
4!/parenleftbig48
12,12,12,12/parenrightbig
/parenleftbig52
13,13,13,13/parenrightbigL.1055 .
Some results in probability are quite surprising when initially encountered. Our
next two examples illustrate this phenomenon.

<<<PAGE 50>>>

A First Course in Probability 37
Example
5iIfnpeople are present in a room, what is the probability that no two of them cele-
brate their birthday on the same day of the year? How large need nbe so that this
probability is less than1
2?
Solution As each person can celebrate his or her birthday on any one of 365 days,
there are a total of (365)npossible outcomes. (We are ignoring the possibility of
someone having been born on February 29.) Assuming that each outcome is equally
likely, we see that the desired probability is (365)(364)(363)...( 365−n+1)/(365)n.
It is a rather surprising fact that when nÚ23, this probability is less than1
2.T h a ti s ,i f
there are 23 or more people in a room, then the probability that at least two of them
have the same birthday exceeds1
2. Many people are initially surprised by this result,
since 23 seems so small in relation to 365, the number of days of the year. However,
every pair of individuals has probability365
(365)2=1
365of having the same birthday,
and in a group of 23 people, there are/parenleftBigg
23
2/parenrightBigg
=253 different pairs of individuals.
Looked at this way, the result no longer seems so surprising.
When there are 50 people in the room, the probability that at least two share the
same birthday is approximately .970, and with 100 persons in the room, the odds are
better than 3,000,000:1. (That is, the probability is greater than3*106
3*106+1that at
least two people have the same birthday.) .
Example
5jA deck of 52 playing cards is shufﬂed, and the cards are turned up one at a time until
the ﬁrst ace appears. Is the next card—that is, the card following the ﬁrst ace—morelikely to be the ace of spades or the two of clubs?
Solution To determine the probability that the card following the ﬁrst ace is the
ace of spades, we need to calculate how many of the (52)! possible orderings of thecards have the ace of spades immediately following the ﬁrst ace. To begin, note that
each ordering of the 52 cards can be obtained by ﬁrst ordering the 51 cards different
from the ace of spades and then inserting the ace of spades into that ordering. Fur-thermore, for each of the (51)! orderings of the other cards, there is only one place
where the ace of spades can be placed so that it follows the ﬁrst ace. For instance, if
the ordering of the other 51 cards is
4c,6h,Jd,5 s,Ac,7 d,...,Kh
then the only insertion of the ace of spades into this ordering that results in its fol-lowing the ﬁrst ace is
4c,6h,Jd,5 s,Ac,As,7d,...,Kh
Therefore, there are (51)! orderings that result in the ace of spades following the ﬁrst
ace, so
P{the ace of spades follows the ﬁrst ace }=(51)!
(52)!=1
52
In fact, by exactly the same argument, it follows that the probability that the two
of clubs (or any other speciﬁed card) follows the ﬁrst ace is also1
52. In other words,
each of the 52 cards of the deck is equally likely to be the one that follows the ﬁrst
ace!
Many people ﬁnd this result rather surprising. Indeed, a common reaction is to
suppose initially that it is more likely that the two of clubs (rather than the ace of

<<<PAGE 51>>>

38Chapter 2 Axioms of Probability
spades) follows the ﬁrst ace, since that ﬁrst ace might itself be the ace of spades. This
reaction is often followed by the realization that the two of clubs might itself appear
before the ﬁrst ace, thus negating its chance of immediately following the ﬁrst ace.
However, as there is one chance in four that the ace of spades will be the ﬁrst ace(because all 4 aces are equally likely to be ﬁrst) and only one chance in ﬁve that
the two of clubs will appear before the ﬁrst ace (because each of the set of 5 cards
consisting of the two of clubs and the 4 aces is equally likely to be the ﬁrst of this setto appear), it again appears that the two of clubs is more likely. However, this is not
the case, and our more complete analysis shows that they are equally likely. .
Example
5kA football team consists of 20 offensive and 20 defensive players. The players are tobe paired in groups of 2 for the purpose of determining roommates. If the pairing is
done at random, what is the probability that there are no offensive–defensive room-mate pairs? What is the probability that there are 2 ioffensive–defensive roommate
pairs, i=1, 2,..., 10?
Solution There are /parenleftBigg
40
2, 2,...,2/parenrightBigg
=(40)!
(2!)20
ways of dividing the 40 players into 20 ordered pairs of two each. (That is, there
are(40)!/220ways of dividing the players into a ﬁrst pair, a second pair, and so on.)
Hence, there are (40)!/220(20)! ways of dividing the players into (unordered) pairs of
2 each. Furthermore, since a division will result in no offensive–defensive pairs if theoffensive (and defensive) players are paired among themselves, it follows that thereare [(20)!/2
10(10)!]2such divisions. Hence, the probability of no offensive–defensive
roommate pairs, call it P0, is given by
P0=/parenleftbigg(20)!
210(10)!/parenrightbigg2
(40)!
220(20)!=[(20)!]3
[(10)!]2(40)!
To determine P2i, the probability that there are 2i offensive–defensive pairs, we ﬁrst
note that there are/parenleftBigg
20
2i/parenrightBigg2
ways of selecting the 2i offensive players and the 2 idefen-
sive players who are to be in the offensive–defensive pairs. These 4 iplayers can then
be paired up into (2i)! possible offensive–defensive pairs. (This is so because theﬁrst offensive player can be paired with any of the 2 idefensive players, the second
offensive player with any of the remaining 2 i−1 defensive players, and so on.)
As the remaining 20 −2ioffensive (and defensive) players must be paired among
themselves, it follows that there are
/parenleftBigg
20
2i/parenrightBigg
2
(2i)!/bracketleftbigg(20−2i)!
210−i(10−i)!/bracketrightbigg2
divisions that lead to 2i offensive–defensive pairs. Hence,
P2i=/parenleftBigg
20
2i/parenrightBigg2
(2i)!/bracketleftbigg(20−2i)!
210−i(10−i)!/bracketrightbigg2
(40)!
220(20)!i=0, 1,...,1 0

<<<PAGE 52>>>

A First Course in Probability 39
TheP2i,i=0, 1,..., 10, can now be computed, or they can be approximated by
making use of a result of Stirling, which shows that n! can be approximated by
nn+1/ 2e−n√
2π. For instance, we obtain
P0L1.3403 *10−6
P10L.345861
P20L7.6068 *10−6.
Our next three examples illustrate the usefulness of the inclusion–exclusion
identity (Proposition 4.4). In Example 5l, the introduction of probability enables us
to obtain a quick solution to a counting problem.
Example
5lA total of 36 members of a club play tennis, 28 play squash, and 18 play badminton.
Furthermore, 22 of the members play both tennis and squash, 12 play both tennis
and badminton, 9 play both squash and badminton, and 4 play all three sports. Howmany members of this club play at least one of three sports?
Solution LetNdenote the number of members of the club, and introduce probabil-
ity by assuming that a member of the club is randomly selected. If, for any subset C
of members of the club, we let P(C)denote the probability that the selected member
is contained in C, then
P(C)=number of members in C
N
Now, with Tbeing the set of members that plays tennis, Sbeing the set that plays
squash, and Bbeing the set that plays badminton, we have, from Proposition 4.4,
P(T∪S∪B)
=P(T)+P(S)+P(B)−P(TS)−P(TB) −P(SB)+P(TSB)
=36+28+18−22−12−9+4
N
=43
N
Hence, we can conclude that 43 members play at least one of the sports. .
The next example in this section not only possesses the virtue of giving rise to a
somewhat surprising answer, but is also of theoretical interest.
Example
5mThe matching problem
Suppose that each of Nmen at a party throws his hat into the center of the room.
The hats are ﬁrst mixed up, and then each man randomly selects a hat. What is the
probability that none of the men selects his own hat?
Solution We ﬁrst calculate the complementary probability of at least one man select-
ing his own hat. Let us denote by Ei,i=1, 2,...,Nthe event that the ith man selects
his own hat. Now, by Proposition 4.4, P/parenleftBigg
N/uniontext
i=1Ei/parenrightBigg
, the probability that at least one of
the men selects his own hat is given by
P⎛
⎝N/uniondisplay
i=1Ei⎞⎠=N/summationdisplay
i=1P(Ei)−/summationdisplay
i1<i2P(Ei1Ei2)+ ···
+(−1)n+1/summationdisplay
i1<i2···<i nP(Ei1Ei2···Ein)
+ ··· + (−1)N+1P(E1E2···EN)

<<<PAGE 53>>>

40Chapter 2 Axioms of Probability
If we regard the outcome of this experiment as a vector of Nnumbers, where the ith
element is the number of the hat drawn by the ith man, then there are N! possible
outcomes. [The outcome (1, 2, 3, ...,N)means, for example, that each man selects his
own hat.] Furthermore, Ei1Ei2...Ein, the event that each of the nmen i1,i2,...,in
selects his own hat, can occur in any of (N−n)(N−n−1)···3·2·1=(N−n)!
possible ways; for, of the remaining N−nmen, the ﬁrst can select any of N−n
hats, the second can then select any of N−n−1 hats, and so on. Hence, assuming
that all N! possible outcomes are equally likely, we see that
P(Ei1Ei2···Ein)=(N−n)!
N!
Also, as there are/parenleftBigg
N
n/parenrightBigg
terms in/summationtext
i1<i2···<i nP(Ei1Ei2···Ein), it follows that
/summationdisplay
i1<i2···<i nP(Ei1Ei2···Ein)=N!(N−n)!
(N−n)!n!N !=1
n!
Thus,
P⎛
⎝N/uniondisplay
i=1Ei⎞⎠=1−1
2!+1
3!− ··· + (−1)N+11
N!
Hence, the probability that none of the men selects his own hat is
1−1+1
2!−1
3!+...+(−1)N
N!=N/summationdisplay
i=0(−1)i/i!
Upon letting x=− 1 in the identity ex=q/summationtext
i=0xi/i! the preceding probability when N
large is seen to be approximately equal to e−1L.3679. In other words, for Nlarge,
the probability that none of the men selects his own hat is approximately .37. (How
many readers would have incorrectly thought that this probability would go to 1 as
N→q?) .
For another illustration of the usefulness of Proposition 4.4, consider the follow-
ing example.
Example
5nCompute the probability that if 10 married couples are seated at random at a round
table, then no wife sits next to her husband.
Solution If we let Ei,i=1, 2,..., 10 denote the event that the ith couple sit next
to each other, it follows that the desired probability is 1 −P/parenleftBigg
10/uniontext
i=1Ei/parenrightBigg
. Now, from
Proposition 4.4,
P⎛
⎝10/uniondisplay
1Ei⎞⎠=10/summationdisplay
1P(Ei)− ··· + (−1)n+1/summationdisplay
i1<i2<···<inP(Ei1Ei2···E in)
+ ··· − P(E1E2···E10)
To compute P(Ei1Ei2···Ein), we ﬁrst note that there are 19! ways of arranging
20 people around a round table. (Why?) The number of arrangements that result in
a speciﬁed set of nmen sitting next to their wives can most easily be obtained by ﬁrst

<<<PAGE 54>>>

A First Course in Probability 41
thinking of each of the nmarried couples as being single entities. If this were the
case, then we would need to arrange 20 −nentities around a round table, and there
are clearly (20−n−1)! such arrangements. Finally, since each of the nmarried
couples can be arranged next to each other in one of two possible ways, it follows
that there are 2n(20−n−1)! arrangements that result in a speciﬁed set of nmen
each sitting next to their wives. Therefore,
P(Ei1Ei2···Ein)=2n(19−n)!
(19)!
Thus, from Proposition 4.4, we obtain that the probability that at least one marriedcouple sits together, namely,
/parenleftBigg
10
1/parenrightBigg
2
1(18)!
(19)!−/parenleftBigg
10
2/parenrightBigg
22(17)!
(19)!+/parenleftBigg
10
3/parenrightBigg
23(16)!
(19)!−···−/parenleftBigg
10
10/parenrightBigg
2109!
(19)!L.6605
and the desired probability is approximately .3395. .
∗Example
5oRuns
Consider an athletic team that had just ﬁnished its season with a ﬁnal record of n
wins and mlosses. By examining the sequence of wins and losses, we are hoping to
determine whether the team had stretches of games in which it was more likely to
win than at other times. One way to gain some insight into this question is to count
the number of runs of wins and then see how likely that result would be when all
(n+m)!/(n! m!)orderings of the nwins and mlosses are assumed equally likely. By
a run of wins, we mean a consecutive sequence of wins. For instance, if n=10,m=6,
and the sequence of outcomes was WWLLWWWLWLLLWWWW , then there would
be 4 runs of wins—the ﬁrst run being of size 2, the second of size 3, the third of size1, and the fourth of size 4.
Suppose now that a team has nwins and mlosses. Assuming that all (n+m)!/
(n!m!)=/parenleftBigg
n+m
n/parenrightBigg
orderings are equally likely, let us determine the probability
that there will be exactly rruns of wins. To do so, consider ﬁrst any vector of positive
integers x
1,x2,...,xrwith x1+ ··· + xr=n, and let us see how many outcomes
result in rruns of wins in which the it hr u ni so fs i z ex i,i=1,...,r. For any such
outcome, if we let y1denote the number of losses before the ﬁrst run of wins, y2the
number of losses between the ﬁrst 2 runs of wins, ...,yr+1the number of losses after
the last run of wins, then the yisatisfy
y1+y2+ ··· + yr+1=my 1Ú0,yr+1Ú0,yi>0,i=2,...,r
and the outcome can be represented schematically as
LL...L/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
y1WW...W/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
x1L...L/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
y2WW...W/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
x2···WW/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
xrL...L/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
yr+1
Hence, the number of outcomes that result in rruns of wins—the ith of size xi,i=
1,...r—is equal to the number of integers y1,...,yr+1that satisfy the foregoing, or,
equivalently, to the number of positive integers
y1=y1+1yi=yi,i=2,...,r,yr+1=yr+1+1
that satisfy
y1+y2+ ··· + yr+1=m+2

<<<PAGE 55>>>

42Chapter 2 Axioms of Probability
By Proposition 6.1 in Chapter 1, there are/parenleftBigg
m+1
r/parenrightBigg
such outcomes. Hence, the
total number of outcomes that result in rruns of wins is/parenleftBigg
m+1
r/parenrightBigg
, multiplied
by the number of positive integral solutions of x1+ ··· + xr=n. Thus, again
from Proposition 6.1, there are/parenleftBigg
m+1
r/parenrightBigg/parenleftBigg
n−1
r−1/parenrightBigg
outcomes resulting in rruns
of wins. As there are/parenleftBigg
n+m
n/parenrightBigg
equally likely outcomes, it follows that
P({rruns of wins })=/parenleftBigg
m+1
r/parenrightBigg/parenleftBigg
n−1
r−1/parenrightBigg
/parenleftBigg
m+n
n/parenrightBigg rÚ1
For example, if n=8 and m=6, then the probability of 7 runs is/parenleftBigg
7
7/parenrightBigg/parenleftBigg
76/parenrightBigg/slashBig
/parenleftBigg
14
8/parenrightBigg
=1/429 if all/parenleftBigg
14
8/parenrightBigg
outcomes are equally likely. Hence, if the outcome
wasWLWLWLWLWWLWLW, then we might suspect that the team’s probability of
winning was changing over time. (In particular, the probability that the team wins
seems to be quite high when it lost its last game and quite low when it won its last
game.) On the other extreme, if the outcome were WWWWWWWWLLLLLL , then
there would have been only 1 run, and as P({1r u n })=/parenleftBigg
7
1/parenrightBigg/parenleftBigg
70/parenrightBigg/slashBig/parenleftBigg
14
8/parenrightBigg
=
1/429, it would thus again seem unlikely that the team’s probability of winning
remained unchanged over its 14 games. .
*2.6 Probability as a Continuous Set Function
A sequence of events {En,nÚ1}is said to be an increasing sequence if
E1(E2(···(En(En+1(···
whereas it is said to be a decreasing sequence if
E1)E2)···)En)En+1)···
If{En,nÚ1}is an increasing sequence of events, then we deﬁne a new event,
denoted by limn→qEn,b y
limn→qEn=q/uniondisplay
i=1Ei
Similarly, if {En,nÚ1}is a decreasing sequence of events, we deﬁne limn→qEn,b y
limn→qEn=q/intersectiondisplay
i=1Ei
We now prove the following Proposition 6.1:

<<<PAGE 56>>>

A First Course in Probability 43
Proposition
6.1If{En,nÚ1}is either an increasing or a decreasing sequence of events, then
limn→qP(En)=P(limn→qEn)
Proof Suppose, ﬁrst, that {En,nÚ1}is an increasing sequence, and deﬁne the events
Fn,nÚ1, by
F1=E1
Fn=En⎛
⎝n−1/uniondisplay
1Ei⎞⎠c
=EnEc
n−1n>1
where we have used the fact thatn−1/uniontext
1Ei=En−1, since the events are increasing. In
words, Fnconsists of those outcomes in Enthat are not in any of the earlier Ei,i<n.
It is easy to verify that the Fnare mutually exclusive events such that
q/uniondisplay
i=1Fi=q/uniondisplay
i=1Eiandn/uniondisplay
i=1Fi=n/uniondisplay
i=1Eifor all nÚ1
Thus,
P⎛
⎝q/uniondisplay
1Ei⎞⎠=P⎛⎝
q/uniondisplay
1Fi⎞⎠
=q/summationdisplay
1P(Fi)(by Axiom 3)
=limn→qn/summationdisplay
1P(Fi)
=limn→qP⎛⎝n/uniondisplay
1Fi⎞⎠
=lim
n→qP⎛
⎝n/uniondisplay
1Ei⎞⎠
=lim
n→qP(En)
which proves the result when {En,nÚ1}is increasing.
If{En,nÚ1}is a decreasing sequence, then {Ec
n,nÚ1}is an increasing sequence;
hence, from the preceding equations,
P⎛⎝q/uniondisplay
1Ec
i⎞⎠=lim
n→qP(Ec
n)
However, becauseq/uniontext
1Ec
i=/parenleftBigg
q/intersectiontext
1Ei/parenrightBiggc
, it follows that
P⎛
⎜⎝⎛
⎝q/intersectiondisplay
1Ei⎞⎠c⎞
⎟⎠=limn→qP(Ec
n)

<<<PAGE 57>>>

44Chapter 2 Axioms of Probability
or, equivalently,
1−P⎛
⎝q/intersectiondisplay
1Ei⎞⎠=lim
n→q[1−P(En)]=1−limn→qP(En)
or
P⎛
⎝q/intersectiondisplay
1Ei⎞⎠=lim
n→qP(En)
which proves the result.
Example
6aProbability and a “paradox”Suppose that we possess an inﬁnitely large urn and an inﬁnite collection of balls
labeled ball number 1, number 2, number 3, and so on. Consider an experimentperformed as follows: At 1 minute to 12
P. M ., balls numbered 1 through 10 are placed
in the urn and ball number 10 is withdrawn. (Assume that the withdrawal takes
no time.) At1
2minute to 12 P. M ., balls numbered 11 through 20 are placed in the
urn and ball number 20 is withdrawn. At1
4minute to 12 P. M ., balls numbered 21
through 30 are placed in the urn and ball number 30 is withdrawn. At1
8minute
to 12 P. M ., and so on. The question of interest is, How many balls are in the urn at
12P. M .?
The answer to this question is clearly that there is an inﬁnite number of
balls in the urn at 12 P. M ., since any ball whose number is not of the form 10 n,
nÚ1, will have been placed in the urn and will not have been withdrawn before
12P. M .Hence, the problem is solved when the experiment is performed as described.
However, let us now change the experiment and suppose that at 1 minute to
12P. M ., balls numbered 1 through 10 are placed in the urn and ball number 1 is with-
drawn; at1
2m i n u t et o1 2 P. M ., balls numbered 11 through 20 are placed in the urn
and ball number 2 is withdrawn; at1
4m i n u t et o1 2 P. M ., balls numbered 21 through
30 are placed in the urn and ball number 3 is withdrawn; at1
8minute to 12 P. M ., balls
numbered 31 through 40 are placed in the urn and ball number 4 is withdrawn, and
so on. For this new experiment, how many balls are in the urn at 12 P. M .?
Surprisingly enough, the answer now is that the urn is empty at 12 P. M . For,
consider any ball—say, ball number n. At some time prior to 12 P. M .[in particular,
at/parenleftBig
1
2/parenrightBign−1
minutes to 12 P. M .], this ball would have been withdrawn from the urn.
Hence, for each n, ball number nis not in the urn at 12 P. M .; therefore, the urn must
be empty at that time.
Because for all n, the number of balls in the urn after the nth interchange is
the same in both variations of the experiment, most people are surprised that thetwo scenarios produce such different results in the limit. It is important to recognize
that the reason the results are different is not because there is an actual paradox,o r
mathematical contradiction, but rather because of the logic of the situation, and alsothat the surprise results because one’s initial intuition when dealing with inﬁnity is
not always correct. (This latter statement is not surprising, for when the theory ofthe inﬁnite was ﬁrst developed by the mathematician Georg Cantor in the second
half of the nineteenth century, many of the other leading mathematicians of the day
called it nonsensical and ridiculed Cantor for making such claims as that the set ofall integers and the set of all even integers have the same number of elements.)
We see from the preceding discussion that the manner in which the balls are
withdrawn makes a difference. For, in the ﬁrst case, only balls numbered 10 n,nÚ1,

<<<PAGE 58>>>

A First Course in Probability 45
are ever withdrawn, whereas in the second case all of the balls are eventually with-
drawn. Let us now suppose that whenever a ball is to be withdrawn, that ball is
randomly selected from among those present. That is, suppose that at 1 minute to
12P. M . balls numbered 1 through 10 are placed in the urn and a ball is randomly
selected and withdrawn, and so on. In this case, how many balls are in the urn at
12P. M .?
Solution We shall show that, with probability 1, the urn is empty at 12 P. M .Let us
ﬁrst consider ball number 1. Deﬁne Ento be the event that ball number 1 is still in
the urn after the ﬁrst nwithdrawals have been made. Clearly,
P(En)=9·18·27···(9n)
10·19·28···(9n+1)
[To understand this equation, just note that if ball number 1 is still to be in theurn after the ﬁrst nwithdrawals, the ﬁrst ball withdrawn can be any one of 9, the
second any one of 18 (there are 19 balls in the urn at the time of the second with-
drawal, one of which must be ball number 1), and so on. The denominator is similarly
obtained.]
Now, the event that ball number 1 is in the urn at 12
P. M .is just the eventq/intersectiontext
n=1En.
Because the events En,nÚ1, are decreasing events, it follows from Proposition 6.1
that
P{ball number 1 is in the urn at 12 P. M .}
=P⎛
⎝q/intersectiondisplay
n=1En⎞⎠
=lim
n→qP(En)
=q/productdisplay
n=1/parenleftbigg9n
9n+1/parenrightbigg
We now show that
q/productdisplay
n=19n
9n+1=0
Since
q/productdisplay
n=1/parenleftbigg9n
9n+1/parenrightbigg
=⎡⎣q/productdisplay
n=1/parenleftbigg9n+1
9n/parenrightbigg⎤⎦−1
this is equivalent to showing that
q/productdisplay
n=1/parenleftbigg
1+1
9n/parenrightbigg
=q

<<<PAGE 59>>>

46Chapter 2 Axioms of Probability
Now, for all mÚ1,
q/productdisplay
n=1/parenleftbigg
1+1
9n/parenrightbigg
Úm/productdisplay
n=1/parenleftbigg
1+1
9n/parenrightbigg
=/parenleftbigg
1+1
9/parenrightbigg/parenleftbigg
1+1
18/parenrightbigg/parenleftbigg
1+1
27/parenrightbigg
···/parenleftbigg
1+1
9m/parenrightbigg
>1
9+1
18+1
27+ ··· +1
9m
=1
9m/summationdisplay
i=11
i
Hence, letting m→q and using the fact thatq/summationtext
i=11/i=qyields
q/productdisplay
n=1/parenleftbigg
1+1
9n/parenrightbigg
=q
Thus, letting Fidenote the event that ball number iis in the urn at 12 P. M ., we have
shown that P(F1)=0. Similarly, we can show that P(Fi)=0 for all i.
(For instance, the same reasoning shows that P(Fi)=q/producttext
n=2[9n/(9n +1)]f o r
i=11, 12, ..., 20.) Therefore, the probability that the urn is not empty at 12 P. M .,
P/parenleftBigg
q/uniontext
1Fi/parenrightBigg
, satisﬁes
P⎛
⎝q/uniondisplay
1Fi⎞⎠…q/summationdisplay
1P(Fi)=0
by Boole’s inequality. (See Self-Test Exercise 14.)
Thus, with probability 1, the urn will be empty at 12 P. M . .
2.7 Probability as a Measure of Belief
Thus far we have interpreted the probability of an event of a given experiment as
being a measure of how frequently the event will occur when the experiment iscontinually repeated. However, there are also other uses of the term probability.
For instance, we have all heard such statements as “It is 90 percent probable thatShakespeare actually wrote Hamlet ” or “The probability that Oswald acted alone in
assassinating Kennedy is .8.” How are we to interpret these statements?
The most simple and natural interpretation is that the probabilities referred to
are measures of the individual’s degree of belief in the statements that he or she
is making. In other words, the individual making the foregoing statements is quite
certain that Oswald acted alone and is even more certain that Shakespeare wroteHamlet . This interpretation of probability as being a measure of the degree of one’s
belief is often referred to as the personal orsubjective view of probability.
It seems logical to suppose that a “measure of the degree of one’s belief” should
satisfy all of the axioms of probability. For example, if we are 70 percent certain
that Shakespeare wrote Julius Caesar and 10 percent certain that it was actually
Marlowe, then it is logical to suppose that we are 80 percent certain that it was either

<<<PAGE 60>>>

A First Course in Probability 47
Shakespeare or Marlowe. Hence, whether we interpret probability as a measure of
belief or as a long-run frequency of occurrence, its mathematical properties remain
unchanged.
Example
7aSuppose that in a 7-horse race, you believe that each of the ﬁrst 2 horses has a 20percent chance of winning, horses 3 and 4 each have a 15 percent chance, and the
remaining 3 horses have a 10 percent chance each. Would it be better for you towager at even money that the winner will be one of the ﬁrst three horses or to wager,
again at even money, that the winner will be one of the horses 1, 5, 6, and 7?
Solution On the basis of your personal probabilities concerning the outcome of the
race, your probability of winning the ﬁrst bet is .2+.2+.15=.55, whereas
it is.2+.1+.1+.1=.5 for the second bet. Hence, the ﬁrst wager is more
attractive. .
Note that in supposing that a person’s subjective probabilities are always consis-
tent with the axioms of probability, we are dealing with an idealized rather than an
actual person. For instance, if we were to ask someone what he thought the chances
were of
(a) rain today,
(b) rain tomorrow,
(c) rain both today and tomorrow,
(d) rain either today or tomorrow,
it is quite possible that, after some deliberation, he might give 30 percent, 40 percent,
20 percent, and 60 percent as answers. Unfortunately, such answers (or such subjec-tive probabilities) are not consistent with the axioms of probability. (Why not?) We
would of course hope that after this was pointed out to the respondent, he would
change his answers. (One possibility we could accept is 30 percent, 40 percent, 10percent, and 60 percent.)
Summary
LetSdenote the set of all possible outcomes of an exper-
iment. Sis called the sample space of the experiment. An
event is a subset of S.I f Ai,i=1,...,n, are events, then
n/uniontext
i=1Ai, called the union of these events, consists of all out-
comes that are in at least one of the events Ai,i=1,...,n.
Similarly,n/intersectiontext
i=1Ai, sometimes written as A1···An, is called
theintersection of the events Aiand consists of all out-
comes that are in all of the events Ai,i=1,...,n.
For any event A, we deﬁne Acto consist of all out-
comes in the sample space that are not in A.W ec a l l Ac
thecomplement of the event A. The event Sc,w h i c hi s
empty of outcomes, is designated by Ø and is called the
null set. If AB=Ø, then we say that AandBaremutually
exclusive.
For each event Aof the sample space S, we suppose
that a number P(A), called the probability of A, is deﬁned
a n di ss u c ht h a t
(i) 0 …P(A)…1
(ii)P(S)=1
(iii) For mutually exclusive events Ai,iÚ1,P⎛
⎝q/uniondisplay
i=1Ai⎞⎠=q/summationdisplay
i=1P(Ai)
P(A) represents the probability that the outcome of the
experiment is in A.
It can be shown that
P(Ac)=1−P(A)
Au s e f u lr e s u l ti st h a t
P(A∪B)=P(A)+P(B)−P(AB)
which can be generalized to give
P⎛⎝n/uniondisplay
i=1Ai⎞⎠=n/summationdisplay
i=1P(Ai)−/summationdisplay/summationdisplay
i<jP(AiAj)
+/summationdisplay/summationdisplay/summationdisplay
i<j<kP(AiAjAk)
+ ··· + (−1)n+1P(A1···An)
This result is known as the inclusion–exclusion identity.

<<<PAGE 61>>>

48Chapter 2 Axioms of Probability
IfSis ﬁnite and each one point set is assumed to have
equal probability, then
P(A)=|A|
|S|where |E|denotes the number of outcomes in the event E.
P(A)can be interpreted either as a long-run relative
frequency or as a measure of one’s degree of belief.
Problems
1.A box contains 3 marbles: 1 red, 1 green, and 1 blue.
Consider an experiment that consists of taking 1 marble
from the box and then replacing it in the box and draw-
ing a second marble from the box. Describe the samplespace. Repeat when the second marble is drawn without
replacing the ﬁrst marble.
2.In an experiment, die is rolled continually until a 6
appears, at which point the experiment stops. What is the
sample space of this experiment? Let E
ndenote the event
that nrolls are necessary to complete the experiment.
What points of the sample space are contained in En?
What is/parenleftBigg
q/uniontext
1En/parenrightBiggc
?
3.Two dice are thrown. Let Ebe the event that the sum of
the dice is odd, let Fbe the event that at least one of the
dice lands on 1, and let Gbe the event that the sum is 5.
Describe the events EF,E∪F,FG,EFc,a n d EFG.
4.A,B,a n d Ctake turns ﬂipping a coin. The ﬁrst one to
get a head wins. The sample space of this experiment canbe deﬁned by
S=/braceleftbigg
1, 01, 001, 0001, ...,
0000···
(a)Interpret the sample space.
(b)Deﬁne the following events in terms of S:
(i)Awins=A.
(ii)Bwins=B.
(iii)(A∪B)
c.
Assume that Aﬂips ﬁrst, then B,t h e n C,t h e nA,
and so on.
5.A system is composed of 5 components, each of which
is either working or failed. Consider an experiment thatconsists of observing the status of each component, and
let the outcome of the experiment be given by the vector(x
1,x2,x3,x4,x5),w h e r ex iis equal to 1 if component iis
working and is equal to 0 if component iis failed.
(a)How many outcomes are in the sample space of this
experiment?
(b)Suppose that the system will work if components 1 and
2 are both working, or if components 3 and 4 are both
working, or if components 1, 3, and 5 are all working. LetWbe the event that the system will work. Specify all the
outcomes in W.(c)LetAbe the event that components 4 and 5 are both
failed. How many outcomes are contained in the event A?
(d)Write out all the outcomes in the event AW.
6.A hospital administrator codes incoming patients suf-
fering gunshot wounds according to whether they haveinsurance (coding 1 if they do and 0 if they do not) andaccording to their condition, which is rated as good (g), fair
(f), or serious (s). Consider an experiment that consists of
the coding of such a patient.
(a)Give the sample space of this experiment.
(b)LetAbe the event that the patient is in serious condi-
tion. Specify the outcomes in A.
(c)LetBbe the event that the patient is uninsured. Specify
the outcomes in B.
(d)Give all the outcomes in the event B
c∪A.
7.Consider an experiment that consists of determining
the type of job—either blue collar or white collar—
and the political afﬁliation—Republican, Democratic, or
Independent—of the 15 members of an adult soccer team.How many outcomes are
(a)in the sample space?
(b)in the event that at least one of the team members is a
blue-collar worker?
(c)in the event that none of the team members considers
himself or herself an Independent?
8.Suppose that AandBare mutually exclusive events for
which P(A)=.3a n d P(B)=.5. What is the probabil-
ity that
(a)either AorBoccurs?
(b)Aoccurs but Bdoes not?
(c)both AandBoccur?
9.A retail establishment accepts either the American
Express or the VISA credit card. A total of 24 percent
of its customers carry an American Express card, 61 per-
cent carry a VISA card, and 11 percent carry both cards.What percentage of its customers carry a credit card that
the establishment will accept?
10.Sixty percent of the students at a certain school wear
neither a ring nor a necklace. Twenty percent wear a ring
and 30 percent wear a necklace. If one of the students is

<<<PAGE 62>>>

A First Course in Probability 49
chosen randomly, what is the probability that this student
is wearing
(a)ar i n go ran e c k l a c e ?
(b)a ring and a necklace?
11.A total of 28 percent of American males smoke
cigarettes, 7 percent smoke cigars, and 5 percent smoke
both cigars and cigarettes.
(a)What percentage of males smokes neither cigars nor
cigarettes?
(b)What percentage smokes cigars but not cigarettes?
12.An elementary school is offering 3 language classes:
one in Spanish, one in French, and one in German. The
classes are open to any of the 100 students in the school.There are 28 students in the Spanish class, 26 in the French
class, and 16 in the German class. There are 12 students
who are in both Spanish and French, 4 who are in bothSpanish and German, and 6 who are in both French and
German. In addition, there are 2 students taking all 3
classes.
(a)If a student is chosen randomly, what is the probability
that he or she is not in any of the language classes?
(b)If a student is chosen randomly, what is the probability
that he or she is taking exactly one language class?(c)If 2 students are chosen randomly, what is the proba-
bility that at least 1 is taking a language class?
13.A certain town with a population of 100,000 has 3
newspapers: I, II, and III. The proportions of townspeople
who read these papers are as follows:
I: 10 percent I and II: 8 percent I and II and
III: 1 percent
II: 30 percent I and III: 2 percent
III: 5 percent II and III: 4 percent
(The list tells us, for instance, that 8000 people read news-papers I and II.)
(a)Find the number of people who read only one newspa-
per.
(b)How many people read at least two newspapers?
(c)If I and III are morning papers and II is an evening
paper, how many people read at least one morning paper
plus an evening paper?
(d)How many people do not read any newspapers?
(e)How many people read only one morning paper and
one evening paper?
14.T h ef o l l o w i n gd a t aw e r eg i v e ni nas t u d yo fag r o u p
of 1000 subscribers to a certain magazine: In reference
to job, marital status, and education, there were 312 pro-
fessionals, 470 married persons, 525 college graduates, 42
professional college graduates, 147 married college gradu-ates, 86 married professionals, and 25 married professional
college graduates. Show that the numbers reported in the
study must be incorrect.Hint :L e tM ,W,a n d Gdenote, respectively, the set
of professionals, married persons, and college graduates.Assume that one of the 1000 persons is chosen at random,
and use Proposition 4.4 to show that if the given numbers
are correct, then P(M∪W∪G)> 1.
15.If it is assumed that all/parenleftbigg
52
5/parenrightbigg
poker hands are
equally likely, what is the probability of being dealt
(a)a ﬂush? (A hand is said to be a ﬂush if all 5 cards are of
the same suit.)
(b)one pair? (This occurs when the cards have denomina-
tions a,a,b,c,d,w h e r e a,b,c,a n d dare all distinct.)
(c)two pairs? (This occurs when the cards have denomi-
nations a,a,b,b,c,w h e r e a,b,a n d care all distinct.)
(d)three of a kind? (This occurs when the cards have
denominations a,a,a,b,c,w h e r ea, b,a n d care all dis-
tinct.)(e)four of a kind? (This occurs when the cards have
denominations a,a,a,a,b.)
16.Poker dice is played by simultaneously rolling 5 dice.
Show that
(a)P{no two alike }=.0926;
(b)P{one pair}=.4630;
(c)P{two pair}=.2315;
(d)P{three alike}= .1543;
(e)P{full house}= .0386;
(f)P{four alike }=.0193;
(g)P{ﬁve alike}=.0008.
17.If 8 rooks (castles) are randomly placed on a chess-
board, compute the probability that none of the rooks can
capture any of the others. That is, compute the probabilitythat no row or ﬁle contains more than one rook.
18.Two cards are randomly selected from an ordinary
playing deck. What is the probability that they form a
blackjack? That is, what is the probability that one of thecards is an ace and the other one is either a ten, a jack, a
queen, or a king?
19.Two symmetric dice have had two of their sides painted
red, two painted black, one painted yellow, and the other
painted white. When this pair of dice is rolled, what is theprobability that both dice land with the same color face
up?
20.Suppose that you are playing blackjack against a
dealer. In a freshly shufﬂed deck, what is the probability
that neither you nor the dealer is dealt a blackjack?
21.A small community organization consists of 20 fam-
ilies, of which 4 have one child, 8 have two children, 5

<<<PAGE 63>>>

50Chapter 2 Axioms of Probability
have three children, 2 have four children, and 1 has ﬁve
children.
(a)If one of these families is chosen at random, what is the
probability it has ichildren, i=1, 2, 3, 4, 5?
(b)If one of the children is randomly chosen, what is the
probability that child comes from a family having ichil-
dren, i=1, 2, 3, 4, 5?
22.Consider the following technique for shufﬂing a deck
ofncards: For any initial ordering of the cards, go through
the deck one card at a time and at each card, ﬂip a fair coin.
If the coin comes up heads, then leave the card where it is;
if the coin comes up tails, then move that card to the endof the deck. After the coin has been ﬂipped ntimes, say
that one round has been completed. For instance, if n=4
and the initial ordering is 1, 2, 3, 4, then if the successiveﬂips result in the outcome h,t,t,h, then the ordering at
the end of the round is 1, 4, 2, 3. Assuming that all possibleoutcomes of the sequence of ncoin ﬂips are equally likely,
what is the probability that the ordering after one round is
the same as the initial ordering?
23.A pair of fair dice is rolled. What is the probability that
the second die lands on a higher value than does the ﬁrst?24.If two dice are rolled, what is the probability that
the sum of the upturned faces equals i?F i n di tf o r i=
2, 3,..., 11, 12.
25.A pair of dice is rolled until a sum of either 5 or 7
appears. Find the probability that a 5 occurs ﬁrst.
Hint :L e tE
ndenote the event that a 5 occurs on the nth
roll and no 5 or 7 occurs on the ﬁrst n−1 rolls. Compute
P(En)and argue thatq/summationtext
n=1P(En)is the desired probability.
26.The game of craps is played as follows: A player rolls
two dice. If the sum of the dice is either a 2, 3, or 12, theplayer loses; if the sum is either a 7 or an 11, the player
wins. If the outcome is anything else, the player continues
to roll the dice until she rolls either the initial outcome or a7. If the 7 comes ﬁrst, the player loses, whereas if the initial
outcome reoccurs before the 7 appears, the player wins.
Compute the probability of a player winning at craps.Hint :L e tE
idenote the event that the initial outcome is
iand the player wins. The desired probability is12/summationtext
i=2P(Ei).
To compute P(Ei), deﬁne the events Ei,nto be the event
that the initial sum is iand the player wins on the nth roll.
Argue that P(Ei)=q/summationtext
n=1P(Ei,n).
27.An urn contains 3 red and 7 black balls. Players Aand
Bwithdraw balls from the urn consecutively until a red
ball is selected. Find the probability that Aselects the red
ball. (A draws the ﬁrst ball, then B, and so on. There is no
replacement of the balls drawn.)28.An urn contains 5 red, 6 blue, and 8 green balls. If a set
of 3 balls is randomly selected, what is the probability thateach of the balls will be (a) of the same color? (b) of differ-
ent colors? Repeat under the assumption that whenever a
ball is selected, its color is noted and it is then replaced inthe urn before the next selection. This is known as sam-
pling with replacement .
29.An urn contains nwhite and mblack balls, where nand
mare positive numbers.
(a)If two balls are randomly withdrawn, what is the prob-
ability that they are the same color?
(b)If a ball is randomly withdrawn and then replaced
before the second one is drawn, what is the probability that
the withdrawn balls are the same color?
(c)Show that the probability in part (b) is always larger
than the one in part (a).
30.The chess clubs of two schools consist of, respectively,
8 and 9 players. Four members from each club are ran-
domly chosen to participate in a contest between the twoschools. The chosen players from one team are then ran-
domly paired with those from the other team, and each
pairing plays a game of chess. Suppose that Rebecca andher sister Elise are on the chess clubs at different schools.
What is the probability that
(a)Rebecca and Elise will be paired?
(b)Rebecca and Elise will be chosen to represent their
schools but will not play each other?
(c)either Rebecca or Elise will be chosen to represent her
school?
31.A 3-person basketball team consists of a guard, a for-
ward, and a center.
(a)If a person is chosen at random from each of three dif-
ferent such teams, what is the probability of selecting a
complete team?
(b)What is the probability that all 3 players selected play
the same position?
32.A group of individuals containing bboys and ggirls
is lined up in random order; that is, each of the (b+g)!
permutations is assumed to be equally likely. What is the
probability that the person in the ith position, 1 …i…b+g,
is a girl?
33.A forest contains 20 elk, of which 5 are captured,
tagged, and then released. A certain time later, 4 of the
20 elk are captured. What is the probability that 2 of these4 have been tagged? What assumptions are you making?
34.The second Earl of Yarborough is reported to have bet
at odds of 1000 to 1 that a bridge hand of 13 cards would
contain at least one card that is ten or higher. (By ten or
higher we mean that a card is either a ten, a jack, a queen,
a king, or an ace.) Nowadays, we call a hand that has nocards higher than 9 a Yarborough. What is the probability
that a randomly selected bridge hand is a Yarborough?

<<<PAGE 64>>>

A First Course in Probability 51
35.Seven balls are randomly withdrawn from an urn that
contains 12 red, 16 blue, and 18 green balls. Find the prob-
ability that
(a)3 red, 2 blue, and 2 green balls are withdrawn;
(b)at least 2 red balls are withdrawn;
(c)all withdrawn balls are the same color;
(d)either exactly 3 red balls or exactly 3 blue balls are
withdrawn.
36.Two cards are chosen at random from a deck of 52
playing cards. What is the probability that they
(a)are both aces?
(b)have the same value?
37.An instructor gives her class a set of 10 problems with
the information that the ﬁnal exam will consist of a ran-
dom selection of 5 of them. If a student has ﬁgured out
how to do 7 of the problems, what is the probability that
he or she will answer correctly
(a)all 5 problems?
(b)at least 4 of the problems?
38.There are nsocks, 3 of which are red, in a drawer. What
is the value of nif, when 2 of the socks are chosen ran-
domly, the probability that they are both red is1
2?
39.There are 5 hotels in a certain town. If 3 people check
into hotels in a day, what is the probability that they each
check into a different hotel? What assumptions are youmaking?
40.A town contains 4 people who repair televisions. If
4 sets break down, what is the probability that exactly i
of the repairers are called? Solve the problem for i=
1, 2, 3, 4. What assumptions are you making?
41.If a die is rolled 4 times, what is the probability that 6
comes up at least once?
42.Two dice are thrown ntimes in succession. Compute
the probability that double 6 appears at least once. How
large need nbe to make this probability at least
1
2?
43.(a)IfNpeople, including AandB, are randomly
arranged in a line, what is the probability that AandB
are next to each other?
(b)What would the probability be if the people were ran-
domly arranged in a circle?
44.Five people, designated as A,B,C,D,E, are arranged
in linear order. Assuming that each possible order is
equally likely, what is the probability that
(a)there is exactly one person between AandB?
(b)there are exactly two people between AandB?
(c)there are three people between AandB?
45.A woman has nkeys, of which one will open her door.(a)If she tries the keys at random, discarding those that
do not work, what is the probability that she will open the
door on her kth try?
(b)What if she does not discard previously tried keys?
46.How many people have to be in a room in order that
the probability that at least two of them celebrate their
birthday in the same month is at least1
2? Assume that all
possible monthly outcomes are equally likely.
47.If there are 12 strangers in a room, what is the proba-
bility that no two of them celebrate their birthday in the
same month?
48.Given 20 people, what is the probability that among
the 12 months in the year, there are 4 months containing
exactly 2 birthdays and 4 containing exactly 3 birthdays?
49.A group of 6 men and 6 women is randomly divided
into 2 groups of size 6 each. What is the probability that
both groups will have the same number of men?
50.In a hand of bridge, ﬁnd the probability that you have
5 spades and your partner has the remaining 8.51.Suppose that nballs are randomly distributed into N
compartments. Find the probability that mballs will fall
into the ﬁrst compartment. Assume that all N
narrange-
ments are equally likely.
52.A closet contains 10 pairs of shoes. If 8 shoes are ran-
domly selected, what is the probability that there will be
(a)no complete pair?
(b)exactly 1 complete pair?
53.If 4 married couples are arranged in a row, ﬁnd the
probability that no husband sits next to his wife.
54.Compute the probability that a bridge hand is void in
at least one suit. Note that the answer is not
/parenleftbigg
4
1/parenrightbigg/parenleftbigg
3913/parenrightbigg
/parenleftbigg
52
13/parenrightbigg
(Why not?)Hint : Use Proposition 4.4.
55.Compute the probability that a hand of 13 cards
contains
(a)the ace and king of at least one suit;
(b)all 4 of at least 1 of the 13 denominations.
56.Two players play the following game: Player Achooses
one of the three spinners pictured in Figure 2.6, and then
player Bchooses one of the remaining two spinners. Both
players then spin their spinner, and the one that lands on
the higher number is declared the winner. Assuming that
each spinner is equally likely to land in any of its 3 regions,would you rather be player Aor player B? Explain your
answer!

<<<PAGE 65>>>

52Chapter 2 Axioms of Probability
95
1a38
4b
76
2c
Figure 2.6 Spinners.
Theoretical Exercises
Prove the following relations:
1.EF(E(E∪F.
2.IfE(F,t h e nFc(Ec.
3.F=FE∪FEcandE∪F=E∪EcF.
4./parenleftBigg
q/uniontext
1Ei/parenrightBigg
F=q/uniontext
1EiFand
/parenleftBigg
q/intersectiontext
1Ei/parenrightBigg
∪F=q/intersectiontext
1(Ei∪F).
5.For any sequence of events E1,E2,...,d e ﬁ n ean e w
sequence F1,F2,...of disjoint events (that is, events such
thatFiFj=Ø whenever iZj) such that for all nÚ1,
n/uniondisplay
1Fi=n/uniondisplay
1Ei6.LetE,F,a n dG be three events. Find expressions for
the events so that, of E,F,a n dG,
(a)onlyEoccurs;
(b)both EandG, but not F, occur;
(c)at least one of the events occurs;
(d)at least two of the events occur;
(e)all three events occur;
(f)none of the events occurs;
(g)at most one of the events occurs;
(h)at most two of the events occur;
(i)exactly two of the events occur;
(j)at most three of the events occur.
7.Use Venn diagrams
(a)to simplify the expressions (E∪F)(E∪Fc);
(b)to prove DeMorgan’s laws for events EandF.[ T h a ti s ,
prove (E∪F)c=EcFc,a n d (EF)c=Ec∪Fc.]
8.LetSbe a given set. If, for some k>0,S1,S2,...,Sk
are mutually exclusive nonempty subsets of Ssuch that

<<<PAGE 66>>>

A First Course in Probability 53
k/uniontext
i=1Si=S, then we call the set {S1,S2,...,Sk}aparti-
tion ofS.L e tT ndenote the number of different parti-
tions of {1, 2,...,n}. Thus, T1=1 (the only partition
being S1={1})a n d T2=2 (the two partitions being
{{1, 2, }},{{1},{2}}).
(a)Show, by computing all partitions, that T3=5,T4=15.
(b)Show that
Tn+1=1+n/summationdisplay
k=1/parenleftbigg
n
k/parenrightbigg
Tk
and use this equation to compute T10.
Hint : One way of choosing a partition of n+1 items is to
call one of the items special . Then we obtain different par-
titions by ﬁrst choosing k,k=0, 1,...,n, then a subset of
sizen−kof the nonspecial items, and then any of the Tk
partitions of the remaining knonspecial items. By adding
the special item to the subset of size n−k, we obtain a
partition of all n+1 items.
9.Suppose that an experiment is performed ntimes. For
any event Eof the sample space, let n(E)denote the num-
ber of times that event Eoccurs and deﬁne f(E)=n(E)/n.
Show that f(·)satisﬁes Axioms 1, 2, and 3.
10.Prove that P(E∪F∪G)=P(E)+P(F)+P(G)−
P(EcFG)−P(EFcG)−P(EFGc)−2P(EFG).
11.IfP(E)=.9a n d P(F)=.8, show that P(EF)Ú.7. In
general, prove Bonferroni’s inequality, namely,
P(EF)ÚP(E)+P(F)−1
12.Show that the probability that exactly one of the events
EorFoccurs equals P(E)+P(F)−2P(EF).
13.Prove that P(EFc)=P(E)−P(EF).
14.Prove Proposition 4.4 by mathematical induction.
15.An urn contains Mwhite and Nblack balls. If a ran-
dom sample of size ris chosen, what is the probability that
it contains exactly kwhite balls?
16.Use induction to generalize Bonferroni’s inequality to
nevents. That is, show that
P(E1E2···En)ÚP(E1)+ ··· + P(En)−(n−1)
17.Consider the matching problem, Example 5m, and
deﬁne ANto be the number of ways in which the N
men can select their hats so that no man selects his own.
Argue thatAN=(N−1)(A N−1+AN−2)
This formula, along with the boundary conditions A1=0,
A2=1, can then be solved for AN, and the desired proba-
bility of no matches would be AN/N!.
Hint : After the ﬁrst man selects a hat that is not his own,
there remain N−1 men to select among a set of N−1
hats that does not contain the hat of one of these men.
Thus, there is one extra man and one extra hat. Argue that
we can get no matches either with the extra man select-ing the extra hat or with the extra man not selecting the
extra hat.
18.Letf
ndenote the number of ways of tossing a coin n
times such that successive heads never appear. Argue that
fn=fn−1+fn−2 nÚ2, where f0K1,f1K2
Hint : How many outcomes are there that start with a head,
and how many start with a tail? If Pndenotes the proba-
bility that successive heads never appear when a coin is
tossed ntimes, ﬁnd Pn(in terms of fn) when all possible
outcomes of the ntosses are assumed equally likely. Com-
pute P10.
19.An urn contains nred and mblue balls. They are with-
drawn one at a time until a total of r,r…n, red balls have
been withdrawn. Find the probability that a total of kballs
are withdrawn.
Hint : A total of kballs will be withdrawn if there are r−1
red balls in the ﬁrst k−1 withdrawals and the kth with-
drawal is a red ball.
20.Consider an experiment whose sample space consists
of a countably inﬁnite number of points. Show that not all
points can be equally likely. Can all points have a positive
probability of occurring?
*21.Consider Example 5o, which is concerned with the
number of runs of wins obtained when nwins and mlosses
are randomly permuted. Now consider the total number
of runs—that is, win runs plus loss runs—and show that
P{2kruns}=2/parenleftbigg
m−1
k−1/parenrightbigg/parenleftbigg
n−1
k−1/parenrightbigg
/parenleftbigg
m+n
n/parenrightbigg
P{2k+1 runs }
=/parenleftbigg
m−1
k−1/parenrightbigg/parenleftbigg
n−1
k/parenrightbigg
+/parenleftbigg
m−1
k/parenrightbigg/parenleftbigg
n−1
k−1/parenrightbigg
/parenleftbigg
m+n
n/parenrightbigg

<<<PAGE 67>>>

54Chapter 2 Axioms of Probability
Self-Test Problems and Exercises
1.A cafeteria offers a three-course meal consisting of an
entree, a starch, and a dessert. The possible choices are
given in the following table:
Course Choices
Entree Chicken or roast beefStarch Pasta or rice or potatoesDessert Ice cream or Jello or apple pie or a peach
A person is to choose one course from each category.
(a)How many outcomes are in the sample space?
(b)LetAbe the event that ice cream is chosen. How many
outcomes are in A?
(c)LetBbe the event that chicken is chosen. How many
outcomes are in B?
(d)List all the outcomes in the event AB.
(e)LetCbe the event that rice is chosen. How many out-
comes are in C?
(f)List all the outcomes in the event ABC.
2.A customer visiting the suit department of a certain
store will purchase a suit with probability .22, a shirt with
probability .30, and a tie with probability .28. The cus-
tomer will purchase both a suit and a shirt with probability
.11, both a suit and a tie with probability .14, and both a
shirt and a tie with probability .10. A customer will pur-
chase all 3 items with probability .06. What is the proba-bility that a customer purchases
(a)none of these items?
(b)exactly 1 of these items?
3.A deck of cards is dealt out. What is the probability that
the 14th card dealt is an ace? What is the probability that
the ﬁrst ace occurs on the 14th card?
4.LetAdenote the event that the midtown temperature
in Los Angeles is 70
◦F, and let Bdenote the event that
the midtown temperature in New York is 70◦F. Also, let
Cdenote the event that the maximum of the midtown
temperatures in New York and in Los Angeles is 70◦F. If
P(A)=.3,P(B)=.4, and P(C)=.2, ﬁnd the probabil-
ity that the minimum of the two midtown temperatures is
70◦F.
5.An ordinary deck of 52 cards is shufﬂed. What is the
probability that the top four cards have
(a)different denominations?
(b)different suits?
6.Urn Acontains 3 red and 3 black balls, whereas urn
Bcontains 4 red and 6 black balls. If a ball is randomlyselected from each urn, what is the probability that the
balls will be the same color?
7.In a state lottery, a player must choose 8 of the num-
bers from 1 to 40. The lottery commission then performs
an experiment that selects 8 of these 40 numbers. Assum-
ing that the choice of the lottery commission is equally
l i k e l yt ob ea n yo ft h e/parenleftbigg
40
8/parenrightbigg
combinations, what is the
probability that a player has
(a)all 8 of the numbers selected by the lottery
commission?(b)7 of the numbers selected by the lottery commission?
(c)at least 6 of the numbers selected by the lottery
commission?
8.From a group of 3 ﬁrst-year students, 4 sophomores, 4
juniors, and 3 seniors, a committee of size 4 is randomly
selected. Find the probability that the committee will con-
sist of
(a)1 from each class;
(b)2 sophomores and 2 juniors;
(c)only sophomores or juniors.
9.For a ﬁnite set A,l e tN (A)denote the number of ele-
ments in A.
(a)Show that
N(A∪B)=N(A)+N(B)−N(AB)
(b)More generally, show that
N⎛
⎝n/uniondisplay
i=1Ai⎞⎠=/summationdisplay
iN(Ai)−/summationdisplay/summationdisplay
i<jN(AiAj)
+ ··· + (−1)n+1N(A1···An)
10.Consider an experiment that consists of 6 horses, num-
bered 1 through 6, running a race, and suppose that the
sample space consists of the 6! possible orders in which the
horses ﬁnish. Let Abe the event that the number-1 horse
is among the top three ﬁnishers, and let Bbe the event that
the number-2 horse comes in second. How many outcomesare in the event A∪B?
11.A 5-card hand is dealt from a well-shufﬂed deck of 52
playing cards. What is the probability that the hand con-
tains at least one card from each of the four suits?
12.A basketball team consists of 6 frontcourt and 4 back-
court players. If players are divided into roommates at ran-
dom, what is the probability that there will be exactly two
roommate pairs made up of a backcourt and a frontcourtplayer?

<<<PAGE 68>>>

A First Course in Probability 55
13.Suppose that a person chooses a letter at random from
R E S E R V E and then chooses one at random from
V E R T I C A L. What is the probability that the same
letter is chosen?
14.Prove Boole’s inequality:
P⎛
⎝q/uniondisplay
i=1Ai⎞⎠…q/summationdisplay
i=1P(Ai)
15.Show that if P(Ai)=1f o ra l l iÚ1, then P/parenleftBigg
q/intersectiontext
i=1Ai/parenrightBigg
=1.
16.LetTk(n)denote the number of partitions of the set
{1,...,n}intoknonempty subsets, where 1 …k…n.( S e e
Theoretical Exercise 8 for the deﬁnition of a partition.)
Argue that
Tk(n)=kTk(n−1)+Tk−1(n−1)
Hint : In how many partitions is {1}a subset, and in how
many is 1 an element of a subset that contains otherelements?
17.Five balls are randomly chosen, without replacement,
from an urn that contains 5 red, 6 white, and 7 blue balls.Find the probability that at least one ball of each color is
chosen.
18.Four red, 8 blue, and 5 green balls are randomly
arranged in a line.
(a)What is the probability that the ﬁrst 5 balls are blue?
(b)What is the probability that none of the ﬁrst 5 balls is
blue?
(c)What is the probability that the ﬁnal 3 balls are of dif-
ferent colors?(d)What is the probability that all the red balls are
together?
19.Ten cards are randomly chosen from a deck of 52 cards
that consists of 13 cards of each of 4 different suits. Each
of the selected cards is put in one of 4 piles, depending onthe suit of the card.
(a)What is the probability that the largest pile has 4 cards,
the next largest has 3, the next largest has 2, and the small-
est has 1 card?
(b)What is the probability that two of the piles have 3
cards, one has 4 cards, and one has no cards?
20.Balls are randomly removed from an urn initially con-
taining 20 red and 10 blue balls. What is the probability
that all of the red balls are removed before all of the blueones have been removed?

<<<PAGE 69>>>

Chapter
Conditional Probability
and Independence 3
Contents
3.1 Introduction
3.2 Conditional Probabilities
3.3 Bayes’s Formula3.4 Independent Events
3.5 P(·|F ) Is a Probability
3.1 Introduction
In this chapter, we introduce one of the most important concepts in probability
theory, that of conditional probability. The importance of this concept is twofold.
In the ﬁrst place, we are often interested in calculating probabilities when somepartial information concerning the result of an experiment is available; in such a
situation, the desired probabilities are conditional. Second, even when no partial
information is available, conditional probabilities can often be used to compute thedesired probabilities more easily.
3.2 Conditional Probabilities
Suppose that we toss 2 dice, and suppose that each of the 36 possible outcomes is
equally likely to occur and hence has probability1
36. Suppose further that we observe
that the ﬁrst die is a 3. Then, given this information, what is the probability that the
sum of the 2 dice equals 8? To calculate this probability, we reason as follows: Giventhat the initial die is a 3, there can be at most 6 possible outcomes of our experiment,
namely, (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), and (3, 6). Since each of these outcomes
originally had the same probability of occurring, the outcomes should still have equalprobabilities. That is, given that the ﬁrst die is a 3, the (conditional) probability of
each of the outcomes (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), and (3, 6) is
1
6, whereas the
(conditional) probability of the other 30 points in the sample space is 0. Hence, the
desired probability will be1
6.
If we let EandFdenote, respectively, the event that the sum of the dice is 8
and the event that the ﬁrst die is a 3, then the probability just obtained is called the
conditional probability that E occurs given that F has occurred and is denoted by
P(E|F)
A general formula for P(E|F)that is valid for all events EandFis derived in the
same manner: If the event Foccurs, then, in order for Eto occur, it is necessary
56

<<<PAGE 70>>>

A First Course in Probability 57
that the actual occurrence be a point both in Eand in F; that is, it must be in EF.
Now, since we know that Fhas occurred, it follows that Fbecomes our new, or
reduced, sample space; hence, the probability that the event EFoccurs will equal
the probability of EFrelative to the probability of F. That is, we have the following
deﬁnition.
Deﬁnition
IfP(F)> 0, then
P(E|F)=P(EF)
P(F)(2.1)
Example
2aJoe is 80 percent certain that his missing key is in one of the two pockets of his
hanging jacket, being 40 percent certain it is in the left-hand pocket and 40 percent
certain it is in the right-hand pocket. If a search of the left-hand pocket does not ﬁnd
the key, what is the conditional probability that it is in the other pocket?
Solution If we let Lbe the event that the key is in the left-hand pocket of the jacket,
andRbe the event that it is in the right-hand pocket, then the desired probability
P(R|Lc)can be obtained as follows:
P(R|Lc)=P(RLc)
P(Lc)
=P(R)
1−P(L)
=2/3 .
If each outcome of a ﬁnite sample space Sis equally likely, then, conditional on
the event that the outcome lies in a subset F(S, all outcomes in Fbecome equally
likely. In such cases, it is often convenient to compute conditional probabilities of
the form P(E|F)by using Fas the sample space. Indeed, working with this reduced
sample space often results in an easier and better understood solution. Our next two
examples illustrate this point.
Example
2bA coin is ﬂipped twice. Assuming that all four points in the sample space S={(h,h),
(h,t),(t,h),(t,t)}are equally likely, what is the conditional probability that both ﬂips
land on heads, given that (a) the ﬁrst ﬂip lands on heads? (b) at least one ﬂip landson heads?
Solution LetB={(h,h)}be the event that both ﬂips land on heads; let F={(h,h),
(h,t)}be the event that the ﬁrst ﬂip lands on heads; and let A={(h,h),(h,t),(t,h)}be
the event that at least one ﬂip lands on heads. The probability for (a) can be obtainedfrom
P(B|F)=P(BF)
P(F)
=P({(h,h)})
P({(h,h),(h,t)})
=1/4
2/4=1/2

<<<PAGE 71>>>

58Chapter 3 Conditional Probability and Independence
For (b), we have
P(B|A) =P(BA)
P(A)
=P({(h,h)})
P({(h,h),(h,t),(t,h)})
=1/4
3/4=1/3
Thus, the conditional probability that both ﬂips land on heads given that the ﬁrst
one does is 1/2, whereas the conditional probability that both ﬂips land on heads
given that at least one does is only 1 /3. Many students initially ﬁnd this latter result
surprising. They reason that given that at least one ﬂip lands on heads, there are two
possible results: Either they both land on heads or only one does. Their mistake,
however, is in assuming that these two possibilities are equally likely. Initially there
are 4 equally likely outcomes. Because the information that at least one ﬂip lands on
heads is equivalent to the information that the outcome is not (t,t), we are left with
the 3 equally likely outcomes (h,h),(h,t),(t,h), only one of which results in both ﬂips
landing on heads. .
Example
2cIn the card game bridge, the 52 cards are dealt out equally to 4 players—called East,West, North, and South. If North and South have a total of 8 spades among them,
what is the probability that East has 3 of the remaining 5 spades?
Solution Probably the easiest way to compute the desired probability is to work
with the reduced sample space. That is, given that North–South have a total of 8
spades among their 26 cards, there remains a total of 26 cards, exactly 5 of them
being spades, to be distributed among the East–West hands. Since each distributionis equally likely, it follows that the conditional probability that East will have exactly
3 spades among his or her 13 cards is
/parenleftBigg
5
3/parenrightBigg/parenleftBigg
2110/parenrightBigg
/parenleftBigg
26
13/parenrightBigg L.339 .
Multiplying both sides of Equation (2.1) by P(F), we obtain
P(EF)=P(F)P(E|F) (2.2)
In words, Equation (2.2) states that the probability that both EandFoccur is equal
to the probability that Foccurs multiplied by the conditional probability of Egiven
thatFoccurred. Equation (2.2) is often quite useful in computing the probability of
the intersection of events.
Example
2dCeline is undecided as to whether to take a French course or a chemistry course. She
estimates that her probability of receiving an A grade would be1
2in a French course
and2
3in a chemistry course. If Celine decides to base her decision on the ﬂip of a
fair coin, what is the probability that she gets an A in chemistry?
Solution LetCbe the event that Celine takes chemistry and Adenote the event
that she receives an A in whatever course she takes, then the desired probability is
P(CA), which is calculated by using Equation (2.2) as follows:

<<<PAGE 72>>>

A First Course in Probability 59
P(CA)=P(C)P(A|C)
=/parenleftbigg1
2/parenrightbigg/parenleftbigg2
3/parenrightbigg
=1
3.
Example
2eSuppose that an urn contains 8 red balls and 4 white balls. We draw 2 balls from the
urn without replacement. (a) If we assume that at each draw, each ball in the urn is
equally likely to be chosen, what is the probability that both balls drawn are red? (b)
Now suppose that the balls have different weights, with each red ball having weightrand each white ball having weight w. Suppose that the probability that a given ball
in the urn is the next one selected is its weight divided by the sum of the weights ofall balls currently in the urn. Now what is the probability that both balls are red?
Solution LetR1andR2denote, respectively, the events that the ﬁrst and second
balls drawn are red. Now, given that the ﬁrst ball selected is red, there are 7 remain-
ing red balls and 4 white balls, so P(R2|R1)=7
11.A sP(R1)is clearly8
12, the desired
probability is
P(R1R2)=P(R1)P(R2|R1)
=/parenleftbigg2
3/parenrightbigg/parenleftbigg7
11/parenrightbigg
=14
33
Of course, this probability could have been computed by P(R1R2)=/parenleftbig8
2/parenrightbig
//parenleftbig12
2/parenrightbig
.
For part (b), we again let Ribe the event that the ith ball chosen is red and use
P(R1R2)=P(R1)P(R2|R1)
Now, number the red balls, and let Bi,i=1,..., 8 be the event that the ﬁrst ball
drawn is red ball number i. Then
P(R1)=P(∪8
i=1Bi)=8/summationdisplay
i=1P(Bi)=8r
8r+4w
Moreover, given that the ﬁrst ball is red, the urn then contains 7 red and 4 white
balls. Thus, by an argument similar to the preceding one,
P(R2|R1)=7r
7r+4w
Hence, the probability that both balls are red is
P(R1R2)=8r
8r+4w7r
7r+4w.
A generalization of Equation (2.2), which provides an expression for the prob-
ability of the intersection of an arbitrary number of events, is sometimes referred to
as the multiplication rule .
The multiplication rule
P(E1E2E3···En)=P(E1)P(E2|E1)P(E3|E1E2)···P(En|E1···En−1)

<<<PAGE 73>>>

60Chapter 3 Conditional Probability and Independence
To prove the multiplication rule, just apply the deﬁnition of conditional proba-
bility to its right-hand side, giving
P(E1)P(E1E2)
P(E1)P(E1E2E3)
P(E1E2)···P(E1E2···En)
P(E1E2···E n−1)=P(E1E2···En)
Example
2fIn the match problem stated in Example 5m of Chapter 2, it was shown that PN,t h e
probability that there are no matches when Npeople randomly select from among
their own Nhats, is given by
PN=N/summationdisplay
i=0(−1)i/i!
What is the probability that exactly kof the Npeople have matches?
Solution Let us ﬁx our attention on a particular set of kpeople and determine the
probability that these kindividuals have matches and no one else does. Letting E
denote the event that everyone in this set has a match, and letting Gbe the event
that none of the other N−kpeople have a match, we have
P(EG) =P(E)P(G|E)
Now, let Fi,i=1,...,k, be the event that the ith member of the set has a match.
Then
P(E)=P(F1F2···Fk)
=P(F1)P(F2|F1)P(F3|F1F2)···P(Fk|F1···F k−1)
=1
N1
N−11
N−2···1
N−k+1
=(N−k)!
N!
Given that everyone in the set of khas a match, the other N−kpeople will be
randomly choosing among their own N−khats, so the probability that none of
them has a match is equal to the probability of no matches in a problem having
N−kpeople choosing among their own N−khats. Therefore,
P(G|E)=PN−k=N−k/summationdisplay
i=0(−1)i/i!
showing that the probability that a speciﬁed set of kpeople have matches and no
one else does is
P(EG) =(N−k)!
N!PN−k
Because there will be exactly kmatches if the preceding is true for any of the/parenleftbigN
k/parenrightbig
sets of kindividuals, the desired probability is
P(exactly kmatches )=PN−k/k!
Le−1/k! when Nis large .
We will now employ the multiplication rule to obtain a second approach to solv-
ing Example 5h(b) of Chapter 2.

<<<PAGE 74>>>

A First Course in Probability 61
Example
2gAn ordinary deck of 52 playing cards is randomly divided into 4 piles of 13 cards
each. Compute the probability that each pile has exactly 1 ace.
Solution Deﬁne events Ei,i=1, 2, 3, 4, as follows:
E1={the ace of spades is in any one of the piles }
E2={the ace of spades and the ace of hearts are in different piles }
E3={the aces of spades, hearts, and diamonds are all in different piles }
E4={all 4 aces are in different piles }
The desired probability is P(E1E2E3E4), and by the multiplication rule,
P(E1E2E3E4)=P(E1)P(E2|E1)P(E3|E1E2)P(E4|E1E2E3)
Now,
P(E1)=1
since E1is the sample space S. To determine P(E2|E1), consider the pile that con-
tains the ace of spades. Because its remaining 12 cards are equally likely to be any12 of the remaining 51 cards, the probability that the ace of hearts is among them is
12/51, giving that
P(E
2|E1)=1−12
51=39
51
Also, given that the ace of spades and ace of hearts are in different piles, it follows
that the set of the remaining 24 cards of these two piles is equally likely to be any set
of 24 of the remaining 50 cards. As the probability that the ace of diamonds is one
of these 24 is 24/50, we see that
P(E3|E1E2)=1−24
50=26
50
Because the same logic as used in the preceding yields that
P(E4|E1E2E3)=1−36
49=13
49
the probability that each pile has exactly 1 ace is
P(E1E2E3E4)=39·26·13
51·50·49L.105
That is, there is approximately a 10.5 percent chance that each pile will contain an
ace. (Problem 13 gives another way of using the multiplication rule to solve this
problem.) .
Remarks Our deﬁnition of P(E|F)is consistent with the interpretation of
probability as being a long-run relative frequency. To see this, suppose that n
repetitions of the experiment are to be performed, where nis large. We claim that
if we consider only those experiments in which Foccurs, then P(E|F)will equal
the long-run proportion of them in which Ealso occurs. To verify this statement,
note that since P(F)is the long-run proportion of experiments in which Foccurs,
it follows that in the nrepetitions of the experiment, Fwill occur approximately
nP(F)times. Similarly, in approximately nP(EF)of these experiments, both Eand
Fwill occur. Hence, out of the approximately nP(F)experiments in which
Foccurs, the proportion of them in which Ealso occurs is approximately equal to
nP(EF)
nP(F)=P(EF)
P(F)

<<<PAGE 75>>>

62Chapter 3 Conditional Probability and Independence
Because this approximation becomes exact as nbecomes larger and larger, we have
the appropriate deﬁnition of P(E|F).
3.3 Bayes’s Formula
LetEandFbe events. We may express Eas
E=EF∪EFc
for, in order for an outcome to be in E, it must either be in both EandFor be in
Ebut not in F. (See Figure 3.1.) As EFandEFcare clearly mutually exclusive, we
have, by Axiom 3,
P(E)=P(EF)+P(EFc)
=P(E|F)P(F)+P(E|Fc)P(Fc)
=P(E|F)P(F)+P(E|Fc)[1−P(F)](3.1)
Equation (3.1) states that the probability of the event Eis a weighted average of the
conditional probability of Egiven that Fhas occurred and the conditional proba-
bility of Egiven that Fhas not occurred—each conditional probability being given
as much weight as the event on which it is conditioned has of occurring. This is an
extremely useful formula, because its use often enables us to determine the prob-
ability of an event by ﬁrst “conditioning” upon whether or not some second eventhas occurred. That is, there are many instances in which it is difﬁcult to compute the
probability of an event directly, but it is straightforward to compute it once we know
whether or not some second event has occurred. We illustrate this idea with someexamples.
EF
EF EFc
Figure 3.1 E=EF∪EFc.EF=Shaded Area; EFc=Striped Area.
Example
3a(Part 1)
An insurance company believes that people can be divided into two classes: those
who are accident prone and those who are not. The company’s statistics show that
an accident-prone person will have an accident at some time within a ﬁxed 1-yearperiod with probability .4, whereas this probability decreases to .2 for a person who
is not accident prone. If we assume that 30 percent of the population is accident
prone, what is the probability that a new policyholder will have an accident within ayear of purchasing a policy?
Solution We shall obtain the desired probability by ﬁrst conditioning upon whether
or not the policyholder is accident prone. Let A1denote the event that the policy-
holder will have an accident within a year of purchasing the policy, and let Adenote

<<<PAGE 76>>>

A First Course in Probability 63
the event that the policyholder is accident prone. Hence, the desired probability is
given by
P(A1)=P(A1|A)P(A)+P(A1|Ac)P(Ac)
=(.4)(.3)+(.2)(.7)=.26 .
Example
3a(Part 2)
Suppose that a new policyholder has an accident within a year of purchasing a policy.
What is the probability that he or she is accident prone?
Solution The desired probability is
P(A|A 1)=P(AA 1)
P(A1)
=P(A)P(A1|A)
P(A1)
=(.3)(.4)
.26=6
13.
Example
3bConsider the following game played with an ordinary deck of 52 playing cards: Thecards are shufﬂed and then turned over one at a time. At any time, the player can
guess that the next card to be turned over will be the ace of spades; if it is, then theplayer wins. In addition, the player is said to win if the ace of spades has not yet
appeared when only one card remains and no guess has yet been made. What is a
good strategy? What is a bad strategy?
Solution Every strategy has probability 1/52 of winning! To show this, we will use
induction to prove the stronger result that for an ncard deck, one of whose cards
is the ace of spades, the probability of winning is 1/ n, no matter what strategy is
employed. Since this is clearly true for n=1, assume it to be true for an n−1
card deck, and now consider an ncard deck. Fix any strategy, and let pdenote the
probability that the strategy guesses that the ﬁrst card is the ace of spades. Given
that it does, the player’s probability of winning is 1/ n. If, however, the strategy does
not guess that the ﬁrst card is the ace of spades, then the probability that the player
wins is the probability that the ﬁrst card is not the ace of spades, namely, (n−1)/n,
multiplied by the conditional probability of winning given that the ﬁrst card is notthe ace of spades. But this latter conditional probability is equal to the probability of
winning when using an n−1 card deck containing a single ace of spades; it is thus,
by the induction hypothesis, 1/(n −1). Hence, given that the strategy does not guess
the ﬁrst card, the probability of winning is
n−1
n1
n−1=1
n
Thus, letting Gbe the event that the ﬁrst card is guessed, we obtain
P{win}=P {win|G}P (G)+P{win|Gc}(1−P(G))=1
np+1
n(1−p)
=1
n.
Example
3cIn answering a question on a multiple-choice test, a student either knows the answeror guesses. Let pbe the probability that the student knows the answer and 1 −p
be the probability that the student guesses. Assume that a student who guesses at

<<<PAGE 77>>>

64Chapter 3 Conditional Probability and Independence
the answer will be correct with probability 1/ m, where mis the number of multiple-
choice alternatives. What is the conditional probability that a student knew the
answer to a question given that he or she answered it correctly?
Solution LetCandKdenote, respectively, the events that the student answers the
question correctly and the event that he or she actually knows the answer. Now,
P(K|C)=P(KC)
P(C)
=P(C|K)P(K)
P(C|K)P(K)+P(C|Kc)P(Kc)
=p
p+(1/m)(1 −p)
=mp
1+(m−1)p
For example, if m=5,p=1
2, then the probability that the student knew the answer
to a question he or she answered correctly is5
6. .
Example
3dA laboratory blood test is 95 percent effective in detecting a certain disease whenit is, in fact, present. However, the test also yields a “false positive” result for 1
percent of the healthy persons tested. (That is, if a healthy person is tested, then,with probability .01, the test result will imply that he or she has the disease.) If .5
percent of the population actually has the disease, what is the probability that a
person has the disease given that the test result is positive?
Solution LetDbe the event that the person tested has the disease and Ethe event
that the test result is positive. Then the desired probability is
P(D|E)=P(DE)
P(E)
=P(E|D)P(D)
P(E|D)P(D)+P(E|Dc)P(Dc)
=(.95)(.005)
(.95)(.005)+(.01)(.995)
=95
294L.323
Thus, only 32 percent of those persons whose test results are positive actually havethe disease. Many students are often surprised at this result (they expect the per-
centage to be much higher, since the blood test seems to be a good one), so it is
probably worthwhile to present a second argument that, although less rigorous thanthe preceding one, is probably more revealing. We now do so.
Since .5 percent of the population actually has the disease, it follows that, on
the average, 1 person out of every 200 tested will have it. The test will correctlyconﬁrm that this person has the disease with probability .95. Thus, on the aver-
age, out of every 200 persons tested, the test will correctly conﬁrm that .95 person
has the disease. On the other hand, out of the (on the average) 199 healthy peo-ple, the test will incorrectly state that (199)(.01) of these people have the disease.
Hence, for every .95 diseased person that the test correctly states is ill, there are (on
the average) (199)(.01) healthy persons who the test incorrectly states are ill. Thus,

<<<PAGE 78>>>

A First Course in Probability 65
the proportion of time that the test result is correct when it states that a person is
ill is
.95
.95+(199)(. 01)=95
294L.323 .
Equation (3.1) is also useful when one has to reassess one’s personal probabil-
ities in the light of additional information. For instance, consider the examples thatfollow.
Example
3eConsider a medical practitioner pondering the following dilemma: “If I’m at least 80percent certain that my patient has this disease, then I always recommend surgery,
whereas if I’m not quite as certain, then I recommend additional tests that are expen-
sive and sometimes painful. Now, initially I was only 60 percent certain that Joneshad the disease, so I ordered the series A test, which always gives a positive result
when the patient has the disease and almost never does when he is healthy. The test
result was positive, and I was all set to recommend surgery when Jones informed me,for the ﬁrst time, that he was diabetic. This information complicates matters because,
although it doesn’t change my original 60 percent estimate of his chances of having
the disease in question, it does affect the interpretation of the results of the A test.This is so because the A test, while never yielding a positive result when the patient
is healthy, does unfortunately yield a positive result 30 percent of the time in the case
ofdiabetic patients who are not suffering from the disease. Now what do I do? More
tests or immediate surgery?”
Solution In order to decide whether or not to recommend surgery, the doctor should
ﬁrst compute her updated probability that Jones has the disease given that the A testresult was positive. Let Ddenote the event that Jones has the disease and Ethe event
that the A test result is positive. The desired conditional probability is then
P(D|E)=P(DE)
P(E)
=P(D)P(E|D)
P(E|D)P(D)+P(E|Dc)P(Dc)
=(.6)1
1(.6)+(.3)(.4)
=.833
Note that we have computed the probability of a positive test result by condition-ing on whether or not Jones has the disease and then using the fact that becauseJones is a diabetic, his conditional probability of a positive result given that he
does not have the disease, P(E|D
c), equals .3. Hence, as the doctor should now
be more than 80 percent certain that Jones has the disease, she should recommend
surgery. .
Example
3fAt a certain stage of a criminal investigation, the inspector in charge is 60 percentconvinced of the guilt of a certain suspect. Suppose, however, that a new piece of
evidence which shows that the criminal has a certain characteristic (such as left-
handedness, baldness, or brown hair) is uncovered. If 20 percent of the population
possesses this characteristic, how certain of the guilt of the suspect should the inspec-tor now be if it turns out that the suspect has the characteristic?

<<<PAGE 79>>>

66Chapter 3 Conditional Probability and Independence
Solution Letting Gdenote the event that the suspect is guilty and Cthe event that
he possesses the characteristic of the criminal, we have
P(G|C)=P(GC)
P(C)
=P(C|G)P(G)
P(C|G)P(G)+P(C|Gc)P(Gc)
=1(.6)
1(.6)+(.2)(.4)
L.882
where we have supposed that the probability of the suspect having the characteristic
if he is, in fact, innocent is equal to .2, the proportion of the population possessing
the characteristic. .
Example
3gIn the world bridge championships held in Buenos Aires in May 1965, the famous
British bridge partnership of Terrence Reese and Boris Schapiro was accused of
cheating by using a system of ﬁnger signals that could indicate the number of hearts
held by the players. Reese and Schapiro denied the accusation, and eventually a
hearing was held by the British bridge league. The hearing was in the form of a legalproceeding with prosecution and defense teams, both having the power to call and
cross-examine witnesses. During the course of the proceeding, the prosecutor exam-
ined speciﬁc hands played by Reese and Schapiro and claimed that their playingthese hands was consistent with the hypothesis that they were guilty of having illicit
knowledge of the heart suit. At this point, the defense attorney pointed out that
their play of these hands was also perfectly consistent with their standard line ofplay. However, the prosecution then argued that as long as their play was consistent
with the hypothesis of guilt, it must be counted as evidence toward that hypothesis.
What do you think of the reasoning of the prosecution?
Solution The problem is basically one of determining how the introduction of new
evidence (in this example, the playing of the hands) affects the probability of a par-ticular hypothesis. If we let Hdenote a particular hypothesis (such as the hypothesis
that Reese and Schapiro are guilty) and Ethe new evidence, then
P(H|E)=P(HE)
P(E)
=P(E|H)P(H)
P(E|H)P(H)+P(E|Hc)[1−P(H)](3.2)
where P(H) is our evaluation of the likelihood of the hypothesis before the intro-
duction of the new evidence. The new evidence will be in support of the hypothesiswhenever it makes the hypothesis more likely—that is, whenever P(H|E)ÚP(H).
From Equation (3.2), this will be the case whenever
P(E|H)ÚP(E|H)P(H)+P(E|H
c)[1−P(H)]
or, equivalently, whenever
P(E|H)ÚP(E|Hc)

<<<PAGE 80>>>

A First Course in Probability 67
In other words, any new evidence can be considered to be in support of a partic-
ular hypothesis only if its occurrence is more likely when the hypothesis is true
than when it is false. In fact, the new probability of the hypothesis depends on
its initial probability and the ratio of these conditional probabilities, since, fromEquation (3.2),
P(H|E)=P(H)
P(H)+[1−P(H)]P(E|Hc)
P(E|H)
Hence, in the problem under consideration, the play of the cards can be con-
sidered to support the hypothesis of guilt only if such play would have been morelikely if the partnership were cheating than if it were not. As the prosecutor nevermade this claim, his assertion that the evidence is in support of the guilt hypothesis is
invalid. .
Example
3hTwins can be either identical or fraternal. Identical, also called monozygotic, twins
form when a single fertilized egg splits into two genetically identical parts. Con-sequently, identical twins always have the same set of genes. Fraternal, also called
dizygotic, twins develop when two eggs are fertilized and implant in the uterus. The
genetic connection of fraternal twins is no more or less the same as siblings born atseparate times. A Los Angeles County, California, scientist wishing to know the cur-
rent fraction of twin pairs born in the county that are identical twins has assigned a
county statistician to study this issue. The statistician initially requested each hospitalin the county to record all twin births, indicating whether or not the resulting twins
were identical. The hospitals, however, told her that to determine whether newborn
twins were identical was not a simple task, as it involved the permission of the twins’parents to perform complicated and expensive DNA studies that the hospitals could
not afford. After some deliberation, the statistician just asked the hospitals for data
listing all twin births along with an indication as to whether the twins were of thesame sex. When such data indicated that approximately 64 percent of twin births
were same-sexed, the statistician declared that approximately 28 percent of all twins
were identical. How did she come to this conclusion?
Solution The statistician reasoned that identical twins are always of the same sex,
whereas fraternal twins, having the same relationship to each other as any pair ofsiblings, will have probability 1/2 of being of the same sex. Letting Ibe the event
that a pair of twins is identical, and SSbe the event that a pair of twins is of the same
sex, she computed the probability P(SS) by conditioning on whether the twin pair
was identical. This gave
P(SS)=P(SS|I)P(I)+P(SS|I
c)P(Ic)
or
P(SS)=1*P(I)+1
2*[1−P(I)]=1
2+1
2P(I)
which, using that P(SS)L.64 yielded the result
P(I)L.28 .
The change in the probability of a hypothesis when new evidence is introduced
can be expressed compactly in terms of the change in the odds of that hypothesis,
where the concept of odds is deﬁned as follows.

<<<PAGE 81>>>

68Chapter 3 Conditional Probability and Independence
Deﬁnition
The odds of an event Aare deﬁned by
P(A)
P(Ac)=P(A)
1−P(A)
That is, the odds of an event Atell how much more likely it is that the event A
occurs than it is that it does not occur. For instance, if P(A)=2
3, then P(A)=
2P(Ac), so the odds are 2. If the odds are equal to α, then it is common to say
that the odds are “α to 1” in favor of the hypothesis.
Consider now a hypothesis Hthat is true with probability P(H), and suppose
that new evidence Eis introduced. Then, the conditional probabilities, given the
evidence E, that His true and that His not true are respectively given by
P(H|E)=P(E|H)P(H)
P(E)P(Hc|E)=P(E|Hc)P(Hc)
P(E)
Therefore, the new odds after the evidence Ehas been introduced are
P(H|E)
P(Hc|E)=P(H)
P(Hc)P(E|H)
P(E|Hc)(3.3)
That is, the new value of the odds of His the old value multiplied by the ratio of the
conditional probability of the new evidence given that His true to the conditional
probability given that His not true. Thus, Equation (3.3) veriﬁes the result of Exam-
ple 3f, since the odds, and thus the probability of H, increase whenever the new evi-
dence is more likely when His true than when it is false. Similarly, the odds decrease
whenever the new evidence is more likely when His false than when it is true.
Example
3iAn urn contains two type Acoins and one type Bcoin. When a type Acoin is ﬂipped,
it comes up heads with probability 1 /4, whereas when a type Bcoin is ﬂipped, it
comes up heads with probability 3/4. A coin is randomly chosen from the urn and
ﬂipped. Given that the ﬂip landed on heads, what is the probability that it was a type
Acoin?
Solution LetAbe the event that a type Acoin was ﬂipped, and let B=Acbe the
event that a type Bcoin was ﬂipped. We want P(A|heads ), where heads is the event
that the ﬂip landed on heads. From Equation (3.3), we see that
P(A|heads )
P(Ac|heads )=P(A)
P(B)P(heads |A)
P(heads |B)
=2/3
1/31/4
3/4
=2/3
Hence, the odds are 2/3 : 1, or, equivalently, the probability is 2 /5 that a type Acoin
was ﬂipped. .
Equation (3.1) may be generalized as follows: Suppose that F1,F2,...,Fnare
mutually exclusive events such that
n/uniondisplay
i=1Fi=S

<<<PAGE 82>>>

A First Course in Probability 69
In other words, exactly one of the events F1,F2,...,Fnmust occur. By writing
E=n/uniondisplay
i=1EFi
and using the fact that the events EFi,i=1,...,nare mutually exclusive, we obtain
P(E)=n/summationdisplay
i=1P(EF i)
=n/summationdisplay
i=1P(E|Fi)P(Fi) (3.4)
Thus, Equation (3.4), often referred to as the law of total probability, shows how,
for given events F1,F2,...,Fn, of which one and only one must occur, we can com-
pute P(E)by ﬁrst conditioning on which one of the Fioccurs. That is, Equation (3.4)
states that P(E)is equal to a weighted average of P(E|Fi), each term being weighted
by the probability of the event on which it is conditioned.
Example
3jIn Example 5j of Chapter 2, we considered the probability that, for a randomly
shufﬂed deck, the card following the ﬁrst ace is some speciﬁed card, and we gave
a combinatorial argument to show that this probability is1
52.Here is a probabilistic
argument based on conditioning: Let Ebe the event that the card following the ﬁrst
ace is some speciﬁed card, say, card x. To compute P(E), we ignore card xand con-
dition on the relative ordering of the other 51cards in the deck. Letting Obe the
ordering gives
P(E)=/summationdisplay
OP(E|O)P(O)
Now, given O, there are 52possible orderings of the cards, corresponding to hav-
ing card xbeing the ith card in the deck, i=1,...,52 .But because all 52! possible
orderings were initially equally likely, it follows that, conditional on O, each of the
52remaining possible orderings is equally likely. Because card xwill follow the
ﬁrst ace for only one of these orderings, we have P(E|O)=1/52, implying that
P(E)=1/52. .
Again, let F1,...,Fnbe a set of mutually exclusive and exhaustive events (mean-
ing that exactly one of these events must occur).
Suppose now that Ehas occurred and we are interested in determining which
one of the Fjalso occurred. Then, by Equation (3.4), we have the following
proposition.
Proposition
3.1P(Fj|E)=P(EF j)
P(E)
=P(E|Fj)P(Fj)
n/summationdisplay
i=1P(E|Fi)P(Fi)(3.5)

<<<PAGE 83>>>

70Chapter 3 Conditional Probability and Independence
Equation (3.5) is known as Bayes’s formula, after the English philosopher Thomas
Bayes. If we think of the events Fjas being possible “hypotheses” about some sub-
ject matter, then Bayes’s formula may be interpreted as showing us how opinions
about these hypotheses held before the experiment was carried out [that is, theP(F
j)] should be modiﬁed by the evidence produced by the experiment.
Example
3kA plane is missing, and it is presumed that it was equally likely to have gone downin any of 3 possible regions. Let 1 −β
i,i=1, 2, 3, denote the probability that
the plane will be found upon a search of the ith region when the plane is, in fact,
in that region. (The constants βiare called overlook probabilities, because they rep-
resent the probability of overlooking the plane; they are generally attributable to
the geographical and environmental conditions of the regions.) What is the condi-
tional probability that the plane is in the ith region given that a search of region 1 is
unsuccessful?
Solution LetRi,i=1, 2, 3, be the event that the plane is in region i, and let Ebe
the event that a search of region 1 is unsuccessful. From Bayes’s formula, we obtain
P(R1|E)=P(ER 1)
P(E)
=P(E|R1)P(R1)
3/summationdisplay
i=1P(E|Ri)P(Ri)
=(β1)1
3
(β1)1
3+(1)1
3+(1)1
3
=β1
β1+2
Forj=2, 3,
P(Rj|E)=P(E|Rj)P(Rj)
P(E)
=(1)1
3
(β1)1
3+1
3+1
3
=1
β1+2j=2, 3
Note that the updated (that is, the conditional) probability that the plane is in
region j, given the information that a search of region 1 did not ﬁnd it, is greater
than the initial probability that it was in region jwhen jZ1 and is less than the
initial probability when j=1. This statement is certainly intuitive, since not ﬁnding
the plane in region 1 would seem to decrease its chance of being in that region andincrease its chance of being elsewhere. Further, the conditional probability that theplane is in region 1 given an unsuccessful search of that region is an increasing func-
tion of the overlook probability β
1. This statement is also intuitive, since the larger
β1is, the more it is reasonable to attribute the unsuccessful search to “bad luck”
as opposed to the plane’s not being there. Similarly, P(Rj|E),jZ1, is a decreasing
function of β1. .

<<<PAGE 84>>>

A First Course in Probability 71
The next example has often been used by unscrupulous probability students to
win money from their less enlightened friends.
Example
3lSuppose that we have 3 cards that are identical in form, except that both sides of the
ﬁrst card are colored red, both sides of the second card are colored black, and one
side of the third card is colored red and the other side black. The 3 cards are mixed
up in a hat, and 1 card is randomly selected and put down on the ground. If the upperside of the chosen card is colored red, what is the probability that the other side is
colored black?
Solution LetRR, BB, and RBdenote, respectively, the events that the chosen card
is all red, all black, or the red–black card. Also, let Rbe the event that the upturned
side of the chosen card is red. Then, the desired probability is obtained by
P(RB|R) =P(RB∩R)
P(R)
=P(R|RB)P (RB)
P(R|RR)P (RR) +P(R|RB)P (RB) +P(R|BB)P (BB)
=/parenleftBig
1
2/parenrightBig/parenleftBig
1
3/parenrightBig
(1)/parenleftBig
1
3/parenrightBig
+/parenleftBig
1
2/parenrightBig/parenleftBig
1
3/parenrightBig
+0/parenleftBig
1
3/parenrightBig=1
3
Hence, the answer is1
3. Some students guess1
2as the answer by incorrectly reasoning
that given that a red side appears, there are two equally likely possibilities: that the
card is the all-red card or the red–black card. Their mistake, however, is in assumingthat these two possibilities are equally likely. For, if we think of each card as con-
sisting of two distinct sides, then we see that there are 6 equally likely outcomes of
the experiment—namely, R
1,R2,B1,B2,R3,B3—where the outcome is R1if the ﬁrst
side of the all-red card is turned face up, R2if the second side of the all-red card
is turned face up, R3if the red side of the red–black card is turned face up, and so
on. Since the other side of the upturned red side will be black only if the outcome isR
3, we see that the desired probability is the conditional probability of R3given that
either R1orR2orR3occurred, which obviously equals1
3. .
Example
3mA new couple, known to have two children, has just moved into town. Suppose that
the mother is encountered walking with one of her children. If this child is a girl,
what is the probability that both children are girls?
Solution Let us start by deﬁning the following events:
G1: the ﬁrst (that is, the oldest) child is a girl.
G2: the second child is a girl.
G: the child seen with the mother is a girl.
Also, let B1,B2, and Bdenote similar events, except that “girl” is replaced by “boy.”
Now, the desired probability is P(G1G2|G), which can be expressed as follows:
P(G1G2|G)=P(G1G2G)
P(G)
=P(G1G2)
P(G)

<<<PAGE 85>>>

72Chapter 3 Conditional Probability and Independence
Also,
P(G)=P(G|G 1G2)P(G1G2)+P(G|G 1B2)P(G1B2)
+P(G|B 1G2)P(B1G2)+P(G|B 1B2)P(B1B2)
=P(G1G2)+P(G|G 1B2)P(G1B2)+P(G|B 1G2)P(B1G2)
where the ﬁnal equation used the results P(G|G 1G2)=1 and P(G|B 1B2)=0. If we
now make the usual assumption that all 4 gender possibilities are equally likely, then
we see that
P(G1G2|G)=1
4
1
4+P(G|G 1B2)/4+P(G|B 1G2)/4
=1
1+P(G|G 1B2)+P(G|B 1G2)
Thus, the answer depends on whatever assumptions we want to make about the con-ditional probabilities that the child seen with the mother is a girl given the eventG
1B2and that the child seen with the mother is a girl given the event G2B1.F o r
instance, if we want to assume, on the one hand, that, independently of the gen-ders of the children, the child walking with the mother is the elder child with someprobability p, then it would follow that
P(G|G
1B2)=p=1−P(G|B 1G2)
implying under this scenario that
P(G1G2|G)=1
2
If, on the other hand, we were to assume that if the children are of different genders,
then the mother would choose to walk with the girl with probability q, independently
of the birth order of the children, then we would have
P(G|G 1B2)=P(G|B 1G2)=q
implying that
P(G1G2|G)=1
1+2q
For instance, if we took q=1, meaning that the mother would always choose to walk
with a daughter, then the conditional probability that she has two daughters would
be1
3, which is in accord with Example 2b because seeing the mother with a daughter
is now equivalent to the event that she has at least one daughter.
Hence, as stated, the problem is incapable of solution. Indeed, even when the
usual assumption about equally likely gender probabilities is made, we still need to
make additional assumptions before a solution can be given. This is because the sam-
ple space of the experiment consists of vectors of the form s1,s2,i, where s1is the
gender of the older child, s2is the gender of the younger child, and iidentiﬁes the
birth order of the child seen with the mother. As a result, to specify the probabilities
of the events of the sample space, it is not enough to make assumptions only aboutthe genders of the children; it is also necessary to assume something about the con-
ditional probabilities as to which child is with the mother given the genders of the
children. .

<<<PAGE 86>>>

A First Course in Probability 73
Example
3nA bin contains 3 types of disposable ﬂashlights. The probability that a type 1 ﬂash-
light will give more than 100 hours of use is .7, with the corresponding probabilities
for type 2 and type 3 ﬂashlights being .4 and .3, respectively. Suppose that 20 per-cent of the ﬂashlights in the bin are type 1, 30 percent are type 2, and 50 percent are
type 3.
(a) What is the probability that a randomly chosen ﬂashlight will give more than
100 hours of use?
(b) Given that a ﬂashlight lasted more than 100 hours, what is the conditional prob-
ability that it was a type jﬂashlight, j=1, 2, 3?
Solution (a) Let Adenote the event that the ﬂashlight chosen will give more than
100 hours of use, and let Fjbe the event that a type jﬂashlight is chosen, j=1, 2, 3.
To compute P(A), we condition on the type of the ﬂashlight, to obtain
P(A)=P(A|F 1)P(F1)+P(A|F 2)P(F2)+P(A|F 3)P(F3)
=(.7)(.2)+(.4)(.3)+(.3)(.5)=.41
There is a 41 percent chance that the ﬂashlight will last for more than 100 hours.
(b) The probability is obtained by using Bayes’s formula:
P(Fj|A)=P(AF j)
P(A)
=P(A|F j)P(Fj)
.41
Thus,
P(F1|A)=(.7)(.2)/.41 =14/41
P(F2|A)=(.4)(.3)/.41 =12/41
P(F3|A)=(.3)(.5)/.41 =15/41
For instance, whereas the initial probability that a type 1 ﬂashlight is chosen is only
.2, the information that the ﬂashlight has lasted more than 100 hours raises the prob-
ability of this event to 14 /41L.341. .
Example
3oA crime has been committed by a solitary individual, who left some DNA at the
scene of the crime. Forensic scientists who studied the recovered DNA noted thatonly ﬁve strands could be identiﬁed and that each innocent person, independently,
would have a probability of 10
−5of having his or her DNA match on all ﬁve strands.
The district attorney supposes that the perpetrator of the crime could be any of the
1 million residents of the town. Ten thousand of these residents have been released
from prison within the past 10 years; consequently, a sample of their DNA is on ﬁle.
Before any checking of the DNA ﬁle, the district attorney thinks that each of the
10,000 ex-criminals has probability αof being guilty of the new crime, whereas each
of the remaining 990,000 residents has probability β, where α=cβ. (That is, the
district attorney supposes that each recently released convict is ct i m e sa sl i k e l yt o
be the crime’s perpetrator as is each town member who is not a recently released
convict.) When the DNA that is analyzed is compared against the database of the10,000 ex-convicts, it turns out that A. J. Jones is the only one whose DNA matches
the proﬁle. Assuming that the district attorney’s estimate of the relationship between
αandβis accurate, what is the probability that A. J. is guilty?

<<<PAGE 87>>>

74Chapter 3 Conditional Probability and Independence
Solution To begin, note that because probabilities must sum to 1, we have
1=10,000α +990,000β =(10,000c +990,000)β
Thus,
β=1
10,000c +990,000,α=c
10,000c +990,000
Now, let Gbe the event that A. J. is guilty, and let Mdenote the event that A. J. is
the only one of the 10,000 on ﬁle to have a match. Then,
P(G|M)=P(GM)
P(M)
=P(G)P(M|G)
P(M|G)P(G)+P(M|Gc)P(Gc)
On the one hand, if A. J. is guilty, then he will be the only one to have a DNA match
if none of the others on ﬁle have a match. Therefore,
P(M|G)=(1−10−5)9999
On the other hand, if A. J. is innocent, then in order for him to be the only match, hisDNA must match (which will occur with probability 10
−5), all others in the database
must be innocent, and none of these others can have a match. Now, given that A. J.
is innocent, the conditional probability that all the others in the database are also
innocent is
P(all others innocent |AJinnocent )=P(all in database innocent )
P(AJinnocent )
=1−10,000α
1−α
Also, the conditional probability, given their innocence, that none of the others in
the database will have a match is (1−10−5)9999. Therefore,
P(M|Gc)=10−5/parenleftbigg1−10,000α
1−α/parenrightbigg
(1−10−5)9999
Because P(G)=α, the preceding formula gives
P(G|M)=α
α+10−5(1−10,000α)=1
.9+10−5
α
Thus, if the district attorney’s initial thoughts were that an arbitrary ex-convict was
100 times more likely to have committed the crime than was a nonconvict (that is,
c=100), then α=1
19,900and
P(G|M)=1
1.099L0.9099
If the district attorney initially thought that the appropriate ratio was c=10, then
α=1
109, 000and
P(G|M)=1
1.99L0.5025

<<<PAGE 88>>>

A First Course in Probability 75
If the district attorney initially thought that the criminal was equally likely to be any
of the members of the town (c=1), then α=10−6and
P(G|M)=1
10.9L0.0917
Thus, the probability ranges from approximately 9 percent when the district attor-ney’s initial assumption is that all the members of the population have the same
chance of being the perpetrator to approximately 91 percent when she assumes
that each ex-convict is 100 times more likely to be the criminal than is a speciﬁedtownsperson who is not an ex-convict. .
3.4 Independent Events
The previous examples in this chapter show that P(E|F), the conditional probability
ofEgiven F, is not generally equal to P(E), the unconditional probability of E.
In other words, knowing that Fhas occurred generally changes the chances of E’s
occurrence. In the special cases where P(E|F)does in fact equal P(E), we say that E
is independent of F. That is, Eis independent of Fif knowledge that Fhas occurred
does not change the probability that Eoccurs.
Since P(E|F)=P(EF)/P(F), it follows that Eis independent of Fif
P(EF)=P(E)P(F) (4.1)
The fact that Equation (4.1) is symmetric in EandFshows that whenever Eis inde-
pendent of F,Fis also independent of E. We thus have the following deﬁnition.
Deﬁnition
Two events EandFa r es a i dt ob e independent if Equation (4.1) holds.
Two events EandFthat are not independent are said to be dependent.
Example
4aA card is selected at random from an ordinary deck of 52 playing cards. If Eis the
event that the selected card is an ace and Fis the event that it is a spade, then E
andFare independent. This follows because P(EF)=1
52, whereas P(E)=4
52and
P(F)=13
52. .
Example
4bTwo coins are ﬂipped, and all 4 outcomes are assumed to be equally likely. If Eis
the event that the ﬁrst coin lands on heads and Fthe event that the second lands
on tails, then EandFare independent, since P(EF)=P({(H,T)})=1
4, whereas
P(E)=P({(H,H),(H,T)})=1
2andP(F)=P({(H,T),(T,T)})=1
2. .
Example
4cSuppose that we toss 2 fair dice. Let E1denote the event that the sum of the dice is
6 and Fdenote the event that the ﬁrst die equals 4. Then
P(E1F)=P({(4, 2)}) =1
36
whereas
P(E1)P(F)=/parenleftbigg5
36/parenrightbigg/parenleftbigg1
6/parenrightbigg
=5
216

<<<PAGE 89>>>

76Chapter 3 Conditional Probability and Independence
Hence, E1andFare not independent. Intuitively, the reason for this is clear because
if we are interested in the possibility of throwing a 6 (with 2 dice), we shall be quite
happy if the ﬁrst die lands on 4 (or, indeed, on any of the numbers 1, 2, 3, 4, and 5),
for then we shall still have a possibility of getting a total of 6. If, however, the ﬁrstdie landed on 6, we would be unhappy because we would no longer have a chance
of getting a total of 6. In other words, our chance of getting a total of 6 depends on
the outcome of the ﬁrst die; thus, E
1andFcannot be independent.
Now, suppose that we let E2be the event that the sum of the dice equals 7. Is E2
independent of F? The answer is yes, since
P(E2F)=P({(4, 3)}) =1
36
whereas
P(E2)P(F)=/parenleftbigg1
6/parenrightbigg/parenleftbigg1
6/parenrightbigg
=1
36
We leave it for the reader to present the intuitive argument why the event that
the sum of the dice equals 7 is independent of the outcome on the ﬁrst die. .
Example
4dIf we let Edenote the event that the next president is a Republican and Fthe event
that there will be a major earthquake within the next year, then most people wouldprobably be willing to assume that EandFare independent. However, there would
probably be some controversy over whether it is reasonable to assume that Eis
independent of G, where Gis the event that there will be a recession within two
years after the election. .
We now show that if Eis independent of F, then Eis also independent of F
c.
Proposition
4.1IfEandFare independent, then so are EandFc.
Proof Assume that EandFare independent. Since E=EF∪EFcandEFandEFc
are obviously mutually exclusive, we have
P(E)=P(EF)+P(EFc)
=P(E)P(F)+P(EFc)
or, equivalently,
P(EFc)=P(E)[1−P(F)]
=P(E)P(Fc)
and the result is proved.
Thus, if Eis independent of F, then the probability of E’s occurrence is unchanged
by information as to whether or not Fhas occurred.
Suppose now that Eis independent of Fand is also independent of G.I s E
then necessarily independent of FG? The answer, somewhat surprisingly, is no, as
the following example demonstrates.
Example
4eTwo fair dice are thrown. Let Edenote the event that the sum of the dice is 7. Let F
denote the event that the ﬁrst die equals 4 and Gdenote the event that the second
die equals 3. From Example 4c, we know that Eis independent of F, and the same
reasoning as applied there shows that Eis also independent of G; but clearly, Eis
not independent of FG[since P(E|FG)=1]. .

<<<PAGE 90>>>

A First Course in Probability 77
It would appear to follow from Example 4e that an appropriate deﬁnition of the
independence of three events E,F, and Gwould have to go further than merely
assuming that all of the/parenleftBigg
3
2/parenrightBigg
pairs of events are independent. We are thus led to
the following deﬁnition.
Deﬁnition
Three events E,F, and Gare said to be independent if
P(EFG) =P(E)P(F)P(G)
P(EF)=P(E)P(F)
P(EG) =P(E)P(G)
P(FG)=P(F)P(G)
Note that if E,F, and Gare independent, then Ewill be independent of any
event formed from FandG. For instance, Eis independent of F∪G, since
P[E(F∪G)]=P(EF∪EG)
=P(EF)+P(EG) −P(EFG)
=P(E)P(F)+P(E)P(G)−P(E)P(FG)
=P(E)[P(F)+P(G)−P(FG)]
=P(E)P(F∪G)
Of course, we may also extend the deﬁnition of independence to more than
three events. The events E1,E2,...,Enare said to be independent if for every subset
E1/prime,E2/prime,...,Er/prime,r…nof these events,
P(E1/primeE2/prime···Er/prime)=P(E1/prime)P(E2/prime)···P(Er/prime)
Finally, we deﬁne an inﬁnite set of events to be independent if every ﬁnite subset ofthose events is independent.
Sometimes, a probability experiment under consideration consists of performing
a sequence of subexperiments. For instance, if the experiment consists of continuallytossing a coin, we may think of each toss as being a subexperiment. In many cases,it is reasonable to assume that the outcomes of any group of the subexperiments
have no effect on the probabilities of the outcomes of the other subexperiments. If
such is the case, we say that the subexperiments are independent. More formally,we say that the subexperiments are independent if E
1,E2,...,En,...is necessarily
an independent sequence of events whenever Eiis an event whose occurrence is
completely determined by the outcome of the ith subexperiment.
If each subexperiment has the same set of possible outcomes, then the subex-
periments are often called trials.
Example
4fAn inﬁnite sequence of independent trials is to be performed. Each trial results in asuccess with probability pand a failure with probability 1 −p. What is the proba-
bility that
(a) at least 1 success occurs in the ﬁrst ntrials;
(b) exactly ksuccesses occur in the ﬁrst ntrials;
(c) all trials result in successes?

<<<PAGE 91>>>

78Chapter 3 Conditional Probability and Independence
Solution In order to determine the probability of at least 1 success in the ﬁrst n
trials, it is easiest to compute ﬁrst the probability of the complementary event: that
of no successes in the ﬁrst ntrials. If we let Eidenote the event of a failure on the ith
trial, then the probability of no successes is, by independence,
P(E1E2···En)=P(E1)P(E2)···P(En)=(1−p)n
Hence, the answer to part (a) is 1 −(1−p)n.
To compute the answer to part (b), consider any particular sequence of the ﬁrst
noutcomes containing ksuccesses and n−kfailures. Each one of these sequences
will, by the assumed independence of trials, occur with probability pk(1−p)n−k.
Since there are/parenleftBigg
n
k/parenrightBigg
such sequences [there are n!/k!(n −k)! permutations of k
successes and n−kfailures], the desired probability in part (b) is
P{exactly ksuccesses }=/parenleftBigg
nk/parenrightBigg
p
k(1−p)n−k
To answer part (c), we note that, by part (a), the probability of the ﬁrst ntrials
all resulting in success is given by
P(Ec
1Ec
2···Ec
n)=pn
Thus, using the continuity property of probabilities (Section 2.6), we see that the
desired probability is given by
P⎛
⎝q/intersectiondisplay
i=1Ec
i⎞⎠=P⎛⎝lim
n→qn/intersectiondisplay
i=1Ec
i⎞⎠
=lim
n→qP⎛
⎝n/intersectiondisplay
i=1Ec
i⎞⎠
=lim
npn=/braceleftBigg
0i fp<1
1i fp=1.
Example
4gA system composed of nseparate components is said to be a parallel system if it
functions when at least one of the components functions. (See Figure 3.2.) For such
a system, if component i, which is independent of the other components, functions
with probability pi,i=1,...,n, what is the probability that the system functions?
Solution LetAidenote the event that component ifunctions. Then,
P{system functions }=1 −P{system does not function }
=1−P{all components do not function }
=1−P⎛
⎝/intersectiondisplay
iAc
i⎞
⎠
=1−n/productdisplay
i=1(1−pi)by independence .

<<<PAGE 92>>>

A First Course in Probability 79
A B1
2
3
n
Figure 3.2 Parallel System: Functions if Current Flows from AtoB.
Example
4hIndependent trials consisting of rolling a pair of fair dice are performed. What is the
probability that an outcome of 5 appears before an outcome of 7 when the outcome
of a roll is the sum of the dice?
Solution If we let Endenote the event that no 5 or 7 appears on the ﬁrst n−1 trials
and a 5 appears on the nth trial, then the desired probability is
P⎛
⎝q/uniondisplay
n=1En⎞⎠=q/summationdisplay
n=1P(En)
Now, since P{5 on any trial}=4
36andP{7 on any trial}=6
36, we obtain, by the
independence of trials,
P(En)=/parenleftbigg
1−10
36/parenrightbiggn−14
36
Thus,
P⎛⎝q/uniondisplay
n=1En⎞⎠=1
9q/summationdisplay
n=1/parenleftbigg13
18/parenrightbiggn−1
=1
91
1−13
18
=2
5
This result could also have been obtained by the use of conditional probabilities.
If we let Ebe the event that a 5 occurs before a 7, then we can obtain the desired
probability, P(E), by conditioning on the outcome of the ﬁrst trial, as follows: Let
Fbe the event that the ﬁrst trial results in a 5, let Gbe the event that it results in
a 7, and let Hbe the event that the ﬁrst trial results in neither a 5 nor a 7. Then,
conditioning on which one of these events occurs gives
P(E)=P(E|F)P(F)+P(E|G)P(G)+P(E|H)P(H)
However,
P(E|F)=1
P(E|G)=0
P(E|H)=P(E)
The ﬁrst two equalities are obvious. The third follows because if the ﬁrst outcome
results in neither a 5 nor a 7, then at that point the situation is exactly as it was when

<<<PAGE 93>>>

80Chapter 3 Conditional Probability and Independence
the problem ﬁrst started—namely, the experimenter will continually roll a pair of fair
dice until either a 5 or 7 appears. Furthermore, the trials are independent; therefore,
the outcome of the ﬁrst trial will have no effect on subsequent rolls of the dice. Since
P(F)=4
36,P(G)=6
36, and P(H)=26
36, it follows that
P(E)=1
9+P(E)13
18
or
P(E)=2
5
The reader should note that the answer is quite intuitive. That is, because a 5
occurs on any roll with probability4
36and a 7 with probability6
36, it seems intuitive
that the odds that a 5 appears before a 7 should be 6 to 4 against. The probability
should then be4
10, as indeed it is.
The same argument shows that if EandFare mutually exclusive events of an
experiment, then, when independent trials of the experiment are performed, the
event Ewill occur before the event Fwith probability
P(E)
P(E)+P(F).
Example
4iSuppose there are ntypes of coupons and that each new coupon collected is, inde-
pendent of previous selections, a type icoupon with probability pi,/summationtextn
i=1pi=1.
Suppose kcoupons are to be collected. If Aiis the event that there is at least one
type icoupon among those collected, then, for iZj, ﬁnd
(a)P(Ai)
(b)P(Ai∪Aj)
(c)P(Ai|Aj)
Solution
P(Ai)=1−P(Ac
i)
=1−P{no coupon is type i}
=1−(1−pi)k
where the preceding used that each coupon is, independently, not of type iwith prob-
ability 1 −pi.Similarly,
P(Ai∪Aj)=1−P/parenleftBig/parenleftbig
Ai∪Aj/parenrightbigc/parenrightBig
=1−P{no coupon is either type ior type j}
=1−(1−pi−pj)k
where the preceding used that each coupon is, independently, neither of type inor
type jwith probability 1 −pi−pj.
To determine P(Ai|Aj), we will use the identity
P(Ai∪Aj)=P(Ai)+P(Aj)−P(AiAj)
which, in conjunction with parts (a) and (b), yields
P(AiAj)=1−(1−pi)k+1−(1−pj)k−[1−(1−pi−pj)k]
=1−(1−pi)k−(1−pj)k+(1−pi−pj)k

<<<PAGE 94>>>

A First Course in Probability 81
Consequently,
P(Ai|Aj)=P(AiAj)
P(Aj)=1−(1−pi)k−(1−pj)k+(1−pi−pj)k
1−(1−pj)k.
The next example presents a problem that occupies an honored place in the his-
tory of probability theory. This is the famous problem of the points . In general terms,
the problem is this: Two players put up stakes and play some game, with the stakes
to go to the winner of the game. An interruption requires them to stop before either
has won and when each has some sort of a “partial score.” How should the stakes be
divided?
This problem was posed to the French mathematician Blaise Pascal in 1654 by
the Chevalier de M ´er´e, who was a professional gambler at that time. In attacking
the problem, Pascal introduced the important idea that the proportion of the prizedeserved by the competitors should depend on their respective probabilities of win-
ning if the game were to be continued at that point. Pascal worked out some special
cases and, more importantly, initiated a correspondence with the famous French-man Pierre de Fermat, who had a reputation as a great mathematician. The resulting
exchange of letters not only led to a complete solution to the problem of the points,
but also laid the framework for the solution to many other problems connected with
games of chance. This celebrated correspondence, considered by some as the birth
date of probability theory, was also important in stimulating interest in probabilityamong the mathematicians in Europe, for Pascal and Fermat were both recognized
as being among the foremost mathematicians of the time. For instance, within a short
time of their correspondence, the young Dutch mathematician Christiaan Huygenscame to Paris to discuss these problems and solutions, and interest and activity in
this new ﬁeld grew rapidly.
Example
4jThe problem of the points
Independent trials resulting in a success with probability pand a failure with proba-
bility 1 −pare performed. What is the probability that nsuccesses occur before m
failures? If we think of AandBas playing a game such that Agains 1 point when a
success occurs and Bgains 1 point when a failure occurs, then the desired probability
is the probability that Awould win if the game were to be continued in a position
where Aneeded nandBneeded mmore points to win.
Solution We shall present two solutions. The ﬁrst comes from Pascal and the second
from Fermat.
Let us denote by Pn,mthe probability that nsuccesses occur before mfailures.
By conditioning on the outcome of the ﬁrst trial, we obtain
Pn,m=pPn−1,m+(1−p)P n,m−1 nÚ1,mÚ1
(Why? Reason it out.) Using the obvious boundary conditions Pn,0=0,P0,m=1,
we can solve these equations for Pn,m. Rather than go through the tedious details,
let us instead consider Fermat’s solution.
Fermat argued that in order for nsuccesses to occur before mfailures, it is nec-
essary and sufﬁcient that there be at least nsuccesses in the ﬁrst m+n−1 trials.
(Even if the game were to end before a total of m+n−1 trials were completed, we
could still imagine that the necessary additional trials were performed.) This is true,
for if there are at least nsuccesses in the ﬁrst m+n−1 trials, there could be at
most m−1 failures in those m+n−1 trials; thus, nsuccesses would occur before
mfailures. If, however, there were fewer than nsuccesses in the ﬁrst m+n−1

<<<PAGE 95>>>

82Chapter 3 Conditional Probability and Independence
trials, there would have to be at least mfailures in that same number of trials; thus,
nsuccesses would not occur before mfailures.
Hence, since, as shown in Example 4f, the probability of exactly ksuccesses in
m+n−1 trials is/parenleftBigg
m+n−1
k/parenrightBigg
pk(1−p)m+n−1−k, it follows that the desired
probability of nsuccesses before mfailures is
Pn,m=m+n−1/summationdisplay
k=n/parenleftBigg
m+n−1
k/parenrightBigg
pk(1−p)m+n−1−k.
The following example gives another instance where determining the probabil-
ity that a player wins a match is made easier by assuming that the play continues
even after the match winner has been determined.
Example
4kService protocol in a serve and rally game
Consider a serve and rally match (such as volleyball, badminton, or squash) between
two players, AandB. The match consists of a sequence of rallies, with each rally
beginning with a serve by one of the players and continuing until one of the players
has won the rally. The winner of the rally receives a point, and the match ends when
one of the players has won a total of npoints, with that player being declared the
winner of the match. Suppose whenever a rally begins with Aas the server, that A
wins that rally with probability pAand that Bwins it with probability qA=1−pA,
and that a rally that begins with Bas the server is won by Awith probability pBand
byBwith probability qB=1−pB.Player Ais to be the initial server. There are two
possible server protocols that are under consideration: “winner serves,” which meansthat the winner of a rally is the server for the next rally, or “alternating serve,” whichmeans that the server alternates from rally to rally, so that no two consecutive rallies
have the same server. Thus, for instance, if n=3, then the successive servers under
the “winner serves” protocol would be A,B,A,AifAwins the ﬁrst point, then Bthe
next, then Awins the next two. On the other hand, the sequence of servers under the
“alternating serve” protocol will always be A,B,A,B,A,...until the match winner is
decided. If you were player A, which protocol would you prefer?
Solution Surprisingly, it turns out that it makes no difference, in that the probability
thatAis the match winner is the same under either protocol. To show that this is
the case, it is advantageous to suppose that the players continue to play until a total
of 2n −1 rallies have been completed. The ﬁrst player to win nrallies would then
be the one who has won at least nof the 2n −1 rallies. To begin, note that if the
alternating serve protocol is being used, then player Awill serve exactly ntimes and
player Bwill serve exactly n−1 times in the 2 n−1 rallies.
Now consider the winner serve protocol, again assuming that the players con-
tinue to play until 2n −1 rallies have been completed. Because it makes no differ-
ence who serves the “extra rallies” after the match winner has been decided, suppose
that at the point at which the match has been decided (because one of the players
has won npoints), the remainder (if there are any) of the 2 n−1 rallies are all served
by the player who lost the match. Note that this modiﬁed service protocol does not
change the fact that the winner of the match will still be the player who wins at least
nof the 2n −1 rallies. We claim that under this modiﬁed service protocol, Awill
always serve ntimes and Bwill always serve n−1 times. Two cases show this.
Case 1: A wins the match.
Because Aserves ﬁrst, it follows that A’s second serve will immediately follow A’s
ﬁrst point; A’s third serve will immediately follow A’s second point; and, in particular,

<<<PAGE 96>>>

A First Course in Probability 83
A’snth serve will immediately follow A’s(n−1)point. But this will be the last serve
ofAbefore the match result is decided. This is so because either Awill win the point
on that serve and so have npoints, or Awill lose the point and so the serve will
switch to, and remain with, Buntil Awins point number n. Thus, provided that A
wins the match, it follows that Awould have served a total of ntimes at the moment
the match is decided. Because, by the modiﬁed service protocol, Awill never again
serve, it follows in this case that Aserves exactly ntimes.
Case 2: B wins the match.
Because Aserves ﬁrst, B’s ﬁrst serve will come immediately after B’s ﬁrst point; B’s
second serve will come immediately after B’s second point; and, in particular, B’s
(n−1)serve will come immediately after B’s(n−1)point. But that will be the last
serve of Bbefore the match is decided because either Bwill win the point on that
serve and so have npoints, or Bwill lose the point and so the serve will switch to, and
remain with, Auntil Bwins point number n. Thus, provided that Bwins the match,
we see that Bwould have served a total of n−1 times at the moment the match
is decided. Because, by the modiﬁed service protocol, Bwill never again serve, it
follows in this case that Bserves exactly n−1 times, and, as there are a total of
2n−1 rallies, that Aserves exactly ntimes.
Thus, we see that under either protocol, Awill always serve ntimes and Bwill
serve n−1 times and the winner of the match will be the one who wins at least
npoints. But since Awins each rally that he serves with probability pAand wins
each rally that Bserves with probability pBit follows that the probability that Ais
the match winner is, under either protocol, equal to the probability that there are at
least nsuccesses in 2n −1 independent trials, when nof these trials result in a success
with probability pAand the other n−1 trials result in a success with probability pB.
Consequently, the win probabilities for both protocols are the same. .
Our next two examples deal with gambling problems, with the ﬁrst having a
surprisingly elegant analysis.∗
Example
4lSuppose that initially there are rplayers, with player ihaving niunits, ni>0,i=
1,...,r.At each stage, two of the players are chosen to play a game, with the winner
of the game receiving 1 unit from the loser. Any player whose fortune drops to 0 is
eliminated, and this continues until a single player has all nK/summationtextr
i=1niunits, with
that player designated as the victor. Assuming that the results of successive games
are independent and that each game is equally likely to be won by either of its two
players, ﬁnd Pi, the probability that player iis the victor.
Solution To begin, suppose that there are nplayers, with each player initially having
1 unit. Consider player i. Each stage she plays will be equally likely to result in her
either winning or losing 1 unit, with the results from each stage being independent.
In addition, she will continue to play stages until her fortune becomes either 0 or
n. Because this is the same for all nplayers, it follows that each player has the same
chance of being the victor, implying that each player has probability 1 /nof being the
victor. Now, suppose these nplayers are divided into rteams, with team icontaining
niplayers, i=1,...,r.Then, the probability that the victor is a member of team iis
ni/n.But because
(a) team iinitially has a total fortune of niunits, i=1,...,r, and
(b) each game played by members of different teams is equally likely to be won
by either player and results in the fortune of members of the winning team
increasing by 1 and the fortune of the members of the losing team decreasing
by1,
∗The remainder of this section should be considered optional.

<<<PAGE 97>>>

84Chapter 3 Conditional Probability and Independence
it is easy to see that the probability that the victor is from team iis exactly the prob-
ability we desire. Thus, Pi=ni/n.Interestingly, our argument shows that this result
does notdepend on how the players in each stage are chosen. .
In the gambler’s ruin problem , there are only 2gamblers, but they are not assumed
to be of equal skill.
Example
4mThe gambler’s ruin problem
Two gamblers, AandB, bet on the outcomes of successive ﬂips of a coin. On each
ﬂip, if the coin comes up heads, Acollects 1 unit from B, whereas if it comes up tails,
Apays 1 unit to B. They continue to do this until one of them runs out of money.
If it is assumed that the successive ﬂips of the coin are independent and each ﬂip
results in a head with probability p, what is the probability that Aends up with all
the money if he starts with iunits and Bstarts with N−iunits?
Solution LetEdenote the event that Aends up with all the money when he starts
with iandBstarts with N−i, and to make clear the dependence on the initial
fortune of A,l e tP i=P(E). We shall obtain an expression for P(E) by conditioning
on the outcome of the ﬁrst ﬂip as follows: Let Hdenote the event that the ﬁrst ﬂip
lands on heads; then
Pi=P(E)=P(E|H)P(H)+P(E|Hc)P(Hc)
=pP(E|H)+(1−p)P(E|Hc)
Now, given that the ﬁrst ﬂip lands on heads, the situation after the ﬁrst bet is that
Ahasi+1 units and BhasN−(i+1). Since the successive ﬂips are assumed to be
independent with a common probability pof heads, it follows that from that point
on,A’s probability of winning all the money is exactly the same as if the game were
just starting with Ahaving an initial fortune of i+1 and Bhaving an initial fortune
ofN−(i+1). Therefore,
P(E|H)=Pi+1
and similarly,
P(E|Hc)=Pi−1
Hence, letting q=1−p, we obtain
Pi=pPi+1+qPi−1 i=1, 2,...,N−1 (4.2)
By making use of the obvious boundary conditions P0=0 and PN=1, we shall
now solve Equation (4.2). Since p+q=1, these equations are equivalent to
pPi+qPi=pPi+1+qPi−1
or
Pi+1−Pi=q
p(Pi−Pi−1) i=1, 2,...,N−1 (4.3)

<<<PAGE 98>>>

A First Course in Probability 85
Hence, since P0=0, we obtain, from Equation (4.3),
P2−P1=q
p(P1−P0)=q
pP1
P3−P2=q
p(P2−P1)=/parenleftbiggq
p/parenrightbigg2
P1
.
.
. (4.4)
Pi−Pi−1=q
p(Pi−1−Pi−2)=/parenleftbiggq
p/parenrightbiggi−1
P1
.
..
P
N−PN−1=q
p(PN−1−PN−2)=/parenleftbiggq
p/parenrightbiggN−1
P1
Adding the ﬁrst i−1 equations of (4.4) yields
Pi−P1=P1/bracketleftBigg/parenleftbiggq
p/parenrightbigg
+/parenleftbiggq
p/parenrightbigg2
+ ··· +/parenleftbiggq
p/parenrightbiggi−1/bracketrightBigg
or
Pi=⎧
⎪⎪⎪⎨
⎪⎪⎪⎩1−(q/p)
i
1−(q/p)P1ifq
pZ1
iP1 ifq
p=1
Using the fact that PN=1, we obtain
P1=⎧
⎪⎪⎪⎨
⎪⎪⎪⎩1−(q/p)
1−(q/p)NifpZ1
2
1
Nifp=1
2
Hence,
Pi=⎧
⎪⎪⎪⎨
⎪⎪⎪⎩1−(q/p)
i
1−(q/p)NifpZ1
2
i
Nifp=1
2(4.5)
LetQidenote the probability that Bwinds up with all the money when Astarts
with iandBstarts with N−i. Then, by symmetry to the situation described, and on
replacing pbyqandibyN−i, it follows that
Qi=⎧
⎪⎪⎪⎨
⎪⎪⎪⎩1−(p/q)
N−i
1−(p/q)NifqZ1
2
N−i
Nifq=1
2

<<<PAGE 99>>>

86Chapter 3 Conditional Probability and Independence
Moreover, since q=1
2is equivalent to p=1
2, we have, when qZ1
2,
Pi+Qi=1−(q/p)i
1−(q/p)N+1−(p/q)N−i
1−(p/q)N
=pN−pN(q/p)i
pN−qN+qN−qN(p/q)N−i
qN−pN
=pN−pN−iqi−qN+qipN−i
pN−qN
=1
This result also holds when p=q=1
2,s o
Pi+Qi=1
In words, this equation states that with probability 1, either AorBwill wind
up with all of the money; in other words, the probability that the game continues
indeﬁnitely with A’s fortune always being between 1 and N−1 is zero. (The reader
must be careful because, a priori, there are three possible outcomes of this gamblinggame, not two: Either Awins, or Bwins, or the game goes on forever with nobody
winning. We have just shown that this last event has probability 0.)
As a numerical illustration of the preceding result, if Awere to start with 5 units
andBwith 10, then the probability of A’s winning would be
1
3ifpwere1
2, whereas
it would jump to
1−/parenleftBig
2
3/parenrightBig5
1−/parenleftBig
2
3/parenrightBig15L.87
ifpwere .6.
A special case of the gambler’s ruin problem, which is also known as the prob-
lem of duration of play , was proposed to Huygens by Fermat in 1657. The version
Huygens proposed, which he himself solved, was that AandBhave 12 coins each.
They play for these coins in a game with 3 dice as follows: Whenever 11 is thrown (byeither—it makes no difference who rolls the dice), Agives a coin to B. Whenever 14
is thrown, Bgives a coin to A. The person who ﬁrst wins all the coins wins the game.
Since P{roll 11 }=
27
216andP{roll 14 }=15
216, we see from Example 4h that, for A,t h i s
is just the gambler’s ruin problem with p=15
42,i=12, and N=24. The general form
of the gambler’s ruin problem was solved by the mathematician James Bernoulli andpublished 8 years after his death in 1713.
For an application of the gambler’s ruin problem to drug testing, suppose that
two new drugs have been developed for treating a certain disease. Drug ihas a cure
rateP
i,i=1, 2, in the sense that each patient treated with drug iwill be cured with
probability Pi. These cure rates are, however, not known, and we are interested in
ﬁnding a method for deciding whether P1>P2orP2>P1. To decide on one of
these alternatives, consider the following test: Pairs of patients are to be treatedsequentially, with one member of the pair receiving drug 1 and the other drug 2.The results for each pair are determined, and the testing stops when the cumulative
number of cures from one of the drugs exceeds the cumulative number of cures from
the other by some ﬁxed, predetermined number. More formally, let
X
j=/braceleftBigg
1 if the patient in the jth pair that receives drug 1 is cured
0 otherwise
Yj=/braceleftBigg
1 if the patient in the jth pair that receives drug 2 is cured
0 otherwise

<<<PAGE 100>>>

A First Course in Probability 87
For a predetermined positive integer M, the test stops after pair N, where Nis
the ﬁrst value of nsuch that either
X1+ ··· + Xn−(Y1+ ··· + Yn)=M
or
X1+ ··· + Xn−(Y1+ ··· + Yn)=−M
In the former case, we assert that P1>P2and in the latter that P2>P1.
In order to help ascertain whether the foregoing is a good test, one thing we
would like to know is the probability that it leads to an incorrect decision. That
is, for given P1andP2, where P1>P2, what is the probability that the test will
incorrectly assert that P2>P1? To determine this probability, note that after each
pair is checked, the cumulative difference of cures using drug 1 versus drug 2 will goup by 1 with probability P
1(1−P2)—since this is the probability that drug 1 leads
to a cure and drug 2 does not—or go down by 1 with probability (1−P1)P2,o r
remain the same with probability P1P2+(1−P1)(1−P2). Hence, if we consider
only those pairs in which the cumulative difference changes, then the difference will
go up by 1 with probability
P=P{up 1|up 1 or down 1 }
=P1(1−P2)
P1(1−P2)+(1−P1)P2
and down by 1 with probability
1−P=P2(1−P1)
P1(1−P2)+(1−P1)P2
Thus, the probability that the test will assert that P2>P1is equal to the prob-
ability that a gambler who wins each (one-unit) bet with probability Pwill go down
Mbefore going up M. But Equation (4.5), with i=M,N=2M, shows that this
probability is given by
P{test asserts that P2>P1}
=1−1−/parenleftbigg1−P
P/parenrightbiggM
1−/parenleftbigg1−P
P/parenrightbigg2M
=1−1
1+/parenleftbigg1−P
P/parenrightbiggM
=1
1+γM
where
γ=P
1−P=P1(1−P2)
P2(1−P1)
For instance, if P1=.6 and P2=.4, then the probability of an incorrect decision is
.017 when M=5 and reduces to .0003 when M=10. .
Suppose that we are presented with a set of elements and we want to determine
whether at least one member of the set has a certain property. We can attack this
question probabilistically by randomly choosing an element of the set in such a way
that each element has a positive probability of being selected. Then the original

<<<PAGE 101>>>

88Chapter 3 Conditional Probability and Independence
question can be answered by a consideration of the probability that the randomly
selected element does not have the property of interest. If this probability is equal
to 1, then none of the elements of the set has the property; if it is less than 1, then at
least one element of the set has the property.
The ﬁnal example of this section illustrates this technique.
Example
4nThe complete graph having nvertices is deﬁned to be a set of npoints (called ver-
tices) in the plane and the/parenleftBigg
n
2/parenrightBigg
lines (called edges) connecting each pair of vertices.
The complete graph having 3 vertices is shown in Figure 3.3. Suppose now that each
edge in a complete graph having nvertices is to be colored either red or blue. For a
ﬁxed integer k, a question of interest is, Is there a way of coloring the edges so that
no set of kvertices has all of its/parenleftBigg
k
2/parenrightBigg
connecting edges the same color? It can be
shown by a probabilistic argument that if nis not too large, then the answer is yes.
Figure 3.3
The argument runs as follows: Suppose that each edge is, independently, equally
likely to be colored either red or blue. That is, each edge is red with probability1
2.
Number the/parenleftBigg
n
k/parenrightBigg
sets of kvertices and deﬁne the events Ei,i=1,...,/parenleftBigg
nk/parenrightBigg
as
follows:
E
i={all of the connecting edges of the ith set
ofkvertices are the same color }
Now, since each of the/parenleftBigg
k
2/parenrightBigg
connecting edges of a set of kvertices is equally likely
to be either red or blue, it follows that the probability that they are all the same
color is
P(Ei)=2/parenleftbigg1
2/parenrightbiggk(k−1)/ 2
Therefore, because
P⎛
⎝/uniondisplay
iEi⎞⎠…/summationdisplay
iP(Ei)(Boole’s inequality )
we ﬁnd that P/parenleftBigg
/uniontext
iEi/parenrightBigg
, the probability that there is a set of kvertices all of whose
connecting edges are similarly colored, satisﬁes
P⎛⎝/uniondisplay
iEi⎞⎠…/parenleftBigg
n
k/parenrightBigg/parenleftbigg1
2/parenrightbiggk(k−1)/ 2−1

<<<PAGE 102>>>

A First Course in Probability 89
Hence, if
/parenleftBigg
n
k/parenrightBigg/parenleftbigg1
2/parenrightbiggk(k−1)/ 2−1
<1
or, equivalently, if
/parenleftBigg
nk/parenrightBigg
<2
k(k−1)/ 2−1
then the probability that at least one of the/parenleftBigg
n
k/parenrightBigg
sets of kvertices has all of its
connecting edges the same color is less than 1. Consequently, under the preceding
condition on nandk, it follows that there is a positive probability that no set of k
vertices has all of its connecting edges the same color. But this conclusion impliesthat there is at least one way of coloring the edges for which no set of kvertices has
all of its connecting edges the same color. .
Remarks (a) Whereas the preceding argument established a condition on nandk
that guarantees the existence of a coloring scheme satisfying the desired property, itgives no information about how to obtain such a scheme (although one possibility
would be simply to choose the colors at random, check to see if the resulting coloring
satisﬁes the property, and repeat the procedure until it does).
(b) The method of introducing probability into a problem whose statement is
purely deterministic has been called the probabilistic method.
†Other examples
of this method are given in Theoretical Exercise 24 and Examples 2t and 2u ofChapter 7.
3.5 P(·|F) Is a Probability
Conditional probabilities satisfy all of the properties of ordinary probabilities, as isproved by Proposition 5.1, which shows that P(E|F)satisﬁes the three axioms of a
probability.
Proposition
5.1(a) 0 …P(E|F)…1.
(b)P(S|F)=1.
(c) If E
i,i=1, 2,...,are mutually exclusive events, then
P⎛
⎝q/uniondisplay
i=1Ei|F⎞⎠=q/summationdisplay
i=1P(Ei|F)
Proof To prove part (a), we must show that 0 …P(EF)/P(F)…1. The left-side
inequality is obvious, whereas the right side follows because EF(F, which implies
thatP(EF)…P(F). Part (b) follows because
P(S|F)=P(SF)
P(F)=P(F)
P(F)=1
†See N. Alon, J. Spencer, and P . Erdos, The Probabilistic Method (New York: John Wiley & Sons, Inc., 1992).

<<<PAGE 103>>>

90Chapter 3 Conditional Probability and Independence
Part (c) follows from
P⎛
⎝q/uniondisplay
i=1Ei|F⎞⎠=P⎛
⎝/parenleftBigg
q/uniontext
i=1Ei/parenrightBigg
F⎞⎠
P(F)
=P/parenleftBigg
q/uniontext
1EiF/parenrightBigg
P(F)since⎛
⎝q/uniondisplay
1Ei⎞⎠F=q/uniondisplay
1EiF
=q/summationdisplay
1P(EiF)
P(F)
=q/summationdisplay
1P(Ei|F)
where the next-to-last equality follows because EiEj=Ø implies that
EiFEjF=Ø.
If we deﬁne Q(E)=P(E|F), then, from Proposition 5.1, Q(E) may be regarded
as a probability function on the events of S. Hence, all of the propositions previously
proved for probabilities apply to Q(E). For instance, we have
Q(E 1∪E2)=Q(E 1)+Q(E 2)−Q(E 1E2)
or, equivalently,
P(E1∪E2|F)=P(E1|F)+P(E2|F)−P(E1E2|F)
Also, if we deﬁne the conditional probability Q(E 1|E2)byQ(E 1|E2)=Q(E 1E2)/
Q(E 2), then, from Equation (3.1), we have
Q(E 1)=Q(E 1|E2)Q(E 2)+Q(E 1|Ec
2)Q(Ec
2) (5.1)
Since
Q(E 1|E2)=Q(E 1E2)
Q(E 2)
=P(E1E2|F)
P(E2|F)
=P(E1E2F)
P(F)
P(E2F)
P(F)
=P(E1|E2F)
Equation (5.1) is equivalent to
P(E1|F)=P(E1|E2F)P(E2|F)+P(E1|Ec
2F)P(Ec
2|F)
Example
5aConsider Example 3a, which is concerned with an insurance company that believes
that people can be divided into two distinct classes: those who are accident prone
and those who are not. During any given year, an accident-prone person will have an

<<<PAGE 104>>>

A First Course in Probability 91
accident with probability .4, whereas the corresponding ﬁgure for a person who is not
prone to accidents is .2. What is the conditional probability that a new policyholder
will have an accident in his or her second year of policy ownership, given that the
policyholder has had an accident in the ﬁrst year?
Solution If we let Abe the event that the policyholder is accident prone and we let
Ai,i=1, 2, be the event that he or she has had an accident in the ith year, then the
desired probability P(A2|A1)may be obtained by conditioning on whether or not
the policyholder is accident prone, as follows:
P(A2|A1)=P(A2|AA 1)P(A|A 1)+P(A2|AcA1)P(Ac|A1)
Now,
P(A|A 1)=P(A1A)
P(A1)=P(A1|A)P(A)
P(A1)
However, P(A)is assumed to equal3
10, and it was shown in Example 3a that P(A1)=
.26. Hence,
P(A|A 1)=(.4)(.3)
.26=6
13
Thus,
P(Ac|A1)=1−P(A|A 1)=7
13
Since P(A2|AA 1)=.4 and P(A2|AcA1)=.2, it follows that
P(A2|A1)=(.4)6
13+(.2)7
13L.29 .
Example
5bA female chimp has given birth. It is not certain, however, which of two male chimps
is the father. Before any genetic analysis has been performed, it is believed that the
probability that male number 1 is the father is pand the probability that male num-
ber 2 is the father is 1 −p.DNA obtained from the mother, male number 1, and
male number 2 indicates that on one speciﬁc location of the genome, the mother hasthe gene pair (A,A), male number 1 has the gene pair (a,a), and male number 2
has the gene pair (A,a). If a DNA test shows that the baby chimp has the gene pair
(A,a), what is the probability that male number 1 is the father?
Solution Let all probabilities be conditional on the event that the mother has the
gene pair (A,A), male number 1 has the gene pair (a,a), and male number 2 has
the gene pair (A,a). Now, let Mibe the event that male number i,i=1, 2, is the
father, and let BA,abe the event that the baby chimp has the gene pair (A,a).Then,
P(M1|BA,a)is obtained as follows:
P(M1|BA,a)=P(M1BA,a)
P(BA,a)
=P(BA,a|M1)P(M1)
P(BA,a|M1)P(M1)+P(BA,a|M2)P(M2)
=1·p
1·p+(1/2)(1 −p)
=2p
1+p
Because2p
1+p>pwhen p<1, the information that the baby’s gene pair is (A,a)
increases the probability that male number 1 is the father. This result is intuitive

<<<PAGE 105>>>

92Chapter 3 Conditional Probability and Independence
because it is more likely that the baby would have gene pair (A,a)ifM1is true than
ifM2is true (the respective conditional probabilities being 1 and 1 /2). .
The next example deals with a problem in the theory of runs.
Example
5cIndependent trials, each resulting in a success with probability por a failure with
probability q=1−p, are performed. We are interested in computing the prob-
ability that a run of nconsecutive successes occurs before a run of mconsecutive
failures.
Solution LetEbe the event that a run of nconsecutive successes occurs before a run
ofmconsecutive failures. To obtain P(E), we start by conditioning on the outcome
of the ﬁrst trial. That is, letting Hdenote the event that the ﬁrst trial results in a
success, we obtain
P(E)=pP(E|H)+qP(E|Hc) (5.2)
Now, given that the ﬁrst trial was successful, one way we can get a run of n
successes before a run of mfailures would be to have the next n−1 trials all result
in successes. So, let us condition on whether or not that occurs. That is, letting Fbe
the event that trials 2 through nall are successes, we obtain
P(E|H)=P(E|FH)P(F|H)+P(E|FcH)P(Fc|H) (5.3)
On the one hand, clearly, P(E|FH)=1; on the other hand, if the event FcHoccurs,
then the ﬁrst trial would result in a success, but there would be a failure some time
during the next n−1 trials. However, when this failure occurs, it would wipe out
all of the previous successes, and the situation would be exactly as if we started outwith a failure. Hence,
P(E|F
cH)=P(E|Hc)
Because the independence of trials implies that FandHare independent, and
because P(F)=pn−1, it follows from Equation (5.3) that
P(E|H)=pn−1+(1−pn−1)P(E|Hc) (5.4)
We now obtain an expression for P(E|Hc)in a similar manner. That is, we let G
denote the event that trials 2 through mare all failures. Then,
P(E|Hc)=P(E|GHc)P(G|Hc)+P(E|GcHc)P(Gc|Hc) (5.5)
Now, GHcis the event that the ﬁrst mtrials all result in failures, so P(E|GHc)=0.
Also, if GcHcoccurs, then the ﬁrst trial is a failure, but there is at least one success
in the next m−1 trials. Hence, since this success wipes out all previous failures, we
see that
P(E|GcHc)=P(E|H)
Thus, because P(Gc|Hc)=P(Gc)=1−qm−1, we obtain, from (5.5),
P(E|Hc)=(1−qm−1)P(E|H) (5.6)
Solving Equations (5.4) and (5.6) yields
P(E|H)=pn−1
pn−1+qm−1−pn−1qm−1
and
P(E|Hc)=(1−qm−1)pn−1
pn−1+qm−1−pn−1qm−1

<<<PAGE 106>>>

A First Course in Probability 93
Thus,
P(E)=pP(E|H)+qP(E|Hc)
=pn+qpn−1(1−qm−1)
pn−1+qm−1−pn−1qm−1
=pn−1(1−qm)
pn−1+qm−1−pn−1qm−1(5.7)
It is interesting to note that by the symmetry of the problem, the probability
of obtaining a run of mfailures before a run of nsuccesses would be given by
Equation (5.7) with pandqinterchanged and nandminterchanged. Hence, this
probability would equal
P{run of mfailures before a run of nsuccesses }
=qm−1(1−pn)
qm−1+pn−1−qm−1pn−1(5.8)
Since Equations (5.7) and (5.8) sum to 1, it follows that, with probability 1, either a
run of nsuccesses or a run of mfailures will eventually occur.
As an example of Equation (5.7), we note that, in tossing a fair coin, the proba-
bility that a run of 2 heads will precede a run of 3 tails is7
10. For 2 consecutive heads
before 4 consecutive tails, the probability rises to5
6. .
In our next example, we return to a matching problem and obtain a solution by
using conditional probabilities.
Example
5dAt a party, nmen take off their hats. The hats are then mixed up, and each man
randomly selects one. We say that a match occurs if a man selects his own hat. Whatis the probability of
(a) no matches?
(b) exactly kmatches?.
Solution (a) Let Edenote the event that no matches occur, and to make explicit the
dependence on n,w r i t e Pn=P(E). We start by conditioning on whether or not the
ﬁrst man selects his own hat—call these events MandMc, respectively. Then,
Pn=P(E)=P(E|M)P(M)+P(E|Mc)P(Mc)
Clearly, P(E|M)=0, so
Pn=P(E|Mc)n−1
n(5.9)
Now, P(E|Mc)is the probability of no matches when n−1 men select from a set
ofn−1 hats that does not contain the hat of one of these men. This can hap-
pen in either of two mutually exclusive ways: Either there are no matches and the
extra man does not select the extra hat (this being the hat of the man who chose
ﬁrst) or there are no matches and the extra man does select the extra hat. Theprobability of the ﬁrst of these events is just P
n−1, which is seen by regarding the
extra hat as “belonging” to the extra man. Because the second event has probability[1/(n−1)]P
n−2, we have
P(E|Mc)=Pn−1+1
n−1Pn−2

<<<PAGE 107>>>

94Chapter 3 Conditional Probability and Independence
Thus, from Equation (5.9),
Pn=n−1
nPn−1+1
nPn−2
or, equivalently,
Pn−Pn−1=−1
n(Pn−1−Pn−2) (5.10)
However, since Pnis the probability of no matches when nmen select among their
own hats, we have
P1=0 P2=1
2
So, from Equation (5.10),
P3−P2=−(P2−P1)
3=−1
3!orP3=1
2!−1
3!
P4−P3=−(P3−P2)
4=1
4!orP4=1
2!−1
3!+1
4!
and, in general,
Pn=1
2!−1
3!+1
4!− ··· +(−1)n
n!
(b) To obtain the probability of exactly kmatches, we consider any ﬁxed group
ofkmen. The probability that they, and only they, select their own hats is
1
n1
n−1···1
n−(k−1)Pn−k=(n−k)!
n!Pn−k
where Pn−kis the conditional probability that the other n−kmen, selecting among
their own hats, have no matches. Since there are/parenleftBigg
n
k/parenrightBigg
choices of a set of kmen, the
desired probability of exactly kmatches is
Pn−k
k!=1
2!−1
3!+ ··· +(−1)n−k
(n−k)!
k!.
An important concept in probability theory is that of the conditional indepen-
dence of events. We say that the events E1andE2areconditionally independent
given Fif given that Foccurs, the conditional probability that E1occurs is unchanged
by information as to whether or not E2occurs. More formally, E1andE2are said to
be conditionally independent given Fif
P(E1|E2F)=P(E1|F) (5.11)
or, equivalently,
P(E1E2|F)=P(E1|F)P(E2|F) (5.12)
The notion of conditional independence can easily be extended to more than
two events, and this extension is left as an exercise.
The reader should note that the concept of conditional independence was implic-
itly employed in Example 5a, where it was assumed that the events that a poli-
cyholder had an accident in his or her ith year, i=1, 2,..., were conditionally
independent given whether or not the person was accident prone. The following

<<<PAGE 108>>>

A First Course in Probability 95
example, sometimes referred to as Laplace’s rule of succession, further illustrates
the concept of conditional independence.
Example
5eLaplace’s rule of succession
There are k+1 coins in a box. When ﬂipped, the ith coin will turn up heads with
probability i/k,i=0, 1,...,k. A coin is randomly selected from the box and is then
repeatedly ﬂipped. If the ﬁrst nﬂips all result in heads, what is the conditional prob-
ability that the (n+1)ﬂip will do likewise?
Solution LetCidenote the event that the ith coin, i=0, 1,...,k, is initially selected;
letFndenote the event that the ﬁrst nﬂips all result in heads; and let Hbe the event
that the (n+1)ﬂip is a head. The desired probability, P(H|Fn), is now obtained as
follows:
P(H|Fn)=k/summationdisplay
i=0P(H|FnCi)P(Ci|Fn)
Now, given that the ith coin is selected, it is reasonable to assume that the out-
comes will be conditionally independent, with each one resulting in a head with
probability i/k. Hence,
P(H|FnCi)=P(H|Ci)=i
k
Also,
P(Ci|Fn)=P(CiFn)
P(Fn)=P(Fn|Ci)P(Ci)
k/summationdisplay
j=0P(Fn|Cj)P(Cj)=(i/k)n[1/(k+1)]
k/summationdisplay
j=0(j/k)n[1/(k+1)]
Thus,
P(H|Fn)=k/summationdisplay
i=0(i/k)n+1
k/summationdisplay
j=0(j/k)n
But if kis large, we can use the integral approximations
1
kk/summationdisplay
i=0/parenleftbiggi
k/parenrightbiggn+1
L/integraldisplay1
0xn+1dx=1
n+2
1
kk/summationdisplay
j=0/parenleftbiggj
k/parenrightbiggn
L/integraldisplay1
0xndx=1
n+1
So, for klarge,
P(H|Fn)Ln+1
n+2.

<<<PAGE 109>>>

96Chapter 3 Conditional Probability and Independence
Example
5fUpdating information sequentially
Suppose there are nmutually exclusive and exhaustive possible hypotheses, with
initial (sometimes referred to as prior ) probabilities P(Hi),/summationtextn
i=1P(Hi)=1. Now, if
information that the event Ehas occurred is received, then the conditional probabil-
ity that Hiis the true hypothesis (sometimes referred to as the updated orposterior
probability of Hi)i s
P(Hi|E)=P(E|Hi)P(Hi)/summationtext
jP(E|Hj)P(Hj)(5.13)
Suppose now that we learn ﬁrst that E1has occurred and then that E2has occurred.
Then, given only the ﬁrst piece of information, the conditional probability that Hiis
the true hypothesis is
P(Hi|E1)=P(E1|Hi)P(Hi)
P(E1)=P(E1|Hi)P(Hi)/summationtext
jP(E1|Hj)P(Hj)
whereas given both pieces of information, the conditional probability that Hiis the
true hypothesis is P(Hi|E1E2), which can be computed by
P(Hi|E1E2)=P(E1E2|Hi)P(Hi)/summationtext
jP(E1E2|Hj)P(Hj)
One might wonder, however, when one can compute P(Hi|E1E2)by using the
right side of Equation (5.13) with E=E2and with P(Hj)replaced by P(Hj|E1),
j=1,...,n. That is, when is it legitimate to regard P(Hj|E1),jÚ1, as the prior
probabilities and then use (5.13) to compute the posterior probabilities?
Solution The answer is that the preceding is legitimate, provided that for each j=
1,...,n, the events E1andE2are conditionally independent, given Hj. For if this is
the case, then
P(E1E2|Hj)=P(E2|Hj)P(E1|Hj),j=1,...,n
Therefore,
P(Hi|E1E2)=P(E2|Hi)P(E1|Hi)P(Hi)
P(E1E2)
=P(E2|Hi)P(E1Hi)
P(E1E2)
=P(E2|Hi)P(Hi|E1)P(E1)
P(E1E2)
=P(E2|Hi)P(Hi|E1)
Q(1, 2)
where Q(1, 2) =P(E1E2)
P(E1). Since the preceding equation is valid for all i, we obtain,
upon summing,
1=n/summationdisplay
i=1P(Hi|E1E2)=n/summationdisplay
i=1P(E2|Hi)P(Hi|E1)
Q(1, 2)
showing that
Q(1, 2) =n/summationdisplay
i=1P(E2|Hi)P(Hi|E1)

<<<PAGE 110>>>

A First Course in Probability 97
and yielding the result
P(Hi|E1E2)=P(E2|Hi)P(Hi|E1)/summationtextn
i=1P(E2|Hi)P(Hi|E1)
For instance, suppose that one of two coins is chosen to be ﬂipped. Let Hibe the
event that coin i,i=1, 2, is chosen, and suppose that when coin iis ﬂipped, it lands
on heads with probability pi,i=1, 2. Then, the preceding equations show that to
sequentially update the probability that coin 1 is the one being ﬂipped, given the
results of the previous ﬂips, all that must be saved after each new ﬂip is the condi-
tional probability that coin 1 is the coin being used. That is, it is not necessary tokeep track of all earlier results. .
Summary
For events EandF, the conditional probability of Egiven
thatFhas occurred is denoted by P(E|F)and is deﬁned by
P(E|F)=P(EF)
P(F)
The identity
P(E1E2···En)=P(E1)P(E2|E1)···P(En|E1···En−1)
is known as the multiplication rule of probability.
A valuable identity is
P(E)=P(E|F)P(F)+P(E|Fc)P(Fc)
which can be used to compute P(E)by “conditioning” on
whether Foccurs.
P(H)/P(Hc)is called the odds of the event H.T h e
identity
P(H|E)
P(Hc|E)=P(H)P(E|H)
P(Hc)P(E|Hc)
shows that when new evidence Eis obtained, the value of
the odds of Hbecomes its old value multiplied by the ratio
of the conditional probability of the new evidence when H
is true to the conditional probability when His not true.
LetFi,i=1,...,n, be mutually exclusive events
whose union is the entire sample space. The identityP(Fj|E)=P(E|Fj)P(Fj)
n/summationdisplay
i=1P(E|Fi)P(Fi)
is known as Bayes’s formula . If the events Fi,i=1,...,n,
are competing hypotheses, then Bayes’s formula shows
how to compute the conditional probabilities of thesehypotheses when additional evidence Ebecomes avail-
able.
The denominator of Bayes’s formula uses that
P(E)=n/summationdisplay
i=1P(E|Fi)P(Fi)
which is called the law of total probability.
IfP(EF)=P(E)P(F), then we say that the events
EandFareindependent . This condition is equivalent to
P(E|F)=P(E)and to P(F|E)=P(F). Thus, the events E
andFare independent if knowledge of the occurrence of
one of them does not affect the probability of the other.
The events E1,...,Enare said to be independent if,
for any subset Ei1,...,Eirof them,
P(Ei1···Eir)=P(Ei1)···P(Eir)
For a ﬁxed event F,P(E|F)can be considered to be a prob-
ability function on the events Eof the sample space.
Problems
3.1.Two fair dice are rolled. What is the conditional prob-
ability that at least one lands on 6 given that the dice landon different numbers?
3.2.If two fair dice are rolled, what is the conditional prob-
ability that the ﬁrst one lands on 6 given that the sum of the
dice is i? Compute for all values of ibetween 2 and 12.3.3.Use Equation (2.1) to compute in a hand of bridge the
conditional probability that East has 3 spades given thatNorth and South have a combined total of 8 spades.
3.4.What is the probability that at least one of a pair of
fair dice lands on 6, given that the sum of the dice is i,
i=2, 3,..., 12?

<<<PAGE 111>>>

98Chapter 3 Conditional Probability and Independence
3.5.An urn contains 6 white and 9 black balls. If 4 balls are
to be randomly selected without replacement, what is the
probability that the ﬁrst 2 selected are white and the last 2
black?
3.6.Consider an urn containing 12 balls, of which 8 are
white. A sample of size 4 is to be drawn with replacement
(without replacement). What is the conditional probabil-
ity (in each case) that the ﬁrst and third balls drawn will be
white given that the sample drawn contains exactly 3 whiteballs?
3.7.The king comes from a family of 2 children. What is
the probability that the other child is his sister?
3.8.A couple has 2 children. What is the probability that
both are girls if the older of the two is a girl?3.9.Consider 3 urns. Urn Acontains 2 white and 4 red
balls, urn Bcontains 8 white and 4 red balls, and urn Ccon-
tains 1 white and 3 red balls. If 1 ball is selected from each
urn, what is the probability that the ball chosen from urn
Awas white given that exactly 2 white balls were selected?
3.10. Three cards are randomly selected, without replace-
ment, from an ordinary deck of 52 playing cards. Compute
the conditional probability that the ﬁrst card selected is a
spade given that the second and third cards are spades.
3.11. Two cards are randomly chosen without replacement
from an ordinary deck of 52 cards. Let Bbe the event that
both cards are aces, let A
sbe the event that the ace of
spades is chosen, and let Abe the event that at least one
ace is chosen. Find
(a)P(B|A s)
(b)P(B|A)
3.12. A recent college graduate is planning to take the
ﬁrst three actuarial examinations in the coming summer.
She will take the ﬁrst actuarial exam in June. If she passes
that exam, then she will take the second exam in July, andif she also passes that one, then she will take the third
exam in September. If she fails an exam, then she is not
allowed to take any others. The probability that she passesthe ﬁrst exam is .9. If she passes the ﬁrst exam, then the
conditional probability that she passes the second one is
.8, and if she passes both the ﬁrst and the second exams,then the conditional probability that she passes the third
exam is .7.
(a)What is the probability that she passes all three exams?
(b)Given that she did not pass all three exams, what is the
conditional probability that she failed the second exam?
3.13. Suppose that an ordinary deck of 52 cards (which
contains 4 aces) is randomly divided into 4 hands of 13
cards each. We are interested in determining p, the prob-
ability that each hand has an ace. Let E
ibe the event
that the ith hand has exactly one ace. Determine p=
P(E1E2E3E4)by using the multiplication rule.3.14. An urn initially contains 5 white and 7 black balls.
Each time a ball is selected, its color is noted and it isreplaced in the urn along with 2 other balls of the same
color. Compute the probability that
(a)the ﬁrst 2 balls selected are black and the next 2 are
white;
(b)of the ﬁrst 4 balls selected, exactly 2 are black.
3.15. An ectopic pregnancy is twice as likely to develop
when the pregnant woman is a smoker as it is when she is
a nonsmoker. If 32 percent of women of childbearing ageare smokers, what percentage of women having ectopic
pregnancies are smokers?
3.16. Ninety-eight percent of all babies survive delivery.
However, 15 percent of all births involve Cesarean (C)
sections, and when a C section is performed, the baby sur-
vives 96 percent of the time. If a randomly chosen preg-
nant woman does not have a C section, what is the proba-
bility that her baby survives?
3.17. In a certain community, 36 percent of the families
own a dog and 22 percent of the families that own a dog
also own a cat. In addition, 30 percent of the families ownac a t .W h a ti s
(a)the probability that a randomly selected family owns
both a dog and a cat?
(b)the conditional probability that a randomly selected
family owns a dog given that it owns a cat?
3.18. A total of 46 percent of the voters in a certain city
classify themselves as Independents, whereas 30 percent
classify themselves as Liberals and 24 percent say that theyare Conservatives. In a recent local election, 35 percent
of the Independents, 62 percent of the Liberals, and 58
percent of the Conservatives voted. A voter is chosen atrandom. Given that this person voted in the local election,
what is the probability that he or she is
(a)an Independent?
(b)a Liberal?
(c)a Conservative?
(d)What percent of voters participated in the local
election?
3.19. A total of 48 percent of the women and 37 percent of
the men who took a certain “quit smoking” class remained
nonsmokers for at least one year after completing the
class. These people then attended a success party at the
end of a year. If 62 percent of the original class was male,
(a)what percentage of those attending the party were
women?
(b)what percentage of the original class attended the
party?
3.20. Fifty-two percent of the students at a certain college
are females. Five percent of the students in this college are
majoring in computer science. Two percent of the students

<<<PAGE 112>>>

A First Course in Probability 99
are women majoring in computer science. If a student is
selected at random, ﬁnd the conditional probability that
(a)the student is female given that the student is majoring
in computer science;
(b)this student is majoring in computer science given that
the student is female.
3.21. A total of 500 married working couples were polled
about their annual salaries, with the following information
resulting:
Husband
Wife Less than More than
$25,000 $25,000
Less than $25,000 212 198
More than $25,000 36 54
For instance, in 36 of the couples, the wife earned more
and the husband earned less than $25,000. If one of thecouples is randomly chosen, what is
(a)the probability that the husband earns less than
$25,000?
(b)the conditional probability that the wife earns more
than $25,000 given that the husband earns more than this
amount?
(c)the conditional probability that the wife earns more
than $25,000 given that the husband earns less than this
amount?
3.22. A red die, a blue die, and a yellow die (all six sided)
are rolled. We are interested in the probability that the
number appearing on the blue die is less than that appear-ing on the yellow die, which is less than that appearing on
the red die. That is, with B,Y,a n d Rdenoting, respec-
tively, the number appearing on the blue, yellow, and red
die, we are interested in P(B<Y<R).
(a)What is the probability that no two of the dice land on
the same number?
(b)Given that no two of the dice land on the same num-
ber, what is the conditional probability that B<Y<R?
(c)What is P(B<Y<R)?
3.23. Urn I contains 2 white and 4 red balls, whereas urn II
contains 1 white and 1 red ball. A ball is randomly chosen
from urn I and put into urn II, and a ball is then randomlyselected from urn II. What is
(a)the probability that the ball selected from urn II is
white?
(b)the conditional probability that the transferred ball
was white given that a white ball is selected from urn II?
3.24. Each of 2 balls is painted either black or gold and
then placed in an urn. Suppose that each ball is colored
black with probability
1
2and that these events are inde-
pendent.(a)Suppose that you obtain information that the gold
paint has been used (and thus at least one of the balls is
painted gold). Compute the conditional probability that
both balls are painted gold.
(b)Suppose now that the urn tips over and 1 ball falls out.
It is painted gold. What is the probability that both balls
are gold in this case? Explain.
3.25. The following method was proposed to estimate the
number of people over the age of 50 who reside in a
town of known population 100,000: “As you walk along
the streets, keep a running count of the percentage of peo-
ple you encounter who are over 50. Do this for a fewdays; then multiply the percentage you obtain by 100,000
to obtain the estimate.” Comment on this method.
Hint:L e tp denote the proportion of people in the town
who are over 50. Furthermore, let α
1denote the propor-
tion of time that a person under the age of 50 spends in
the streets, and let α2be the corresponding value for those
over 50. What quantity does the method suggested esti-
mate? When is the estimate approximately equal to p?
3.26. Suppose that 5 percent of men and 0.25 percent of
women are color blind. A color-blind person is chosen
at random. What is the probability of this person beingmale? Assume that there are an equal number of males
and females. What if the population consisted of twice as
many males as females?
3.27. All the workers at a certain company drive to work
and park in the company’s lot. The company is interested
in estimating the average number of workers in a car.Which of the following methods will enable the company
to estimate this quantity? Explain your answer.
1. Randomly choose nworkers, ﬁnd out how many were
in the cars in which they were driven, and take the aver-
age of the nvalues.
2. Randomly choose ncars in the lot, ﬁnd out how many
were driven in those cars, and take the average of the n
values.
3.28. Suppose that an ordinary deck of 52 cards is shufﬂed
and the cards are then turned over one at a time until theﬁrst ace appears. Given that the ﬁrst ace is the 20th cardto appear, what is the conditional probability that the card
following it is the
(a)ace of spades?
(b)two of clubs?
3.29. There are 15 tennis balls in a box, of which 9 have
not previously been used. Three of the balls are randomly
chosen, played with, and then returned to the box. Later,
another 3 balls are randomly chosen from the box. Find
the probability that none of these balls has ever beenused.
3.30. Consider two boxes, one containing 1 black and 1
white marble, the other 2 black and 1 white marble. A

<<<PAGE 113>>>

100 Chapter 3 Conditional Probability and Independence
box is selected at random, and a marble is drawn from
it at random. What is the probability that the marble isblack? What is the probability that the ﬁrst box was the
one selected given that the marble is white?
3.31. Ms. Aquina has just had a biopsy on a possibly can-
cerous tumor. Not wanting to spoil a weekend family
event, she does not want to hear any bad news in the next
few days. But if she tells the doctor to call only if the news
is good, then if the doctor does not call, Ms. Aquina canconclude that the news is bad. So, being a student of prob-
ability, Ms. Aquina instructs the doctor to ﬂip a coin. If it
comes up heads, the doctor is to call if the news is good andnot call if the news is bad. If the coin comes up tails, the
doctor is not to call. In this way, even if the doctor doesn’t
call, the news is not necessarily bad. Let αbe the proba-
bility that the tumor is cancerous; let βbe the conditional
probability that the tumor is cancerous given that the doc-tor does not call.
(a)Which should be larger, αorβ?
(b)Findβin terms of α, and prove your answer in part (a).
3.32. A family has jchildren with probability p
j,w h e r e
p1=.1,p2=.25,p3=.35,p4=.3. A child from this fam-
ily is randomly chosen. Given that this child is the eldest
child in the family, ﬁnd the conditional probability that the
family has
(a)only 1 child;
(b)4 children.
Redo (a) and (b) when the randomly selected child is the
youngest child of the family.
3.33. On rainy days, Joe is late to work with probability
.3; on nonrainy days, he is late with probability .1. With
probability .7, it will rain tomorrow.
(a)Find the probability that Joe is early tomorrow.
(b)Given that Joe was early, what is the conditional prob-
ability that it rained?
3.34. In Example 3f, suppose that the new evidence is sub-
ject to different possible interpretations and in fact shows
only that it is 90 percent likely that the criminal pos-
sesses the characteristic in question. In this case, how likely
would it be that the suspect is guilty (assuming, as before,
that he has the characteristic)?
3.35. With probability .6, the present was hidden by mom;
with probability .4, it was hidden by dad. When mom hides
the present, she hides it upstairs 70 percent of the time and
downstairs 30 percent of the time. Dad is equally likely tohide it upstairs or downstairs.
(a)What is the probability that the present is upstairs?
(b)Given that it is downstairs, what is the probability it
was hidden by dad?
3.36. Stores A,B,a n d Chave 50, 75, and 100 employees,
respectively, and 50, 60, and 70 percent of them respec-
tively are women. Resignations are equally likely amongall employees, regardless of sex. One woman employeeresigns. What is the probability that she works in store C?
3.37. (a)A gambler has a fair coin and a two-headed coin
in his pocket. He selects one of the coins at random; whenhe ﬂips it, it shows heads. What is the probability that it is
the fair coin?
(b)Suppose that he ﬂips the same coin a second time and,
again, it shows heads. Now what is the probability that it is
the fair coin?
(c)Suppose that he ﬂips the same coin a third time and it
shows tails. Now what is the probability that it is the fair
coin?
3.38. Urn Ahas 5 white and 7 black balls. Urn Bhas 3
white and 12 black balls. We ﬂip a fair coin. If the outcome
is heads, then a ball from urn Ais selected, whereas if the
outcome is tails, then a ball from urn Bis selected. Sup-
pose that a white ball is selected. What is the probabilitythat the coin landed tails?
3.39. In Example 3a, what is the probability that someone
has an accident in the second year given that he or she had
no accidents in the ﬁrst year?
3.40. Consider a sample of size 3 drawn in the following
manner: We start with an urn containing 5 white and 7 red
balls. At each stage, a ball is drawn and its color is noted.
The ball is then returned to the urn, along with an addi-
tional ball of the same color. Find the probability that thesample will contain exactly
(a)0 white balls;
(b)1 white ball;
(c)3 white balls;
(d)2 white balls.
3.41. A deck of cards is shufﬂed and then divided into two
halves of 26 cards each. A card is drawn from one of the
halves; it turns out to be an ace. The ace is then placed in
the second half-deck. The half is then shufﬂed, and a cardis drawn from it. Compute the probability that this drawn
card is an ace.
Hint: Condition on whether or not the interchanged card
is selected.
3.42. Twelve percent of all U.S. households are in
California. A total of 1.3 percent of all U.S. households
earn more than $250,000 per year, while a total of 3.3 per-
cent of all California households earn more than $250,000per year.
(a)What proportion of all non-California households earn
more than $250,000 per year?
(b)Given that a randomly chosen U.S. household earns
more than $250,000 per year, what is the probability it is
a California household?

<<<PAGE 114>>>

A First Course in Probability 101
3.43. There are 3 coins in a box. One is a two-headed coin,
another is a fair coin, and the third is a biased coin that
comes up heads 75 percent of the time. When one of the
3 coins is selected at random and ﬂipped, it shows heads.
What is the probability that it was the two-headed coin?
3.44. Three prisoners are informed by their jailer that one
of them has been chosen at random to be executed and
the other two are to be freed. Prisoner Aasks the jailer to
tell him privately which of his fellow prisoners will be set
free, claiming that there would be no harm in divulging
this information because he already knows that at leastone of the two will go free. The jailer refuses to answer the
question, pointing out that if Aknew which of his fellow
prisoners were to be set free, then his own probability of
being executed would rise from
1
3to1
2because he would
then be one of two prisoners. What do you think of the
jailer’s reasoning?
3.45. Suppose we have 10 coins such that if the ith coin
is ﬂipped, heads will appear with probability i/10, i=
1, 2,..., 10. When one of the coins is randomly selected
and ﬂipped, it shows heads. What is the conditional prob-
ability that it was the ﬁfth coin?
3.46. In any given year, a male automobile policyholder
will make a claim with probability pmand a female pol-
icyholder will make a claim with probability pf,w h e r e
pfZpm. The fraction of the policyholders that are male
isα,0<α< 1. A policyholder is randomly chosen. If Ai
denotes the event that this policyholder will make a claim
in year i, show that
P(A2|A1)>P (A1)
Give an intuitive explanation of why the preceding
inequality is true.
3.47. An urn contains 5 white and 10 black balls. A fair die
is rolled and that number of balls is randomly chosen from
the urn. What is the probability that all of the balls selectedare white? What is the conditional probability that the die
landed on 3 if all the balls selected are white?
3.48. Each of 2 cabinets identical in appearance has 2
drawers. Cabinet Acontains a silver coin in each drawer,
and cabinet Bcontains a silver coin in one of its draw-
ers and a gold coin in the other. A cabinet is randomly
selected, one of its drawers is opened, and a silver coin is
found. What is the probability that there is a silver coin inthe other drawer?
3.49. Prostate cancer is the most common type of cancer
found in males. As an indicator of whether a male has
prostate cancer, doctors often perform a test that measures
the level of the prostate-speciﬁc antigen (PSA) that is pro-
duced only by the prostate gland. Although PSA levelsare indicative of cancer, the test is notoriously unreliable.
Indeed, the probability that a noncancerous man will have
an elevated PSA level is approximately .135, increasing toapproximately .268 if the man does have cancer. If, on the
basis of other factors, a physician is 70 percent certain thata male has prostate cancer, what is the conditional proba-
bility that he has the cancer given that
(a)the test indicated an elevated PSA level?
(b)the test did not indicate an elevated PSA level?
Repeat the preceding calculation, this time assuming that
the physician initially believes that there is a 30 percent
chance that the man has prostate cancer.
3.50. Suppose that an insurance company classiﬁes people
into one of three classes: good risks, average risks, and bad
risks. The company’s records indicate that the probabilities
that good-, average-, and bad-risk persons will be involvedin an accident over a 1-year span are, respectively, .05, .15,
and .30. If 20 percent of the population is a good risk, 50
percent an average risk, and 30 percent a bad risk, whatproportion of people have accidents in a ﬁxed year? If
policyholder Ahad no accidents in 2012, what is the prob-
ability that he or she is a good risk? is an average risk?
3.51. A worker has asked her supervisor for a letter of
recommendation for a new job. She estimates that there
is an 80 percent chance that she will get the job if shereceives a strong recommendation, a 40 percent chance if
she receives a moderately good recommendation, and a
10 percent chance if she receives a weak recommendation.She further estimates that the probabilities that the rec-
ommendation will be strong, moderate, and weak are .7,
.2, and .1, respectively.
(a)How certain is she that she will receive the new job
offer?
(b)Given that she does receive the offer, how likely
should she feel that she received a strong recommenda-
tion? a moderate recommendation? a weak recommenda-
tion?
(c)Given that she does not receive the job offer, how
likely should she feel that she received a strong recommen-
dation? a moderate recommendation? a weak recommen-
dation?
3.52. A high school student is anxiously waiting to receive
mail telling her whether she has been accepted to a certain
college. She estimates that the conditional probabilities of
receiving notiﬁcation on each day of next week, given thatshe is accepted and that she is rejected, are as follows:
Day P(mail|accepted ) P(mail|rejected)
Monday .15 .05
Tuesday .20 .10
Wednesday .25 .10
Thursday .15 .15
Friday .10 .20

<<<PAGE 115>>>

102 Chapter 3 Conditional Probability and Independence
She estimates that her probability of being accepted is .6.
(a)What is the probability that she receives mail on Mon-
day?
(b)What is the conditional probability that she receives
mail on Tuesday given that she does not receive mail on
Monday?
(c)If there is no mail through Wednesday, what is the con-
ditional probability that she will be accepted?(d)What is the conditional probability that she will be
accepted if mail comes on Thursday?(e)What is the conditional probability that she will be
accepted if no mail arrives that week?
3.53. A parallel system functions whenever at least one
of its components works. Consider a parallel system of
ncomponents, and suppose that each component works
independently with probability
1
2. Find the conditional
probability that component 1 works given that the system
is functioning.
3.54. If you had to construct a mathematical model for
events EandF, as described in parts (a) through (e),
would you assume that they were independent events?
Explain your reasoning.
(a)Eis the event that a businesswoman has blue eyes, and
Fis the event that her secretary has blue eyes.
(b)Eis the event that a professor owns a car, and Fis the
event that he is listed in the telephone book.
(c)Eis the event that a man is under 6 feet tall, and Fis
the event that he weighs more than 200 pounds.(d)Eis the event that a woman lives in the United States,
andFis the event that she lives in the Western Hemi-
sphere.(e)Eis the event that it will rain tomorrow, and Fis the
event that it will rain the day after tomorrow.
3.55. In a class, there are 4 ﬁrst-year boys, 6 ﬁrst-year girls,
and 6 sophomore boys. How many sophomore girls must
be present if sex and class are to be independent when astudent is selected at random?
3.56. Suppose that you continually collect coupons and
that there are mdifferent types. Suppose also that each
time a new coupon is obtained, it is a type icoupon with
probability p
i,i=1,...,m. Suppose that you have just col-
lected your nth coupon. What is the probability that it is a
new type?
Hint: Condition on the type of this coupon.
3.57. A simpliﬁed model for the movement of the price of
a stock supposes that on each day the stock’s price either
moves up 1 unit with probability por moves down 1 unit
with probability 1 −p. The changes on different days are
assumed to be independent.
(a)What is the probability that after 2 days the stock will
be at its original price?(b)What is the probability that after 3 days the stock’s
price will have increased by 1 unit?
(c)Given that after 3 days the stock’s price has increased
by 1 unit, what is the probability that it went up on the ﬁrst
day?
3.58. Suppose that we want to generate the outcome of the
ﬂip of a fair coin, but that all we have at our disposal is a
biased coin that lands on heads with some unknown proba-
bility pthat need not be equal to1
2. Consider the following
procedure for accomplishing our task:
1. Flip the coin.
2. Flip the coin again.3. If both ﬂips land on heads or both land on tails, return
to step 1.
4. Let the result of the last ﬂip be the result of the experi-
ment.
(a)Show that the result is equally likely to be either heads
or tails.(b)Could we use a simpler procedure that continues to ﬂip
the coin until the last two ﬂips are different and then lets
the result be the outcome of the ﬁnal ﬂip?
3.59. Independent ﬂips of a coin that lands on heads with
probability pare made. What is the probability that the
ﬁrst four outcomes are
(a)H,H,H,H?
(b)T,H,H,H?
(c)What is the probability that the pattern T,H,H,H
occurs before the pattern H,H,H,H?
Hint for part (c) : How can the pattern H,H,H,Hoccur
ﬁrst?
3.60. The color of a person’s eyes is determined by a single
pair of genes. If they are both blue-eyed genes, then the
person will have blue eyes; if they are both brown-eyed
genes, then the person will have brown eyes; and if one of
them is a blue-eyed gene and the other a brown-eyed gene,then the person will have brown eyes. (Because of the
latter fact, we say that the brown-eyed gene is dominant
over the blue-eyed one.) A newborn child independentlyreceives one eye gene from each of its parents, and the
gene it receives from a parent is equally likely to be eitherof the two eye genes of that parent. Suppose that Smith
and both of his parents have brown eyes, but Smith’s sister
has blue eyes.
(a)What is the probability that Smith possesses a blue-
eyed gene?
(b)Suppose that Smith’s wife has blue eyes. What is the
probability that their ﬁrst child will have blue eyes?(c)If their ﬁrst child has brown eyes, what is the probabil-
ity that their next child will also have brown eyes?

<<<PAGE 116>>>

A First Course in Probability 103
3.61. Genes relating to albinism are denoted by Aanda.
Only those people who receive the agene from both par-
ents will be albino. Persons having the gene pair A,aare
normal in appearance and, because they can pass on the
trait to their offspring, are called carriers. Suppose that anormal couple has two children, exactly one of whom is
an albino. Suppose that the nonalbino child mates with a
person who is known to be a carrier for albinism.
(a)What is the probability that their ﬁrst offspring is an
albino?
(b)What is the conditional probability that their second
offspring is an albino given that their ﬁrstborn is not?
3.62. Barbara and Dianne go target shooting. Suppose
that each of Barbara’s shots hits a wooden duck target
with probability p
1, while each shot of Dianne’s hits it with
probability p2. Suppose that they shoot simultaneously at
the same target. If the wooden duck is knocked over (indi-cating that it was hit), what is the probability that
(a)both shots hit the duck?
(b)Barbara’s shot hit the duck?
What independence assumptions have you made?
3.63. AandBare involved in a duel. The rules of the duel
are that they are to pick up their guns and shoot at each
other simultaneously. If one or both are hit, then the duel
is over. If both shots miss, then they repeat the process.Suppose that the results of the shots are independent and
that each shot of Awill hit Bwith probability p
A, and each
shot of Bwill hit Awith probability pB.W h a ti s
(a)the probability that Ais not hit?
(b)the probability that both duelists are hit?
(c)the probability that the duel ends after the nth round
of shots?(d)the conditional probability that the duel ends after the
nth round of shots given that Ais not hit?
(e)the conditional probability that the duel ends after the
nth round of shots given that both duelists are hit?
3.64. A true–false question is to be posed to a husband-
and-wife team on a quiz show. Both the husband and the
wife will independently give the correct answer with prob-ability p. Which of the following is a better strategy for the
couple?
(a)Choose one of them and let that person answer the
question.
(b)Have them both consider the question, and then either
give the common answer if they agree or, if they disagree,
ﬂip a coin to determine which answer to give.
3.65. Assume, as in Example 3h, that 64 percent of twins
are of the same sex. Given that a newborn set of twins is of
the same sex, what is the conditional probability that thetwins are identical?
3.66. The probability of the closing of the ith relay in the
circuits shown in Figure 3.4 is given by p
i,i=1, 2, 3, 4, 5.
If all relays function independently, what is the probability
that a current ﬂows between AandBfor the respective
circuits?
Hint for (b) : Condition on whether relay 3 closes.
3.67. An engineering system consisting of ncomponents
is said to be a k-out-of-n system (k…n)if the system
functions if and only if at least kof the ncomponents func-
tion. Suppose that all components function independently
of one another.
(a)If the ith component functions with probability Pi,i=
1, 2, 3, 4, compute the probability that a 2-out-of-4 system
functions.
(b)Repeat part (a) for a 3-out-of-5 system.
A B14
253A B1
32
45(a)
(b)
Figure 3.4 Circuits for Problem 3.66.

<<<PAGE 117>>>

104 Chapter 3 Conditional Probability and Independence
(c)Repeat for a k-out-of-n system when all the Piequal p
(that is, Pi=p,i=1, 2,...,n).
3.68. In Problem 3.66a, ﬁnd the conditional probability
that relays 1 and 2 are both closed given that a current
ﬂows from AtoB.
3.69. A certain organism possesses a pair of each of 5 dif-
ferent genes (which we will designate by the ﬁrst 5 lettersof the English alphabet). Each gene appears in 2 forms
(which we designate by lowercase and capital letters). The
capital letter will be assumed to be the dominant gene,in the sense that if an organism possesses the gene pair
xX, then it will outwardly have the appearance of the X
gene. For instance, if Xstands for brown eyes and xfor
blue eyes, then an individual having either gene pair XX
orxXwill have brown eyes, whereas one having gene pair
xxwill have blue eyes. The characteristic appearance of
an organism is called its phenotype, whereas its geneticconstitution is called its genotype. (Thus, 2 organisms with
respective genotypes aA, bB, cc, dD, ee andAA, BB, cc,
DD, ee would have different genotypes but the same phe-
notype.) In a mating between 2 organisms, each one con-tributes, at random, one of its gene pairs of each type.The 5 contributions of an organism (one of each of the 5
types) are assumed to be independent and are also inde-
pendent of the contributions of the organism’s mate. In amating between organisms having genotypes aA, bB, cC,
dD, eE anda a ,b B ,c c ,D d ,e ewhat is the probability that
the progeny will (i) phenotypically and (ii) genotypicallyresemble
(a)the ﬁrst parent?
(b)the second parent?
(c)either parent?
(d)neither parent?
3.70. There is a 50–50 chance that the queen carries the
gene for hemophilia. If she is a carrier, then each prince
has a 50–50 chance of having hemophilia. If the queen has
had three princes without the disease, what is the proba-bility that the queen is a carrier? If there is a fourth prince,
what is the probability that he will have hemophilia?
3.71. On the morning of September 30, 1982, the won–lost
records of the three leading baseball teams in the Western
Division of the National League were as follows:
Team Won Lost
Atlanta Braves 87 72
San Francisco Giants 86 73
Los Angeles Dodgers 86 73
Each team had 3 games remaining. All 3 of the Giants’games were with the Dodgers, and the 3 remaining games
of the Braves were against the San Diego Padres. Suppose
that the outcomes of all remaining games are independentand each game is equally likely to be won by either par-ticipant. For each team, what is the probability that it willwin the division title? If two teams tie for ﬁrst place, they
have a playoff game, which each team has an equal chance
of winning.
3.72. A town council of 7 members contains a steering
committee of size 3. New ideas for legislation go ﬁrst to the
steering committee and then on to the council as a wholeif at least 2 of the 3 committee members approve the leg-
islation. Once at the full council, the legislation requires a
majority vote (of at least 4) to pass. Consider a new pieceof legislation, and suppose that each town council member
will approve it, independently, with probability p.W h a t
is the probability that a given steering committee mem-
ber’s vote is decisive in the sense that if that person’s vote
were reversed, then the ﬁnal fate of the legislation would
be reversed? What is the corresponding probability for agiven council member not on the steering committee?
3.73. Suppose that each child born to a couple is equally
likely to be a boy or a girl, independently of the sex dis-
tribution of the other children in the family. For a couple
having 5 children, compute the probabilities of the follow-ing events:
(a)All children are of the same sex.
(b)The 3 eldest are boys and the others girls.
(c)Exactly 3 are boys.
(d)The 2 oldest are girls.
(e)There is at least 1 girl.
3.74. AandBalternate rolling a pair of dice, stopping
either when Ar o l l st h es u m9o rw h e n Brolls the sum
6. Assuming that Arolls ﬁrst, ﬁnd the probability that the
ﬁnal roll is made by A.
3.75. In a certain village, it is traditional for the eldest son
(or the older son in a two-son family) and his wife to be
responsible for taking care of his parents as they age. Inrecent years, however, the women of this village, not want-
ing that responsibility, have not looked favorably upon
marrying an eldest son.
(a)If every family in the village has two children, what
proportion of all sons are older sons?
(b)If every family in the village has three children, what
proportion of all sons are eldest sons?
Assume that each child is, independently, equally likely tobe either a boy or a girl.
3.76. Suppose that EandFare mutually exclusive events
of an experiment. Show that if independent trials of this
experiment are performed, then Ewill occur before Fwith
probability P(E)/[P(E)+P(F)].
3.77. Consider an unending sequence of independent tri-
als, where each trial is equally likely to result in any of the
outcomes 1, 2, or 3. Given that outcome 3 is the last of the

<<<PAGE 118>>>

A First Course in Probability 105
three outcomes to occur, ﬁnd the conditional probability
that
(a)the ﬁrst trial results in outcome 1;
(b)the ﬁrst two trials both result in outcome 1.
3.78. AandBplay a series of games. Each game is inde-
pendently won by Awith probability pand by Bwith
probability 1 −p. They stop when the total number of
wins of one of the players is two greater than that of the
other player. The player with the greater number of total
wins is declared the winner of the series.
(a)Find the probability that a total of 4 games are played.
(b)Find the probability that A is the winner of the series.
3.79. In successive rolls of a pair of fair dice, what is the
probability of getting 2 sevens before 6 even numbers?
3.80. In a certain contest, the players are of equal skill and
the probability is1
2that a speciﬁed one of the two contes-
tants will be the victor. In a group of 2nplayers, the players
are paired off against each other at random. The 2n−1win-
ners are again paired off randomly, and so on, until a single
winner remains. Consider two speciﬁed contestants, Aand
B, and deﬁne the events Ai,i…n,Eby
Ai:Aplays in exactly icontests
E:AandBnever play each other
(a)Find P(Ai),i=1,...,n.
(b)Find P(E).
(c)LetPn=P(E). Show that
Pn=1
2n−1+2n−2
2n−1/parenleftbigg1
2/parenrightbigg2
Pn−1
and use this formula to check the answer you obtained in
part (b).
Hint:F i n d P(E)by conditioning on which of the events
Ai,i=1,...,noccur. In simplifying your answer, use the
algebraic identity
n−1/summationdisplay
i=1ixi−1=1−nxn−1+(n−1)xn
(1−x)2
For another approach to solving this problem, note thatt h e r ea r eat o t a lo f2
n−1 games played.
(d)Explain why 2n−1 games are played.
Number these games, and let Bidenote the event that A
andBplay each other in game i,i=1,...,2n−1.
(e)What is P(Bi)?
(f)Use part (e) to ﬁnd P(E).
3.81. An investor owns shares in a stock whose present
value is 25. She has decided that she must sell her stock if it
goes either down to 10 or up to 40. If each change of price
is either up 1 point with probability .55 or down 1 pointwith probability .45, and the successive changes are inde-pendent, what is the probability that the investor retires awinner?
3.82. AandBﬂip coins. Astarts and continues ﬂipping
until a tail occurs, at which point Bstarts ﬂipping and con-
tinues until there is a tail. Then Atakes over, and so on.
LetP
1be the probability of the coin landing on heads
when Aﬂips and P2when Bﬂips. The winner of the game
is the ﬁrst one to get
(a)2 heads in a row;
(b)a total of 2 heads;
(c)3 heads in a row;
(d)a total of 3 heads.
In each case, ﬁnd the probability that Awins.
3.83. DieAhas 4 red and 2 white faces, whereas die Bhas
2 red and 4 white faces. A fair coin is ﬂipped once. If it
lands on heads, the game continues with die A; if it lands
on tails, then die Bis to be used.
(a)Show that the probability of red at any throw is1
2.
(b)If the ﬁrst two throws result in red, what is the proba-
bility of red at the third throw?
(c)If red turns up at the ﬁrst two throws, what is the prob-
ability that it is die Athat is being used?
3.84. An urn contains 12 balls, of which 4 are white. Three
players— A,B,a n d C—successively draw from the urn, A
ﬁrst, then B,t h e n C,t h e n A, and so on. The winner is the
ﬁrst one to draw a white ball. Find the probability of win-
ning for each player if
(a)each ball is replaced after it is drawn;
(b)the balls that are withdrawn are not replaced.
3.85. Repeat Problem 3.84 when each of the 3 players
selects from his own urn. That is, suppose that there are
3 different urns of 12 balls with 4 white balls in each urn.
3.86. LetS={1, 2,...,n}and suppose that AandBare,
independently, equally likely to be any of the 2nsubsets
(including the null set and Sitself) of S.
(a)Show that
P{A(B}=/parenleftbigg3
4/parenrightbiggn
Hint:L e tN (B)denote the number of elements in B.U s e
P{A(B}=n/summationdisplay
i=0P{A(B|N(B)=i}P{N(B)=i}
Show that P{AB=Ø}=/parenleftBig
3
4/parenrightBign
.
3.87. Consider Example 2a, but now suppose that when
the key is in a certain pocket, there is a 10 percent chance
that a search of that pocket will not ﬁnd the key. Let R
andLbe, respectively, the events that the key is in the

<<<PAGE 119>>>

106 Chapter 3 Conditional Probability and Independence
right-hand pocket of the jacket and that it is in the left-
hand pocket. Also, let SRbe the event that a search of
the right-hand jacket pocket will be successful in ﬁnding
the key, and let ULbe the event that a search of the left-
hand jacket pocket will be unsuccessful and, thus, not ﬁnd
the key. Find P(SR|UL), the conditional probability that a
search of the right-hand pocket will ﬁnd the key given thata search of the left-hand pocket did not, by
(a)using the identity
P(S
R|UL)=P(SRUL)
P(UL)
determining P(SRUL)by conditioning on whether or not
the key is in the right-hand pocket, and determining P(UL)
by conditioning on whether or not the key is in the left-
hand pocket;
(b)using the identity
P(SR|UL)=P(SR|RU L)P(R|U L)
+P(SR|RcUL)P(Rc|UL)
3.88. In Example 5e, what is the conditional probability
that the ith coin was selected given that the ﬁrst ntrials
all result in heads?3.89. In Laplace’s rule of succession (Example 5e), are the
outcomes of the successive ﬂips independent? Explain.
3.90. A person tried by a 3-judge panel is declared guilty
if at least 2 judges cast votes of guilty. Suppose that when
the defendant is in fact guilty, each judge will indepen-
dently vote guilty with probability .7, whereas when the
defendant is in fact innocent, this probability drops to.2. If 70 percent of defendants are guilty, compute the
conditional probability that judge number 3 votes guilty
given that
(a)judges 1 and 2 vote guilty;
(b)judges 1 and 2 cast 1 guilty and 1 not guilty vote;
(c)judges 1 and 2 both cast not guilty votes.
LetE
i,i=1, 2, 3 denote the event that judge icasts a guilty
vote. Are these events independent? Are they condition-
ally independent? Explain.
3.91. Suppose that nindependent trials, each of which
results in any of the outcomes 0, 1, or 2, with respective
probabilities p0,p1,a n d p2,/summationtext2
i=0pi=1, are performed.
Find the probability that outcomes 1 and 2 both occur at
least once.
Theoretical Exercises
3.1.Show that if P(A)> 0, then
P(AB|A) ÚP(AB|A ∪B)
3.2.LetA(B. Express the following probabilities as sim-
ply as possible:
P(A|B), P(A|Bc),P(B|A), P(B|Ac)
3.3.Consider a school community of mfamilies, with niof
them having ichildren, i=1,...,k,k/summationtext
i=1ni=m. Consider
the following two methods for choosing a child:
1. Choose one of the mfamilies at random and then ran-
domly choose a child from that family.
2. Choose one of thek/summationtext
i=1inichildren at random.
Show that method 1 is more likely than method 2 to result
in the choice of a ﬁrstborn child.
Hint: In solving this problem, you will need to show that
k/summationdisplay
i=1inik/summationdisplay
j=1nj
jÚk/summationdisplay
i=1nik/summationdisplay
j=1njTo do so, multiply the sums and show that for all pairs i,j,
the coefﬁcient of the term ninjis greater in the expression
on the left than in the one on the right.
3.4.A ball is in any one of nboxes and is in the ith box
with probability Pi. If the ball is in box i, a search of that
box will uncover it with probability αi. Show that the con-
ditional probability that the ball is in box j, given that a
search of box idid not uncover it, is
Pj
1−αiPiifjZi
(1−αi)Pi
1−αiPiifj=i
3.5. (a)Prove that if EandFare mutually exclusive, then
P(E|E∪F)=P(E)
P(E)+P(F)
(b)Prove that if Ei,iÚ1 are mutually exclusive, then
P(Ej|∪q
i=1Ei)=P(Ej)/summationtextq
i=1P(Ei)

<<<PAGE 120>>>

A First Course in Probability 107
3.6.Prove that if E1,E2,...,Enare independent events,
then
P(E1∪E2∪ ··· ∪ En)=1−n/productdisplay
i=1[1−P(Ei)]
3.7.(a)An urn contains nwhite and mblack balls. The
balls are withdrawn one at a time until only those of the
same color are left. Show that with probability n/(n+m),
they are all white.
Hint: Imagine that the experiment continues until all the
balls are removed, and consider the last ball withdrawn.
(b)A pond contains 3 distinct species of ﬁsh, which we
will call the Red, Blue, and Green ﬁsh. There are rRed, b
Blue, and gGreen ﬁsh. Suppose that the ﬁsh are removed
from the pond in a random order. (That is, each selection is
equally likely to be any of the remaining ﬁsh.) What is the
probability that the Red ﬁsh are the ﬁrst species to becomeextinct in the pond?
Hint:W r i t eP {R}= P{RBG}+ P{RGB}, and compute the
probabilities on the right by ﬁrst conditioning on the last
species to be removed.
3.8.LetA,B,a n d Cbe events relating to the experiment
of rolling a pair of dice.
(a)If
P(A|C)>P(B|C)and P(A|C
c)>P (B|Cc)
either prove that P(A)>P (B)or give a counterexample
by deﬁning events A,B,a n d Cfor which that relationship
is not true.
(b)If
P(A|C)>P(A|Cc)and P(B|C)>P(B|Cc)
either prove that P(AB|C )> P(AB|Cc)or give a coun-
terexample by deﬁning events A,B,a n d Cfor which that
relationship is not true.
Hint:L e t Cbe the event that the sum of a pair of dice is
10; let Abe the event that the ﬁrst die lands on 6; let Bbe
the event that the second die lands on 6.
3.9.Consider two independent tosses of a fair coin. Let
Abe the event that the ﬁrst toss results in heads, let B
be the event that the second toss results in heads, and let
Cbe the event that in both tosses the coin lands on the
same side. Show that the events A,B,a n dC are pairwise
independent—that is, AandBare independent, AandC
are independent, and BandCare independent—but not
independent.
3.10. Two percent of women age 45 who participate in rou-
tine screening have breast cancer. Ninety percent of those
with breast cancer have positive mammographies. Eight
percent of the women who do not have breast cancer will
also have positive mammographies. Given that a womanhas a positive mammography, what is the probability shehas breast cancer?
3.11. In each of nindependent tosses of a coin, the coin
lands on heads with probability p. How large need nbe
so that the probability of obtaining at least one head is at
least
1
2?
3.12. Show that 0 …ai…1,i=1, 2,...,t h e n
q/summationdisplay
i=1⎡
⎢⎣aii−1/productdisplay
j=1(1−aj)⎤
⎥⎦+q/productdisplay
i=1(1−ai)=1
Hint: Suppose that an inﬁnite number of coins are to be
ﬂipped. Let aibe the probability that the ith coin lands on
heads, and consider when the ﬁrst head occurs.
3.13. The probability of getting a head on a single toss of
ac o i ni sp. Suppose that Astarts and continues to ﬂip the
coin until a tail shows up, at which point Bstarts ﬂipping.
Then Bcontinues to ﬂip until a tail comes up, at which
point Atakes over, and so on. Let Pn,mdenote the prob-
ability that Aaccumulates a total of nheads before B
accumulates m. Show that
Pn,m=pPn−1,m+(1−p)(1−Pm,n)
*3.14. Suppose that you are gambling against an inﬁnitely
rich adversary and at each stage you either win or lose 1
unit with respective probabilities pand 1 −p. Show that
the probability that you eventually go broke is
1i f p…1
2
(q/p)iifp>1
2
where q=1−pand where iis your initial fortune.
3.15. Independent trials that result in a success with prob-
ability pare successively performed until a total of rsuc-
cesses is obtained. Show that the probability that exactly n
trials are required is
/parenleftbigg
n−1
r−1/parenrightbigg
pr(1−p)n−r
Use this result to solve the problem of the points (Exam-
ple 4j).
Hint:I no r d e rf o ri tt ot a k en trials to obtain rsuccesses,
how many successes must occur in the ﬁrst n−1 trials?
3.16. Independent trials that result in a success with
probability pand a failure with probability 1 −p
are called Bernoulli trials.L e t Pndenote the probability
that nBernoulli trials result in an even number of suc-
cesses (0 being considered an even number). Show that
Pn=p(1−Pn−1)+(1−p)P n−1nÚ1

<<<PAGE 121>>>

108 Chapter 3 Conditional Probability and Independence
and use this formula to prove (by induction) that
Pn=1+(1−2p)n
2
3.17. Suppose that nindependent trials are performed,
with trial ibeing a success with probability 1/(2i +1).
LetPndenote the probability that the total number of suc-
cesses that result is an odd number.
(a)Find Pnforn=1, 2, 3, 4, 5.
(b)Conjecture a general formula for Pn.
(c)Derive a formula for Pnin terms of Pn−1.
(d)Verify that your conjecture in part (b) satisﬁes the
recursive formula in part (c). Because the recursive for-
mula has a unique solution, this then proves that yourconjecture is correct.
3.18. LetQ
ndenote the probability that no run of 3 con-
secutive heads appears in ntosses of a fair coin. Show that
Qn=1
2Qn−1+1
4Qn−2+1
8Qn−3
Q0=Q1=Q2=1
Find Q8.
Hint: Condition on the ﬁrst tail.
3.19. Consider the gambler’s ruin problem, with the excep-
tion that AandBagree to play no more than ngames. Let
Pn,idenote the probability that Awinds up with all the
money when Astarts with iandBstarts with N−i.D e r i v e
an equation for Pn,iin terms of Pn−1,i+1andPn−1,i−1,a n d
compute P7, 3,N=5.
3.20. Consider two urns, each containing both white and
black balls. The probabilities of drawing white balls from
the ﬁrst and second urns are, respectively, pandp/prime. Balls
are sequentially selected with replacement as follows:
With probability α, a ball is initially chosen from the ﬁrst
urn, and with probability 1 −α, it is chosen from the sec-
ond urn. The subsequent selections are then made accord-ing to the rule that whenever a white ball is drawn (and
replaced), the next ball is drawn from the same urn, but
when a black ball is drawn, the next ball is taken from theother urn. Let α
ndenote the probability that the nth ball
is chosen from the ﬁrst urn. Show that
αn+1=αn(p+p/prime−1)+1−p/primenÚ1
and use this formula to prove that
αn=1−p/prime
2−p−p/prime+/parenleftBigg
α−1−p/prime
2−p−p/prime/parenrightBigg
*(p+p/prime−1)n−1
LetPndenote the probability that the nth ball selected
is white. Find Pn. Also, compute lim n→qαnand
limn→qPn.3.21. The Ballot Problem. In an election, candidate A
receives nvotes and candidate Breceives mvotes, where
n>m. Assuming that all of the (n+m)!/n! m! orderings
of the votes are equally likely, let Pn,mdenote the proba-
bility that Ais always ahead in the counting of the votes.
(a)Compute P2,1,P3,1,P3,2,P4,1,P4,2,P4,3.
(b)Find Pn,1,Pn,2.
(c)On the basis of your results in parts (a) and (b), con-
jecture the value of Pn,m.
(d)Derive a recursion for Pn,min terms of Pn−1,mand
Pn,m−1by conditioning on who receives the last vote.
(e)Use part (d) to verify your conjecture in part (c) by an
induction proof on n+m.
3.22. As a simpliﬁed model for weather forecasting, sup-
pose that the weather (either wet or dry) tomorrow will
be the same as the weather today with probability p. Show
that the weather is dry on January 1, then Pn, the proba-
bility that it will be dry ndays later, satisﬁes
Pn=(2p−1)Pn−1+(1−p) nÚ1
P0=1
Prove that
Pn=1
2+1
2(2p−1)nnÚ0
3.23. A bag contains awhite and bblack balls. Balls are
chosen from the bag according to the following method:
1. A ball is chosen at random and is discarded.
2. A second ball is then chosen. If its color is different
from that of the preceding ball, it is replaced in the
bag and the process is repeated from the beginning. If
its color is the same, it is discarded and we start fromstep 2.
In other words, balls are sampled and discarded until achange in color occurs, at which point the last ball is
returned to the urn and the process starts anew. Let P
a,b
denote the probability that the last ball in the bag is white.Prove that
P
a,b=1
2
Hint: Use induction on kKa+b.
*3.24. A round-robin tournament of ncontestants is a tour-
nament in which each of the/parenleftbigg
n
2/parenrightbigg
pairs of contestants
play each other exactly once, with the outcome of any
play being that one of the contestants wins and the other
loses. For a ﬁxed integer k,k<n, a question of interest is
whether it is possible that the tournament outcome is such

<<<PAGE 122>>>

A First Course in Probability 109
that for every set of kplayers, there is a player who beat
each member of that set. Show that if
/parenleftbigg
n
k/parenrightbigg/bracketleftBigg
1−/parenleftbigg1
2/parenrightbiggk/bracketrightBiggn−k
<1
then such an outcome is possible.
Hint: Suppose that the results of the games are indepen-
dent and that each game is equally likely to be won by
either contestant. Number the/parenleftbigg
n
k/parenrightbigg
sets of kcontestants,
and let Bidenote the event that no contestant beat all of
thekplayers in the ith set. Then use Boole’s inequality to
bound P/parenleftBigg
/uniontext
iBi/parenrightBigg
.
3.25. Prove directly that
P(E|F)=P(E|FG)P (G|F)+P(E|FGc)P(Gc|F)
3.26. Prove the equivalence of Equations (5.11) and
(5.12).
3.27. Extend the deﬁnition of conditional independence to
more than 2 events.
3.28. Prove or give a counterexample. If E1andE2are
independent, then they are conditionally independent
given F.
3.29. In Laplace’s rule of succession (Example 5e), show
that if the ﬁrst nﬂips all result in heads, then theconditional probability that the next mﬂips also result in
all heads is (n+1)/(n +m+1).
3.30. In Laplace’s rule of succession (Example 5e), sup-
pose that the ﬁrst nﬂips resulted in rheads and n−rtails.
Show that the probability that the (n+1)ﬂip turns up
heads is (r+1)/(n +2). To do so, you will have to prove
and use the identity
/integraldisplay1
0yn(1−y)mdy=n!m!
(n+m+1)!
Hint: To prove the identity, let C(n,m)=/integraltext1
0yn(1−y)mdy.
Integrating by parts yields
C(n,m)=m
n+1C(n+1,m−1)
Starting with C(n,0)=1/(n+1), prove the identity by
induction on m.
3.31. Suppose that a nonmathematical, but philosophically
minded, friend of yours claims that Laplace’s rule of suc-
cession must be incorrect because it can lead to ridicu-
lous conclusions. “For instance,” says he, “the rule states
that if a boy is 10 years old, having lived 10 years, the
boy has probability11
12of living another year. On the
other hand, if the boy has an 80-year-old grandfather,
then, by Laplace’s rule, the grandfather has probability
81
82of surviving another year. However, this is ridiculous.
Clearly, the boy is more likely to survive an additional
year than the grandfather is.” How would you answer yourfriend?
Self-Test Problems and Exercises
3.1.In a game of bridge, West has no aces. What is the
probability of his partner’s having (a) no aces? (b) 2 ormore aces? (c) What would the probabilities be if Westhad exactly 1 ace?
3.2.The probability that a new car battery functions for
more than 10,000 miles is .8, the probability that it func-
tions for more than 20,000 miles is .4, and the probability
that it functions for more than 30,000 miles is .1. If a new
car battery is still working after 10,000 miles, what is theprobability that
(a)its total life will exceed 20,000 miles?
(b)its additional life will exceed 20,000 miles?
3.3.How can 20 balls, 10 white and 10 black, be put into
two urns so as to maximize the probability of drawing a
white ball if an urn is selected at random and a ball is
drawn at random from it?
3.4.UrnAcontains 2 white balls and 1 black ball, whereas
urnBcontains 1 white ball and 5 black balls. A ball is
drawn at random from urn Aa n dp l a c e di nu r n B.Aball is then drawn from urn B. It happens to be white.
What is the probability that the ball transferred was
white?
3.5.An urn has rred and wwhite balls that are randomly
removed one at a time. Let R
ibe the event that the ith ball
removed is red. Find
(a)P(Ri)
(b)P(R5|R3)
(c)P(R3|R5)
3.6.An urn contains bblack balls and rred balls. One of
the balls is drawn at random, but when it is put back in
the urn, cadditional balls of the same color are put in with
it. Now, suppose that we draw another ball. Show that the
probability that the ﬁrst ball was black, given that the sec-
ond ball drawn was red, is b/(b+r+c).
3.7.A friend randomly chooses two cards, without
replacement, from an ordinary deck of 52 playing cards. Ineach of the following situations, determine the conditional
probability that both cards are aces.

<<<PAGE 123>>>

110 Chapter 3 Conditional Probability and Independence
(a)You ask your friend if one of the cards is the ace of
spades, and your friend answers in the afﬁrmative.
(b)You ask your friend if the ﬁrst card selected is an ace,
and your friend answers in the afﬁrmative.(c)You ask your friend if the second card selected is an
ace, and your friend answers in the afﬁrmative.
(d)You ask your friend if either of the cards selected is an
ace, and your friend answers in the afﬁrmative.
3.8.Show that
P(H|E)
P(G|E)=P(H)
P(G)P(E|H)
P(E|G)
Suppose that, before new evidence is observed, the
hypothesis His three times as likely to be true as is the
hypothesis G. If the new evidence is twice as likely when
Gis true than it is when His true, which hypothesis is more
likely after the evidence has been observed?
3.9.You ask your neighbor to water a sickly plant while
you are on vacation. Without water, it will die with prob-
ability .8; with water, it will die with probability .15. You
are 90 percent certain that your neighbor will remember
to water the plant.
(a)What is the probability that the plant will be alive when
you return?
(b)If the plant is dead upon your return, what is the prob-
ability that your neighbor forgot to water it?
3.10. Six balls are to be randomly chosen from an urn con-
taining 8 red, 10 green, and 12 blue balls.
(a)What is the probability at least one red ball is chosen?
(b)Given that no red balls are chosen, what is the con-
ditional probability that there are exactly 2 green balls
among the 6 chosen?
3.11. A type C battery is in working condition with proba-
bility .7, whereas a type D battery is in working condition
with probability .4. A battery is randomly chosen from a
bin consisting of 8 type C and 6 type D batteries.
(a)What is the probability that the battery works?
(b)Given that the battery does not work, what is the con-
ditional probability that it was a type C battery?
3.12. Maria will take two books with her on a trip. Sup-
pose that the probability that she will like book 1 is .6, the
probability that she will like book 2 is .5, and the probabil-
ity that she will like both books is .4. Find the conditional
probability that she will like book 2 given that she did not
like book 1.
3.13. Balls are randomly removed from an urn that ini-
tially contains 20 red and 10 blue balls.
(a)What is the probability that all of the red balls are
removed before all of the blue ones have been removed?
Now suppose that the urn initially contains 20 red, 10 blue,
and 8 green balls.(b)Now what is the probability that all of the red balls are
removed before all of the blue ones have been removed?
(c)What is the probability that the colors are depleted in
the order blue, red, green?(d)What is the probability that the group of blue balls is
the ﬁrst of the three groups to be removed?
3.14. A coin having probability .8 of landing on heads is
ﬂipped. Aobserves the result—either heads or tails—and
rushes off to tell B. However, with probability .4,Awill
have forgotten the result by the time he reaches B.I fA
has forgotten, then, rather than admitting this to B,h ei s
equally likely to tell Bt h a tt h ec o i nl a n d e do nh e a d so r
that it landed tails. (If he does remember, then he tells B
the correct result.)
(a)What is the probability that Bis told that the coin
l a n d e do nh e a d s ?
(b)What is the probability that Bis told the correct result?
(c)Given that Bis told that the coin landed on heads, what
is the probability that it did in fact land on heads?
3.15. In a certain species of rats, black dominates over
brown. Suppose that a black rat with two black parents
has a brown sibling.
(a)What is the probability that this rat is a pure black
rat (as opposed to being a hybrid with one black and one
brown gene)?
(b)Suppose that when the black rat is mated with a brown
rat, all 5 of their offspring are black. Now what is the prob-
ability that the rat is a pure black rat?
3.16. (a)In Problem 3.66b, ﬁnd the probability that a cur-
rent ﬂows from AtoB, by conditioning on whether relay
1c l o s e s .
(b)Find the conditional probability that relay 3 is closed
given that a current ﬂows from AtoB.
3.17. For the k-out-of-n system described in Problem 3.67,
assume that each component independently works with
probability
1
2. Find the conditional probability that com-
ponent 1 is working, given that the system works, when
(a)k=1,n=2;
(b)k=2,n=3.
3.18. Mr. Jones has devised a gambling system for winning
at roulette. When he bets, he bets on red and places a
bet only when the 10 previous spins of the roulette have
landed on a black number. He reasons that his chance of
winning is quite large because the probability of 11 con-secutive spins resulting in black is quite small. What do
you think of this system?
3.19. Three players simultaneously toss coins. The coin
tossed by A(B)[C ] turns up heads with probability
P
1(P2)[P3]. If one person gets an outcome different from
those of the other two, then he is the odd man out. If there
is no odd man out, the players ﬂip again and continue to do

<<<PAGE 124>>>

A First Course in Probability 111
so until they get an odd man out. What is the probability
thatAwill be the odd man?
3.20. Suppose that there are npossible outcomes of a
trial, with outcome iresulting with probability pi,i=
1,...,n,n/summationtext
i=1pi=1. If two independent trials are observed,
what is the probability that the result of the second trial islarger than that of the ﬁrst?
3.21. IfAﬂipsn+1a n d Bﬂipsnfair coins, show that the
probability that Agets more heads than Bis
1
2.
Hint: Condition on which player has more heads after each
has ﬂipped ncoins. (There are three possibilities.)
3.22. Prove or give counterexamples to the following
statements:
(a)IfEis independent of FandEis independent of G,
then Eis independent of F∪G.
(b)IfEis independent of F,a n dE is independent of G,
andFG=Ø, then Eis independent of F∪G.
(c)IfEis independent of F,a n dF is independent of G,
andEis independent of FG,t h e n Gis independent of EF.
3.23. LetAandBbe events having positive probability.
State whether each of the following statements is (i) nec-
essarily true, (ii) necessarily false, or (iii) possibly true.
(a)IfAandBare mutually exclusive, then they are inde-
pendent.
(b)IfAandBare independent, then they are mutually
exclusive.(c)P(A)=P(B)=.6, and AandBare mutually exclusive.
(d)P(A)=P(B)=.6, and AandBare independent.
3.24. Rank the following from most likely to least likely to
occur:
1. A fair coin lands on heads.2. Three independent trials, each of which is a success with
probability .8, all result in successes.
3. Seven independent trials, each of which is a success with
probability .9, all result in successes.
3.25. Two local factories, AandB, produce radios. Each
radio produced at factory Ais defective with probability
.05, whereas each one produced at factory Bis defective
with probability .01. Suppose you purchase two radios that
were produced at the same factory, which is equally likely
to have been either factory Aor factory B. If the ﬁrst radiothat you check is defective, what is the conditional proba-
bility that the other one is also defective?
3.26. Show that if P(A|B) =1, then P(B
c|Ac)=1.
3.27. An urn initially contains 1 red and 1 blue ball. At
each stage, a ball is randomly withdrawn and replaced by
two other balls of the same color. (For instance, if the red
ball is initially chosen, then there would be 2 red and 1
blue balls in the urn when the next selection occurs.) Showby mathematical induction that the probability that there
are exactly ired balls in the urn after nstages have been
completed is
1
n+1,1…i…n+1.
3.28. A total of 2n c a r d s ,o fw h i c h2a r ea c e s ,a r et ob er a n -
domly divided among two players, with each player receiv-ingncards. Each player is then to declare, in sequence,
whether he or she has received any aces. What is theconditional probability that the second player has no
aces, given that the ﬁrst player declares in the afﬁrma-tive, when (a) n=2? (b) n=10? (c) n=100? To
what does the probability converge as ngoes to inﬁnity?
Why?
3.29. There are ndistinct types of coupons, and each
coupon obtained is, independently of prior types collected,
of type iwith probability p
i,/summationtextn
i=1pi=1.
(a)Ifncoupons are collected, what is the probability that
one of each type is obtained?
(b)Now suppose that p1=p2= ··· = pn=1/n.L e t Ei
be the event that there are no type icoupons among the
ncollected. Apply the inclusion–exclusion identity for the
probability of the union of events to P(∪iEi)to prove the
identity
n!=n/summationdisplay
k=0(−1)k/parenleftbiggn
k/parenrightbigg
(n−k)n
3.30. Show that for any events EandF,
P(E|E∪F)ÚP(E|F)
Hint: Compute P(E|E∪F)by conditioning on whether F
occurs.
3.31. There is a 60 percent chance that event Awill occur.
IfAdoes not occur, then there is a 10 percent chance that
Bwill occur.
What is the probability that at least one of the events Aor
Bwill occur?

<<<PAGE 125>>>

Chapter
Random Variables4
Contents
4.1 Random Variables
4.2 Discrete Random Variables
4.3 Expected Value
4.4 Expectation of a Function of a Random
Variable
4.5 Variance
4.6 The Bernoulli and Binomial RandomVariables4.7 The Poisson Random Variable
4.8 Other Discrete Probability Distributions
4.9 Expected Value of Sums of RandomVariables
4.10 Properties of the Cumulative Distribution
Function
4.1 Random Variables
When an experiment is performed, we are frequently interested mainly in some func-
tion of the outcome as opposed to the actual outcome itself. For instance, in tossing
dice, we are often interested in the sum of the two dice and are not really concerned
about the separate values of each die. That is, we may be interested in knowingthat the sum is 7 and may not be concerned over whether the actual outcome was
(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), or (6, 1). Also, in ﬂipping a coin, we may be inter-
ested in the total number of heads that occur and not care at all about the actualhead–tail sequence that results. These quantities of interest, or, more formally, these
real-valued functions deﬁned on the sample space, are known as random variables.
Because the value of a random variable is determined by the outcome of the
experiment, we may assign probabilities to the possible values of the randomvariable.
Example
1aSuppose that our experiment consists of tossing 3 fair coins. If we let Ydenote the
number of heads that appear, then Yis a random variable taking on one of the values
0, 1, 2, and 3 with respective probabilities
P{Y=0}=P {(T,T,T)}=1
8
P{Y=1}=P {(T,T,H),(T,H,T),(H,T,T)}=3
8
P{Y=2}=P {(T,H,H),(H,T,H),(H,H,T)}=3
8
P{Y=3}=P {(H,H,H)}=1
8
112

<<<PAGE 126>>>

A First Course in Probability 113
Since Ymust take on one of the values 0 through 3, we must have
1=P⎛
⎝3/uniondisplay
i=0{Y=i}⎞⎠=3/summationdisplay
i=0P{Y=i}
which, of course, is in accord with the preceding probabilities. .
Example
1bA life insurance agent has 2 elderly clients, each of whom has a life insurance policy
that pays $100,000 upon death. Let Ybe the event that the younger one dies in the
following year, and let Obe the event that the older one dies in the following year.
Assume that Yand Oare independent, with respective probabilities P(Y)=.05
and P(O)=.10.IfXdenotes the total amount of money (in units of $100, 000) that
will be paid out this year to any of these clients’ beneﬁciaries, then Xis a random
variable that takes on one of the possible values 0, 1, 2 with respective probabilities
P{X=0}= P(YcOc)=P(Yc)P(Oc)=(.95)(. 9)=.855
P{X=1}= P(YOc)+P(YcO)=(.05)(. 9)+(.95)(. 1)=.140
P{X=2}= P(YO)=(.05)(. 1)=.005 .
Example
1cFour balls are to be randomly selected, without replacement, from an urn that con-
tains 20 balls numbered 1 through 20. If Xis the largest numbered ball selected, then
Xis a random variable that takes on one of the values 4, 5, ...,2 0.Because each of
the/parenleftbig20
4/parenrightbig
possible selections of 4 of the 20 balls is equally likely, the probability that X
takes on each of its possible values is
P{X=i}=/parenleftbigi−1
3/parenrightbig
/parenleftbig20
4/parenrightbig,i=4,...,2 0
This is so because the number of selections that result in X=iis the number of
selections that result in ball numbered iand three of the balls numbered 1 through
i−1 being selected. As there are/parenleftbig1
1/parenrightbig/parenleftbigi−1
3/parenrightbig
such selections, the preceding equation
follows.
Suppose now that we want to determine P{X>10}. One way, of course, is to
just use the preceding to obtain
P{X>10}=20/summationdisplay
i=11P{X=i}=20/summationdisplay
i=11/parenleftbigi−1
3/parenrightbig
/parenleftbig20
4/parenrightbig
However, a more direct approach for determining P(X>10)w o u l db et ou s e
P{X>10}= 1−P{X…10}= 1−/parenleftbig10
4/parenrightbig
/parenleftbig20
4/parenrightbig
where the preceding results because Xwill be less than or equal to 10 when the 4
balls chosen are among balls numbered 1 through 10. .
Example
1dIndependent trials consisting of the ﬂipping of a coin having probability pof coming
up heads are continually performed until either a head occurs or a total of nﬂips is
made. If we let Xdenote the number of times the coin is ﬂipped, then Xis a random
variable taking on one of the values 1, 2, 3, ...,nwith respective probabilities

<<<PAGE 127>>>

114 Chapter 4 Random Variables
P{X=1}= P{H}=p
P{X=2}= P{(T,H)}=(1 −p)p
P{X=3}= P{(T,T,H)}=(1 −p)2p
#
##
P{X=n−1}= P{(T,T,...,T,/bracehtipupleft
/bracehtipdownright/bracehtipdownleft/bracehtipupright
n−2H)}=(1 −p)n−2p
P{X=n}= P{(T,T,...,T,/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
n−1T),(T,T,...,T,/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
n−1H)}=(1 −p)n−1
As a check, note that
P⎛
⎝n/uniondisplay
i=1{X=i}⎞⎠=n/summationdisplay
i=1P{X=i}
=n−1/summationdisplay
i=1p(1−p)i−1+(1−p)n−1
=p/bracketleftBigg
1−(1−p)n−1
1−(1−p)/bracketrightBigg
+(1−p)n−1
=1−(1−p)n−1+(1−p)n−1
=1 .
Example
1eSuppose that there are Ndistinct types of coupons and that each time one obtains
a coupon, it is, independently of previous selections, equally likely to be any one of
theNtypes. One random variable of interest is T, the number of coupons that need
to be collected until one obtains a complete set of at least one of each type. Ratherthan derive P{T=n}directly, let us start by considering the probability that Tis
greater than n.T od os o ,ﬁ xn and deﬁne the events A
1,A2,...,ANas follows: Aj
is the event that no type jcoupon is contained among the ﬁrst ncoupons collected,
j=1,...,N. Hence,
P{T>n}= P⎛
⎜⎝N/uniondisplay
j=1Aj⎞
⎟⎠
=/summationdisplay
jP(Aj)−/summationdisplay/summationdisplay
j1<j2P(Aj1Aj2)+ ···
+(−1)k+1/summationdisplay/summationdisplay/summationdisplay
j1<j2<···<j kP(Aj1Aj2···A jk)···
+(−1)N+1P(A1A2···AN)

<<<PAGE 128>>>

A First Course in Probability 115
Now, Ajwill occur if each of the ncoupons collected is not of type j. Since each of the
coupons will not be of type jwith probability (N−1)/N , we have, by the assumed
independence of the types of successive coupons,
P(Aj)=/parenleftbiggN−1
N/parenrightbiggn
Also, the event Aj1Aj2will occur if none of the ﬁrst ncoupons collected is of either
type j1or type j2. Thus, again using independence, we see that
P(Aj1Aj2)=/parenleftbiggN−2
N/parenrightbiggn
The same reasoning gives
P(Aj1Aj2···A jk)=/parenleftbiggN−k
N/parenrightbiggn
and we see that for n>0,
P{T>n}=N/parenleftbiggN−1
N/parenrightbiggn
−/parenleftBigg
N
2/parenrightBigg/parenleftbiggN−2
N/parenrightbiggn
+/parenleftBigg
N
3/parenrightBigg/parenleftbiggN−3
N/parenrightbiggn
− ···
+(−1)N/parenleftBigg
N
N−1/parenrightBigg/parenleftbigg1
N/parenrightbiggn
=N−1/summationdisplay
i=1/parenleftBigg
N
i/parenrightBigg/parenleftbiggN−i
N/parenrightbiggn
(−1)i+1(1.1)
The probability that Tequals ncan now be obtained from the preceding formula by
the use of
P{T>n−1}=P {T=n}+P {T>n}
or, equivalently,
P{T=n}= P{T>n−1}−P {T>n}
Another random variable of interest is the number of distinct types of coupons
that are contained in the ﬁrst nselections—call this random variable Dn. To compute
P{Dn=k}, let us start by ﬁxing attention on a particular set of kdistinct types,
and let us then determine the probability that this set constitutes the set of distinct
types obtained in the ﬁrst nselections. Now, in order for this to be the situation, it is
necessary and sufﬁcient that of the ﬁrst ncoupons obtained,
A: each is one of these ktypes
B: each of these ktypes is represented
Now, each coupon selected will be one of the ktypes with probability k/N,s ot h e
probability that Awill be valid is (k/N)n. Also, given that a coupon is of one of the
ktypes under consideration, it is easy to see that it is equally likely to be of any one
of these ktypes. Hence, the conditional probability of Bgiven that Aoccurs is the
same as the probability that a set of ncoupons, each equally likely to be any of k
possible types, contains a complete set of all ktypes. But this is just the probability
that the number needed to amass a complete set, when choosing among ktypes, is
less than or equal to nand is thus obtainable from Equation (1.1) with kreplacing
N. Thus, we have

<<<PAGE 129>>>

116 Chapter 4 Random Variables
P(A)=/parenleftbiggk
N/parenrightbiggn
P(B|A) =1−k−1/summationdisplay
i=1/parenleftBigg
k
i/parenrightBigg/parenleftbiggk−i
k/parenrightbiggn
(−1)i+1
Finally, as there are/parenleftBigg
N
k/parenrightBigg
possible choices for the set of ktypes, we arrive at
P{Dn=k}=/parenleftBigg
N
k/parenrightBigg
P(AB)
=/parenleftBigg
N
k/parenrightBigg/parenleftbiggk
N/parenrightbiggn⎡
⎣1−k−1/summationdisplay
i=1/parenleftBigg
k
i/parenrightBigg/parenleftbiggk−i
k/parenrightbiggn
(−1)i+1⎤⎦
Remark Since one must collect at least Ncoupons to obtain a complete set, it
follows that P{T>n}= 1i fn<N. Therefore, from Equation (1.1), we obtain the
interesting combinatorial identity that for integers 1 …n<N,
N−1/summationdisplay
i=1/parenleftBigg
N
i/parenrightBigg/parenleftbiggN−i
N/parenrightbiggn
(−1)i+1=1
which can be written as
N−1/summationdisplay
i=0/parenleftBigg
N
i/parenrightBigg/parenleftbiggN−i
N/parenrightbiggn
(−1)i+1=0
or, upon multiplying by (−1)NNnand letting j=N−i,
N/summationdisplay
j=1/parenleftBigg
N
j/parenrightBigg
jn(−1)j−1=01 …n<N .
For a random variable X, the function Fdeﬁned by
F(x)=P{X…x}− q<x<q
is called the cumulative distribution function or, more simply, the distribution func-
tion ofX. Thus, the distribution function speciﬁes, for all real values x, the probabil-
ity that the random variable is less than or equal to x.
Now, suppose that a…b. Then, because the event {X…a}is contained in the
event {X…b}, it follows that F(a), the probability of the former, is less than or equal
toF(b), the probability of the latter. In other words, F(x)is a nondecreasing function
ofx. Other general properties of the distribution function are given in Section 4.10.
4.2 Discrete Random Variables
A random variable that can take on at most a countable number of possible values is
said to be discrete. For a discrete random variable X, we deﬁne the probability mass
function p(a) ofXby
p(a)=P{X=a}

<<<PAGE 130>>>

A First Course in Probability 117
The probability mass function p(a) is positive for at most a countable number of
values of a. That is, if Xmust assume one of the values x1,x2,..., then
p(x i)Ú0f o r i=1, 2,...
p(x)=0 for all other values of x
Since Xmust take on one of the values xi, we have
q/summationdisplay
i=1p(x i)=1
It is often instructive to present the probability mass function in a graphical
format by plotting p(x i)on the y-axis against xion the x-axis. For instance, if the
probability mass function of Xis
p(0)=1
4p(1)=1
2p(2)=1
4
we can represent this function graphically as shown in Figure 4.1. Similarly, a graph
of the probability mass function of the random variable representing the sum when
two dice are rolled looks like Figure 4.2.
1–4
0xp(x)
1–21
12
Figure 4.1
1—36
0xp(x)
2—36
126—36
3—364—365—36
3 4 56 7 8 9 10 11 12
Figure 4.2

<<<PAGE 131>>>

118 Chapter 4 Random Variables
Example
2aThe probability mass function of a random variable Xis given by p(i)=cλi/i!,
i=0, 1, 2, ..., where λis some positive value. Find (a) P{X=0}and (b) P{X>2}.
Solution Sinceq/summationtext
i=0p(i)=1, we have
cq/summationdisplay
i=0λi
i!=1
which, because ex=q/summationtext
i=0xi/i!, implies that
ceλ=1o rc =e−λ
Hence,
(a) P{X=0}=e−λλ0/0!=e−λ
(b) P{X>2}=1 −P{X…2}=1 −P{X=0}− P{X=1}
−P{X=2}
=1−e−λ−λe−λ−λ2e−λ
2 .
The cumulative distribution function Fcan be expressed in terms of p(a) by
F(a)=/summationdisplay
allx…ap(x)
IfXis a discrete random variable whose possible values are x1,x2,x3,..., where
x1<x2<x3<···, then the distribution function FofXis a step function. That
is, the value of Fis constant in the intervals (xi−1,xi)and then takes a step (or jump)
of size p(x i)atxi. For instance, if Xhas a probability mass function given by
p(1)=1
4p(2)=1
2p(3)=1
8p(4)=1
8
then its cumulative distribution function is
F(a)=⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩0a<1
1
41…a<2
3
42…a<3
7
83…a<4
14…a
This function is depicted graphically in Figure 4.3.
1–41
123aF(a)
43–47–8
Figure 4.3

<<<PAGE 132>>>

A First Course in Probability 119
Note that the size of the step at any of the values 1, 2, 3, and 4 is equal to the
probability that Xassumes that particular value.
4.3 Expected Value
One of the most important concepts in probability theory is that of the expectation
of a random variable. If Xis a discrete random variable having a probability mass
function p(x), then the expectation ,o rt h eexpected value,o f X, denoted by E[X], is
deﬁned by
E[X]=/summationdisplay
x:p(x)>0xp(x)
In words, the expected value of Xis a weighted average of the possible values that
Xcan take on, each value being weighted by the probability that Xassumes it. For
instance, on the one hand, if the probability mass function of Xis given by
p(0)=1
2=p(1)
then
E[X]=0/parenleftbigg1
2/parenrightbigg
+1/parenleftbigg1
2/parenrightbigg
=1
2
is just the ordinary average of the two possible values, 0 and 1, that Xcan assume.
On the other hand, if
p(0)=1
3p(1)=2
3
then
E[X]=0/parenleftbigg1
3/parenrightbigg
+1/parenleftbigg2
3/parenrightbigg
=2
3
is a weighted average of the two possible values 0 and 1, where the value 1 is given
twice as much weight as the value 0, since p(1)=2p(0).
Another motivation of the deﬁnition of expectation is provided by the frequency
interpretation of probabilities. This interpretation (partially justiﬁed by the stronglaw of large numbers, to be presented in Chapter 8) assumes that if an inﬁnite
sequence of independent replications of an experiment is performed, then, for anyevent E, the proportion of time that Eoccurs will be P(E). Now, consider a random
variable Xthat must take on one of the values x
1,x2,...xnwith respective probabil-
ities p(x 1),p(x 2),...,p(x n), and think of Xas representing our winnings in a single
game of chance. That is, with probability p(x i),w es h a l lw i n xiunits i=1, 2,...,n.B y
the frequency interpretation, if we play this game continually, then the proportionof time that we win x
iwill be p(x i). Since this is true for all i,i=1, 2,...,n, it follows
that our average winnings per game will be
n/summationdisplay
i=1xip(x i)=E[X]
Example
3aFind E[X], where Xis the outcome when we roll a fair die.
Solution Since p(1)=p(2)=p(3)=p(4)=p(5)=p(6)=1
6, we obtain
E[X]=1/parenleftbigg1
6/parenrightbigg
+2/parenleftbigg1
6/parenrightbigg
+3/parenleftbigg1
6/parenrightbigg
+4/parenleftbigg1
6/parenrightbigg
+5/parenleftbigg1
6/parenrightbigg
+6/parenleftbigg1
6/parenrightbigg
=7
2 .

<<<PAGE 133>>>

120 Chapter 4 Random Variables
Example
3bWe say that Iis an indicator variable for the event Aif
I=/braceleftBigg
1i f Aoccurs
0i f Acoccurs
Find E[I].
Solution Since p(1)=P(A), p(0)=1−P(A), we have
E[I]=P(A)
That is, the expected value of the indicator variable for the event Ais equal to the
probability that Aoccurs. .
Example
3cA contestant on a quiz show is presented with two questions, questions 1 and 2,
which he is to attempt to answer in some order he chooses. If he decides to try
question iﬁrst, then he will be allowed to go on to question j,jZi, only if his answer
to question iis correct. If his initial answer is incorrect, he is not allowed to answer
the other question. The contestant is to receive Vidollars if he answers question
icorrectly, i=1, 2. For instance, he will receive V1+V2dollars if he answers
both questions correctly. If the probability that he knows the answer to question iis
Pi,i=1, 2, which question should he attempt to answer ﬁrst so as to maximize his
expected winnings? Assume that the events Ei,i=1, 2, that he knows the answer to
question iare independent events.
Solution On the one hand, if he attempts to answer question 1 ﬁrst, then he will win
0 with probability 1 −P1
V1 with probability P1(1−P2)
V1+V2with probability P1P2
Hence, his expected winnings in this case will be
V1P1(1−P2)+(V1+V2)P1P2
On the other hand, if he attempts to answer question 2 ﬁrst, his expected winnings
will be
V2P2(1−P1)+(V1+V2)P1P2
Therefore, it is better to try question 1 ﬁrst if
V1P1(1−P2)ÚV2P2(1−P1)
or, equivalently, if
V1P1
1−P1ÚV2P2
1−P2
For example, if he is 60 percent certain of answering question 1, worth $200, correctlyand he is 80 percent certain of answering question 2, worth $100, correctly, then heshould attempt to answer question 2 ﬁrst because
400=(100)(. 8)
.2>(200)(. 6)
.4=300 .

<<<PAGE 134>>>

A First Course in Probability 121
Example
3dA school class of 120 students is driven in 3 buses to a symphonic performance. There
are 36 students in one of the buses, 40 in another, and 44 in the third bus. When the
buses arrive, one of the 120 students is randomly chosen. Let Xdenote the number
of students on the bus of that randomly chosen student, and ﬁnd E[X].
Solution Since the randomly chosen student is equally likely to be any of the 120
students, it follows that
P{X=36}=36
120P{X=40}=40
120P{X=44}=44
120
Hence,
E[X]=36/parenleftbigg3
10/parenrightbigg
+40/parenleftbigg1
3/parenrightbigg
+44/parenleftbigg11
30/parenrightbigg
=1208
30=40.2667
However, the average number of students on a bus is 120 /3=40, showing that
the expected number of students on the bus of a randomly chosen student is larger
than the average number of students on a bus. This is a general phenomenon, and
it occurs because the more students there are on a bus, the more likely it is thata randomly chosen student would have been on that bus. As a result, buses with
many students are given more weight than those with fewer students. (See Self-Test
Problem 4.4) .
Remark The probability concept of expectation is analogous to the physical con-
cept of the center of gravity of a distribution of mass. Consider a discrete random
variable Xhaving probability mass function p(x i),iÚ1. If we now imagine a weight-
less rod in which weights with mass p(x i),iÚ1, are located at the points xi,iÚ1
(see Figure 4.4), then the point at which the rod would be in balance is known as thecenter of gravity. For those readers acquainted with elementary statics, it is now asimple matter to show that this point is at E[X].
†.
–1 0 2 ^ 1
p(–1) = .10, p(0) = .25, p(2) = .35 p(1) = .30,
^ = center of gravity = .9
Figure 4.4
4.4 Expectation of a Function of a Random Variable
Suppose that we are given a discrete random variable along with its probability mass
function and that we want to compute the expected value of some function of X,s a y ,
g(X). How can we accomplish this? One way is as follows: Since g(X)is itself a dis-
crete random variable, it has a probability mass function, which can be determinedfrom the probability mass function of X. Once we have determined the probability
mass function of g(X), we can compute E[g(X)] by using the deﬁnition of expected
value.
†To prove this, we must show that the sum of the torques tending to turn the point around E[X] is equal to 0.
That is, we must show that 0 =/summationtext
i(xi−E[X])p(x i), which is immediate.

<<<PAGE 135>>>

122 Chapter 4 Random Variables
Example
4aLetXdenote a random variable that takes on any of the values −1, 0, and 1 with
respective probabilities
P{X=−1}=.2P{X=0}=.5P{X=1}=.3
Compute E[X2].
Solution LetY=X2. Then the probability mass function of Yis given by
P{Y=1}=P {X=−1}+ P{X=1}=.5
P{Y=0}=P {X=0}=.5
Hence,
E[X2]=E[Y]=1(.5)+0(.5)=.5
Note that
.5=E[X2]Z(E[X])2=.01 .
Although the preceding procedure will always enable us to compute the expec-
ted value of any function of Xfrom a knowledge of the probability mass function
ofX, there is another way of thinking about E[g(X)]: Since g(X)will equal g(x)
whenever Xis equal to x, it seems reasonable that E[g(X)] should just be a weighted
average of the values g(x),w i t h g(x)being weighted by the probability that Xis equal
tox. That is, the following result is quite intuitive.
Proposition
4.1IfXis a discrete random variable that takes on one of the values xi,iÚ1, with
respective probabilities p(x i), then, for any real-valued function g,
E[g(X)]=/summationdisplay
ig(xi)p(x i)
Before proving this proposition, let us check that it is in accord with the results
of Example 4a. Applying it to that example yields
E{X2}=(−1)2(.2)+02(.5)+12(.3)
=1(.2+.3)+0(.5)
=.5
which is in agreement with the result given in Example 4a.
Proof of Proposition 4.1 The proof of Proposition 4.1 proceeds, as in the preceding
veriﬁcation, by grouping together all the terms in/summationtext
ig(xi)p(x i)having the same value
ofg(xi). Speciﬁcally, suppose that yj,jÚ1, represent the different values of g(xi),iÚ
1. Then, grouping all the g(xi)having the same value gives
/summationdisplay
ig(xi)p(x i)=/summationdisplay
j/summationdisplay
i:g(xi)=y jg(xi)p(x i)
=/summationdisplay
j/summationdisplay
i:g(xi)=y jyjp(x i)
=/summationdisplay
jyj/summationdisplay
i:g(xi)=y jp(x i)
=/summationdisplay
jyjP{g(X)=yj}
=E[g(X)] /square

<<<PAGE 136>>>

A First Course in Probability 123
Example
4bA product that is sold seasonally yields a net proﬁt of bdollars for each unit sold and
a net loss of /lscriptdollars for each unit left unsold when the season ends. The number
of units of the product that are ordered at a speciﬁc department store during any
season is a random variable having probability mass function p(i), iÚ0. If the store
must stock this product in advance, determine the number of units the store shouldstock so as to maximize its expected proﬁt.
Solution LetXdenote the number of units ordered. If sunits are stocked, then the
proﬁt—call it P(s)—can be expressed as
P(s)=bX−(s−X)/lscriptifX…s
=sb ifX>s
Hence, the expected proﬁt equals
E[P(s)]=s/summationdisplay
i=0[bi−(s−i)/lscript]p(i) +q/summationdisplay
i=s+1sbp(i)
=(b+/lscript)s/summationdisplay
i=0ip(i)−s/lscripts/summationdisplay
i=0p(i)+sb⎡
⎣1−s/summationdisplay
i=0p(i)⎤⎦
=(b+/lscript)s/summationdisplay
i=0ip(i)−(b+/lscript)ss/summationdisplay
i=0p(i)+sb
=sb+(b+/lscript)s/summationdisplay
i=0(i−s)p(i)
To determine the optimum value of s, let us investigate what happens to the proﬁt
when we increase sby 1 unit. By substitution, we see that the expected proﬁt in this
case is given by
E[P(s+1)]=b(s+1)+(b+/lscript)s+1/summationdisplay
i=0(i−s−1)p(i)
=b(s+1)+(b+/lscript)s/summationdisplay
i=0(i−s−1)p(i)
Therefore,
E[P(s+1)]−E[P(s)]=b−(b+/lscript)s/summationdisplay
i=0p(i)
Thus, stocking s+1 units will be better than stocking sunits whenever
s/summationdisplay
i=0p(i)<b
b+/lscript(4.1)
Because the left-hand side of Equation (4.1) is increasing in swhile the right-hand
side is constant, the inequality will be satisﬁed for all values of s…s∗, where s∗is the
largest value of ssatisfying Equation (4.1). Since
E[P(0)]<···<E[P(s∗)]<E[P(s∗+1)]>E[P(s∗+2)]>···
it follows that stocking s∗+1 items will lead to a maximum expected proﬁt. .

<<<PAGE 137>>>

124 Chapter 4 Random Variables
Example
4cUtility
Suppose that you must choose one of two possible actions, each of which can result
in any of nconsequences, denoted as C1,...,Cn. Suppose that if the ﬁrst action is
chosen, then consequence Ciwill result with probability pi,i=1,...,n, whereas
if the second action is chosen, then consequence Ciwill result with probability qi,
i=1,...,n, wheren/summationtext
i=1pi=n/summationtext
i=1qi=1. The following approach can be used to deter-
mine which action to choose: Start by assigning numerical values to the different
consequences. First, identify the least and the most desirable consequences—call
them cand C, respectively; give consequence cthe value 0 and give Cthe value 1.
Now consider any of the other n−2 consequences, say, Ci. To value this conse-
quence, imagine that you are given the choice between either receiving Cior taking
part in a random experiment that either earns you consequence Cwith probabil-
ityuor consequence cwith probability 1 −u. Clearly, your choice will depend on
the value of u. On the one hand, if u=1, then the experiment is certain to result
in consequence C, and since Cis the most desirable consequence, you will prefer
participating in the experiment to receiving Ci. On the other hand, if u=0, then
the experiment will result in the least desirable consequence—namely, c—so in this
case you will prefer the consequence Cito participating in the experiment. Now,
asudecreases from 1 to 0, it seems reasonable that your choice will at some point
switch from participating in the experiment to the certain return of Ci, and at that
critical switch point you will be indifferent between the two alternatives. Take that
indifference probability uas the value of the consequence Ci. In other words, the
value of Ciis that probability usuch that you are indifferent between either receiv-
ing the consequence Cior taking part in an experiment that returns consequence C
with probability uor consequence cwith probability 1 −u. We call this indifference
probability the utility of the consequence Ci, and we designate it as u(C i).
To determine which action is superior, we need to evaluate each one. Consider
the ﬁrst action, which results in consequence Ciwith probability pi,i=1,...,n.W e
can think of the result of this action as being determined by a two-stage experiment.In the ﬁrst stage, one of the values 1, ...,nis chosen according to the probabilities
p
1,...,pn; if value iis chosen, you receive consequence Ci. However, since Ciis
equivalent to obtaining consequence Cwith probability u(C i)or consequence cwith
probability 1 −u(C i), it follows that the result of the two-stage experiment is equiv-
alent to an experiment in which either consequence Cor consequence cis obtained,
with Cbeing obtained with probability
n/summationdisplay
i=1piu(C i)
Similarly, the result of choosing the second action is equivalent to taking part in anexperiment in which either consequence Cor consequence cis obtained, with C
being obtained with probability
n/summationdisplay
i=1qiu(C i)
Since Cis preferable to c, it follows that the ﬁrst action is preferable to the
second action if
n/summationdisplay
i=1piu(C i)>n/summationdisplay
i=1qiu(C i)

<<<PAGE 138>>>

A First Course in Probability 125
In other words, the worth of an action can be measured by the expected value of the
utility of its consequence, and the action with the largest expected utility is the most
preferable. .
A simple logical consequence of Proposition 4.1 is Corollary 4.1.
Corollary
4.1Ifaandbare constants, then
E[aX+b]=aE[X]+b
Proof
E[aX+b]=/summationdisplay
x:p(x)>0(ax+b)p(x)
=a/summationdisplay
x:p(x)>0xp(x) +b/summationdisplay
x:p(x)>0p(x)
=aE[X]+b
The expected value of a random variable X,E[X], is also referred to as the mean
or the ﬁrst moment of X. The quantity E[Xn],nÚ1, is called the nth moment of X.
By Proposition 4.1, we note that
E[Xn]=/summationdisplay
x:p(x)>0xnp(x)
4.5 Variance
Given a random variable Xalong with its distribution function F, it would be
extremely useful if we were able to summarize the essential properties of Fby
certain suitably deﬁned measures. One such measure would be E[X], the expected
value of X. However, although E[X] yields the weighted average of the possible
values of X, it does not tell us anything about the variation, or spread, of these val-
ues. For instance, although random variables W,Y, and Zhaving probability mass
functions determined by
W=0 with probability 1
Y=⎧
⎨
⎩−1 with probability1
2
+1 with probability1
2
Z=⎧
⎨
⎩−100 with probability1
2
+100 with probability1
2
all have the same expectation—namely, 0—there is a much greater spread in the
possible values of Ythan in those of W(which is a constant) and in the possible
values of Zthan in those of Y.
Because we expect Xto take on values around its mean E[X], it would appear
that a reasonable way of measuring the possible variation of Xwould be to look
at how far apart Xwould be from its mean, on the average. One possible way to
measure this variation would be to consider the quantity E[|X−μ|], where μ=
E[X]. However, it turns out to be mathematically inconvenient to deal with this
quantity, so a more tractable quantity is usually considered—namely, the expectationof the square of the difference between Xand its mean. We thus have the following
deﬁnition.

<<<PAGE 139>>>

126 Chapter 4 Random Variables
Deﬁnition
IfXis a random variable with mean μ, then the variance of X, denoted by
Var(X), is deﬁned by
Var(X)=E[(X−μ)2]
An alternative formula for Var (X)is derived as follows:
Var(X)=E[(X−μ)2]
=/summationdisplay
x(x−μ)2p(x)
=/summationdisplay
x(x2−2μx+μ2)p(x)
=/summationdisplay
xx2p(x)−2μ/summationdisplay
xxp(x) +μ2/summationdisplay
xp(x)
=E[X2]−2μ2+μ2
=E[X2]−μ2
That is,
Var(X)=E[X2]−(E[X])2
In words, the variance of Xis equal to the expected value of X2minus the square
of its expected value. In practice, this formula frequently offers the easiest way to
compute Var (X).
Example
5aCalculate Var (X)ifXrepresents the outcome when a fair die is rolled.
Solution It was shown in Example 3a that E[X]=7
2.A l s o ,
E[X2]=12/parenleftbigg1
6/parenrightbigg
+22/parenleftbigg1
6/parenrightbigg
+32/parenleftbigg1
6/parenrightbigg
+42/parenleftbigg1
6/parenrightbigg
+52/parenleftbigg1
6/parenrightbigg
+62/parenleftbigg1
6/parenrightbigg
=/parenleftbigg1
6/parenrightbigg
(91)
Hence,Var(X)=91
6−/parenleftbigg7
2/parenrightbigg2
=35
12.
A useful identity is that for any constants aandb,
Var(aX+b)=a2Var(X)
To prove this equality, let μ=E[X] and note from Corollary 4.1 that E[aX+b]=
aμ+b. Therefore,
Var(aX+b)=E[(aX +b−aμ−b)2]
=E[a2(X−μ)2]
=a2E[(X−μ)2]
=a2Var(X)
Remarks (a) Analogous to the means being the center of gravity of a distribution
of mass, the variance represents, in the terminology of mechanics, the moment ofinertia.

<<<PAGE 140>>>

A First Course in Probability 127
(b) The square root of the Var(X )is called the standard deviation ofX, and we
denote it by SD(X ). That is,
SD(X)=/radicalbig
Var(X)
Discrete random variables are often classiﬁed according to their probability
mass functions. In the next few sections, we consider some of the more common
types.
4.6 The Bernoulli and Binomial Random Variables
Suppose that a trial, or an experiment, whose outcome can be classiﬁed as either a
success or a failure is performed. If we let X=1 when the outcome is a success and
X=0 when it is a failure, then the probability mass function of Xis given by
p(0)=P{X=0}=1 −p
p(1)=P{X=1}=p(6.1)
where p,0…p…1, is the probability that the trial is a success.
A random variable Xis said to be a Bernoulli random variable (after the Swiss
mathematician James Bernoulli) if its probability mass function is given by Equa-
tions (6.1) for some p∈(0, 1).
Suppose now that nindependent trials, each of which results in a success with
probability por in a failure with probability 1 −p, are to be performed. If Xrepre-
sents the number of successes that occur in the ntrials, then Xis said to be a binomial
random variable with parameters (n, p). Thus, a Bernoulli random variable is just a
binomial random variable with parameters (1, p).
The probability mass function of a binomial random variable having parameters
(n,p) is given by
p(i)=/parenleftBigg
n
i/parenrightBigg
pi(1−p)n−ii=0, 1,...,n (6.2)
The validity of Equation (6.2) may be veriﬁed by ﬁrst noting that the probability ofany particular sequence of noutcomes containing isuccesses and n−ifailures is, by
the assumed independence of trials, p
i(1−p)n−i. Equation (6.2) then follows, since
there are/parenleftBigg
n
i/parenrightBigg
different sequences of the noutcomes leading to isuccesses and
n−ifailures. This perhaps can most easily be seen by noting that there are/parenleftBigg
n
i/parenrightBigg
different choices of the itrials that result in successes. For instance, if n=4,i=2,
then there are/parenleftBigg
42/parenrightBigg
=6 ways in which the four trials can result in two successes,
namely, any of the outcomes ( s,s,f,f), (s, f,s,f), (s, f,f,s), (f ,s,s,f), (f,s,f,s), and
(f,f,s,s), where the outcome (s, s,f,f) means, for instance, that the ﬁrst two trials
are successes and the last two failures. Since each of these outcomes has probabilityp
2(1−p)2of occurring, the desired probability of two successes in the four trials
is/parenleftBigg
42/parenrightBigg
p
2(1−p)2.

<<<PAGE 141>>>

128 Chapter 4 Random Variables
Note that, by the binomial theorem, the probabilities sum to 1; that is,
q/summationdisplay
i=0p(i)=n/summationdisplay
i=0/parenleftBigg
n
i/parenrightBigg
pi(1−p)n−i=[p+(1−p)]n=1
Example
6aFive fair coins are ﬂipped. If the outcomes are assumed independent, ﬁnd the prob-
ability mass function of the number of heads obtained.
Solution If we let Xequal the number of heads (successes) that appear, then X
is a binomial random variable with parameters/parenleftBig
n=5,p=1
2/parenrightBig
. Hence, by Equa-
tion (6.2),
P{X=0}=/parenleftBigg
5
0/parenrightBigg/parenleftbigg1
2/parenrightbigg0/parenleftbigg1
2/parenrightbigg5
=1
32
P{X=1}=/parenleftBigg
5
1/parenrightBigg/parenleftbigg1
2/parenrightbigg1/parenleftbigg1
2/parenrightbigg4
=5
32
P{X=2}=/parenleftBigg
5
2/parenrightBigg/parenleftbigg1
2/parenrightbigg2/parenleftbigg1
2/parenrightbigg3
=10
32
P{X=3}=/parenleftBigg
53/parenrightBigg/parenleftbigg1
2/parenrightbigg3/parenleftbigg1
2/parenrightbigg2
=10
32
P{X=4}=/parenleftBigg
5
4/parenrightBigg/parenleftbigg1
2/parenrightbigg4/parenleftbigg1
2/parenrightbigg1
=5
32
P{X=5}=/parenleftBigg
5
5/parenrightBigg/parenleftbigg1
2/parenrightbigg5/parenleftbigg1
2/parenrightbigg0
=1
32.
Example
6bIt is known that screws produced by a certain company will be defective with prob-
ability .01, independently of one another. The company sells the screws in packages
of 10 and offers a money-back guarantee that at most 1 of the 10 screws is defective.What proportion of packages sold must the company replace?
Solution IfXis the number of defective screws in a package, then Xis a binomial
random variable with parameters (10, .01). Hence, the probability that a package
will have to be replaced is
1−P{X=0}−P {X=1}=1 −/parenleftBigg
10
0/parenrightBigg
(.01)0(.99)10−/parenleftBigg
10
1/parenrightBigg
(.01)1(.99)9
L.004
Thus, only .4 percent of the packages will have to be replaced. .
Example
6cThe following gambling game, known as the wheel of fortune (or chuck-a-luck), is
quite popular at many carnivals and gambling casinos: A player bets on one of the
numbers 1 through 6. Three dice are then rolled, and if the number bet by the player
appears itimes, i=1, 2, 3, then the player wins iunits; if the number bet by the player
does not appear on any of the dice, then the player loses 1 unit. Is this game fair to
the player? (Actually, the game is played by spinning a wheel that comes to rest on
a slot labeled by three of the numbers 1 through 6, but this variant is mathematicallyequivalent to the dice version.)

<<<PAGE 142>>>

A First Course in Probability 129
Solution If we assume that the dice are fair and act independently of one another,
then the number of times that the number bet appears is a binomial random variable
with parameters/parenleftBig
3,1
6/parenrightBig
. Hence, letting Xdenote the player’s winnings in the game,
we have
P{X=−1}=/parenleftBigg
3
0/parenrightBigg/parenleftbigg1
6/parenrightbigg0/parenleftbigg5
6/parenrightbigg3
=125
216
P{X=1}=/parenleftBigg
3
1/parenrightBigg/parenleftbigg1
6/parenrightbigg1/parenleftbigg5
6/parenrightbigg2
=75
216
P{X=2}=/parenleftBigg
3
2/parenrightBigg/parenleftbigg1
6/parenrightbigg2/parenleftbigg5
6/parenrightbigg1
=15
216
P{X=3}=/parenleftBigg
33/parenrightBigg/parenleftbigg1
6/parenrightbigg3/parenleftbigg5
6/parenrightbigg0
=1
216
In order to determine whether or not this is a fair game for the player, let us
calculate E[X]. From the preceding probabilities, we obtain
E[X]=−125 +75+30+3
216
=−17
216
Hence, in the long run, the player will lose 17 units per every 216 games he plays. .
In the next example, we consider the simplest form of the theory of inheritance
as developed by Gregor Mendel (1822–1884).
Example
6dSuppose that a particular trait (such as eye color or left-handedness) of a person isclassiﬁed on the basis of one pair of genes, and suppose also that drepresents a domi-
nant gene and ra recessive gene. Thus, a person with ddgenes is purely dominant,
one with rris purely recessive, and one with rdis hybrid. The purely dominant and
the hybrid individuals are alike in appearance. Children receive 1 gene from eachparent. If, with respect to a particular trait, 2 hybrid parents have a total of 4 children,
what is the probability that 3 of the 4 children have the outward appearance of the
dominant gene?
(a) (b)Pure yellow
Yellow hybridy, y g, g
y, gPure green
Pure yellow HybridHybrid Hybrid
Hybridy1, g1
1
4
y1, g2 y1, y2 y2, g1y2, g2
g1, g2
Pure green141414
Figure 4.5 (a) Crossing pure yellow seeds with pure green seeds; (b) Crossing hybrid
ﬁrst-generation seeds.

<<<PAGE 143>>>

130 Chapter 4 Random Variables
The preceding Figure 4.5a and b shows what can happen when hybrid yellow (dom-
inant) and green (recessive) seeds are crossed.
Solution If we assume that each child is equally likely to inherit either of 2 genes
from each parent, the probabilities that the child of 2 hybrid parents will have dd,
rr, and rdpairs of genes are, respectively,1
4,1
4, and1
2. Hence, since an offspring will
have the outward appearance of the dominant gene if its gene pair is either ddorrd,
it follows that the number of such children is binomially distributed with parameters/parenleftBig
4,3
4/parenrightBig
. Thus, the desired probability is
/parenleftBigg
4
3/parenrightBigg/parenleftbigg3
4/parenrightbigg3/parenleftbigg1
4/parenrightbigg1
=27
64 .
Example
6eConsider a jury trial in which it takes 8 of the 12 jurors to convict the defendant;
that is, in order for the defendant to be convicted, at least 8 of the jurors must vote
him guilty. If we assume that jurors act independently and that whether or not thedefendant is guilty, each makes the right decision with probability θ, what is the
probability that the jury renders a correct decision?
Solution The problem, as stated, is incapable of solution, for there is not yet enough
information. For instance, if the defendant is innocent, the probability of the jury
rendering a correct decision is
12/summationdisplay
i=5/parenleftBigg
12
i/parenrightBigg
θi(1−θ)12−i
whereas, if he is guilty, the probability of a correct decision is
12/summationdisplay
i=8/parenleftBigg
12
i/parenrightBigg
θi(1−θ)12−i
Therefore, if αrepresents the probability that the defendant is guilty, then, by condi-
tioning on whether or not he is guilty, we obtain the probability that the jury rendersa correct decision:
α12/summationdisplay
i=8/parenleftBigg
12
i/parenrightBigg
θi(1−θ)12−i+(1−α)12/summationdisplay
i=5/parenleftBigg
12
i/parenrightBigg
θi(1−θ)12−i.
Example
6fA communication system consists of ncomponents, each of which will, indepen-
dently, function with probability p. The total system will be able to operate effec-
tively if at least one-half of its components function.
(a) For what values of pis a 5-component system more likely to operate effectively
than a 3-component system?
(b) In general, when is a (2k+1)-component system better than a (2k−1)-
component system?
Solution (a) Because the number of functioning components is a binomial random
variable with parameters ( n,p), it follows that the probability that a 5-component
system will be effective is
/parenleftBigg
5
3/parenrightBigg
p3(1−p)2+/parenleftBigg
54/parenrightBigg
p
4(1−p)+p5

<<<PAGE 144>>>

A First Course in Probability 131
whereas the corresponding probability for a 3-component system is
/parenleftBigg
3
2/parenrightBigg
p2(1−p)+p3
Hence, the 5-component system is better if
10p3(1−p)2+5p4(1−p)+p5>3p2(1−p)+p3
which reduces to
3(p−1)2(2p−1)>0
or
p>1
2
(b) In general, a system with 2 k+1 components will be better than one with
2k−1 components if (and only if) p>1
2. To prove this, consider a system of 2 k+1
components and let Xdenote the number of the ﬁrst 2k −1 that function. Then
P2k+1(effective )=P{XÚk+1}+ P{X=k}(1−(1−p)2)
+P{X=k−1}p2
which follows because the (2k+1)-component system will be effective if either
(i)XÚk+1;
(ii) X=kand at least one of the remaining 2 components function; or
(iii) X=k−1 and both of the next 2 components function.
Since
P2k−1(effective )=P{XÚk}
=P{X=k}+ P{XÚk+1}
we obtain
P2k+1(effective) −P2k−1(effective )
=P{X=k−1}p2−(1−p)2P{X=k}
=/parenleftBigg
2k−1
k−1/parenrightBigg
pk−1(1−p)kp2−(1−p)2/parenleftBigg
2k−1
k/parenrightBigg
pk(1−p)k−1
=/parenleftBigg
2k−1
k/parenrightBigg
pk(1−p)k[p−(1−p)] since/parenleftBigg
2k−1
k−1/parenrightBigg
=/parenleftBigg
2k−1
k/parenrightBigg
>03p>1
2.
4.6.1 Properties of Binomial Random Variables
We will now examine the properties of a binomial random variable with parametersnandp. To begin, let us compute its expected value and variance. To begin, note that
E[X
k]=n/summationdisplay
i=0ik/parenleftBigg
n
i/parenrightBigg
pi(1−p)n−i
=n/summationdisplay
i=1ik/parenleftBigg
n
i/parenrightBigg
pi(1−p)n−i

<<<PAGE 145>>>

132 Chapter 4 Random Variables
Using the identity
i/parenleftBigg
n
i/parenrightBigg
=n/parenleftBigg
n−1
i−1/parenrightBigg
gives
E[Xk]=npn/summationdisplay
i=1ik−1/parenleftBigg
n−1
i−1/parenrightBigg
pi−1(1−p)n−i
=npn−1/summationdisplay
j=0(j+1)k−1/parenleftBigg
n−1
j/parenrightBigg
pj(1−p)n−1−jby letting
j=i−1
=npE[(Y+1)k−1]
where Yis a binomial random variable with parameters n−1,p. Setting k=1i n
the preceding equation yields
E[X]=np
That is, the expected number of successes that occur in nindependent trials when
each is a success with probability pis equal to np. Setting k=2 in the preced-
ing equation and using the preceding formula for the expected value of a binomial
random variable yields
E[X2]=npE[Y+1]
=np[(n −1)p+1]
Since E[X]=np, we obtain
Var(X)=E[X2]−(E[X])2
=np[(n −1)p+1]−(np)2
=np(1−p)
Summing up, we have shown the following:
IfXis a binomial random variable with parameters nandp, then
E[X]=np
Var(X)=np(1−p)
The following proposition details how the binomial probability mass function
ﬁrst increases and then decreases.
Proposition
6.1IfXis a binomial random variable with parameters (n, p), where 0 <p<1, then
askgoes from 0 to n,P{X=k}ﬁrst increases monotonically and then decreases
monotonically, reaching its largest value when kis the largest integer less than or
equal to (n+1)p.
Proof We prove the proposition by considering P{X=k}/P{X=k−1}and deter-
mining for what values of kit is greater or less than 1. Now,
P{X=k}
P{X=k−1}=n!
(n−k)!k!pk(1−p)n−k
n!
(n−k+1)!(k −1)!pk−1(1−p)n−k+1
=(n−k+1)p
k(1−p)

<<<PAGE 146>>>

A First Course in Probability 133
0k1024 /H11003 p (k)
123 4 56 7 8 9101045120210252
1
Figure 4.6 Graph of p(k)=/parenleftbig10
k/parenrightbig/parenleftBig
1
2/parenrightBig10
.
Hence, P{X=k}ÚP{X=k−1}if and only if
(n−k+1)pÚk(1−p)
or, equivalently, if and only if
k…(n+1)p
and the proposition is proved. /square
As an illustration of Proposition 6.1, consider Figure 4.6, the graph of the prob-
ability mass function of a binomial random variable with parameters (10,1
2).
Example
6gIn a U.S. presidential election, the candidate who gains the maximum number of
votes in a state is awarded the total number of electoral college votes allocated to
that state. The number of electoral college votes of a given state is roughly propor-
tional to the population of that state—that is, a state with population nhas roughly
ncelectoral votes. (Actually, it is closer to nc+2, as a state is given an electoral
vote for each member it has in the House of Representatives, with the number ofsuch representatives being roughly proportional to the population of the state, andone electoral college vote for each of its two senators.) Let us determine the average
power of a citizen in a state of size nin a close presidential election, where, by aver-
age power in a close election, we mean that a voter in a state of size n=2k+1 will be
decisive if the other n−1 voters split their votes evenly between the two candidates.
(We are assuming here that nis odd, but the case where nis even is quite similar.)
Because the election is close, we shall suppose that each of the other n−1=2k
voters acts independently and is equally likely to vote for either candidate. Hence,
the probability that a voter in a state of size n=2k+1 will make a difference to
the outcome is the same as the probability that 2 ktosses of a fair coin land heads
and tails an equal number of times. That is,
P{voter in state of size 2 k+1 makes a difference }
=/parenleftBigg
2k
k/parenrightBigg/parenleftbigg1
2/parenrightbiggk/parenleftbigg1
2/parenrightbiggk
=(2k)!
k!k!22k

<<<PAGE 147>>>

134 Chapter 4 Random Variables
To approximate the preceding equality, we make use of Stirling’s approximation,
which says that for klarge,
k!∼kk+1/ 2e−k√
2π
where we say that ak∼bkwhen the ratio ak/bkapproaches 1 as kapproaches q.
Hence, it follows that
P{voter in state of size 2 k+1 makes a difference }
∼(2k)2k+1/ 2e−2k√
2π
k2k+1e−2k(2π) 22k=1√
kπ
Because such a voter (if he or she makes a difference) will affect ncelectoral votes,
the expected number of electoral votes a voter in a state of size nwill affect—or the
voter’s average power—is given by
average power =ncP{makes a difference }
∼nc/radicalbig
nπ/2
=c/radicalbig
2n/π
Thus, the average power of a voter in a state of size nis proportional to the square
root of n, showing that in presidential elections, voters in large states have more
power than do those in smaller states. .
4.6.2 Computing the Binomial Distribution Function
Suppose that Xis binomial with parameters ( n,p). The key to computing its distri-
bution function
P{X…i}=i/summationdisplay
k=0/parenleftBigg
n
k/parenrightBigg
pk(1−p)n−ki=0, 1,...,n
is to utilize the following relationship between P{X=k+1}and P{X=k}, which
was established in the proof of Proposition 6.1:
P{X=k+1}=p
1−pn−k
k+1P{X=k} (6.3)
Example
6hLetXbe a binomial random variable with parameters n=6,p=.4. Then, starting
with P{X=0}=(. 6)6and recursively employing Equation (6.3), we obtain
P{X=0}=(.6)6L.0467
P{X=1}=4
66
1P{X=0}L.1866
P{X=2}=4
65
2P{X=1}L.3110
P{X=3}=4
64
3P{X=2}L.2765
P{X=4}=4
63
4P{X=3}L.1382
P{X=5}=4
62
5P{X=4}L.0369
P{X=6}=4
61
6P{X=5}L.0041 .

<<<PAGE 148>>>

A First Course in Probability 135
A computer program that utilizes the recursion (6.3) to compute the binomial
distribution function is easily written. To compute P{X…i}, the program should
ﬁrst compute P{X=i}and then use the recursion to successively compute P{X=
i−1},P{X=i−2}, and so on.
Historical note
Independent trials having a common probability of success pwere ﬁrst stud-
ied by the Swiss mathematician Jacques Bernoulli (1654–1705). In his book Ars
Conjectandi (The Art of Conjecturing) , published by his nephew Nicholas eight
years after his death in 1713, Bernoulli showed that if the number of such trials
were large, then the proportion of them that were successes would be close to p
with a probability near 1.
Jacques Bernoulli was from the ﬁrst generation of the most famous mathe-
matical family of all time. Altogether, there were between 8 and 12 Bernoullis,
spread over three generations, who made fundamental contributions to proba-
bility, statistics, and mathematics. One difﬁculty in knowing their exact number
is the fact that several had the same name. (For example, two of the sons of
Jacques’s brother Jean were named Jacques and Jean.) Another difﬁculty is thatseveral of the Bernoullis were known by different names in different places.
Our Jacques (sometimes written Jaques) was, for instance, also known as Jakob
(sometimes written Jacob) and as James Bernoulli. But whatever their num-ber, their inﬂuence and output were prodigious. Like the Bachs of music, the
Bernoullis of mathematics were a family for the ages!
Example
6iIfXis a binomial random variable with parameters n=100 and p=.75, ﬁnd
P{X=70}and P{X…70}.
Solution A binomial calculator can be used to obtain the following solutions:
Enter Value For p: .75
Enter Value For n: 100
Enter Value For i: 70Binomial Distribution
Start
Quit
Probability (Number of Successes = i) = .04575381
Probability (Number of Successes < = i) = .14954105
Figure 4.7
4.7 The Poisson Random Variable
A random variable Xthat takes on one of the values 0, 1, 2, ...is said to be a Poisson
random variable with parameter λif, for some λ> 0,
p(i)=P{X=i}=e−λλi
i!i=0, 1, 2, ... (7.1)

<<<PAGE 149>>>

136 Chapter 4 Random Variables
Equation (7.1) deﬁnes a probability mass function, since
q/summationdisplay
i=0p(i)=e−λq/summationdisplay
i=0λi
i!=e−λeλ=1
The Poisson probability distribution was introduced by Sim ´eon Denis Poisson in a
book he wrote regarding the application of probability theory to lawsuits, criminal
trials, and the like. This book, published in 1837, was entitled Recherches sur la prob-
abilit ´e des jugements en mati `ere criminelle et en mati `ere civile (Investigations into the
Probability of Verdicts in Criminal and Civil Matters) .
The Poisson random variable has a tremendous range of applications in diverse
areas because it may be used as an approximation for a binomial random variablewith parameters (n, p) when nis large and pis small enough so that npis of moderate
size. To see this, suppose that Xis a binomial random variable with parameters (n, p),
and let λ=np. Then
P{X=i}=n!
(n−i)!i!pi(1−p)n−i
=n!
(n−i)!i!/parenleftbiggλ
n/parenrightbiggi/parenleftbigg
1−λ
n/parenrightbiggn−i
=n(n−1)···(n−i+1)
niλi
i!(1−λ/n)n
(1−λ/n)i
Now, for nlarge and λmoderate,
/parenleftbigg
1−λ
n/parenrightbiggn
Le−λ n(n−1)···(n−i+1)
niL1/parenleftbigg
1−λ
n/parenrightbiggi
L1
Hence, for nlarge and λmoderate,
P{X=i}Le−λλi
i!
In other words, if nindependent trials, each of which results in a success with
probability p, are performed, then when nis large and pis small enough to make
npmoderate, the number of successes occurring is approximately a Poisson random
variable with parameter λ=np. This value λ(which will later be shown to equal the
expected number of successes) will usually be determined empirically.
Some examples of random variables that generally obey the Poisson probability
law [that is, they obey Equation (7.1)] are as follows:
1. The number of misprints on a page (or a group of pages) of a book
2. The number of people in a community who survive to age 100
3. The number of wrong telephone numbers that are dialed in a day
4. The number of packages of dog biscuits sold in a particular store each day5. The number of customers entering a post ofﬁce on a given day
6. The number of vacancies occurring during a year in the federal judicial system
7. The number of α-particles discharged in a ﬁxed period of time from some
radioactive material
Each of the preceding and numerous other random variables are approximately
Poisson for the same reason—namely, because of the Poisson approximation to the
binomial. For instance, we can suppose that there is a small probability pthat each
letter typed on a page will be misprinted. Hence, the number of misprints on a pagewill be approximately Poisson with λ=np, where nis the number of letters on a
page. Similarly, we can suppose that each person in a community has some small

<<<PAGE 150>>>

A First Course in Probability 137
probability of reaching age 100. Also, each person entering a store may be thought
of as having some small probability of buying a package of dog biscuits, and so on.
Example
7aSuppose that the number of typographical errors on a single page of this book has a
Poisson distribution with parameter λ=1
2. Calculate the probability that there is at
least one error on this page.
Solution Letting Xdenote the number of errors on this page, we have
P{XÚ1}=1 −P{X=0}=1 −e−1/2L.393 .
Example
7bSuppose that the probability that an item produced by a certain machine will be
defective is .1. Find the probability that a sample of 10 items will contain at most 1
defective item.
Solution The desired probability is/parenleftBigg
10
0/parenrightBigg
(.1)0(.9)10+/parenleftBigg
10
1/parenrightBigg
(.1)1(.9)9=.7361,
whereas the Poisson approximation yields the value e−1+e−1L.7358. .
Example
7cConsider an experiment that consists of counting the number of αparticles given
off in a 1-second interval by 1 gram of radioactive material. If we know from past
experience that on the average, 3.2 such αparticles are given off, what is a good
approximation to the probability that no more than 2 αparticles will appear?
Solution If we think of the gram of radioactive material as consisting of a large
number nof atoms, each of which has probability of 3.2/n of disintegrating and send-
ing off an αparticle during the second considered, then we see that to a very close
approximation, the number of αparticles given off will be a Poisson random variable
with parameter λ=3.2. Hence, the desired probability is
P{X…2}=e−3.2+3.2e−3.2+(3.2)2
2e−3.2
L.3799 .
Before computing the expected value and variance of the Poisson random vari-
able with parameter λ, recall that this random variable approximates a binomial
random variable with parameters nand pwhen nis large, pis small, and λ=np.
Since such a binomial random variable has expected value np=λand variance
np(1−p)=λ(1−p)Lλ(since pis small), it would seem that both the expected
value and the variance of a Poisson random variable would equal its parameter λ.
We now verify this result:
E[X]=q/summationdisplay
i=0ie−λλi
i!
=λq/summationdisplay
i=1e−λλi−1
(i−1)!
=λe−λq/summationdisplay
j=0λj
j!by letting
j=i−1
=λ sinceq/summationdisplay
j=0λj
j!=eλ

<<<PAGE 151>>>

138 Chapter 4 Random Variables
Thus, the expected value of a Poisson random variable Xis indeed equal to its
parameter λ. To determine its variance, we ﬁrst compute E[X2]:
E[X2]=q/summationdisplay
i=0i2e−λλi
i!
=λq/summationdisplay
i=1ie−λλi−1
(i−1)!
=λq/summationdisplay
j=0(j+1)e−λλj
j!by letting
j=i−1
=λ⎡
⎢⎣q/summationdisplay
j=0je−λλj
j!+q/summationdisplay
j=0e−λλj
j!⎤
⎥⎦
=λ(λ+1)
where the ﬁnal equality follows because the ﬁrst sum is the expected value of a
Poisson random variable with parameter λand the second is the sum of the proba-
bilities of this random variable. Therefore, since we have shown that E[X]=λ,w e
obtain
Var(X)=E[X2]−(E[X])2
=λ
Hence, the expected value and variance of a Poisson random variable are both
equal to its parameter λ.
We have shown that the Poisson distribution with parameter npis a very good
approximation to the distribution of the number of successes in nindependent trials
when each trial has probability pof being a success, provided that nis large and p
small. In fact, it remains a good approximation even when the trials are not inde-
pendent, provided that their dependence is weak. For instance, recall the matchingproblem (Example 5m of Chapter 2) in which nmen randomly select hats from a set
consisting of one hat from each person. From the point of view of the number ofmen who select their own hat, we may regard the random selection as the result of n
trials where we say that trial iis a success if person iselects his own hat, i=1,...,n.
Deﬁning the events E
i,i=1,...,n,b y
Ei={trial iis a success }
it is easy to see that
P{Ei}=1
nand P{Ei|Ej}=1
n−1,jZi
Thus, we see that although the events Ei,i=1,...,nare not independent, their
dependence, for large n, appears to be weak. Because of this, it seems reasonable to
expect that the number of successes will approximately have a Poisson distributionwith parameter n*1/n=1 and indeed this is veriﬁed in Example 5m of Chapter 2.
For a second illustration of the strength of the Poisson approximation when the
trials are weakly dependent, let us consider again the birthday problem presented inExample 5i of Chapter 2. In this example, we suppose that each of npeople is equally
likely to have any of the 365 days of the year as his or her birthday, and the problemis to determine the probability that a set of nindependent people all have different

<<<PAGE 152>>>

A First Course in Probability 139
birthdays. A combinatorial argument was used to determine this probability, which
was shown to be less than1
2when n=23.
We can approximate the preceding probability by using the Poisson approxima-
tion as follows: Imagine that we have a trial for each of the/parenleftBigg
n
2/parenrightBigg
pairs of individuals
iand j,iZj, and say that trial i,jis a success if persons iand jhave the same
birthday. If we let Eijdenote the event that trial i,jis a success, then, whereas the/parenleftBigg
n
2/parenrightBigg
events Eij,1…i<j…n, are not independent (see Theoretical Exercise 21),
their dependence appears to be rather weak. (Indeed, these events are even pair-
wise independent , in that any 2 of the events EijandEklare independent—again, see
Theoretical Exercise 21). Since P(Eij)=1/365, it is reasonable to suppose that the
number of successes should approximately have a Poisson distribution with mean/parenleftBigg
n
2/parenrightBigg/slashBig
365=n(n−1)/730. Therefore,
P{no 2 people have the same birthday}=P {0 successes }
Lexp/braceleftbigg−n(n −1)
730/bracerightbigg
To determine the smallest integer nfor which this probability is less than1
2, note that
exp/braceleftbigg−n(n −1)
730/bracerightbigg
…1
2
is equivalent to
exp/braceleftbiggn(n−1)
730/bracerightbigg
Ú2
Taking logarithms of both sides, we obtain
n(n−1)Ú730 log 2
L505.997
which yields the solution n=23, in agreement with the result of Example 5i of
Chapter 2.
Suppose now that we wanted the probability that among the npeople, no 3 of
them have their birthday on the same day. Whereas this now becomes a difﬁcult
combinatorial problem, it is a simple matter to obtain a good approximation. To
begin, imagine that we have a trial for each of the/parenleftBigg
n
3/parenrightBigg
triplets i,j,k, where 1 …i<
j<k…n, and call the i,j,ktrial a success if persons i,j, and kall have their birthday
on the same day. As before, we can then conclude that the number of successes is
approximately a Poisson random variable with parameter
/parenleftBigg
n
3/parenrightBigg
P{i,j,khave the same birthday }=/parenleftBigg
n
3/parenrightBigg/parenleftbigg1
365/parenrightbigg2
=n(n−1)(n−2)
6*(365)2

<<<PAGE 153>>>

140 Chapter 4 Random Variables
Hence,
P{no 3 have the same birthday }Lexp/braceleftbigg−n(n −1)(n−2)
799350/bracerightbigg
This probability will be less than1
2when nis such that
n(n−1)(n−2)Ú799350 log 2 L554067.1
which is equivalent to nÚ84. Thus, the approximate probability that at least 3 people
in a group of size 84 or larger will have the same birthday exceeds1
2.
For the number of events to occur to approximately have a Poisson distribution,
it is not essential that all the events have the same probability of occurrence, but
only that all of these probabilities be small. The following is referred to as the Pois-
son paradigm.
Poisson Paradigm. Consider nevents, with piequal to the probability that
event ioccurs, i=1,...,n.If all the piare “small” and the trials are either inde-
pendent or at most “weakly dependent,” then the number of these events that occurapproximately has a Poisson distribution with mean/summationtext
n
i=1pi.
Our next example not only makes use of the Poisson paradigm, but also illus-
trates a variety of the techniques we have studied so far.
Example
7dLength of the longest run
A coin is ﬂipped ntimes. Assuming that the ﬂips are independent, with each one
coming up heads with probability p, what is the probability that there is a string of k
consecutive heads?
Solution We will ﬁrst use the Poisson paradigm to approximate this probability.
Now, if for i=1,...,n−k+1, we let Hidenote the event that ﬂips i,i+1,...,i+
k−1 all land on heads, then the desired probability is that at least one of the events
Hioccur. Because Hiis the event that starting with ﬂip i, the next kﬂips all land
on heads, it follows that P(Hi)=pk. Thus, when pkis small, we might think that
the number of the Hithat occur should have an approximate Poisson distribution.
However, such is not the case, because, although the events all have small proba-
bilities, some of their dependencies are too great for the Poisson distribution to be
a good approximation. For instance, because the conditional probability that ﬂips
2,...,k+1 are all heads given that ﬂips 1, ...,kare all heads is equal to the proba-
bility that ﬂip k+1 is a head, it follows that
P(H2|H1)=p
which is far greater than the unconditional probability of H2.
The trick that enables us to use a Poisson approximation is to note that there
will be a string of kconsecutive heads either if there is such a string that is imme-
diately followed by a tail or if the ﬁnal kﬂips all land on heads. Consequently, for
i=1,...,n−k,l e t Eibe the event that ﬂips i,...,i+k−1 are all heads and ﬂip
i+kis a tail; also, let En−k+1 be the event that ﬂips n−k+1,...,nare all heads.
Note that
P(Ei)=pk(1−p), i…n−k
P(En−k+1)=pk
Thus, when pkis small, each of the events Eihas a small probability of occurring.
Moreover, for iZj, if the events EiandEjrefer to nonoverlapping sequences of ﬂips,

<<<PAGE 154>>>

A First Course in Probability 141
then P(Ei|Ej)=P(Ei); if they refer to overlapping sequences, then P(Ei|Ej)=0.
Hence, in both cases, the conditional probabilities are close to the unconditional
ones, indicating that N, the number of the events Eithat occur, should have an
approximate Poisson distribution with mean
E[N]=n−k+1/summationdisplay
i=1P(Ei)=(n−k)pk(1−p)+pk
Because there will not be a run of kheads if (and only if) N=0, thus the preceding
gives
P(no head strings of length k)=P(N=0)Lexp{−(n−k)pk(1−p)−pk}
If we let Lndenote the largest number of consecutive heads in the nﬂips, then,
because Lnwill be less than kif (and only if) there are no head strings of length k,
the preceding equation can be written as
P{Ln<k}Lexp{−(n−k)pk(1−p)−pk}
Now, let us suppose that the coin being ﬂipped is fair; that is, suppose that p=1/2.
Then the preceding gives
P{Ln<k}Lexp/braceleftbigg
−n−k+2
2k+1/bracerightbigg
Lexp/braceleftbigg
−n
2k+1/bracerightbigg
where the ﬁnal approximation supposes that ek−2
2k+1L1 (that is, thatk−2
2k+1L0). Let
j=log2n, and assume that jis an integer. For k=j+i,
n
2k+1=n
2j2i+1=1
2i+1
Consequently,
P{Ln<j+i}Lexp{−(1/2)i+1}
which implies that
P{Ln=j+i}= P{Ln<j+i+1}− P{Ln<j+i}
Lexp{−(1/2)i+2}− exp{−(1/2)i+1}
For instance,
P{Ln<j−3}Le−4L.0183
P{Ln=j−3}Le−2−e−4L.1170
P{Ln=j−2}Le−1−e−2L.2325
P{Ln=j−1}Le−1/2−e−1L.2387
P{Ln=j}Le−1/4−e−1/2L.1723
P{Ln=j+1}Le−1/8−e−1/4L.1037
P{Ln=j+2}Le−1/16−e−1/8L.0569
P{Ln=j+3}Le−1/32−e−1/16L.0298
P{LnÚj+4}L1−e−1/32L.0308
Thus, we observe the rather interesting fact that no matter how large nis, the length
of the longest run of heads in a sequence of nﬂips of a fair coin will be within 2 of
log2(n)−1 with a probability approximately equal to .86.

<<<PAGE 155>>>

142 Chapter 4 Random Variables
We now derive an exact expression for the probability that there is a string of
kconsecutive heads when a coin that lands on heads with probability pis ﬂipped
ntimes. With the events Ei,i=1,...,n−k+1, as deﬁned earlier, and with Ln
denoting, as before, the length of the longest run of heads,
P(LnÚk)=P(there is a string of kconsecutive heads )=P(∪n−k+1
i=1Ei)
The inclusion–exclusion identity for the probability of a union can be written as
P(∪n−k+1i=1Ei)=n−k+1/summationdisplay
r=1(−1)r+1/summationdisplay
i1<···<i rP(Ei1···Eir)
LetSidenote the set of ﬂip numbers to which the event Eirefers. (So, for instance,
S1={1,...,k+1}.) Now, consider one of the r-way intersection probabilities that
does not include the event En−k+1 . That is, consider P(Ei1···Eir)where i1<···<
ir<n−k+1. On the one hand, if there is any overlap in the sets Si1,...,Sir
then this probability is 0. On the other hand, if there is no overlap, then the events
Ei1,...,Eirare independent. Therefore,
P(Ei1···Eir)=/braceleftBigg
0, if there is any overlap in Si1,...,Sir
prk(1−p)r, if there is no overlap
We must now determine the number of different choices of i1<···<ir<n−k+1
for which there is no overlap in the sets Si1,...,Sir. To do so, note ﬁrst that each
of the Sij,j=1,...,r, refer to k+1 ﬂips, so, without any overlap, they together
refer to r(k+1)ﬂips. Now consider any permutation of ridentical letters aand
ofn−r(k+1)identical letters b. Interpret the number of b’s before the ﬁrst a
as the number of ﬂips before Si1, the number of b’s between the ﬁrst and second
aas the number of ﬂips between Si1and Si2, and so on, with the number of b’s
after the ﬁnal arepresenting the number of ﬂips after Sir. Because there are/parenleftbign−rk
r/parenrightbig
permutations of rletters aand of n−r(k+1)letters b, with every such permuta-
tion corresponding (in a one-to-one fashion) to a different nonoverlapping choice, it
follows that
/summationdisplay
i1<···<i r<n−k+1P(Ei1···Eir)=/parenleftbiggn−rk
r/parenrightbigg
prk(1−p)r
We must now consider r-way intersection probabilities of the form
P(Ei1···E ir−1En−k+1),
where i1<···<ir−1<n−k+1. Now, this probability will equal 0 if there
is any overlap in Si1,...,Sir−1,Sn−k;if there is no overlap, then the events of the
intersection will be independent, so
P(Ei1···E ir−1En−k+1)=[pk(1−p)]r−1pk=pkr(1−p)r−1
By a similar argument as before, the number of nonoverlapping sets Si1,...,Sir−1,
Sn−kwill equal the number of permutations of r−1 letters a(one for each of the
setsSi1,...,Sir−1)and of n−(r−1)(k+1)−k=n−rk−(r−1)letters b(one
for each of the trials that are not part of any of Si1,...,Sir−1,Sn−k+1). Since there are/parenleftbign−rk
r−1/parenrightbig
permutations of r−1 letters aand of n−rk−(r−1)letters b, we have

<<<PAGE 156>>>

A First Course in Probability 143
/summationdisplay
i1<...< ir−1<n−k+1P(Ei1···E ir−1En−k+1)=/parenleftbiggn−rk
r−1/parenrightbigg
pkr(1−p)r−1
Putting it all together yields the exact expression, namely,
P(LnÚk)=n−k+1/summationdisplay
r=1(−1)r+1/bracketleftBigg/parenleftbiggn−rk
r/parenrightbigg
+1
p/parenleftbiggn−rk
r−1/parenrightbigg/bracketrightBigg
pkr(1−p)r
where we utilize the convention that/parenleftbigm
j/parenrightbig
=0i fm <j.
From a computational point of view, a more efﬁcient method for computing the
desired probability than the use of the preceding identity is to derive a set of recur-
sive equations. To do so, let Anbe the event that there is a string of kconsecutive
heads in a sequence of nﬂips of a fair coin, and let Pn=P(An).We will derive a
set of recursive equations for Pnby conditioning on when the ﬁrst tail appears. For
j=1,...,k,l e tF jbe the event that the ﬁrst tail appears on ﬂip j, and let Hbe the
event that the ﬁrst kﬂips are all heads. Because the events F1,...,Fk,Hare mutually
exclusive and exhaustive (that is, exactly one of these events must occur), we have
P(An)=k/summationdisplay
j=1P(An|Fj)P(Fj)+P(An|H)P(H)
Now, given that the ﬁrst tail appears on ﬂip j, where j<k, it follows that those j
ﬂips are wasted as far as obtaining a string of kheads in a row; thus, the conditional
probability of this event is the probability that such a string will occur among theremaining n−jﬂips. Therefore,
P(A
n|Fj)=Pn−j
Because P(An|H)=1, the preceding equation gives
Pn=P(An)
=k/summationdisplay
j=1Pn−jP(Fj)+P(H)
=k/summationdisplay
j=1Pn−jpj−1(1−p)+pk
Starting with Pj=0,j<k, and Pk=pk, we can use the latter formula to recur-
sively compute Pk+1,Pk+2, and so on, up to Pn. For instance, suppose we want the
probability that there is a run of 2 consecutive heads when a fair coin is ﬂipped 4times. Then, with k=2, we have P
1=0,P2=(1/2)2. Because, when p=1/2, the
recursion becomes
Pn=k/summationdisplay
j=1Pn−j(1/2)j+(1/2)k
we obtain
P3=P2(1/2) +P1(1/2)2+(1/2)2=3/8
and
P4=P3(1/2) +P2(1/2)2+(1/2)2=1/2

<<<PAGE 157>>>

144 Chapter 4 Random Variables
which is clearly true because there are 8 outcomes that result in a string of 2 consecu-
tive heads: hhhh, hhht ,hhth, hthh, thhh, hhtt, thht, and tthh. Each of these outcomes
occurs with probability 1/16. .
Another use of the Poisson probability distribution arises in situations where
“events” occur at certain points in time. One example is to designate the occurrenceof an earthquake as an event; another possibility would be for events to correspond
to people entering a particular establishment (bank, post ofﬁce, gas station, and so
on); and a third possibility is for an event to occur whenever a war starts. Let ussuppose that events are indeed occurring at certain (random) points of time, and let
us assume that for some positive constant λ, the following assumptions hold true:
1. The probability that exactly 1 event occurs in a given interval of length his
equal to λh+o(h), where o(h) stands for any function f(h)for which
lim
h→0f(h)/h =0. [For instance, f(h)=h2iso(h), whereas f(h)=his not.]
2. The probability that 2 or more events occur in an interval of length his equal
too(h).
3. For any integers n,j1,j2,...,jnand any set of nnonoverlapping intervals, if
we deﬁne Eito be the event that exactly jiof the events under consideration
occur in the ith of these intervals, then events E1,E2,...,Enare independent.
Loosely put, assumptions 1 and 2 state that for small values of h, the probability
that exactly 1 event occurs in an interval of size hequals λhplus something that is
small compared with h, whereas the probability that 2 or more events occur is small
compared with h. Assumption 3 states that whatever occurs in one interval has no
(probability) effect on what will occur in other, nonoverlapping intervals.
We now show that under assumptions 1, 2, and 3, the number of events occurring
in any interval of length tis a Poisson random variable with parameter λt.T ob e
precise, let us call the interval [0, t] and denote the number of events occurring in
that interval by N(t). To obtain an expression for P{N(t)=k}, we start by breaking
the interval [0, t]i n t on nonoverlapping subintervals, each of length t/n(Figure 4.8).
0
t–n2t—n3t—nt–n(n – 1)nt—nt =
Figure 4.8
Now,
P{N(t)=k}= P{kof the nsubintervals contain exactly 1 event
and the other n−kcontain 0 events } (7.2)
+P{N(t)=kand at least 1 subinterval contains
2 or more events }
The preceding equation holds because the event on the left side of Equation (7.2),
that is, {N(t)=k}, is clearly equal to the union of the two mutually exclusive events
on the right side of the equation. Letting AandBdenote the two mutually exclusive
events on the right side of Equation (7.2), we have

<<<PAGE 158>>>

A First Course in Probability 145
P(B)…P{at least one subinterval contains 2 or more events }
=P⎛
⎝n/uniondisplay
i=1{ith subinterval contains 2 or more events}⎞⎠
…n/summationdisplay
i=1P{ith subinterval contains 2 or more events}by Boole’s
inequality
=n/summationdisplay
i=1o/parenleftbiggt
n/parenrightbigg
by assumption 2
=no/parenleftbiggt
n/parenrightbigg
=t/bracketleftbiggo(t/n)
t/n/bracketrightbigg
Now, in addition, for any t,t/n→ 0a s n→q,s oo(t /n)/(t /n)→ 0a s n→q, by the def-
inition of o(h). Hence,
P(B)→0a s n→q (7.3)
Moreover, since assumptions 1 and 2 imply that†
P{0 events occur in an interval of length h}
=1−[λh+o(h)+o(h)]=1−λh−o(h)
we see from the independence assumption (number 3) that
P(A)=P{kof the subintervals contain exactly 1 event and the other
n−kcontain 0 events }
=/parenleftBigg
n
k/parenrightBigg/bracketleftBigg
λt
n+o/parenleftbiggt
n/parenrightbigg/bracketrightBiggk/bracketleftBigg
1−/parenleftbiggλt
n/parenrightbigg
−o/parenleftbiggt
n/parenrightbigg/bracketrightBiggn−k
However, since
n/bracketleftBigg
λt
n+o/parenleftbiggt
n/parenrightbigg/bracketrightBigg
=λt+t/bracketleftbiggo(t/n)
t/n/bracketrightbigg
→λt as n→q
it follows, by the same argument that veriﬁed the Poisson approximation to the bino-mial, that
P(A)→ e
−λt(λt)k
k!as n→q (7.4)
Thus, from Equations (7.2), (7.3), and (7.4), by letting n→q, we obtain
P{N(t)=k}= e−λt(λt)k
k!k=0, 1,... (7.5)
Hence, if assumptions 1, 2, and 3 are satisﬁed, then the number of events occur-
ring in any ﬁxed interval of length tis a Poisson random variable with mean λt, and
we say that the events occur in accordance with a Poisson process having rate λ.T h e
value λ, which can be shown to equal the rate per unit time at which events occur, is
a constant that must be empirically determined.
†The sum of two functions, both of which are o(h),i sa l s o o(h). This is so because if lim h→ 0f(h)/h=
limh→ 0g(h)/h=0, then lim h→ 0[f(h)+g(h)]/h=0.

<<<PAGE 159>>>

146 Chapter 4 Random Variables
The preceding discussion explains why a Poisson random variable is usually a
good approximation for such diverse phenomena as the following:
1. The number of earthquakes occurring during some ﬁxed time span
2. The number of wars per year3. The number of electrons emitted from a heated cathode during a ﬁxed time
period
4. The number of deaths, in a given period of time, of the policyholders of a life
insurance company
Example
7eSuppose that earthquakes occur in the western portion of the United States in accor-
dance with assumptions 1, 2, and 3, with λ=2 and with 1 week as the unit of time.
(That is, earthquakes occur in accordance with the three assumptions at a rate of 2
per week.)
(a) Find the probability that at least 3 earthquakes occur during the next 2 weeks.
(b) Find the probability distribution of the time, starting from now, until the next
earthquake.
Solution (a) From Equation (7.5), we have
P{N(2)Ú3}= 1−P{N(2)=0}− P{N(2)=1}− P{N(2)=2}
=1−e−4−4e−4−42
2e−4
=1−13e−4
(b) Let Xdenote the amount of time (in weeks) until the next earthquake.
Because Xwill be greater than tif and only if no events occur within the next t
units of time, we have, from Equation (7.5),
P{X>t}=P{N(t)=0}=e−λt
so the probability distribution function Fof the random variable Xis given by
F(t)=P{X…t}=1−P{X>t}=1 −e−λt
=1−e−2t.
4.7.1 Computing the Poisson Distribution Function
IfXis Poisson with parameter λ, then
P{X=i+1}
P{X=i}=e−λλi+1/(i+1)!
e−λλi/i!=λ
i+1(7.6)
Starting with P{X=0}= e−λ, we can use (7.6) to compute successively
P{X=1}=λP{X=0}
P{X=2}=λ
2P{X=1}
...
P{X=i+1}=λ
i+1P{X=i}
We can use a module to compute the Poisson probabilities for Equation (7.6).

<<<PAGE 160>>>

A First Course in Probability 147
Example
7f(a) Determine P{X…90}when Xis Poisson with mean 100.
(b) Determine P{Y…1075} when Yis Poisson with mean 1000.
Solution Using the Poisson calculator of StatCrunch yields the solutions:
(a) P{X…90}= .17138
(b) P{Y…1075}= .99095 .
4.8 Other Discrete Probability Distributions
4.8.1 The Geometric Random Variable
Suppose that independent trials, each having a probability p,0<p<1, of being a
success, are performed until a success occurs. If we let Xequal the number of trials
required, then
P{X=n}=(1−p)n−1pn =1, 2,... (8.1)
Equation (8.1) follows because, in order for Xto equal n, it is necessary and sufﬁ-
cient that the ﬁrst n−1 trials are failures and the nth trial is a success. Equation (8.1)
then follows, since the outcomes of the successive trials are assumed to be indepen-
dent.
Sinceq/summationdisplay
n=1P{X=n}= pq/summationdisplay
n=1(1−p)n−1=p
1−(1−p)=1
it follows that with probability 1, a success will eventually occur. Any random vari-
able Xwhose probability mass function is given by Equation (8.1) is said to be a
geometric random variable with parameter p.
Example
8aAn urn contains Nwhite and Mblack balls. Balls are randomly selected, one at a
time, until a black one is obtained. If we assume that each ball selected is replaced
before the next one is drawn, what is the probability that
(a) exactly ndraws are needed?
(b) at least kdraws are needed?
Solution If we let Xdenote the number of draws needed to select a black ball, then
Xsatisﬁes Equation (8.1) with p=M/(M+N). Hence,
(a)
P{X=n}=/parenleftbiggN
M+N/parenrightbiggn−1M
M+N=MNn−1
(M+N)n
(b)
P{XÚk}=M
M+Nq/summationdisplay
n=k/parenleftbiggN
M+N/parenrightbiggn−1
=/parenleftbiggM
M+N/parenrightbigg/parenleftbiggN
M+N/parenrightbiggk−1/slashBigg/bracketleftbigg
1−N
M+N/bracketrightbigg
=/parenleftbiggN
M+N/parenrightbiggk−1

<<<PAGE 161>>>

148 Chapter 4 Random Variables
Of course, part (b) could have been obtained directly, since the probability that at
least ktrials are necessary to obtain a success is equal to the probability that the ﬁrst
k−1 trials are all failures. That is, for a geometric random variable,
P{XÚk}=(1 −p)k−1.
Example
8bFind the expected value of a geometric random variable.
Solution With q=1−p, we have
E[X]=q/summationdisplay
i=1iqi−1p
=q/summationdisplay
i=1(i−1+1)qi−1p
=q/summationdisplay
i=1(i−1)qi−1p+q/summationdisplay
i=1qi−1p
=q/summationdisplay
j=0jqjp+1
=qq/summationdisplay
j=1jqj−1p+1
=qE[X]+1
Hence,
pE[X]=1
yielding the result
E[X]=1
p
In other words, if independent trials having a common probability pof being suc-
cessful are performed until the ﬁrst success occurs, then the expected number of
required trials equals 1/p. For instance, the expected number of rolls of a fair die
that it takes to obtain the value 1 is 6. .
Example
8cFind the variance of a geometric random variable.
Solution To determine Var (X), let us ﬁrst compute E[X2]. With q=1−p, we have
E[X2]=q/summationdisplay
i=1i2qi−1p
=q/summationdisplay
i=1(i−1+1)2qi−1p
=q/summationdisplay
i=1(i−1)2qi−1p+q/summationdisplay
i=12(i−1)qi−1p+q/summationdisplay
i=1qi−1p
=q/summationdisplay
j=0j2qjp+2q/summationdisplay
j=1jqjp+1
=qE[X2]+2qE[X]+1

<<<PAGE 162>>>

A First Course in Probability 149
Using E[X]=1/p, the equation for E[X2] yields
pE[X2]=2q
p+1
Hence,
E[X2]=2q+p
p2=q+1
p2
giving the result
Var(X)=q+1
p2−1
p2=q
p2=1−p
p2.
4.8.2 The Negative Binomial Random Variable
Suppose that independent trials, each having probability p,0<p<1, of being a
success are performed until a total of rsuccesses is accumulated. If we let Xequal
the number of trials required, then
P{X=n}=/parenleftBigg
n−1
r−1/parenrightBigg
pr(1−p)n−rn=r,r+1,... (8.2)
Equation (8.2) follows because, in order for the rth success to occur at the nth trial,
there must be r−1 successes in the ﬁrst n−1 trials and the nth trial must be a
success. The probability of the ﬁrst event is
/parenleftBigg
n−1
r−1/parenrightBigg
pr−1(1−p)n−r
and the probability of the second is p; thus, by independence, Equation (8.2) is estab-
lished. To verify that a total of rsuccesses must eventually be accumulated, either
we can prove analytically that
q/summationdisplay
n=rP{X=n}=q/summationdisplay
n=r/parenleftBigg
n−1
r−1/parenrightBigg
pr(1−p)n−r=1 (8.3)
or we can give a probabilistic argument as follows: The number of trials required
to obtain rsuccesses can be expressed as Y1+Y2+ ··· + Yr, where Y1equals
the number of trials required for the ﬁrst success, Y2the number of additional trials
after the ﬁrst success until the second success occurs, Y3the number of additional
trials until the third success, and so on. Because the trials are independent and allhave the same probability of success, it follows that Y
1,Y2,...,Yrare all geometric
random variables. Hence, each is ﬁnite with probability 1, sor/summationtext
i=1Yimust also be ﬁnite,
establishing Equation (8.3).
Any random variable Xwhose probability mass function is given by
Equation (8.2) is said to be a negative binomial random variable with parameters
(r,p). Note that a geometric random variable is just a negative binomial with param-
eter (1, p).
In the next example, we use the negative binomial to obtain another solution of
the problem of the points.

<<<PAGE 163>>>

150 Chapter 4 Random Variables
Example
8dIf independent trials, each resulting in a success with probability p, are performed,
what is the probability of rsuccesses occurring before mfailures?
Solution The solution will be arrived at by noting that rsuccesses will occur before
mfailures if and only if the rth success occurs no later than the (r +m−1) trial.
This follows because if the rth success occurs before or at the ( r+m−1) trial, then
it must have occurred before the mth failure, and conversely. Hence, from Equa-
tion (8.2), the desired probability is
r+m−1/summationdisplay
n=r/parenleftBigg
n−1
r−1/parenrightBigg
pr(1−p)n−r.
Example
8eThe Banach match problem
At all times, a pipe-smoking mathematician carries 2 matchboxes—1 in his left-hand
pocket and 1 in his right-hand pocket. Each time he needs a match, he is equally
likely to take it from either pocket. Consider the moment when the mathematicianﬁrst discovers that one of his matchboxes is empty. If it is assumed that both match-
boxes initially contained Nmatches, what is the probability that there are exactly k
matches, k=0, 1,...,N, in the other box?
Solution Let Edenote the event that the mathematician ﬁrst discovers that the
right-hand matchbox is empty and that there are kmatches in the left-hand box
at the time. Now, this event will occur if and only if the (N+1)choice of the right-
hand matchbox is made at the ( N+1+N−k) trial. Hence, from Equation (8.2)
(with p=1
2,r=N+1, and n=2N−k+1), we see that
P(E)=/parenleftBigg
2N−k
N/parenrightBigg/parenleftbigg1
2/parenrightbigg2N−k+1
Since there is an equal probability that it is the left-hand box that is ﬁrst discoveredto be empty and there are kmatches in the right-hand box at that time, the desired
result is
2P(E)=/parenleftBigg
2N−k
N/parenrightBigg/parenleftbigg1
2/parenrightbigg2N−k
.
Example
8fCompute the expected value and the variance of a negative binomial random vari-able with parameters randp.
Solution We have
E[Xk]=q/summationdisplay
n=rnk/parenleftBigg
n−1
r−1/parenrightBigg
pr(1−p)n−r
=r
pq/summationdisplay
n=rnk−1/parenleftBigg
n
r/parenrightBigg
pr+1(1−p)n−rsince n/parenleftBigg
n−1
r−1/parenrightBigg
=r/parenleftBigg
n
r/parenrightBigg
=r
pq/summationdisplay
m=r+1(m−1)k−1/parenleftBigg
m−1
r/parenrightBigg
pr+1(1−p)m−(r+1)by setting
m=n+1
=r
pE[(Y−1)k−1]

<<<PAGE 164>>>

A First Course in Probability 151
where Yis a negative binomial random variable with parameters r+1,p. Setting
k=1 in the preceding equation yields
E[X]=r
p
Setting k=2 in the equation for E[Xk] and using the formula for the expected value
of a negative binomial random variable gives
E[X2]=r
pE[Y−1]
=r
p/parenleftbiggr+1
p−1/parenrightbigg
Therefore,
Var(X)=r
p/parenleftbiggr+1
p−1/parenrightbigg
−/parenleftbiggr
p/parenrightbigg2
=r(1−p)
p2.
Thus, from Example 8f, if independent trials, each of which is a success with
probability p, are performed, then the expected value and variance of the number
of trials that it takes to amass rsuccesses is r/pandr(1−p)/p2, respectively.
Since a geometric random variable is just a negative binomial with parameter
r=1, it follows from the preceding example that the variance of a geometric random
variable with parameter pis equal to (1−p)/p2, which checks with the result of
Example 8c.
Example
8gFind the expected value and the variance of the number of times one must throw a
die until the outcome 1 has occurred 4 times.
Solution Since the random variable of interest is a negative binomial with parame-
tersr=4 and p=1
6, it follows that
E[X]=24
Var(X)=4/parenleftBig
5
6/parenrightBig
/parenleftBig
1
6/parenrightBig2=120 .
4.8.3 The Hypergeometric Random Variable
Suppose that a sample of size nis to be chosen randomly (without replacement)
from an urn containing Nballs, of which mare white and N−mare black. If we let
Xdenote the number of white balls selected, then
P{X=i}=/parenleftBigg
m
i/parenrightBigg/parenleftBigg
N−m
n−i/parenrightBigg
/parenleftBigg
N
n/parenrightBigg i=0, 1,...,n (8.4)
A random variable Xwhose probability mass function is given by Equation (8.4) for
some values of n,N,mis said to be a hypergeometric random variable.

<<<PAGE 165>>>

152 Chapter 4 Random Variables
Remark Although we have written the hypergeometric probability mass function
with igoing from 0 to n,P{X=i}will actually be 0, unless isatisﬁes the inequalities
n−(N−m)…i…min(n,m). However, Equation (8.4) is always valid because of
our convention that/parenleftBigg
r
k/parenrightBigg
is equal to 0 when either k<0o r r<k. .
Example
8hAn unknown number, say, N, of animals inhabit a certain region. To obtain some
information about the size of the population, ecologists often perform the follow-
ing experiment: They ﬁrst catch a number, say, m, of these animals, mark them in
some manner, and release them. After allowing the marked animals time to dispersethroughout the region, a new catch of size, say, n, is made. Let Xdenote the number
of marked animals in this second capture. If we assume that the population of ani-
mals in the region remained ﬁxed between the time of the two catches and that each
time an animal was caught it was equally likely to be any of the remaining uncaught
animals, it follows that Xis a hypergeometric random variable such that
P{X=i}=/parenleftBigg
m
i/parenrightBigg/parenleftBigg
N−m
n−i/parenrightBigg
/parenleftBigg
N
n/parenrightBigg KPi(N)
Suppose now that Xis observed to equal i. Then, since Pi(N)represents the
probability of the observed event when there are actually Nanimals present in the
region, it would appear that a reasonable estimate of Nwould be the value of N
that maximizes Pi(N). Such an estimate is called a maximum likelihood estimate.
(See Theoretical Exercises 13 and 18 for other examples of this type of estimationprocedure.)
The maximization of P
i(N)can be done most simply by ﬁrst noting that
Pi(N)
Pi(N−1)=(N−m)(N −n)
N(N−m−n+i)
Now, the preceding ratio is greater than 1 if and only if
(N−m)(N −n)ÚN(N−m−n+i)
or, equivalently, if and only if
N…mn
i
Thus, Pi(N)is ﬁrst increasing and then decreasing and reaches its maximum value at
the largest integral value not exceeding mn/i. This value is the maximum
likelihood estimate of N. For example, suppose that the initial catch consisted of
m=50 animals, which are marked and then released. If a subsequent catch consists
ofn=40 animals of which i=4 are marked, then we would estimate that there are
some 500 animals in the region. (Note that the preceding estimate could also havebeen obtained by assuming that the proportion of marked animals in the region,
m/N, is approximately equal to the proportion of marked animals in our secondcatch, i/n.) .
Example
8iA purchaser of electrical components buys them in lots of size 10. It is his policyto inspect 3 components randomly from a lot and to accept the lot only if all 3 are
nondefective. If 30 percent of the lots have 4 defective components and 70 percent
have only 1, what proportion of lots does the purchaser reject?

<<<PAGE 166>>>

A First Course in Probability 153
Solution LetAdenote the event that the purchaser accepts a lot. Now,
P(A)=P(A|lot has 4 defectives )3
10+P(A|lot has 1 defective )7
10
=/parenleftBigg
4
0/parenrightBigg/parenleftBigg
63/parenrightBigg
/parenleftBigg
10
3/parenrightBigg/parenleftbigg3
10/parenrightbigg
+/parenleftBigg
10/parenrightBigg/parenleftBigg
93/parenrightBigg
/parenleftBigg
10
3/parenrightBigg/parenleftbigg7
10/parenrightbigg
=54
100
Hence, 46 percent of the lots are rejected. .
Ifnballs are randomly chosen without replacement from a set of Nballs of
which the fraction p=m/N is white, then the number of white balls selected is
hypergeometric. Now, it would seem that when mand Nare large in relation to
n, it shouldn’t make much difference whether the selection is being done with orwithout replacement, because, no matter which balls have previously been selected,
when mand Nare large, each additional selection will be white with a probability
approximately equal to p. In other words, it seems intuitive that when mand Nare
large in relation to n, the probability mass function of Xshould approximately be
that of a binomial random variable with parameters nandp. To verify this intuition,
note that if Xis hypergeometric, then, for i…n,
P{X=i}=/parenleftBigg
m
i/parenrightBigg/parenleftBigg
N−m
n−i/parenrightBigg
/parenleftBigg
N
n/parenrightBigg
=m!
(m−i)!i!(N−m)!
(N−m−n+i)!(n−i)!(N−n)!n!
N!
=/parenleftBigg
n
i/parenrightBigg
m
Nm−1
N−1···m−i+1
N−i+1N−m
N−iN−m−1
N−i−1
···N−m−(n−i−1)
N−i−(n−i−1)
L/parenleftBigg
n
i/parenrightBigg
pi(1−p)n−iwhen p=m/N andmandNare
large in relation to nandi
Example
8jDetermine the expected value and the variance of X, a hypergeometric random vari-
able with parameters n,N, and m.
Solution
E[Xk]=n/summationdisplay
i=0ikP{X=i}
=n/summationdisplay
i=1ik/parenleftBigg
m
i/parenrightBigg/parenleftBigg
N−m
n−i/parenrightBigg/slashBigg/parenleftBigg
N
n/parenrightBigg
Using the identities
i/parenleftBigg
m
i/parenrightBigg
=m/parenleftBigg
m−1
i−1/parenrightBigg
and n/parenleftBigg
N
n/parenrightBigg
=N/parenleftBigg
N−1
n−1/parenrightBigg

<<<PAGE 167>>>

154 Chapter 4 Random Variables
we obtain
E[Xk]=nm
Nn/summationdisplay
i=1ik−1/parenleftBigg
m−1
i−1/parenrightBigg/parenleftBigg
N−m
n−i/parenrightBigg/slashBigg/parenleftBigg
N−1
n−1/parenrightBigg
=nm
Nn−1/summationdisplay
j=0(j+1)k−1/parenleftBigg
m−1
j/parenrightBigg/parenleftBigg
N−m
n−1−j/parenrightBigg/slashBigg/parenleftBigg
N−1
n−1/parenrightBigg
=nm
NE[(Y+1)k−1]
where Yis a hypergeometric random variable with parameters n−1,N−1, and
m−1. Hence, upon setting k=1, we have
E[X]=nm
N
In words, if nballs are randomly selected from a set of Nballs, of which mare white,
then the expected number of white balls selected is nm/N.
Upon setting k=2 in the equation for E[Xk], we obtain
E[X2]=nm
NE[Y+1]
=nm
N/bracketleftbigg(n−1)(m −1)
N−1+1/bracketrightbigg
where the ﬁnal equality uses our preceding result to compute the expected value of
the hypergeometric random variable Y.
Because E[X]=nm/N , we can conclude that
Var(X)=nm
N/bracketleftbigg(n−1)(m −1)
N−1+1−nm
N/bracketrightbigg
Letting p=m/N and using the identity
m−1
N−1=Np−1
N−1=p−1−p
N−1
shows that
Var(X)=np[(n −1)p−(n−1)1−p
N−1+1−np]
=np(1−p)/parenleftbigg
1−n−1
N−1/parenrightbigg
.
Remark We have shown in Example 8j that if nballs are randomly selected with-
out replacement from a set of Nballs, of which the fraction pare white, then the
expected number of white balls chosen is np. In addition, if Nis large in relation to
n[so that (N−n)/(N −1)is approximately equal to 1], then
Var(X)Lnp(1−p)
In other words, E[X] is the same as when the selection of the balls is done with
replacement (so that the number of white balls is binomial with parameters n
andp), and if the total collection of balls is large, then Var (X)is approximately equal
to what it would be if the selection were done with replacement. This is, of course,
exactly what we would have guessed, given our earlier result that when the number
of balls in the urn is large, the number of white balls chosen approximately has the
mass function of a binomial random variable. .

<<<PAGE 168>>>

A First Course in Probability 155
4.8.4 The Zeta (or Zipf) Distribution
A random variable is said to have a zeta (sometimes called the Zipf) distribution if
its probability mass function is given by
P{X=k}=C
kα+1k=1, 2,...
for some value of α> 0. Since the sum of the foregoing probabilities must equal 1,
it follows that
C=⎡
⎣q/summationdisplay
k=1/parenleftbigg1
k/parenrightbiggα+1⎤⎦−1
The zeta distribution owes its name to the fact that the function
ζ(s)=1+/parenleftbigg1
2/parenrightbiggs
+/parenleftbigg1
3/parenrightbiggs
+ ··· +/parenleftbigg1
k/parenrightbiggs
+ ···
is known in mathematical disciplines as the Riemann zeta function (after the
German mathematician G. F. B. Riemann).
The zeta distribution was used by the Italian economist V . Pareto to describe
the distribution of family incomes in a given country. However, it was G. K. Zipfwho applied zeta distribution to a wide variety of problems in different areas and, in
doing so, popularized its use.
4.9 Expected Value of Sums of Random Variables
A very important property of expectations is that the expected value of a sum of
random variables is equal to the sum of their expectations. In this section, we will
prove this result under the assumption that the set of possible values of the proba-
bility experiment—that is, the sample space S—is either ﬁnite or countably inﬁnite.
Although the result is true without this assumption (and a proof is outlined in the
theoretical exercises), not only will the assumption simplify the argument, but it will
also result in an enlightening proof that will add to our intuition about expectations.
So, for the remainder of this section, suppose that the sample space Sis either a ﬁnite
or a countably inﬁnite set.
For a random variable X,l e t X(s)denote the value of Xwhen s∈Sis the
outcome of the experiment. Now, if Xand Yare both random variables, then so
is their sum. That is, Z=X+Yis also a random variable. Moreover, Z(s)=
X(s)+Y(s).
Example
9aSuppose that the experiment consists of ﬂipping a coin 5 times, with the outcome
being the resulting sequence of heads and tails. Suppose Xis the number of heads
in the ﬁrst 3 ﬂips and Yis the number of heads in the ﬁnal 2 ﬂips. Let Z=X+Y.
Then, for instance, for the outcome s=(h,t,h,t,h),
X(s)=2
Y(s)=1
Z(s)=X(s)+Y(s)=3
meaning that the outcome (h,t,h,t,h)results in 2 heads in the ﬁrst three ﬂips, 1 head
in the ﬁnal two ﬂips, and a total of 3 heads in the ﬁve ﬂips. .

<<<PAGE 169>>>

156 Chapter 4 Random Variables
Let p(s)=P({s}) be the probability that sis the outcome of the experiment.
Because we can write any event Aas the ﬁnite or countably inﬁnite union of the
mutually exclusive events {s},s∈A, it follows by the axioms of probability that
P(A)=/summationdisplay
s∈Ap(s)
When A=S, the preceding equation gives
1=/summationdisplay
s∈Sp(s)
Now, let Xbe a random variable, and consider E[X]. Because X(s)is the value of X
when sis the outcome of the experiment, it seems intuitive that E[X]—the weighted
average of the possible values of X, with each value weighted by the probability that
Xassumes that value—should equal a weighted average of the values X(s),s∈S,
with X(s)weighted by the probability that sis the outcome of the experiment. We
now prove this intuition.
Proposition
9.1E[X]=/summationdisplay
s∈SX(s)p(s)
Proof Suppose that the distinct values of Xarexi,iÚ1.For each i,l e tS ibe the
event that Xis equal to xi. That is, Si={s:X(s)=xi}. Then,
E[X]=/summationdisplay
ixiP{X=xi}
=/summationdisplay
ixiP(Si)
=/summationdisplay
ixi/summationdisplay
s∈S ip(s)
=/summationdisplay
i/summationdisplay
s∈S ixip(s)
=/summationdisplay
i/summationdisplay
s∈S iX(s)p(s)
=/summationdisplay
s∈SX(s)p(s)
where the ﬁnal equality follows because S1,S2,...are mutually exclusive events
whose union is S.
Example
9bSuppose that two independent ﬂips of a coin that comes up heads with probability p
are made, and let Xdenote the number of heads obtained. Because
P(X=0)=P(t,t)=(1−p)2,
P(X=1)=P(h,t)+P(t,h)=2p(1−p)
P(X=2)=P(h,h)=p2
it follows from the deﬁnition of expected value that
E[X]=0·(1−p)2+1·2p(1−p)+2·p2=2p

<<<PAGE 170>>>

A First Course in Probability 157
which agrees with
E[X]=X(h,h)p2+X(h,t)p(1−p)+X(t,h)(1−p)p+X(t,t)(1−p)2
=2p2+p(1−p)+(1−p)p
=2p .
We now prove the important and useful result that the expected value of a sum of
random variables is equal to the sum of their expectations.
Corollary
9.2For random variables X1,X2,...,Xn,
E⎡
⎣n/summationdisplay
i=1Xi⎤⎦=n/summationdisplay
i=1E[Xi]
Proof Let Z=/summationtextn
i=1Xi. Then, by Proposition 9.1,
E[Z]=/summationdisplay
s∈SZ(s)p(s)
=/summationdisplay
s∈S/parenleftbig
X1(s)+X2(s)+...+Xn(s)/parenrightbig
p(s)
=/summationdisplay
s∈SX1(s)p(s) +/summationdisplay
s∈SX2(s)p(s) +...+/summationdisplay
s∈SXn(s)p(s)
=E[X1]+E[X2]+...+E[Xn] .
Example
9cFind the expected value of the sum obtained when nfair dice are rolled.
Solution LetXbe the sum. We will compute E[X] by using the representation
X=n/summationdisplay
i=1Xi
where Xiis the upturned value on die i. Because Xiis equally likely to be any of the
values from 1 to 6, it follows that
E[Xi]=6/summationdisplay
i=1i(1/6) =21/6=7/2
which yields the result
E[X]=E⎡
⎣n/summationdisplay
i=1Xi⎤⎦=n/summationdisplay
i=1E[Xi]=3.5n .
Example
9dFind the expected total number of successes that result from ntrials when trial iis a
success with probability pi,i=1,...,n.
Solution Letting
Xi=/braceleftBigg
1, if trial iis a success
0, if trial iis a failure
we have the representation
X=n/summationdisplay
i=1Xi

<<<PAGE 171>>>

158 Chapter 4 Random Variables
Consequently,
E[X]=n/summationdisplay
i=1E[Xi]=n/summationdisplay
i=1pi
Note that this result does not require that the trials be independent. It includes as
a special case the expected value of a binomial random variable, which assumes
independent trials and all pi=p, and thus has mean np. It also gives the expected
value of a hypergeometric random variable representing the number of white ballsselected when nballs are randomly selected, without replacement, from an urn of
Nballs of which mare white. We can interpret the hypergeometric as representing
the number of successes in ntrials, where trial iis said to be a success if the ith ball
selected is white. Because the ith ball selected is equally likely to be any of the Nballs
and thus has probability m/N of being white, it follows that the hypergeometric is
the number of successes in ntrials in which each trial is a success with probability
p=m/N . Hence, even though these hypergeometric trials are dependent, it follows
from the result of Example 9d that the expected value of the hypergeometric is np=
nm/N . .
Example
9eDerive an expression for the variance of the number of successful trials in Example
9d, and apply it to obtain the variance of a binomial random variable with parame-
tersnandp, and of a hypergeometric random variable equal to the number of white
balls chosen when nballs are randomly chosen from an urn containing Nballs of
which mare white.
Solution Letting Xbe the number of successful trials, and using the same represen-
tation for X—namely, X=/summationtextn
i=1Xi—as in the previous example, we have
E[X2]=E⎡
⎢⎢⎣⎛
⎝n/summationdisplay
i=1Xi⎞⎠⎛
⎜⎝n/summationdisplay
j=1Xj⎞
⎟⎠⎤
⎥⎥⎦
=E⎡
⎢⎣n/summationdisplay
i=1Xi⎛
⎝Xi+/summationdisplay
jZiXj⎞⎠⎤
⎥⎦
=E⎡
⎣n/summationdisplay
i=1X2
i+n/summationdisplay
i=1/summationdisplay
jZiXiXj⎤⎦
=n/summationdisplay
i=1E[X2
i]+n/summationdisplay
i=1/summationdisplay
jZiE[XiXj]
=/summationdisplay
ipi+n/summationdisplay
i=1/summationdisplay
jZiE[XiXj] (9.1)
where the ﬁnal equation used that X2
i=Xi.However, because the possible values
of both XiandXjare 0 or 1, it follows that
XiXj=/braceleftBigg
1, if Xi=1,Xj=1
0, otherwise
Hence,
E[XiXj]=P{Xi=1,Xj=1}= P(trials iandjare successes )

<<<PAGE 172>>>

A First Course in Probability 159
Now, on the one hand, if Xis binomial, then, for iZj, the results of trial iand trial j
are independent, with each being a success with probability p. Therefore,
E[XiXj]=p2, iZj
Together with Equation (9.1), the preceding equation shows that for a binomial ran-
dom variable X,
E[X2]=np+n(n−1)p2
implying that
Var(X)=E[X2]−(E[X])2=np+n(n−1)p2−n2p2=np(1−p)
On the other hand, if Xis hypergeometric, then, given that a white ball is chosen
in trial i, each of the other N−1 balls, of which m−1 are white, is equally likely
to be the jth ball chosen, for jZi. Consequently, for jZi,
P{Xi=1,Xj=1}= P{Xi=1}P{Xj=1|X i=1}=m
Nm−1
N−1
Using pi=m/N , we now obtain, from Equation (9.1),
E[X2]=nm
N+n(n−1)m
Nm−1
N−1
Consequently,
Var(X)=nm
N+n(n−1)m
Nm−1
N−1−/parenleftbiggnm
N/parenrightbigg2
which, as shown in Example 8j, can be simpliﬁed to yield
Var(X)=np(1−p)/parenleftbigg
1−n−1
N−1/parenrightbigg
where p=m/N. .
4.10 Properties of the Cumulative Distribution Function
Recall that for the distribution function FofX,F(b)denotes the probability that
the random variable Xtakes on a value that is less than or equal to b. Following are
some properties of the cumulative distribution function (c.d.f.) F:
1.Fis a nondecreasing function; that is, if a<b, then F(a)…F(b).
2. lim
b→qF(b)=1.
3. lim
b→−qF(b)=0.
4.Fis right continuous. That is, for any band any decreasing sequence bn,nÚ1,
that converges to b, limn→qF(bn)=F(b).
Property 1 follows, as was noted in Section 4.1, because, for a<b, the event
{X…a}is contained in the event {X…b}and so cannot have a larger probabil-
ity. Properties 2, 3, and 4 all follow from the continuity property of probabilities
(Section 2.6). For instance, to prove property 2, we note that if bnincreases to q,

<<<PAGE 173>>>

160 Chapter 4 Random Variables
then the events {X…bn},nÚ1, are increasing events whose union is the event
{X<q}. Hence, by the continuity property of probabilities,
limn→qP{X…bn}=P{X<q}=1
which proves property 2.
The proof of property 3 is similar and is left as an exercise. To prove property 4,
we note that if bndecreases to b, then {X…bn},nÚ1, are decreasing events whose
intersection is {X…b}. The continuity property then, yields
limn→qP{X…bn}=P{X…b}
which veriﬁes property 4.
All probability questions about Xcan be answered in terms of the c.d.f., F.F o r
example,
P{a<X…b}= F(b)−F(a) for all a<b (10.1)
This equation can best be seen to hold if we write the event {X…b}as the union of
the mutually exclusive events {X…a}and{a<X…b}. That is,
{X…b}={ X…a}∪{ a<X…b}
so
P{X…b}=P {X…a}+ P{a<X…b}
which establishes Equation (10.1).
If we want to compute the probability that Xis strictly less than b, we can again
apply the continuity property to obtain
P{X<b}= P/parenleftBigg
limn→q/braceleftbigg
X…b−1
n/bracerightbigg/parenrightBigg
=limn→qP/parenleftbigg
X…b−1
n/parenrightbigg
=limn→qF/parenleftbigg
b−1
n/parenrightbigg
Note that P{X<b}does not necessarily equal F(b), since F(b)also includes the
probability that Xequals b.

<<<PAGE 174>>>

A First Course in Probability 161
Example
10aThe distribution function of the random variable Xis given by
F(x)=⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩0 x<0
x
20…x<1
2
31…x<2
11
122…x<3
13 …x
A graph of F(x)is presented in Figure 4.9. Compute (a) P{X<3},( b ) P{X=1},
(c)P{X>1
2}, and (d) P{2<X…4}.
Solution (a) P{X<3}= limnP/braceleftbigg
X…3−1
n/bracerightbigg
=limnF/parenleftbigg
3−1
n/parenrightbigg
=11
12
(b)P{X=1}= P{X…1}−P {X<1}
=F(1)−limnF/parenleftbigg
1−1
n/parenrightbigg
=2
3−1
2=1
6
(c)
P/braceleftbigg
X>1
2/bracerightbigg
=1−P/braceleftbigg
X…1
2/bracerightbigg
=1−F/parenleftbigg1
2/parenrightbigg
=3
4
(d)P{2<X…4}=F (4)−F(2)
=1
12.
1–22–31
11—12
123xF(x)
Figure 4.9 Graph of F(x).

<<<PAGE 175>>>

162 Chapter 4 Random Variables
Summary
A real-valued function deﬁned on the outcome of a prob-
ability experiment is called a random variable.
IfXis a random variable, then the function F(x)
deﬁned by
F(x)=P{X…x}
is called the distribution function ofX. All probabilities
concerning Xcan be stated in terms of F.
A random variable whose set of possible values is
either ﬁnite or countably inﬁnite is called discrete.I f Xis a
discrete random variable, then the function
p(x)=P{X=x}
is called the probability mass function ofX. Also, the
quantity E[X] deﬁned by
E[X]=/summationdisplay
x:p(x)>0xp(x)
is called the expected value ofX.E[X] is also commonly
called the mean or the expectation ofX.
A useful identity states that for a function g,
E[g(X)]=/summationdisplay
x:p(x)>0g(x)p(x)
The variance of a random variable X, denoted by Var (X),
is deﬁned by
Var(X)=E[(X−E[X])2]
The variance, which is equal to the expected square of
the difference between Xand its expected value, is a mea-
sure of the spread of the possible values of X. A useful
identity is
Var(X)=E[X2]−(E[X])2
The quantity√Var(X)is called the standard deviation
ofX.
We now note some common types of discrete random
variables. The random variable Xwhose probability mass
function is given by
p(i)=/parenleftbigg
n
i/parenrightbigg
pi(1−p)n−ii=0,...,n
is said to be a binomial random variable with parameters n
andp. Such a random variable can be interpreted as being
the number of successes that occur when nindependent
trials, each of which results in a success with probability p,
are performed. Its mean and variance are given by
E[X]=np Var(X)=np(1−p)The random variable Xwhose probability mass function is
given by
p(i)=e−λλi
i!iÚ0
is said to be a Poisson random variable with parameter λ.
If a large number of (approximately) independent trials
are performed, each having a small probability of beingsuccessful, then the number of successful trials that result
will have a distribution that is approximately that of a Pois-
son random variable. The mean and variance of a Poisson
random variable are both equal to its parameter λ.T h a ti s ,
E[X]=Var(X)=λ
The random variable Xwhose probability mass function is
given by
p(i)=p(1−p)
i−1i=1, 2,...
is said to be a geometric random variable with parameter
p. Such a random variable represents the trial number ofthe ﬁrst success when each trial is independently a success
with probability p. Its mean and variance are given by
E[X]=1
pVar(X)=1−p
p2
The random variable Xwhose probability mass function is
given by
p(i)=/parenleftbigg
i−1
r−1/parenrightbigg
pr(1−p)i−riÚr
is said to be a negative binomial random variable with
parameters randp. Such a random variable represents the
trial number of the rth success when each trial is indepen-
dently a success with probability p. Its mean and variance
are given by
E[X]=r
pVar(X)=r(1−p)
p2
Ahypergeometric random variable Xwith parameters n,
N,a n dm represents the number of white balls selected
when nballs are randomly chosen from an urn that con-
tains Nballs of which mare white. The probability mass
function of this random variable is given by
p(i)=/parenleftbigg
m
i/parenrightbigg/parenleftbigg
N−m
n−i/parenrightbigg
/parenleftbigg
N
n/parenrightbigg i=0,...,m
With p=m/N , its mean and variance are
E[X]=np Var(X)=N−n
N−1np(1−p)

<<<PAGE 176>>>

A First Course in Probability 163
An important property of the expected value is that the
expected value of a sum of random variables is equal tothe sum of their expected values. That is,E⎡
⎣n/summationdisplay
i=1Xi⎤⎦=n/summationdisplay
i=1E[Xi]
Problems
4.1.Two balls are chosen randomly from an urn contain-
ing 8 white, 4 black, and 2 orange balls. Suppose that we
win $2 for each black ball selected and we lose $1 for eachwhite ball selected. Let Xdenote our winnings. What are
the possible values of X, and what are the probabilities
associated with each value?
4.2. Two fair dice are rolled. Let Xequal the
product of the 2 dice. Compute P{X=i}fori=1,..., 36.
4.3. Three dice are rolled. By assuming that each of the
6
3=216 possible outcomes is equally likely, ﬁnd the
probabilities attached to the possible values that Xcan
take on, where Xis the sum of the 3 dice.
4.4. Five men and 5 women are ranked according to their
scores on an examination. Assume that no two scores are
alike and all 10! possible rankings are equally likely. Let
Xdenote the highest ranking achieved by a woman. (For
instance, X=1 if the top-ranked person is female.) Find
P{X=i },i=1, 2, 3, ...,8 ,9 ,1 0 .
4.5. LetXrepresent the difference between the number
of heads and the number of tails obtained when a coin is
tossed ntimes. What are the possible values of X?
4.6. In Problem 4.5, for n=3 ,i ft h ec o i ni sa s s u m e df a i r ,
what are the probabilities associated with the values thatXcan take on?
4.7.Suppose that a die is rolled twice. What are the
possible values that the following random variables cantake on:
(a)the maximum value to appear in the two rolls;
(b)the minimum value to appear in the two rolls;
(c)the sum of the two rolls;
(d)the value of the ﬁrst roll minus the value of the second
roll?
4.8. If the die in Problem 4.7 is assumed fair, calculate the
probabilities associated with the random variables in parts
(a) through (d).
4.9. Repeat Example 1c when the balls are selected with
replacement.
4.10. Let Xbe the winnings of a gambler. Let p(i)=
P(X=i)and suppose that
p(0)=1/3; p(1)=p(−1) =13/55;
p(2)=p(−2) =1/11; p(3)=p(−3) =1/165
Compute the conditional probability that the gambler
wins i,i=1, 2, 3, given that he wins a positive amount.4.11. (a)An integer Nis to be selected at random from
{1, 2,...,(10)
3}in the sense that each integer has the same
probability of being selected. What is the probability thatNwill be divisible by 3? by 5? by 7? by 15? by 105? How
would your answer change if (10)
3is replaced by (10)kas
kbecame larger and larger?
(b)An important function in number theory—one whose
properties can be shown to be related to what is proba-
bly the most important unsolved problem of mathemat-
ics, the Riemann hypothesis—is the M ¨obius function μ(n),
deﬁned for all positive integral values nas follows: Factor n
into its prime factors. If there is a repeated prime factor, asin 12=2·2·3o r4 9 =7·7, then μ(n)is deﬁned to equal
0. Now let Nbe chosen at random from {1, 2,...(10)
k},
where kis large. Determine P{μ(N)=0}ask→q.
Hint : To compute P{μ(N)Z0}, use the identity
q/productdisplay
i=1P2
i−1
P2
i=/parenleftbigg3
4/parenrightbigg/parenleftbigg8
9/parenrightbigg/parenleftbigg24
25/parenrightbigg/parenleftbigg48
49/parenrightbigg
···=6
π2
where Piis the ith-smallest prime. (The number 1 is not a
prime.)
4.12. In the game of Two-Finger Morra, 2 players show 1
or 2 ﬁngers and simultaneously guess the number of ﬁn-
gers their opponent will show. If only one of the players
guesses correctly, he wins an amount (in dollars) equal to
the sum of the ﬁngers shown by him and his opponent. Ifboth players guess correctly or if neither guesses correctly,
then no money is exchanged. Consider a speciﬁed player,
and denote by Xthe amount of money he wins in a single
game of Two-Finger Morra.
(a)If each player acts independently of the other, and if
each player makes his choice of the number of ﬁngers he
will hold up and the number he will guess that his oppo-
nent will hold up in such a way that each of the 4 possibili-
ties is equally likely, what are the possible values of Xand
what are their associated probabilities?
(b)Suppose that each player acts independently of the
other. If each player decides to hold up the same num-
ber of ﬁngers that he guesses his opponent will hold up,and if each player is equally likely to hold up 1 or 2 ﬁn-
gers, what are the possible values of Xand their associated
probabilities?
4.13. A salesman has scheduled two appointments to sell
encyclopedias. His ﬁrst appointment will lead to a sale with
probability .3, and his second will lead independently to a
sale with probability .6. Any sale made is equally likely to
be either for the deluxe model, which costs $1000, or thestandard model, which costs $500. Determine the proba-
bility mass function of X, the total dollar value of all sales.

<<<PAGE 177>>>

164 Chapter 4 Random Variables
4.14. Five distinct numbers are randomly distributed to
players numbered 1 through 5. Whenever two players
compare their numbers, the one with the higher one is
declared the winner. Initially, players 1 and 2 compare
their numbers; the winner then compares her number withthat of player 3, and so on. Let Xdenote the number of
times player 1 is a winner. Find P{X=i},i=0, 1, 2, 3, 4.
4.15. The National Basketball Association (NBA) draft
lottery involves the 11 teams that had the worst won–lostrecords during the year. A total of 66 balls are placed inan urn. Each of these balls is inscribed with the name of a
team: Eleven have the name of the team with the worst
record, 10 have the name of the team with the second-worst record, 9 have the name of the team with the third-
worst record, and so on (with 1 ball having the name of
the team with the 11th-worst record). A ball is then cho-
sen at random, and the team whose name is on the ball is
given the ﬁrst pick in the draft of players about to enter theleague. Another ball is then chosen, and if it “belongs” to
a team different from the one that received the ﬁrst draft
pick, then the team to which it belongs receives the seconddraft pick. (If the ball belongs to the team receiving the
ﬁrst pick, then it is discarded and another one is chosen;
this continues until the ball of another team is chosen.)Finally, another ball is chosen, and the team named on the
ball (provided that it is different from the previous two
teams) receives the third draft pick. The remaining draftpicks 4 through 11 are then awarded to the 8 teams that
did not “win the lottery,” in inverse order of their won–lost
records. For instance, if the team with the worst record didnot receive any of the 3 lottery picks, then that team would
receive the fourth draft pick. Let Xdenote the draft pick
of the team with the worst record. Find the probability
mass function of X.
4.16. In Problem 4.15, let team number 1 be the team with
the worst record, let team number 2 be the team with thesecond-worst record, and so on. Let Y
idenote the team
that gets draft pick number i. (Thus, Y1=3 if the ﬁrst ball
chosen belongs to team number 3.) Find the probabilitymass function of (a) Y
1,( b )Y 2,a n d( c )Y 3.
4.17. Suppose that the distribution function of Xis
given by
F(b)=⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩0 b<0
b
40…b<1
1
2+b−1
41…b<2
11
122…b<3
13 …b
(a)Find P{X=i},i=1, 2, 3.
(b)Find P{1
2<X<3
2}.4.18. Four independent ﬂips of a fair coin are made. Let X
denote the number of heads obtained. Plot the probability
mass function of the random variable X−2.
4.19. If the distribution function of Xis given by
F(b)=⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩0 b<0
1
20…b<1
3
51…b<2
4
52…b<3
9
103…b<3.5
1 bÚ3.5
calculate the probability mass function of X.
4.20. A gambling book recommends the following “win-
ning strategy” for the game of roulette: Bet $1 on red. If
red appears/parenleftbig
which has probability18
38/parenrightbig
, then take the $1
proﬁt and quit. If red does not appear and you lose this
bet/parenleftbig
which has probability20
38of occurring/parenrightbig
, make addi-
tional $1 bets on red on each of the next two spins of the
roulette wheel and then quit. Let Xdenote your winnings
when you quit.
(a)Find P{X>0}.
(b)Are you convinced that the strategy is indeed a “win-
ning” strategy? Explain your answer!
(c)Find E[X].
4.21. Four buses carrying 148 students from the same
school arrive at a football stadium. The buses carry, respec-
tively, 40, 33, 25, and 50 students. One of the students is
randomly selected. Let Xdenote the number of students
who were on the bus carrying the randomly selected stu-dent. One of the 4 bus drivers is also randomly selected.LetYdenote the number of students on her bus.
(a)Which of E[X]o rE [Y] do you think is larger? Why?
(b)Compute E[X]a n dE [Y].
4.22. Suppose that two teams play a series of games that
ends when one of them has won igames. Suppose that
each game played is, independently, won by team Awith
probability p. Find the expected number of games that are
played when (a) i=2a n d( b ) i=3. Also, show in both
cases that this number is maximized when p=
1
2.
4.23. You have $1000, and a certain commodity presently
sells for $2 per ounce. Suppose that after one week the
commodity will sell for either $1 or $4 an ounce, with these
two possibilities being equally likely.
(a)If your objective is to maximize the expected amount
of money that you possess at the end of the week, what
strategy should you employ?

<<<PAGE 178>>>

A First Course in Probability 165
(b)If your objective is to maximize the expected amount
of the commodity that you possess at the end of the week,
what strategy should you employ?
4.24. Aand Bplay the following game: Awrites down
either number 1 or number 2, and Bmust guess which
one. If the number that Ahas written down is iand Bhas
guessed correctly, Breceives iunits from A.I fB makes a
wrong guess, Bpays3
4unit to A.I fB randomizes his deci-
sion by guessing 1 with probability pand 2 with probability
1−p, determine his expected gain if (a) Ahas written
down number 1 and (b) Ahas written down number 2.
What value of pmaximizes the minimum possible value
ofB’s expected gain, and what is this maximin value?
(Note that B’s expected gain depends not only on p, but
also on what Adoes.)
Consider now player A. Suppose that she also random-
izes her decision, writing down number 1 with probability
q.W h a ti s A’s expected loss if (c) Bchooses number 1 and
(d)Bchooses number 2?
What value of qminimizes A’s maximum expected loss?
Show that the minimum of A’s maximum expected loss
is equal to the maximum of B’s minimum expected gain.
This result, known as the minimax theorem , was ﬁrst
established in generality by the mathematician John von
Neumann and is the fundamental result in the mathemati-
cal discipline known as the theory of games. The commonvalue is called the value of the game to player B.
4.25. Two coins are to be ﬂipped. The ﬁrst coin will land
on heads with probability .6, the second with probability.7. Assume that the results of the ﬂips are independent,and let Xequal the total number of heads that result.
(a)Find P{X=1}.
(b)Determine E[X].
4.26. One of the numbers 1 through 10 is randomly cho-
sen. You are to try to guess the number chosen by askingquestions with “yes–no” answers. Compute the expectednumber of questions you will need to ask in each of the
following two cases:
(a)Your ith question is to be “Is it i?” i=
1, 2, 3, 4, 5, 6, 7, 8, 9, 10.
(b)With each question you try to eliminate one-half of the
remaining numbers, as nearly as possible.
4.27. An insurance company writes a policy to the effect
that an amount of money Amust be paid if some event
Eoccurs within a year. If the company estimates that E
will occur within a year with probability p, what should it
charge the customer in order that its expected proﬁt will
be 10 percent of A?
4.28. A sample of 3 items is selected at random from a
box containing 20 items of which 4 are defective. Find theexpected number of defective items in the sample.4.29. There are two possible causes for a breakdown of a
machine. To check the ﬁrst possibility would cost C
1dol-
lars, and, if that were the cause of the breakdown, the
trouble could be repaired at a cost of R1dollars. Similarly,
there are costs C2and R2associated with the second pos-
sibility. Let pand 1 −pdenote, respectively, the probabil-
ities that the breakdown is caused by the ﬁrst and second
possibilities. Under what conditions on p,Ci,Ri,i=1, 2,
should we check the ﬁrst possible cause of breakdown
and then the second, as opposed to reversing the check-
ing order, so as to minimize the expected cost involved inreturning the machine to working order?
Note : If the ﬁrst check is negative, we must still check the
other possibility.
4.30. A person tosses a fair coin until a tail appears for the
ﬁrst time. If the tail appears on the nth ﬂip, the person wins
2
ndollars. Let Xdenote the player’s winnings. Show that
E[X]=+q. This problem is known as the St. Petersburg
paradox.
(a)Would you be willing to pay $1 million to play this
game once?
(b)Would you be willing to pay $1 million for each game
if you could play for as long as you liked and only had to
settle up when you stopped playing?
4.31. Each night different meteorologists give us the prob-
ability that it will rain the next day. To judge how well these
people predict, we will score each of them as follows: If a
meteorologist says that it will rain with probability p,t h e n
he or she will receive a score of
1−(1−p)2if it does rain
1−p2if it does not rain
We will then keep track of scores over a certain time span
and conclude that the meteorologist with the highest aver-age score is the best predictor of weather. Suppose now
that a given meteorologist is aware of our scoring mecha-
nism and wants to maximize his or her expected score. Ifthis person truly believes that it will rain tomorrow with
probability p
∗, what value of pshould he or she assert so
as to maximize the expected score?
4.32. To determine whether they have a certain disease,
100 people are to have their blood tested. However, rather
than testing each individual separately, it has been decided
ﬁrst to place the people into groups of 10. The blood sam-
ples of the 10 people in each group will be pooled andanalyzed together. If the test is negative, one test will suf-
ﬁce for the 10 people, whereas if the test is positive, each
of the 10 people will also be individually tested and, in all,11 tests will be made on this group. Assume that the prob-
ability that a person has the disease is .1 for all people,
independently of one another, and compute the expectednumber of tests necessary for each group. (Note that we
are assuming that the pooled test will be positive if at least
one person in the pool has the disease.)

<<<PAGE 179>>>

166 Chapter 4 Random Variables
4.33. A newsboy purchases papers at 10 cents and sells
them at 15 cents. However, he is not allowed to return
unsold papers. If his daily demand is a binomial random
variable with n=10,p=1
3, approximately how many
papers should he purchase so as to maximize his expected
proﬁt?
4.34. In Example 4b, suppose that the department store
incurs an additional cost of cfor each unit of unmet
demand. (This type of cost is often referred to as a good-
will cost because the store loses the goodwill of those
customers whose demands it cannot meet.) Compute the
expected proﬁt when the store stocks sunits, and deter-
mine the value of sthat maximizes the expected proﬁt.
4.35. A box contains 5 red and 5 blue marbles. Two mar-
bles are withdrawn randomly. If they are the same color,then you win $1.10; if they are different colors, then you
win−$1.00. (That is, you lose $1.00.) Calculate
(a)the expected value of the amount you win;
(b)the variance of the amount you win.
4.36. Consider Problem 4.22 with i=2. Find the variance
of the number of games played, and show that this number
is maximized when p=
1
2.
4.37. Find Var (X)and Var(Y )forXand Yas given in
Problem 4.21.
4.38. IfE[X]=1a n dV a r (X)=5, ﬁnd
(a)E[(2+X)2];
(b)Var(4+3X).
4.39. A ball is drawn from an urn containing 3 white and
3 black balls. After the ball is drawn, it is replaced and
another ball is drawn. This process goes on indeﬁnitely.
What is the probability that of the ﬁrst 4 balls drawn,
exactly 2 are white?
4.40. On a multiple-choice exam with 3 possible answers
for each of the 5 questions, what is the probability that a
student will get 4 or more correct answers just by guessing?
4.41. A man claims to have extrasensory perception. As a
test, a fair coin is ﬂipped 10 times and the man is asked to
predict the outcome in advance. He gets 7 out of 10 cor-
rect. What is the probability that he would have done at
least this well if he did not have ESP?
4.42. AandBwill take the same 10-question examination.
Each question will be answered correctly by Awith prob-
ability .7, independently of her results on other questions.
Each question will be answered correctly by Bwith prob-
ability .4, independently both of her results on the other
questions and on the performance of A.
(a)Find the expected number of questions that are
answered correctly by both AandB.
(b)Find the variance of the number of questions that are
answered correctly by either AorB.4.43. A communications channel transmits the digits 0 and
1. However, due to static, the digit transmitted is incor-
rectly received with probability .2. Suppose that we want
to transmit an important message consisting of one binary
digit. To reduce the chance of error, we transmit 00000instead of 0 and 11111 instead of 1. If the receiver of the
message uses “majority” decoding, what is the probabil-
ity that the message will be wrong when decoded? Whatindependence assumptions are you making?
4.44. A satellite system consists of ncomponents and
functions on any given day if at least kof the ncom-
ponents function on that day. On a rainy day, each of
the components independently functions with probability
p
1, whereas on a dry day, each independently functions
with probability p2. If the probability of rain tomorrow
isα, what is the probability that the satellite system will
function?
4.45. A student is getting ready to take an important oral
examination and is concerned about the possibility of hav-
ing an “on” day or an “off” day. He ﬁgures that if he has
an on day, then each of his examiners will pass him, inde-
pendently of one another, with probability .8, whereas ifhe has an off day, this probability will be reduced to .4.
Suppose that the student will pass the examination if a
majority of the examiners pass him. If the student believesthat he is twice as likely to have an off day as he is to have
an on day, should he request an examination with 3 exam-
iners or with 5 examiners?
4.46. Suppose that it takes at least 9 votes from a 12-
member jury to convict a defendant. Suppose also that the
probability that a juror votes a guilty person innocent is .2,whereas the probability that the juror votes an innocent
person guilty is .1. If each juror acts independently and if
65 percent of the defendants are guilty, ﬁnd the probabilitythat the jury renders a correct decision. What percentage
of defendants is convicted?
4.47. In some military courts, 9 judges are appointed.
However, both the prosecution and the defense attorneys
are entitled to a peremptory challenge of any judge, in
which case that judge is removed from the case and is not
replaced. A defendant is declared guilty if the majorityof judges cast votes of guilty, and he or she is declared
innocent otherwise. Suppose that when the defendant is,
in fact, guilty, each judge will (independently) vote guilty
with probability .7, whereas when the defendant is, in fact,
innocent, this probability drops to .3.
(a)What is the probability that a guilty defendant is
declared guilty when there are (i) 9, (ii) 8, and (iii) 7
judges?
(b)Repeat part (a) for an innocent defendant.
(c)If the prosecuting attorney does not exercise the right
to a peremptory challenge of a judge, and if the defense
is limited to at most two such challenges, how many chal-
lenges should the defense attorney make if he or she is 60
percent certain that the client is guilty?

<<<PAGE 180>>>

A First Course in Probability 167
4.48. It is known that diskettes produced by a certain com-
pany will be defective with probability .01, independently
of one another. The company sells the diskettes in pack-
ages of size 10 and offers a money-back guarantee that at
most 1 of the 10 diskettes in the package will be defective.The guarantee is that the customer can return the entire
package of diskettes if he or she ﬁnds more than 1 defec-
tive diskette in it. If someone buys 3 packages, what is theprobability that he or she will return exactly 1 of them?
4.49. When coin 1 is ﬂipped, it lands on heads with prob-
ability .4; when coin 2 is ﬂipped, it lands on heads with
probability .7. One of these coins is randomly chosen and
ﬂipped 10 times.
(a)What is the probability that the coin lands on heads on
exactly 7 of the 10 ﬂips?
(b)Given that the ﬁrst of these 10 ﬂips lands heads, what
is the conditional probability that exactly 7 of the 10 ﬂips
land on heads?
4.50. Suppose that a biased coin that lands on heads with
probability pis ﬂipped 10 times. Given that a total of 6
heads results, ﬁnd the conditional probability that the ﬁrst
3 outcomes are
(a)h,t,t(meaning that the ﬁrst ﬂip results in heads, the
second in tails, and the third in tails);
(b)t,h,t.
4.51. The expected number of typographical errors on a
page of a certain magazine is .2. What is the probability
that the next page you read contains (a) 0 and (b) 2 or
more typographical errors? Explain your reasoning!
4.52. The monthly worldwide average number of airplane
crashes of commercial airlines is 3.5. What is the probabil-
ity that there will be
(a)at least 2 such accidents in the next month;
(b)at most 1 accident in the next month?
Explain your reasoning!
4.53. Approximately 80,000 marriages took place in the
state of New York last year. Estimate the probability that
for at least one of these couples,
(a)both partners were born on April 30;
(b)both partners celebrated their birthday on the same
day of the year.
State your assumptions.
4.54. Suppose that the average number of cars abandoned
weekly on a certain highway is 2.2. Approximate the prob-
ability that there will be
(a)no abandoned cars in the next week;
(b)at least 2 abandoned cars in the next week.
4.55. A certain typing agency employs 2 typists. The aver-
age number of errors per article is 3 when typed by the ﬁrst
typist and 4.2 when typed by the second. If your article isequally likely to be typed by either typist, approximate theprobability that it will have no errors.
4.56. How many people are needed so that the probability
that at least one of them has the same birthday as you is
greater than
1
2?
4.57. Suppose that the number of accidents occurring on
a highway each day is a Poisson random variable with
parameter λ=3.
(a)Find the probability that 3 or more accidents occur
today.
(b)Repeat part (a) under the assumption that at least 1
accident occurs today.
4.58. Compare the Poisson approximation with the correct
binomial probability for the following cases:
(a)P{X=2}when n=8,p=.1;
(b)P{X=9}when n=10,p=.95;
(c)P{X=0}when n=10,p=.1;
(d)P{X=4}when n=9,p=.2.
4.59. If you buy a lottery ticket in 50 lotteries, in each of
which your chance of winning a prize is1
100, what is the
(approximate) probability that you will win a prize(a)at least once?
(b)exactly once?
(c)at least twice?
4.60. The number of times that a person contracts a cold
in a given year is a Poisson random variable with param-
eterλ=5. Suppose that a new wonder drug (based on
large quantities of vitamin C) has just been marketed that
reduces the Poisson parameter to λ=3 for 75 percent of
the population. For the other 25 percent of the population,the drug has no appreciable effect on colds. If an individ-ual tries the drug for a year and has 2 colds in that time,
how likely is it that the drug is beneﬁcial for him or her?
4.61. The probability of being dealt a full house in a hand
of poker is approximately .0014. Find an approximation
for the probability that in 1000 hands of poker, you will be
dealt at least 2 full houses.
4.62. Consider nindependent trials, each of which results
in one of the outcomes 1, ...,kwith respective probabil-
ities p
1,...,pk,/summationtextk
i=1pi=1.Show that if all the piare
small, then the probability that no trial outcome occurs
more than once is approximately equal to exp (−n(n −1)/summationtext
ip2
i/2).
4.63. People enter a gambling casino at a rate of 1 every 2
minutes.
(a)What is the probability that no one enters between
12:00 and 12:05?
(b)What is the probability that at least 4 people enter the
casino during that time?

<<<PAGE 181>>>

168 Chapter 4 Random Variables
4.64. The suicide rate in a certain state is 1 suicide per
100,000 inhabitants per month.
(a)Find the probability that in a city of 400,000 inhabitants
within this state, there will be 8 or more suicides in a given
month.
(b)What is the probability that there will be at least 2
months during the year that will have 8 or more suicides?(c)Counting the present month as month number 1, what
is the probability that the ﬁrst month to have 8 or more
suicides will be month number i,iÚ1?
What assumptions are you making?
4.65. Each of 500 soldiers in an army company indepen-
dently has a certain disease with probability 1 /10
3.T h i s
disease will show up in a blood test, and to facilitate mat-
ters, blood samples from all 500 soldiers are pooled and
tested.
(a)What is the (approximate) probability that the blood
test will be positive (that is, at least one person has the
disease)?
Suppose now that the blood test yields a positive result.
(b)What is the probability, under this circumstance, that
more than one person has the disease?
Now, suppose one of the 500 people is Jones, who knows
that he has the disease.
(c)What does Jones think is the probability that more than
one person has the disease?
Because the pooled test was positive, the authorities have
decided to test each individual separately. The ﬁrst i−1
of these tests were negative, and the ith one—which was
on Jones—was positive.
(d)Given the preceding scenario, what is the probability,
as a function of i, that any of the remaining people have
the disease?
4.66. A total of 2n people, consisting of nmarried cou-
ples, are randomly seated (all possible orderings being
equally likely) at a round table. Let Cidenote the event
that the members of couple iare seated next to each other,
i=1,...,n.
(a)Find P(Ci).
(b)ForjZi, ﬁnd P(Cj|Ci).
(c)Approximate the probability, for nlarge, that there
are no married couples who are seated next to each
other.
4.67. Repeat the preceding problem when the seating is
random but subject to the constraint that the men and
women alternate.
4.68. In response to an attack of 10 missiles, 500 antiballis-
tic missiles are launched. The missile targets of the antibal-
listic missiles are independent, and each antiballstic missile
is equally likely to go towards any of the target missiles. Ifeach antiballistic missile independently hits its target withprobability .1, use the Poisson paradigm to approximate
the probability that all missiles are hit.
4.69. A fair coin is ﬂipped 10 times. Find the probability
that there is a string of 4 consecutive heads by
(a)using the formula derived in the text;
(b)using the recursive equations derived in the text.
(c)Compare your answer with that given by the Poisson
approximation.
4.70. At time 0, a coin that comes up heads with proba-
bility pis ﬂipped and falls to the ground. Suppose it lands
on heads. At times chosen according to a Poisson process
with rate λ, the coin is picked up and ﬂipped. (Between
these times, the coin remains on the ground.) What is the
probability that the coin is on its head side at time t?
Hint : What would be the conditional probability if there
were no additional ﬂips by time t, and what would it be if
there were additional ﬂips by time t?
4.71. Consider a roulette wheel consisting of 38 numbers 1
through 36, 0, and double 0. If Smith always bets that theoutcome will be one of the numbers 1 through 12, what isthe probability that
(a)Smith will lose his ﬁrst 5 bets;
(b)his ﬁrst win will occur on his fourth bet?
4.72. Two athletic teams play a series of games; the ﬁrst
team to win 4 games is declared the overall winner. Sup-
pose that one of the teams is stronger than the other and
wins each game with probability .6, independently of the
outcomes of the other games. Find the probability, for
i=4, 5, 6, 7, that the stronger team wins the series in
exactly igames. Compare the probability that the stronger
team wins with the probability that it would win a 2-out-of-3 series.
4.73. Suppose in Problem 4.72 that the two teams are
evenly matched and each has probability
1
2of win-
ning each game. Find the expected number of games
played.
4.74. An interviewer is given a list of people she can inter-
view. If the interviewer needs to interview 5 people, and
if each person (independently) agrees to be interviewed
with probability2
3, what is the probability that her list of
people will enable her to obtain her necessary number
of interviews if the list consists of (a) 5 people and (b) 8people? For part (b), what is the probability that the inter-
viewer will speak to exactly (c) 6 people and (d) 7 people
on the list?
4.75. A fair coin is continually ﬂipped until heads appears
for the 10th time. Let Xdenote the number of tails that
occur. Compute the probability mass function of X.

<<<PAGE 182>>>

A First Course in Probability 169
4.76. Solve the Banach match problem (Example 8e)
when the left-hand matchbox originally contained N1
matches and the right-hand box contained N2matches.
4.77. In the Banach matchbox problem, ﬁnd the probabil-
ity that at the moment when the ﬁrst box is emptied (as
opposed to being found empty), the other box contains
exactly kmatches.
4.78. An urn contains 4 white and 4 black balls. We ran-
domly choose 4 balls. If 2 of them are white and 2 areblack, we stop. If not, we replace the balls in the urn and
again randomly select 4 balls. This continues until exactly
2 of the 4 chosen are white. What is the probability that weshall make exactly nselections?
4.79. Suppose that a batch of 100 items contains 6 that are
defective and 94 that are not defective. If Xis the number
of defective items in a randomly drawn sample of 10 itemsfrom the batch, ﬁnd (a) P{X=0}and (b) P{X>2}.
4.80. A game popular in Nevada gambling casinos is Keno,
which is played as follows: Twenty numbers are selected at
random by the casino from the set of numbers 1 through
80. A player can select from 1 to 15 numbers; a win occursif some fraction of the player’s chosen subset matches any
of the 20 numbers drawn by the house. The payoff is a
function of the number of elements in the player’s selec-tion and the number of matches. For instance, if the player
selects only 1 number, then he or she wins if this number is
among the set of 20, and the payoff is $2.20 won for everydollar bet. (As the player’s probability of winning in this
case is
1
4, it is clear that the “fair” payoff should be $3 won
for every $1 bet.) When the player selects 2 numbers, a
payoff (of odds) of $12 won for every $1 bet is made when
both numbers are among the 20.
(a)What would be the fair payoff in this case?
Let Pn,kdenote the probability that exactly kof the n
numbers chosen by the player are among the 20 selected
by the house.
(b)Compute Pn,k
(c)The most typical wager at Keno consists of selecting 10
numbers. For such a bet, the casino pays off as shown in
the following table. Compute the expected payoff:Keno Payoffs in 10 Number Bets
Number of matches Dollars won for each $1 bet
0–4 –1
51
61 7
7 179
8 1, 299
9 2, 599
10 24, 999
4.81. In Example 8i, what percentage of idefective lots
does the purchaser reject? Find it for i=1, 4. Given that
a lot is rejected, what is the conditional probability that itcontained 4 defective components?
4.82. A purchaser of transistors buys them in lots of 20.
It is his policy to randomly inspect 4 components from a
lot and to accept the lot only if all 4 are nondefective. Ifeach component in a lot is, independently, defective with
probability .1, what proportion of lots is rejected?
4.83. There are three highways in the county. The number
of daily accidents that occur on these highways are Poisson
random variables with respective parameters .3,.5, and .7.
Find the expected number of accidents that will happen on
any of these highways today.
4.84. Suppose that 10 balls are put into 5 boxes, with each
ball independently being put in box iwith probability
p
i,/summationtext5
i=1pi=1.
(a)Find the expected number of boxes that do not have
any balls.
(b)Find the expected number of boxes that have exactly 1
ball.
4.85. There are ktypes of coupons. Independently of the
types of previously collected coupons, each new coupon
collected is of type iwith probability pi,/summationtextk
i=1pi=1.
Ifncoupons are collected, ﬁnd the expected number of
distinct types that appear in this set. (That is, ﬁnd the
expected number of types of coupons that appear at least
once in the set of ncoupons.)
Theoretical Exercises
4.1.There are Ndistinct types of coupons, and each time
one is obtained it will, independently of past choices, be oftype iwith probability P
i,i=1,...,N.L e tT denote the
number one need select to obtain at least one of each type.
Compute P{T=n}.
Hint : Use an argument similar to the one used in Exam-
ple 1e.4.2. IfXhas distribution function F, what is the distribu-
tion function of eX?
4.3. IfXhas distribution function F, what is the distribu-
tion function of the random variable αX+β,w h e r e αand
βare constants, αZ0?
4.4. The random variable Xis said to have the Yule-
Simons distribution if

<<<PAGE 183>>>

170 Chapter 4 Random Variables
P{X=n}=4
n(n+1)(n+2),nÚ1
(a)Show that the preceding is actually a probability mass
function. That is, show that/summationtextq
n=1P{X=n}= 1.
(b)Show that E[X]=2.
(c)Show that E[X2]=q.
Hint: For (a), ﬁrst use that1
n(n+1)(n+2)=1
n(n+1)−
1
n(n+2),t h e nu s et h a tk
n(n+k)=1
n−1
n+k.
4.5. LetNbe a nonnegative integer-valued random vari-
able. For nonnegative values aj,jÚ1, show that
q/summationdisplay
j=1(a1+...+aj)P{N=j}=q/summationdisplay
i=1aiP{NÚi}
Then show that
E[N]=q/summationdisplay
i=1P{NÚi}
and
E[N(N+1)]=2q/summationdisplay
i=1iP{NÚi}
4.6. LetXbe such that
P{X=1}=p =1−P{X=−1}
Find cZ1 such that E[cX]=1.
4.7.LetXbe a random variable having expected value μ
and variance σ2. Find the expected value and variance of
Y=X−μ
σ
4.8. Find Var (X)if
P(X=a)=p=1−P(X=b)
4.9. Show how the derivation of the binomial probabilities
P{X=i}=/parenleftbiggn
i/parenrightbigg
pi(1−p)n−i,i=0,...,n
leads to a proof of the binomial theorem
(x+y)n=n/summationdisplay
i=0/parenleftbiggn
i/parenrightbigg
xiyn−i
when xandyare nonnegative.
Hint: Let p=x
x+y.
4.10. LetXbe a binomial random variable with parame-
tersnandp. Show thatE/bracketleftbigg1
X+1/bracketrightbigg
=1−(1−p)n+1
(n+1)p
4.11. Consider nindependent sequential trials, each of
which is successful with probability p. If there is a total
ofksuccesses, show that each of the n!/[k!(n −k)!] pos-
sible arrangements of the ksuccesses and n−kfailures is
equally likely.
4.12. There are ncomponents lined up in a linear arrange-
ment. Suppose that each component independently func-
tions with probability p. What is the probability that no 2
neighboring components are both nonfunctional?
Hint : Condition on the number of defective components
and use the results of Example 4c of Chapter 1.
4.13. Let Xbe a binomial random variable with param-
eters (n, p). What value of pmaximizes P{X=k},k=
0, 1,...,n? This is an example of a statistical method used
to estimate pwhen a binomial ( n,p) random variable is
observed to equal k. If we assume that nis known, then
we estimate pby choosing that value of pthat maximizes
P{X=k}. This is known as the method of maximum like-
lihood estimation .
4.14. A family has nchildren with probability αpn,nÚ1,
where α…(1−p)/p.
(a)What proportion of families has no children?
(b)If each child is equally likely to be a boy or a girl
(independently of each other), what proportion of families
consists of kboys (and any number of girls)?
4.15. Suppose that nindependent tosses of a coin having
probability pof coming up heads are made. Show that
the probability that an even number of heads results is
1
2[1+(q−p)n], where q=1−p. Do this by proving
and then utilizing the identity
[n/2]/summationdisplay
i=0/parenleftbigg
n
2i/parenrightbigg
p2iqn−2i=1
2/bracketleftbig
(p+q)n+(q−p)n/bracketrightbig
where [n/2] is the largest integer less than or equal ton/2. Compare this exercise with Theoretical Exercise 3.5of Chapter 3.
4.16. LetXbe a Poisson random variable with parameter
λ. Show that P{X=i}increases monotonically and then
decreases monotonically as iincreases, reaching its maxi-
mum when iis the largest integer not exceeding λ.
Hint : Consider P{X=i}/P{X=i−1}.
4.17. Let Xbe a Poisson random variable with parame-
terλ.
(a)Show that
P{Xis even }=1
2/bracketleftBig
1+e−2λ/bracketrightBig

<<<PAGE 184>>>

A First Course in Probability 171
by using the result of Theoretical Exercise 4.15 and the
relationship between Poisson and binomial random vari-ables.
(b)Verify the formula in part (a) directly by making use of
the expansion of e
−λ+eλ.
4.18. LetXbe a Poisson random variable with parameter
λ. What value of λmaximizes P{X=k},kÚ0?
4.19. Show that Xis a Poisson random variable with
parameter λ,t h e n
E[Xn]=λE[(X+1)n−1]
Now use this result to compute E[X3].
4.20. Consider ncoins, each of which independently comes
up heads with probability p. Suppose that nis large and p
is small, and let λ=np. Suppose that all ncoins are tossed;
if at least one comes up heads, the experiment ends; if not,
we again toss all ncoins, and so on. That is, we stop the ﬁrst
time that at least one of the ncoins come up heads. Let
Xdenote the total number of heads that appear. Which
of the following reasonings concerned with approximating
P{X=1}is correct (in all cases, Yis a Poisson random
variable with parameter λ)?
(a)Because the total number of heads that occur when all
ncoins are rolled is approximately a Poisson random vari-
able with parameter λ,
P{X=1}LP{Y=1}=λe−λ
(b)Because the total number of heads that occur when all
ncoins are rolled is approximately a Poisson random vari-
able with parameter λ, and because we stop only when this
number is positive,
P{X=1}LP{Y=1|Y>0}=λe−λ
1−e−λ
(c)Because at least one coin comes up heads, Xwill equal
1 if none of the other n−1 coins come up heads. Because
the number of heads resulting from these n−1 coins is
approximately Poisson with mean (n−1)pLλ,
P{X=1}LP{Y=0}= e−λ
4.21. From a set of nrandomly chosen people, let Eij
denote the event that persons iand jhave the same birth-
day. Assume that each person is equally likely to have any
of the 365 days of the year as his or her birthday. Find
(a)P(E3,4|E1,2);
(b)P(E1,3|E1,2);
(c)P(E2,3|E1,2∩E1,3).
What can you conclude from your answers to parts (a)–(c)
about the independence of the/parenleftbigg
n
2/parenrightbigg
events Eij?4.22. An urn contains 2n balls, of which 2 are numbered 1,
2 are numbered 2, ..., and 2 are numbered n. Balls are suc-
cessively withdrawn 2 at a time without replacement. Let
Tdenote the ﬁrst selection in which the balls withdrawn
have the same number (and let it equal inﬁnity if none of
the pairs withdrawn has the same number). We want to
show that, for 0 <α< 1,
limnP{T>αn}= e−α/2
To verify the preceding formula, let Mkdenote the number
of pairs withdrawn in the ﬁrst kselections, k=1,...,n.
(a)Argue that when nis large, Mkcan be regarded as the
number of successes in k(approximately) independent tri-
als.
(b)Approximate P{Mk=0}when nis large.
(c)Write the event {T>αn}in terms of the value of one
of the variables Mk.
(d)Verify the limiting probability given for P{T>αn}.
4.23. Consider a random collection of nindividuals. In
approximating the probability that no 3 of these individ-
uals share the same birthday, a better Poisson approxima-
tion than that obtained in the text (at least for values of n
between 80 and 90) is obtained by letting Eibe the event
that there are at least 3 birthdays on day i,i=1,..., 365.
(a)Find P(Ei).
(b)Give an approximation for the probability that no 3
individuals share the same birthday.
(c)Evaluate the preceding when n=88 (which can be
shown to be the smallest value of nfor which the probabil-
ity exceeds .5).
4.24. Here is another way to obtain a set of recursive equa-
tions for determining Pn, the probability that there is a
string of kconsecutive heads in a sequence of nﬂips of
a fair coin that comes up heads with probability p:
(a)Argue that for k<n, there will be a string of kcon-
secutive heads if either
1. there is a string of kconsecutive heads within the
ﬁrst n−1ﬂ i p s ,o r
2. there is no string of kconsecutive heads within the
ﬁrst n−k−1ﬂ i p s ,ﬂ i pn −kis a tail, and ﬂips
n−k+1,...,nare all heads.
(b)Using the preceding, relate PntoPn−1. Starting with
Pk=pk, the recursion can be used to obtain Pk+1,t h e n
Pk+2, and so on, up to Pn.
4.25. Suppose that the number of events that occur in a
speciﬁed time is a Poisson random variable with parameter
λ. If each event is counted with probability p, indepen-
dently of every other event, show that the number of
events that are counted is a Poisson random variable with
parameter λp. Also, give an intuitive argument as to why

<<<PAGE 185>>>

172 Chapter 4 Random Variables
this should be so. As an application of the preceding result,
suppose that the number of distinct uranium deposits ina given area is a Poisson random variable with param-
eterλ=10. If, in a ﬁxed period of time, each deposit
is discovered independently with probability
1
50, ﬁnd the
probability that (a) exactly 1, (b) at least 1, and (c) at most
1 deposit is discovered during that time.
4.26. Prove
n/summationdisplay
i=0e−λλi
i!=1
n!/integraldisplayq
λe−xxndx
Hint : Use integration by parts.
4.27. IfXis a geometric random variable, show analyti-
cally that
P{X=n+k|X>n}=P {X=k}
Using the interpretation of a geometric random variable,
give a verbal argument as to why the preceding equationis true.
4.28. LetXbe a negative binomial random variable with
parameters rand p,a n dl e tY be a binomial random vari-
able with parameters nandp. Show that
P{X>n}=P {Y<r}
Hint : Either one could attempt an analytical proof of the
preceding equation, which is equivalent to proving the
identity
q/summationdisplay
i=n+1/parenleftbigg
i−1
r−1/parenrightbigg
pr(1−p)i−r=r−1/summationdisplay
i=0/parenleftbigg
n
i/parenrightbigg
*pi(1−p)n−i
or one could attempt a proof that uses the probabilisticinterpretation of these random variables. That is, in the
latter case, start by considering a sequence of independent
trials having a common probability pof success. Then try
to express the events {X>n}and{Y<r}in terms of the
outcomes of this sequence.
4.29. For a hypergeometric random variable, determine
P{X=k+1}/P{X=k}
4.30. Balls numbered 1 through Nare in an urn. Sup-
pose that n,n…N, of them are randomly selected without
replacement. Let Ydenote the largest number selected.
(a)Find the probability mass function of Y.
(b)Derive an expression for E[Y] and then use Fermat’s
combinatorial identity (see Theoretical Exercise 11 of
Chapter 1) to simplify the expression.4.31. A jar contains m+nchips, numbered 1, 2, ...,
n+m.As e to fs i z en is drawn. If we let Xdenote the
number of chips drawn having numbers that exceed each
of the numbers of those remaining, compute the probabil-
ity mass function of X.
4.32. A jar contains nchips. Suppose that a boy succes-
sively draws a chip from the jar, each time replacing theone drawn before drawing another. The process continuesuntil the boy draws a chip that he has previously drawn.
LetXdenote the number of draws, and compute its prob-
ability mass function.
4.33. Repeat Theoretical Exercise 4.32, this time assum-
ing that withdrawn chips are not replaced before the next
selection.
4.34. From a set of nelements, a nonempty subset is cho-
sen at random in the sense that all of the nonempty subsets
are equally likely to be selected. Let Xdenote the number
of elements in the chosen subset. Using the identities given
in Theoretical Exercise 12 of Chapter 1, show that
E[X]=n
2−/parenleftBig
1
2/parenrightBign−1
Var(X)=n·22n−2−n(n+1)2n−2
(2n−1)2
Show also that for nlarge,
Var(X)/revsimilarn
4
in the sense that the ratio Var(X )t on/4 approaches 1 as
napproaches q. Compare this formula with the limiting
form of Var (Y)when P{Y=i}= 1/n, i=1,...,n.
4.35. An urn initially contains one red and one blue ball.
At each stage, a ball is randomly chosen and then replaced
along with another of the same color. Let Xdenote the
selection number of the ﬁrst chosen ball that is blue. For
instance, if the ﬁrst selection is red and the second blue,
then Xis equal to 2.
(a)Find P{X>i},iÚ1.
(b)Show that with probability 1, a blue ball is eventually
chosen. (That is, show that P{X<q}= 1.)
(c)Find E[X].
4.36. Suppose the possible values of Xare{xi}, the possi-
ble values of Yare{yj}, and the possible values of X+Y
are{zk}.LetAkdenote the set of all pairs of indices (i,j)
such that xi+yj=zk;t h a ti s , Ak={(i,j):xi+yj=zk}.
(a)Argue that
P{X+Y=zk}=/summationdisplay
(i,j)∈A kP{X=xi,Y=yj}

<<<PAGE 186>>>

A First Course in Probability 173
(b)Show that
E[X+Y]=/summationdisplay
k/summationdisplay
(i,j)∈A k(xi+yj)P{X=xi,
Y=yj}
(c)Using the formula from part (b), argue that
E[X+Y]=/summationdisplay
i/summationdisplay
j(xi+yj)P{X=xi,
Y=yj}(d)Show that
P(X=xi)=/summationdisplay
jP(X=xi,Y=yj),
P(Y=yj)=/summationdisplay
iP{X=xi,Y=yj}
(e)Prove that
E[X+Y]=E[X]+E[Y]
Self-Test Problems and Exercises
4.1.Suppose that the random variable Xis equal to the
number of hits obtained by a certain baseball player in his
next 3 at bats. If P{X=1}=.3, P{X=2}=.2, and
P{X=0}=3P {X=3}, ﬁnd E[X].
4.2. Suppose that Xtakes on one of the values 0, 1, and 2.
If for some constant c,P{X=i}= cP{X=i−1},i=1, 2,
ﬁnd E[X].
4.3. A coin that when ﬂipped comes up heads with prob-
ability pis ﬂipped until either heads or tails has occurred
twice. Find the expected number of ﬂips.
4.4. A certain community is composed of mfamilies, niof
which have ichildren,r/summationtext
i=1ni=m. If one of the families is
randomly chosen, let Xdenote the number of children in
that family. If one of ther/summationtext
i=1inichildren is randomly cho-
sen, let Ydenote the total number of children in the family
of that child. Show that E[Y]ÚE[X].
4.5. Suppose that P{X=0}=1 −P{X=1}.I f E[X]=
3Var(X ), ﬁnd P{X=0}.
4.6. There are 2 coins in a bin. When one of them is
ﬂipped, it lands on heads with probability .6, and when the
other is ﬂipped, it lands on heads with probability .3. One
of these coins is to be randomly chosen and then ﬂipped.Without knowing which coin is chosen, you can bet any
amount up to $10, and you then either win that amount if
the coin comes up heads or lose it if it comes up tails. Sup-pose, however, that an insider is willing to sell you, for an
amount C, the information as to which coin was selected.
What is your expected payoff if you buy this information?
Note that if you buy it and then bet x, you will end up
either winning x−Cor−x−C(that is, losing x+Cin
the latter case). Also, for what values of Cdoes it pay to
purchase the information?
4.7.A philanthropist writes a positive number xon a piece
of red paper, shows the paper to an impartial observer,
and then turns it face down on the table. The observer
then ﬂips a fair coin. If it shows heads, she writes thevalue 2x and, if tails, the value x/2, on a piece of blue
paper, which she then turns face down on the table. With-out knowing either the value xor the result of the coin
ﬂip, you have the option of turning over either the red orthe blue piece of paper. After doing so and observing thenumber written on that paper, you may elect to receive
as a reward either that amount or the (unknown) amount
written on the other piece of paper. For instance, if youelect to turn over the blue paper and observe the value
100, then you can elect either to accept 100 as your reward
or to take the amount (either 200 or 50) on the red paper.
Suppose that you would like your expected reward to be
large.
(a)Argue that there is no reason to turn over the red
paper ﬁrst, because if you do so, then no matter what value
you observe, it is always better to switch to the blue paper.
(b)Letybe a ﬁxed nonnegative value, and consider the
following strategy: Turn over the blue paper, and if its
value is at least y, then accept that amount. If it is less
than y, then switch to the red paper. Let R
y(x)denote the
reward obtained if the philanthropist writes the amountxand you employ this strategy. Find E[R
y(x)]. Note that
E[R0(x)] is the expected reward if the philanthropist writes
the amount xwhen you employ the strategy of always
choosing the blue paper.
4.8. LetB(n, p) represent a binomial random variable with
parameters nandp.A r g u et h a t
P{B(n, p)…i}= 1−P{B(n,1 −p)…n−i−1}
Hint : The number of successes less than or equal to iis
equivalent to what statement about the number of fail-
ures?
4.9. IfXis a binomial random variable with expected
value 6 and variance 2.4, ﬁnd P{X=5}.
4.10. An urn contains nballs numbered 1 through n.I f
you withdraw mballs randomly in sequence, each time
replacing the ball selected previously, ﬁnd P{X=k},k=

<<<PAGE 187>>>

174 Chapter 4 Random Variables
1,...,m,w h e r e Xis the maximum of the mchosen
numbers.
Hint : First ﬁnd P{X…k}.
4.11. Teams Aand Bplay a series of games, with the ﬁrst
team to win 3 games being declared the winner of the
series. Suppose that team Aindependently wins each game
with probability p. Find the conditional probability that
team Awins
(a)the series given that it wins the ﬁrst game;
(b)the ﬁrst game given that it wins the series.
4.12. A local soccer team has 5 more games left to play.
If it wins its game this weekend, then it will play its ﬁnal4 games in the upper bracket of its league, and if it loses,then it will play its ﬁnal games in the lower bracket. If it
plays in the upper bracket, then it will independently win
each of its games in this bracket with probability .4, and
if it plays in the lower bracket, then it will independently
win each of its games with probability .7. If the probabilitythat the team wins its game this weekend is .5, what is the
probability that it wins at least 3 of its ﬁnal 4 games?
4.13. Each of the members of a 7-judge panel indepen-
dently makes a correct decision with probability .7. If the
panel’s decision is made by majority rule, what is the prob-
ability that the panel makes the correct decision? Giventhat 4 of the judges agreed, what is the probability that the
panel made the correct decision?
4.14. On average, 5.2 hurricanes hit a certain region in a
year. What is the probability that there will be 3 or fewer
hurricanes hitting this year?
4.15. The number of eggs laid on a tree leaf by an insect
of a certain type is a Poisson random variable with param-
eterλ. However, such a random variable can be observed
only if it is positive, since if it is 0, then we cannot know
that such an insect was on the leaf. If we let Ydenote the
observed number of eggs, then
P{Y=i}= P{X=i|X>0}
where Xis Poisson with parameter λ.F i n dE [Y].
4.16. Each of nboys and ngirls, independently and ran-
domly, chooses a member of the other sex. If a boy and
girl choose each other, they become a couple. Number the
girls, and let G
ibe the event that girl number iis part of a
couple. Let P0=1−P(∪n
i=1Gi)be the probability that no
couples are formed.
(a)What is P(Gi)?
(b)What is P(Gi|Gj)?
(c)When nis large, approximate P0.
(d)When nis large, approximate Pk, the probability that
exactly kcouples are formed.
(e)Use the inclusion–exclusion identity to evaluate P0.4.17. A total of 2n people, consisting of nmarried couples,
are randomly divided into npairs. Arbitrarily number the
women, and let Widenote the event that woman iis paired
with her husband.(a)Find P(W
i).
(b)ForiZj, ﬁnd P(Wi|Wj).
(c)When nis large, approximate the probability that no
wife is paired with her husband.
(d)If each pairing must consist of a man and a woman,
what does the problem reduce to?
4.18. A casino patron will continue to make $5 bets on red
in roulette until she has won 4 of these bets.
(a)What is the probability that she places a total of 9
bets?
(b)What are her expected winnings when she stops?
Remark : On each bet, she will either win $5 with probabil-
ity18
38or lose $5 with probability20
38.
4.19. When three friends go for coffee, they decide who
will pay the check by each ﬂipping a coin and then letting
the “odd person” pay. If all three ﬂips produce the same
result (so that there is no odd person), then they make
a second round of ﬂips, and they continue to do so untilthere is an odd person. What is the probability that
(a)exactly 3 rounds of ﬂips are made?
(b)more than 4 rounds are needed?
4.20. Show that if Xis a geometric random variable with
parameter p,t h e n
E[1/X ]=−plog(p)
1−p
Hint : You will need to evaluate an expression of the form
q/summationtext
i=1ai/i.T od os o ,w r i t e ai/i=/integraltexta
0xi−1dx, and then inter-
change the sum and the integral.
4.21. Suppose that
P{X=a}=p, P{X=b}=1 −p
(a)Show thatX−b
a−bis a Bernoulli random variable.
(b)Find Var (X).
4.22. Each game you play is a win with probability p.Y o u
plan to play 5 games, but if you win the ﬁfth game, then
you will keep on playing until you lose.
(a)Find the expected number of games that you play.
(b)Find the expected number of games that you lose.
4.23. Balls are randomly withdrawn, one at a time without
replacement, from an urn that initially has Nwhite and
Mblack balls. Find the probability that nwhite balls are
drawn before mblack balls, n…N,m…M.
4.24. Ten balls are to be distributed among 5 urns, with
each ball going into urn iwith probability pi,/summationtext5
i=1pi=1.

<<<PAGE 188>>>

A First Course in Probability 175
Let Xidenote the number of balls that go into urn i.
Assume that events corresponding to the locations of dif-
ferent balls are independent.
(a)What type of random variable is Xi?B ea ss p e c i ﬁ ca s
possible.
(b)ForiZj, what type of random variable is Xi+Xj?
(c)Find P{X1+X2+X3=7}.
4.25. For the match problem (Example 5m in Chapter 2),
ﬁnd
(a)the expected number of matches.
(b)the variance of the number of matches.
4.26. Letαbe the probability that a geometric random
variable Xwith parameter pis an even number.
(a)Findαby using the identity α=/summationtextq
i=1P{X=2 i }.
(b)Findαby conditioning on whether X=1o rX >1.
4.27. The National Basketball Association championship
series is a best of 7 series, meaning that the ﬁrst team towin 4 games is declared the champion. In its history, no
team has ever come back to win the championship seriesafter being behind 3 games to 1. Assuming that each of
the games played in this year’s series is equally likely to be
won by either team, independent of the results of earliergames, what is the probability that the upcoming champi-
onship series will result in a team coming back from a 3
games to 1 deﬁcit to win the series?
4.28. An urn has nwhite and mblack balls. Balls are ran-
domly withdrawn, without replacement, until a total of
k,k…nwhite balls have been withdrawn. The random
variable Xequal to the total number of balls that are
withdrawn is said to be a negative hypergeometric random
variable.
(a)Explain how such a random variable differs from a
negative binomial random variable.
(b)Find P{X=r}.
Hint for (b) :I no r d e rf o rX =rto happen, what must be
the results of the ﬁrst r−1 withdrawals?

<<<PAGE 189>>>

Chapter
Continuous Random
V ariables 5
Contents
5.1 Introduction
5.2 Expectation and Variance of Continuous
Random Variables
5.3 The Uniform Random Variable
5.4 Normal Random Variables5.5 Exponential Random Variables
5.6 Other Continuous Distributions
5.7 The Distribution of a Function
of a Random Variable
5.1 Introduction
In Chapter 4, we considered discrete random variables—that is, random variables
whose set of possible values is either ﬁnite or countably inﬁnite. However, there alsoexist random variables whose set of possible values is uncountable. Two examples
are the time that a train arrives at a speciﬁed stop and the lifetime of a transistor.
LetXbe such a random variable. We say that Xis acontinuous
†random variable
if there exists a nonnegative function f, deﬁned for all real x∈(−q, q), having the
property that for any set Bof real numbers,‡
P{X∈B}=/integraldisplay
Bf(x)dx (1.1)
The function fis called the probability density function of the random variable X.
(See Figure 5.1.)
In words, Equation (1.1) states that the probability that Xwill be in Bmay be
obtained by integrating the probability density function over the set B. Since Xmust
assume some value, fmust satisfy
1=P{X∈(−q, q)}=/integraldisplayq
−qf(x)dx
All probability statements about Xcan be answered in terms of f. For instance, from
Equation (1.1), letting B=[a,b], we obtain
P{a…X…b}=/integraldisplayb
af(x)dx (1.2)
†Sometimes called absolutely continuous .
‡Actually, for technical reasons, Equation (1.1) is true only for the measurable setsB, which, fortunately, include
all sets of practical interest.
176

<<<PAGE 190>>>

A First Course in Probability 177
x
b af
P(a /H11349 X /H11349 b ) = area of shaded region
Figure 5.1 Probability density function f.
If we let a=bin Equation (1.2), we get
P{X=a}=/integraldisplaya
af(x)dx=0
In words, this equation states that the probability that a continuous random variable
will assume any ﬁxed value is zero. Hence, for a continuous random variable,
P{X<a}= P{X…a}= F(a)=/integraldisplaya
−qf(x)dx
Example
1aSuppose that Xis a continuous random variable whose probability density function
is given by
f(x)=/braceleftBigg
C(4x−2x2)0<x<2
0 otherwise
(a)What is the value of C?
(b)Find P{X>1}.
Solution (a) Since fis a probability density function, we must have/integraltextq
−qf(x)dx=1,
implying that
C/integraldisplay2
0(4x−2x2)dx=1
or
C/bracketleftBigg
2x2−2x3
3/bracketrightBigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglex=2
x=0=1
or
C=3
8
Hence,
(b)P{X>1}=/integraltextq
1f(x)dx=3
8/integraltext2
1(4x−2x2)dx=1
2.
Example
1bThe amount of time in hours that a computer functions before breaking down is a
continuous random variable with probability density function given by
f(x)=/braceleftBigg
λe−x/100xÚ0
0 x<0

<<<PAGE 191>>>

178 Chapter 5 Continuous Random Variables
What is the probability that
(a)a computer will function between 50 and 150 hours before breaking down?
(b)it will function for fewer than 100 hours?
Solution (a) Since
1=/integraldisplayq
−qf(x)dx=λ/integraldisplayq
0e−x/100dx
we obtain
1=−λ(100)e−x/100/vextendsingle/vextendsingleq
0=100λ orλ=1
100
Hence, the probability that a computer will function between 50 and 150 hours
before breaking down is given by
P{50<X<150}=/integraldisplay150
501
100e−x/100dx=−e−x/100/vextendsingle/vextendsingle150
50
=e−1/2−e−3/2L.383
(b) Similarly,
P{X<100}=/integraldisplay100
01
100e−x/100dx=−e−x/100/vextendsingle/vextendsingle100
0=1−e−1L.632
In other words, approximately 63.2 percent of the time, a computer will fail before
registering 100 hours of use. .
Example
1cThe lifetime in hours of a certain kind of radio tube is a random variable having aprobability density function given by
f(x)=⎧
⎪⎨
⎪⎩0 x…100
100
x2x>100
What is the probability that exactly 2 of 5 such tubes in a radio set will have to
be replaced within the ﬁrst 150 hours of operation? Assume that the events Ei,i=
1, 2, 3, 4, 5, that the ith such tube will have to be replaced within this time are
independent.
Solution From the statement of the problem, we have
P(Ei)=/integraldisplay150
0f(x)dx
=100/integraldisplay150
100x−2dx
=1
3
Hence, from the independence of the events Ei, it follows that the desired probabil-
ity is
/parenleftbigg5
2/parenrightbigg/parenleftbigg1
3/parenrightbigg2/parenleftbigg2
3/parenrightbigg3
=80
243.

<<<PAGE 192>>>

A First Course in Probability 179
The relationship between the cumulative distribution Fand the probability den-
sityfis expressed by
F(a)=P{X∈(−q, a]}=/integraldisplaya
−qf(x)dx
Differentiating both sides of the preceding equation yields
d
daF(a)=f(a)
That is, the density is the derivative of the cumulative distribution function. A some-
what more intuitive interpretation of the density function may be obtained fromEquation (1.2) as follows:
P/braceleftbigg
a−ε
2…X…a+ε
2/bracerightbigg
=/integraldisplaya+ε/ 2
a−ε/ 2f(x)dxLεf(a)
when εis small and when f(·)is continuous at x=a. In other words, the probability
thatXwill be contained in an interval of length εaround the point ais approximately
εf(a). From this result, we see that f(a)is a measure of how likely it is that the
random variable will be near a.
Example
1dIfXis continuous with distribution function FXand density function fX, ﬁnd the
density function of Y=2X.
Solution We will determine fYin two ways. The ﬁrst way is to derive, and then dif-
ferentiate, the distribution function of Y:
FY(a)=P{Y…a}
=P{2X…a}
=P{X…a/2}
=FX(a/2)
Differentiation gives
fY(a)=1
2fX(a/2)
Another way to determine fYis to note that
/epsilon1fY(a)LP/braceleftbigg
a−/epsilon1
2…Y…a+/epsilon1
2/bracerightbigg
=P/braceleftbigg
a−/epsilon1
2…2X…a+/epsilon1
2/bracerightbigg
=P/braceleftbigga
2−/epsilon1
4…X…a
2+/epsilon1
4/bracerightbigg
L/epsilon1
2fX(a/2)
Dividing through by /epsilon1gives the same result as before. .
5.2 Expectation and Variance of Continuous Random Variables
In Chapter 4, we deﬁned the expected value of a discrete random variable Xby
E[X]=/summationdisplay
xxP{X=x}

<<<PAGE 193>>>

180 Chapter 5 Continuous Random Variables
IfXis a continuous random variable having probability density function f(x), then,
because
f(x)dxLP{x…X…x+dx} fordxsmall
it is easy to see that the analogous deﬁnition is to deﬁne the expected value of Xby
E[X]=/integraldisplayq
−qxf(x)dx
Example
2aFind E[X] when the density function of Xis
f(x)=/braceleftBigg
2xif 0 …x…1
0 otherwise
Solution
E[X]=/integraldisplay
xf(x)dx
=/integraldisplay1
02x2dx
=2
3.
Example
2bThe density function of Xis given by
f(x)=/braceleftBigg
1i f 0 …x…1
0 otherwise
Find E[eX].
Solution LetY=eX. We start by determining FY, the cumulative distribution func-
tion of Y.N o w ,f o r1 …x…e,
FY(x)=P{Y…x}
=P{eX…x}
=P{X…log(x)}
=/integraldisplaylog(x)
0f(y)dy
=log(x)
By differentiating FY(x), we can conclude that the probability density function of Y
is given by
fY(x)=1
x1…x…e
Hence,
E[eX]=E[Y]=/integraldisplayq
−qxfY(x)dx
=/integraldisplaye
1dx
=e−1 .

<<<PAGE 194>>>

A First Course in Probability 181
Although the method employed in Example 2b to compute the expected value
of a function of Xis always applicable, there is, as in the discrete case, an alternative
way of proceeding. The following is a direct analog of Proposition 4.1 of Chapter 4.
Proposition
2.1IfXis a continuous random variable with probability density function f(x), then, for
any real-valued function g,
E[g(X)]=/integraldisplayq
−qg(x)f(x)dx
An application of Proposition 2.1 to Example 2b yields
E[eX]=/integraldisplay1
0exdx since f(x)=1, 0 <x<1
=e−1
which is in accord with the result obtained in that example.
The proof of Proposition 2.1 is more involved than that of its discrete random
variable analog. We will present such a proof under the provision that the random
variable g(X)is nonnegative. (The general proof, which follows the argument in the
case we present, is indicated in Theoretical Exercises 5.2 and 5.3.) We will need the
following lemma, which is of independent interest.
Lemma
2.1For a nonnegative random variable Y,
E[Y]=/integraldisplayq
0P{Y>y}dy
Proof We present a proof when Yis a continuous random variable with probability
density function fY. We have
/integraldisplayq
0P{Y>y}dy=/integraldisplayq
0/integraldisplayq
yfY(x)dx dy
where we have used the fact that P{Y>y}=/integraltextq
yfY(x)dx. Interchanging the order
of integration in the preceding equation yields
/integraldisplayq
0P{Y>y}dy=/integraldisplayq
0/parenleftbigg/integraldisplayx
0dy/parenrightbigg
fY(x)dx
=/integraldisplayq
0xfY(x)dx
=E[Y] .
Proof of Proposition 2.1 From Lemma 2.1, for any function gfor which g(x)Ú0,
E[g(X)]=/integraldisplayq
0P{g(X)>y} dy
=/integraldisplayq
0/integraldisplay
x:g(x)>yf(x)dx dy
=/integraldisplay
x:g(x)>0/integraldisplayg(x)
0dy f(x)dx
=/integraldisplay
x:g(x)>0g(x)f(x)dx
which completes the proof.

<<<PAGE 195>>>

182 Chapter 5 Continuous Random Variables
Example
2cA stick of length 1 is split at a point Uhaving density function f(u)=1, 0<u<1.
Determine the expected length of the piece that contains the point p,0…p…1.
Solution LetLp(U)denote the length of the substick that contains the point p, and
note that
Lp(U)=/braceleftBigg
1−UU <p
UU >p
(See Figure 5.2.) Hence, from Proposition 2.1,
E[Lp(U)]=/integraldisplay1
0Lp(u)du
=/integraldisplayp
0(1−u)du+/integraldisplay1
pud u
=1
2−(1−p)2
2+1
2−p2
2
=1
2+p(1−p)
0 Up 1(a)
0 U p 1(b)1 – U
U
Figure 5.2 Substick containing point p:( a )U <p;( b ) U>p.
Since p(1−p)is maximized when p=1
2, it is interesting to note that the expected
length of the substick containing the point pis maximized when pis the midpoint of
the original stick. .
Example
2dSuppose that if you are sminutes early for an appointment, then you incur the cost
cs, and if you are sminutes late, then you incur the cost ks. Suppose also that the
travel time from where you presently are to the location of your appointment is a
continuous random variable having probability density function f. Determine the
time at which you should depart if you want to minimize your expected cost.
Solution LetXdenote the travel time. If you leave tminutes before your appoint-
ment, then your cost—call it Ct(X)—is given by
Ct(X)=/braceleftBigg
c(t−X)ifX…t
k(X−t)ifXÚt
Therefore,
E[Ct(X)]=/integraldisplayq
0Ct(x)f(x)dx
=/integraldisplayt
0c(t−x)f(x)dx+/integraldisplayq
tk(x−t)f(x)dx
=ct/integraldisplayt
0f(x)dx−c/integraldisplayt
0xf(x)dx+k/integraldisplayq
txf(x)dx−kt/integraldisplayq
tf(x)dx

<<<PAGE 196>>>

A First Course in Probability 183
The value of tthat minimizes E[Ct(X)] can now be obtained by calculus. Differenti-
ation yields
d
dtE[Ct(X)]=ct f(t)+cF(t)−ct f(t)−kt f(t)+kt f(t)−k[1−F(t)]
=(k+c)F(t)−k
Equating the rightmost side to zero shows that the minimal expected cost is obtained
when you leave t∗minutes before your appointment, where t∗satisﬁes
F(t∗)=k
k+c.
As in Chapter 4, we can use Proposition 2.1 to show the following.
Corollary
2.1Ifaandbare constants, then
E[aX+b]=aE[X]+b
The proof of Corollary 2.1 for a continuous random variable Xis the same as
the one given for a discrete random variable. The only modiﬁcation is that the sumis replaced by an integral and the probability mass function by a probability density
function.
The variance of a continuous random variable is deﬁned exactly as it is for a
discrete random variable, namely, if Xis a random variable with expected value μ,
then the variance of Xis deﬁned (for any type of random variable) by
Var(X)=E[(X−μ)
2]
The alternative formula,
Var(X)=E[X2]−(E[X])2
is established in a manner similar to its counterpart in the discrete case.
Example
2eFind Var (X)forXas given in Example 2a.
Solution We ﬁrst compute E[X2].
E[X2]=/integraldisplayq
−qx2f(x)dx
=/integraldisplay1
02x3dx
=1
2
Hence, since E[X]=2
3, we obtain
Var(X)=1
2−/parenleftbigg2
3/parenrightbigg2
=1
18.
It can be shown that, for constants aandb,
Var(aX+b)=a2Var(X)
The proof mimics the one given for discrete random variables.
There are several important classes of continuous random variables that appear
frequently in applications of probability; the next few sections are devoted to a study
of some of them.

<<<PAGE 197>>>

184 Chapter 5 Continuous Random Variables
5.3 The Uniform Random Variable
A random variable is said to be uniformly distributed over the interval (0, 1) if its
probability density function is given by
f(x)=/braceleftBigg
10<x<1
0 otherwise(3.1)
Note that Equation (3.1) is a density function, since f(x)Ú0 and/integraltextq
−qf(x)dx=/integraltext1
0dx=1. Because f(x)>0 only when x∈(0, 1), it follows that Xmust assume a
value in interval (0, 1). Also, since f(x)is constant for x∈(0, 1), Xis just as likely to
be near any value in (0, 1) as it is to be near any other value. To verify this statement,
note that for any 0 <a<b<1,
P{a…X…b}=/integraldisplayb
af(x)dx=b−a
In other words, the probability that Xis in any particular subinterval of (0, 1) equals
the length of that subinterval.
In general, we say that Xis a uniform random variable on the interval (α,β)if
the probability density function of Xis given by
f(x)=⎧
⎪⎨
⎪⎩1
β−αifα< x<β
0 otherwise(3.2)
Since F(a)=/integraltexta
−qf(x)dx, it follows from Equation (3.2) that the distribution func-
tion of a uniform random variable on the interval (α,β)is given by
F(a)=⎧
⎪⎪⎪⎨
⎪⎪⎪⎩0 a…α
a−α
β−αα< a<β
1 aÚβ
Figure 5.3 presents a graph of f(a)andF(a).
/H9251af(a)
/H92521——–   –   
(a)/H9251aF(a)
/H9252
(b)1
/H9252/H9251
Figure 5.3 Graph of (a) f(a)and (b) F(a)for a uniform (α,β)random variable.

<<<PAGE 198>>>

A First Course in Probability 185
Example
3aLetXbe uniformly distributed over (α,β).F i n d( a )E [X] and (b) Var (X).
Solution (a)
E[X]=/integraldisplayq
−qxf(x)dx
=/integraldisplayβ
αx
β−αdx
=β2−α2
2(β−α)
=β+α
2
In words, the expected value of a random variable that is uniformly distributed
over some interval is equal to the midpoint of that interval.
(b) To ﬁnd Var(X ), we ﬁrst calculate E[X2].
E[X2]=/integraldisplayβ
α1
β−αx2dx
=β3−α3
3(β−α)
=β2+αβ+α2
3
Hence,
Var(X)=β2+αβ+α2
3−(α+β)2
4
=(β−α)2
12
Therefore, the variance of a random variable that is uniformly distributed over
some interval is the square of the length of that interval divided by 12. .
Example
3bIfXis uniformly distributed over (0, 10), calculate the probability that (a) X<3,
(b)X>6, and (c) 3 <X<8.
Solution (a)P{X<3}=/integraldisplay3
01
10dx=3
10
(b)P{X>6}=/integraldisplay10
61
10dx=4
10
(c)P{3<X<8}=/integraldisplay8
31
10dx=1
2.
Example
3cBuses arrive at a speciﬁed stop at 15-minute intervals starting at 7 a.m.That is, they
arrive at 7, 7:15, 7:30, 7:45, and so on. If a passenger arrives at the stop at a time thatis uniformly distributed between 7 and 7:30, ﬁnd the probability that he waits
(a)less than 5 minutes for a bus;
(b)more than 10 minutes for a bus.
Solution LetXdenote the number of minutes past 7 that the passenger arrives at
the stop. Since Xis a uniform random variable over the interval (0, 30), it follows
that the passenger will have to wait less than 5 minutes if (and only if) he arrivesbetween 7:10 and 7:15 or between 7:25 and 7:30. Hence, the desired probability forpart (a) is

<<<PAGE 199>>>

186 Chapter 5 Continuous Random Variables
P{10<X<15}+ P{25<X<30}=/integraldisplay15
101
30dx+/integraldisplay30
251
30dx=1
3
Similarly, he would have to wait more than 10 minutes if he arrives between 7 and
7:05 or between 7:15 and 7:20, so the probability for part (b) is
P{0<X<5}+ P{15<X<20}=1
3.
The next example was ﬁrst considered by the French mathematician Joseph
L. F. Bertrand in 1889 and is often referred to as Bertrand’s paradox. It represents
our initial introduction to a subject commonly referred to as geometrical probability .
Example
3dConsider a random chord of a circle. What is the probability that the length of the
chord will be greater than the side of the equilateral triangle inscribed in that circle?
Solution As stated, the problem is incapable of solution because it is not clear what
is meant by a random chord. To give meaning to this phrase, we shall reformulatethe problem in two distinct ways.
The ﬁrst formulation is as follows: The position of the chord can be determined
by its distance from the center of the circle. This distance can vary between 0 andr, the radius of the circle. Now, the length of the chord will be greater than the side
of the equilateral triangle inscribed in the circle if the distance from the chord to
the center of the circle is less than r/2. Hence, by assuming that a random chord
is a chord whose distance Dfrom the center of the circle is uniformly distributed
between 0 and r, we see that the probability that the length of the chord is greater
than the side of an inscribed equilateral triangle is
P/braceleftbigg
D<r
2/bracerightbigg
=r/2
r=1
2
For our second formulation of the problem, consider an arbitrary chord of the
circle; through one end of the chord, draw a tangent. The angle θbetween the chord
and the tangent, which can vary from 0◦to 180◦, determines the position of the
chord. (See Figure 5.4.) Furthermore, the length of the chord will be greater than
the side of the inscribed equilateral triangle if the angle θis between 60◦and 120◦.
Hence, assuming that a random chord is a chord whose angle θis uniformly dis-
tributed between 0◦and 180◦, we see that the desired answer in this formulation is
P{60<θ<120}=120−60
180=1
3
Note that random experiments could be performed in such a way that1
2or1
3would
be the correct probability. For instance, if a circular disk of radius ris thrown on a
table ruled with parallel lines a distance 2r apart, then one and only one of these
lines would cross the disk and form a chord. All distances from this chord to the
A/H9258
Figure 5.4

<<<PAGE 200>>>

A First Course in Probability 187
center of the disk would be equally likely, so that the desired probability that the
chord’s length will be greater than the side of an inscribed equilateral triangle is1
2.
In contrast, if the experiment consisted of rotating a needle freely about a point A
on the edge (see Figure 5.4) of the circle, the desired answer would be1
3. .
5.4 Normal Random Variables
We say that Xis a normal random variable, or simply that Xis normally distributed,
with parameters μandσ2if the density of Xis given by
f(x)=1√
2πσe−(x−μ)2/2σ2−q<x<q
This density function is a bell-shaped curve that is symmetric about μ. (See
Figure 5.5.)
(b)– 23
(a)–3 –2 –1 0 1 2.399
.399——–/H9268
/H9262 /H9268 /H9262 + 2/H9268 /H9262–/H9268 /H9262 +/H9268 /H9262
Figure 5.5 Normal density function: (a) μ=0,σ=1; (b) arbitrary μ,σ2.
The normal distribution was introduced by the French mathematician Abraham
DeMoivre in 1733, who used it to approximate probabilities associated with bino-
mial random variables when the binomial parameter nis large. This result was later
extended by Laplace and others and is now encompassed in a probability theoremknown as the central limit theorem, which is discussed in Chapter 8. The central limit
theorem, one of the two most important results in probability theory,
†gives a theo-
retical base to the often noted empirical observation that, in practice, many randomphenomena obey, at least approximately, a normal probability distribution. Some
examples of random phenomena obeying this behavior are the height of a man or
woman, the velocity in any direction of a molecule in gas, and the error made inmeasuring a physical quantity.
†The other is the strong law of large numbers.

<<<PAGE 201>>>

188 Chapter 5 Continuous Random Variables
To prove that f(x)is indeed a probability density function, we need to show that
1√
2πσ/integraldisplayq
−qe−(x−μ)2/2σ2dx=1
Making the substitution y=(x−μ)/σ , we see that
1√
2πσ/integraldisplayq
−qe−(x−μ)2/2σ2dx=1√
2π/integraldisplayq
−qe−y2/2dy
Hence, we must show that /integraldisplayq
−qe−y2/2dy=√
2π
Toward this end, let I=/integraltextq
−qe−y2/2dy. Then
I2=/integraldisplayq
−qe−y2/2dy/integraldisplayq
−qe−x2/2dx
=/integraldisplayq
−q/integraldisplayq
−qe−(y2+x2)/2dy dx
We now evaluate the double integral by means of a change of variables to polar
coordinates. (That is, let x=rcosθ,y=rsinθ, and dy dx =rdθdr.) Thus,
I2=/integraldisplayq
0/integraldisplay2π
0e−r2/2rdθdr
=2π/integraldisplayq
0re−r2/2dr
=−2πe−r2/2/vextendsingle/vextendsingleq
0
=2π
Hence, I=√
2π, and the result is proved.
An important fact about normal random variables is that if Xis normally dis-
tributed with parameters μandσ2, then Y=aX+bis normally distributed with
parameters aμ+banda2σ2. To prove this statement, suppose that a>0. (The
proof when a<0 is similar.) Let FYdenote the cumulative distribution function of
Y. Then
FY(x)=P{Y…x}
=P{aX+b…x}
=P/braceleftbigg
X…x−b
a/bracerightbigg
=FX/parenleftbiggx−b
a/parenrightbigg
where FXis the cumulative distribution function of X. By differentiation, the density
function of Yis then
fY(x)=1
afX/parenleftbiggx−b
a/parenrightbigg
=1√
2πaσexp/braceleftBigg
−/parenleftbiggx−b
a−μ/parenrightbigg2/slashBig
2σ2/bracerightBigg
=1√
2πaσexp{−(x−b−aμ)2/2(aσ)2}
which shows that Yis normal with parameters aμ+banda2σ2.

<<<PAGE 202>>>

A First Course in Probability 189
An important implication of the preceding result is that if Xis normally dis-
tributed with parameters μandσ2, then Z=(X−μ)/σ is normally distributed
with parameters 0 and 1. Such a random variable is said to be a standard,o raunit,
normal random variable.
We now show that the parameters μandσ2of a normal random variable repre-
sent, respectively, its expected value and variance.
Example
4aFind E[X] and Var (X)when Xis a normal random variable with parameters μ
andσ2.
Solution Let us start by ﬁnding the mean and variance of the standard normal ran-
dom variable Z=(X−μ)/σ . We have
E[Z]=/integraldisplayq
−qxfZ(x)dx
=1√
2π/integraldisplayq
−qxe−x2/2dx
=−1√
2πe−x2/2|q
−q
=0
Thus,
Var(Z)=E[Z2]
=1√
2π/integraldisplayq
−qx2e−x2/2dx
Integration by parts (with u=xanddv=xe−x2/2)now gives
Var(Z)=1√
2π/parenleftBigg
−xe−x2/2|q−q+/integraldisplayq
−qe−x2/2dx/parenrightBigg
=1√
2π/integraldisplayq
−qe−x2/2dx
=1
Because X=μ+σZ, the preceding yields the results
E[X]=μ+σE[Z]=μ
and
Var(X)=σ2Var(Z)=σ2.
It is customary to denote the cumulative distribution function of a standard nor-
mal random variable by /Phi1(x). That is,
/Phi1(x)=1√
2π/integraldisplayx
−qe−y2/2dy

<<<PAGE 203>>>

190 Chapter 5 Continuous Random Variables
Table 5.1 Area/Phi1(x)Under the Standard Normal Curve to the Left of X.
X .00 .01 .02 .03 .04 .05 .06 .07 .08 .09
.0 .5000 .5040 .5080 .5120 .5160 .5199 .5239 .5279 .5319 .5359
.1 .5398 .5438 .5478 .5517 .5557 .5596 .5636 .5675 .5714 .5753.2 .5793 .5832 .5871 .5910 .5948 .5987 .6026 .6064 .6103 .6141
.3 .6179 .6217 .6255 .6293 .6331 .6368 .6406 .6443 .6480 .6517
.4 .6554 .6591 .6628 .6664 .6700 .6736 .6772 .6808 .6844 .6879.5 .6915 .6950 .6985 .7019 .7054 .7088 .7123 .7157 .7190 .7224
.6 .7257 .7291 .7324 .7357 .7389 .7422 .7454 .7486 .7517 .7549
.7 .7580 .7611 .7642 .7673 .7704 .7734 .7764 .7794 .7823 .7852.8 .7881 .7910 .7939 .7967 .7995 .8023 .8051 .8078 .8106 .8133
.9 .8159 .8186 .8212 .8238 .8264 .8289 .8315 .8340 .8365 .8389
1.0 .8413 .8438 .8461 .8485 .8508 .8531 .8554 .8577 .8599 .8621
1.1 .8643 .8665 .8686 .8708 .8729 .8749 .8770 .8790 .8810 .8830
1.2 .8849 .8869 .8888 .8907 .8925 .8944 .8962 .8980 .8997 .9015
1.3 .9032 .9049 .9066 .9082 .9099 .9115 .9131 .9147 .9162 .9177
1.4 .9192 .9207 .9222 .9236 .9251 .9265 .9279 .9292 .9306 .9319
1.5 .9332 .9345 .9357 .9370 .9382 .9394 .9406 .9418 .9429 .94411.6 .9452 .9463 .9474 .9484 .9495 .9505 .9515 .9525 .9535 .9545
1.7 .9554 .9564 .9573 .9582 .9591 .9599 .9608 .9616 .9625 .9633
1.8 .9641 .9649 .9656 .9664 .9671 .9678 .9686 .9693 .9699 .97061.9 .9713 .9719 .9726 .9732 .9738 .9744 .9750 .9756 .9761 .9767
2.0 .9772 .9778 .9783 .9788 .9793 .9798 .9803 .9808 .9812 .9817
2.1 .9821 .9826 .9830 .9834 .9838 .9842 .9846 .9850 .9854 .98572.2 .9861 .9864 .9868 .9871 .9875 .9878 .9881 .9884 .9887 .9890
2.3 .9893 .9896 .9898 .9901 .9904 .9906 .9909 .9911 .9913 .9916
2.4 .9918 .9920 .9922 .9925 .9927 .9929 .9931 .9932 .9934 .99362.5 .9938 .9940 .9941 .9943 .9945 .9946 .9948 .9949 .9951 .9952
2.6 .9953 .9955 .9956 .9957 .9959 .9960 .9961 .9962 .9963 .9964
2.7 .9965 .9966 .9967 .9968 .9969 .9970 .9971 .9972 .9973 .99742.8 .9974 .9975 .9976 .9977 .9977 .9978 .9979 .9979 .9980 .9981
2.9 .9981 .9982 .9982 .9983 .9984 .9984 .9985 .9985 .9986 .9986
3.0 .9987 .9987 .9987 .9988 .9988 .9989 .9989 .9989 .9990 .99903.1 .9990 .9991 .9991 .9991 .9992 .9992 .9992 .9992 .9993 .9993
3.2 .9993 .9993 .9994 .9994 .9994 .9994 .9994 .9995 .9995 .9995
3.3 .9995 .9995 .9995 .9996 .9996 .9996 .9996 .9996 .9996 .9997
3.4 .9997 .9997 .9997 .9997 .9997 .9997 .9997 .9997 .9997 .9998
The values of /Phi1(x)for nonnegative xare given in Table 5.1. For negative values of
x,/Phi1(x)can be obtained from the relationship
/Phi1(−x)=1−/Phi1(x) −q<x<q (4.1)
The proof of Equation (4.1), which follows from the symmetry of the standard nor-
mal density, is left as an exercise. This equation states that if Zis a standard normal
random variable, then
P{Z…−x}= P{Z>x}− q<x<q

<<<PAGE 204>>>

A First Course in Probability 191
Since Z=(X−μ)/σ is a standard normal random variable whenever Xis normally
distributed with parameters μandσ2, it follows that the distribution function of X
can be expressed as
FX(a)=P{X…a}= P/parenleftbiggX−μ
σ…a−μ
σ/parenrightbigg
=/Phi1/parenleftbigga−μ
σ/parenrightbigg
Example
4bIfXis a normal random variable with parameters μ=3 and σ2=9, ﬁnd
(a)P{2<X<5};( b ) P{X>0};( c ) P{|X−3|>6}.
Solution (a)
P{2<X<5}=P/braceleftbigg2−3
3<X−3
3<5−3
3/bracerightbigg
=P/braceleftbigg
−1
3<Z<2
3/bracerightbigg
=/Phi1/parenleftbigg2
3/parenrightbigg
−/Phi1/parenleftbigg
−1
3/parenrightbigg
=/Phi1/parenleftbigg2
3/parenrightbigg
−/bracketleftBigg
1−/Phi1/parenleftbigg1
3/parenrightbigg/bracketrightBigg
L.3779
(b)
P{X>0}= P/braceleftbiggX−3
3>0−3
3/bracerightbigg
=P{Z>−1}
=1−/Phi1(−1)
=/Phi1(1)
L.8413
(c)
P{|X−3|>6}= P{X>9}+ P{X<−3}
=P/braceleftbiggX−3
3>9−3
3/bracerightbigg
+P/braceleftbiggX−3
3<−3−3
3/bracerightbigg
=P{Z>2}+ P{Z<−2}
=1−/Phi1(2)+/Phi1(−2)
=2[1−/Phi1(2)]
L.0456 .
Example
4cAn examination is frequently regarded as being good (in the sense of determining
a valid grade spread for those taking it) if the test scores of those taking the exami-
nation can be approximated by a normal density function. (In other words, a graph
of the frequency of grade scores should have approximately the bell-shaped form ofthe normal density.) The instructor often uses the test scores to estimate the normal
parameters μandσ
2and then assigns the letter grade A to those whose test score
is greater than μ+σ, B to those whose score is between μandμ+σ, C to those
whose score is between μ−σandμ, D to those whose score is between μ−2σ
andμ−σ, and F to those getting a score below μ−2σ. (This strategy is sometimes
referred to as grading “on the curve.”) Since

<<<PAGE 205>>>

192 Chapter 5 Continuous Random Variables
P{X>μ+ σ}=P/braceleftbiggX−μ
σ>1/bracerightbigg
=1−/Phi1(1)L.1587
P{μ< X<μ+ σ}=P/braceleftbigg
0<X−μ
σ<1/bracerightbigg
=/Phi1(1)−/Phi1(0)L.3413
P{μ−σ< X<μ}=P/braceleftbigg
−1<X−μ
σ<0/bracerightbigg
=/Phi1(0)−/Phi1(−1)L.3413
P{μ−2σ< X<μ− σ}=P/braceleftbigg
−2<X−μ
σ<−1/bracerightbigg
=/Phi1(2)−/Phi1(1)L.1359
P{X<μ−2σ}=P/braceleftbiggX−μ
σ<−2/bracerightbigg
=/Phi1(−2)L.0228
it follows that approximately 16 percent of the class will receive an A grade on the
examination, 34 percent a B grade, 34 percent a C grade, and 14 percent a D grade;
2 percent will fail. .
Example
4dAn expert witness in a paternity suit testiﬁes that the length (in days) of human
gestation is approximately normally distributed with parameters μ=270 and σ2=
100. The defendant in the suit is able to prove that he was out of the country duringa period that began 290 days before the birth of the child and ended 240 days beforethe birth. If the defendant was, in fact, the father of the child, what is the probability
that the mother could have had the very long or very short gestation indicated by
the testimony?
Solution LetXdenote the length of the gestation, and assume that the defendant
is the father. Then the probability that the birth could occur within the indicatedperiod is
P{X>290 or X<240}= P{X>290}+P {X<240}
=P/braceleftbiggX−270
10>2/bracerightbigg
+P/braceleftbiggX−270
10<−3/bracerightbigg
=1−/Phi1(2)+1−/Phi1(3)
L.0241 .
Example
4eSuppose that a binary message—either 0 or 1—must be transmitted by wire fromlocation Ato location B. However, the data sent over the wire are subject to a chan-
nel noise disturbance, so, to reduce the possibility of error, the value 2 is sent over
the wire when the message is 1 and the value −2 is sent when the message is 0. If
x,x=;2, is the value sent at location A, then R, the value received at location B,i s
given by R=x+N, where Nis the channel noise disturbance. When the message
is received at location B, the receiver decodes it according to the following rule:
IfRÚ.5, then 1 is concluded.
IfR<.5, then 0 is concluded.
Because the channel noise is often normally distributed, we will determine the error
probabilities when Nis a standard normal random variable.
Two types of errors can occur: One is that the message 1 can be incorrectly
determined to be 0, and the other is that 0 can be incorrectly determined to be 1.

<<<PAGE 206>>>

A First Course in Probability 193
The ﬁrst type of error will occur if the message is 1 and 2 +N<.5, whereas the
second will occur if the message is 0 and −2+NÚ.5. Hence,
P{error|message is 1 }=P{N<−1.5}
=1−/Phi1(1.5)L.0668
and
P{error|message is 0 }=P{NÚ2.5}
=1−/Phi1(2.5)L.0062.
Example
4fValue at Risk (VAR) has become a key concept in ﬁnancial calculations. The VAR of
an investment is deﬁned as that value vsuch that there is only a 1 percent chance that
the loss from the investment will be greater than v.I fX, the gain from an investment,
is a normal random variable with mean μand variance σ2, then because the loss is
equal to the negative of the gain, the VAR of such an investment is that value vsuch
that
.01=P{−X>ν}
Using that −Xis normal with mean −μand variance σ2, we see that
.01=P/braceleftbigg−X+μ
σ>ν+μ
σ/bracerightbigg
=1−/Phi1/parenleftbiggv+μ
σ/parenrightbigg
Because, as indicated by Table 5 .1,/Phi1(2.33)=.99, we see that
ν+μ
σ=2.33
That is,
ν=VAR=2.33σ −μ
Consequently, among a set of investments all of whose gains are normally distributed,the investment having the smallest VAR is the one having the largest value ofμ−2.33σ. .
5.4.1 The Normal Approximation to the Binomial Distribution
An important result in probability theory known as the DeMoivre–Laplace limittheorem states that when nis large, a binomial random variable with parameters n
andpwill have approximately the same distribution as a normal random variable
with the same mean and variance as the binomial. This result was proved originally
for the special case of p=
1
2by DeMoivre in 1733 and was then extended to general
pby Laplace in 1812. It formally states that if we “standardize” the binomial by
ﬁrst subtracting its mean npand then dividing the result by its standard deviation/radicalbig
np(1−p), then the distribution function of this standardized random variable
(which has mean 0 and variance 1) will converge to the standard normal distribution
function as n→q.

<<<PAGE 207>>>

194 Chapter 5 Continuous Random Variables
The DeMoivre–Laplace limit theorem
IfSndenotes the number of successes that occur when nindependent trials, each
resulting in a success with probability p, are performed, then, for any a<b,
P/braceleftBigg
a…Sn−np/radicalbig
np(1−p)…b/bracerightBigg
→/Phi1( b)−/Phi1(a)
asn→q.
Because the preceding theorem is only a special case of the central limit theo-
rem, which is presented in Chapter 8, we shall not present a proof.
Note that we now have two possible approximations to binomial probabilities:
the Poisson approximation, which is good when nis large and pis small, and the
normal approximation, which can be shown to be quite good when np(1 −p)is
large. (See Figure 5.6.) [The normal approximation will, in general, be quite good
for values of nsatisfying np(1−p)Ú10.]
Example
4gLetXbe the number of times that a fair coin that is ﬂipped 40 times lands on heads.
Find the probability that X=20. Use the normal approximation and then compare
it with the exact solution.
Solution To employ the normal approximation, note that because the binomial is
a discrete integer-valued random variable, whereas the normal is a continuous ran-dom variable, it is best to write P{X=i}asP{i−1/2<X<i+1/2} before
0246(10, 0.7)
81 0 0
050.00.020.040.060.080.100.120.140.16
0.00.14
0.120.100.080.060.04
0.020.00.30
0.250.200.150.100.05
0.00.20
0.150.100.05
10 15(30, 0.7)
20 25 30 0 10 20(50, 0.7)
30 40 5051 0(20, 0.7)
x
xxx15 20
Figure 5.6 The probability mass function of a binomial ( n,p) random variable becomes
more and more “normal” as nbecomes larger and larger.

<<<PAGE 208>>>

A First Course in Probability 195
applying the normal approximation (this is called the continuity correction ). Doing
so gives
P{X=20}= P{19.5 <X<20.5}
=P/braceleftBigg
19.5−20√
10<X−20√
10<20.5−20√
10/bracerightBigg
LP/braceleftBigg
−.16 <X−20√
10<.16/bracerightBigg
L/Phi1(.16)−/Phi1(−.16) L.1272
The exact result is
P{X=20}=/parenleftBigg
40
20/parenrightBigg/parenleftbigg1
2/parenrightbigg40
L.1254 .
Example
4hThe ideal size of a ﬁrst-year class at a particular college is 150 students. The college,knowing from past experience that, on the average, only 30 percent of those accepted
for admission will actually attend, uses a policy of approving the applications of 450students. Compute the probability that more than 150 ﬁrst-year students attend this
college.
Solution IfXdenotes the number of students who attend, then Xis a binomial ran-
dom variable with parameters n=450 and p=.3. Using the continuity correction,
we see that the normal approximation yields
P{XÚ150.5}= P/braceleftBigg
X−(450)(. 3)√450(. 3)(.7)Ú150.5 −(450)(. 3)√450(. 3)(.7)/bracerightBigg
L1−/Phi1(1.59)
L.0559
Hence, less than 6 percent of the time do more than 150 of the ﬁrst 450 accepted
actually attend. (What independence assumptions have we made?) .
Example
4iTo determine the effectiveness of a certain diet in reducing the amount of cholesterol
in the bloodstream, 100 people are put on the diet. After they have been on the diet
for a sufﬁcient length of time, their cholesterol count will be taken. The nutritionistrunning this experiment has decided to endorse the diet if at least 65 percent of the
people have a lower cholesterol count after going on the diet. What is the proba-
bility that the nutritionist endorses the new diet if, in fact, it has no effect on thecholesterol level?
Solution Let us assume that if the diet has no effect on the cholesterol count, then,
strictly by chance, each person’s count will be lower than it was before the diet with
probability1
2. Hence, if Xis the number of people whose count is lowered, then the
probability that the nutritionist will endorse the diet when it actually has no effect
on the cholesterol count is
100/summationdisplay
i=65/parenleftBigg
100
i/parenrightBigg/parenleftbigg1
2/parenrightbigg100
=P{XÚ64.5}
=P⎧
⎪⎨
⎪⎩X−(100)(1
2)/radicalBig
100(1
2)(1
2)Ú2.9⎫
⎪⎬
⎪⎭
L1−/Phi1(2.9)
L.0019 .

<<<PAGE 209>>>

196 Chapter 5 Continuous Random Variables
Example
4jFifty-two percent of the residents of New York City are in favor of outlawing cigarette
smoking on university campuses. Approximate the probability that more than 50
percent of a random sample of npeople from New York are in favor of this prohibi-
tion when
(a)n=11
(b)n=101
(c)n=1001
How large would nhave to be to make this probability exceed .95?
Solution LetNdenote the number of residents of New York City. To answer the
preceding question, we must ﬁrst understand that a random sample of size nis a
sample such that the npeople were chosen in such a manner that each of the/parenleftBigg
N
n/parenrightBigg
subsets of npeople had the same chance of being the chosen subset. Consequently,
Sn, the number of people in the sample who are in favor of the smoking prohibition,
is a hypergeometric random variable. That is, Snhas the same distribution as the
number of white balls obtained when nballs are chosen from an urn of Nballs, of
which .52N are white. But because Nand .52N are both large in comparison with the
sample size n, it follows from the binomial approximation to the hypergeometric (see
Section 4.8.3) that the distribution of Snis closely approximated by a binomial dis-
tribution with parameters nandp=.52. The normal approximation to the binomial
distribution then shows that
P{Sn>.5n}= P/braceleftBigg
Sn−.52n√n(.52)(.48)>.5n−.52n√n(.52)(.48)/bracerightBigg
=P/braceleftBigg
Sn−.52n√n(.52)(.48)>−.04√n/bracerightBigg
L/Phi1(.04√n)
Thus,
P{Sn>.5n}L⎧
⎪⎨
⎪⎩/Phi1(.1328) =.5528, if n=11
/Phi1(.4020) =.6562, if n=101
/Phi1(1.2665) =.8973, if n=1001
In order for this probability to be at least .95, we would need /Phi1(.04√n)>. 95.
Because /Phi1(x)is an increasing function and /Phi1(1.645) =.95, this means that
.04√n>1.645
or
nÚ1691.266
That is, the sample size would have to be at least 1692. .
Historical notes concerning the normal distribution
The normal distribution was introduced by the French mathematician Abra-
ham DeMoivre in 1733. DeMoivre, who used this distribution to approximate
probabilities connected with coin tossing, called it the exponential bell-shaped
curve. Its usefulness, however, became truly apparent only in 1809, when thefamous German mathematician Karl Friedrich Gauss used it as an integral part
of his approach to predicting the location of astronomical entities. As a result, it
became common after this time to call it the Gaussian distribution .

<<<PAGE 210>>>

A First Course in Probability 197
During the mid- to late 19th century, however, most statisticians started to
believe that the majority of data sets would have histograms conforming to the
Gaussian bell-shaped form. Indeed, it came to be accepted that it was “normal”
for any well-behaved data set to follow this curve. As a result, following the leadof the British statistician Karl Pearson, people began referring to the Gaussian
curve by calling it simply the normal curve. (A partial explanation as to why
so many data sets conform to the normal curve is provided by the central limit
theorem, which is presented in Chapter 8.)
Abraham DeMoivre (1667–1754)
Today there is no shortage of statistical consultants, many of whom ply theirtrade in the most elegant of settings. However, the ﬁrst of their breed worked,in the early years of the 18th century, out of a dark, grubby betting shop in
Long Acres, London, known as Slaughter’s Coffee House. He was Abraham
DeMoivre, a Protestant refugee from Catholic France, and, for a price, he wouldcompute the probability of gambling bets in all types of games of chance.
Although DeMoivre, the discoverer of the normal curve, made his living at
the coffee shop, he was a mathematician of recognized abilities. Indeed, he was
a member of the Royal Society and was reported to be an intimate of Isaac
Newton.
Listen to Karl Pearson imagining DeMoivre at work at Slaughter’s Coffee
House: “I picture DeMoivre working at a dirty table in the coffee house with abroken-down gambler beside him and Isaac Newton walking through the crowdto his corner to fetch out his friend. It would make a great picture for an inspired
artist.”
Karl Friedrich Gauss
Karl Friedrich Gauss (1777–1855), one of the earliest users of the normal curve,
was one of the greatest mathematicians of all time. Listen to the words of the
well-known mathematical historian E. T. Bell, as expressed in his 1954 book
Men of Mathematics : In a chapter entitled “The Prince of Mathematicians,” he
writes, “Archimedes, Newton, and Gauss; these three are in a class by themselves
among the great mathematicians, and it is not for ordinary mortals to attempt to
rank them in order of merit. All three started tidal waves in both pure and appliedmathematics. Archimedes esteemed his pure mathematics more highly than its
applications;
Newton appears to have found the chief justiﬁcation for his mathematical inven-tions in the scientiﬁc uses to which he put them; while Gauss declared it was all
one to him whether he worked on the pure or on the applied side .”
5.5 Exponential Random Variables
A continuous random variable whose probability density function is given, for some
λ> 0, by
f(x)=/braceleftBigg
λe−λxifxÚ0
0i f x<0
is said to be an exponential random variable (or, more simply, is said to be exponen-
tially distributed) with parameter λ. The cumulative distribution function F(a)of an
exponential random variable is given by

<<<PAGE 211>>>

198 Chapter 5 Continuous Random Variables
F(a)=P{X…a}
=/integraldisplaya
0λe−λxdx
=−e−λx/vextendsingle/vextendsinglea
0
=1−e−λaaÚ0
Note that F(q)=/integraltextq
0λe−λxdx=1, as, of course, it must. The parameter λwill now
be shown to equal the reciprocal of the expected value.
Example
5aLetXbe an exponential random variable with parameter λ. Calculate (a) E[X] and
(b) Var (X).
Solution (a) Since the density function is given by
f(x)=/braceleftBigg
λe−λxxÚ0
0 x<0
we obtain, for n>0,
E[Xn]=/integraldisplayq
0xnλe−λxdx
Integrating by parts (with λe−λx=dvandu=xn) yields
E[Xn]=−xne−λx|q
0+/integraldisplayq
0e−λxnxn−1dx
=0+n
λ/integraldisplayq
0λe−λxxn−1dx
=n
λE[Xn−1]
Letting n=1 and then n=2 gives
E[X]=1
λ
E[X2]=2
λE[X]=2
λ2
(b) Hence,
Var(X)=2
λ2−/parenleftbigg1
λ/parenrightbigg2
=1
λ2
Thus, the mean of the exponential is the reciprocal of its parameter λ, and the vari-
ance is the mean squared. .
In practice, the exponential distribution often arises as the distribution of the
amount of time until some speciﬁc event occurs. For instance, the amount of time
(starting from now) until an earthquake occurs, or until a new war breaks out, or
until a telephone call you receive turns out to be a wrong number are all randomvariables that tend in practice to have exponential distributions. (For a theoretical
explanation of this phenomenon, see Section 4.7.)
Example
5bSuppose that the length of a phone call in minutes is an exponential random variable
with parameter λ=1
10. If someone arrives immediately ahead of you at a public
telephone booth, ﬁnd the probability that you will have to wait
(a)more than 10 minutes;
(b)between 10 and 20 minutes.

<<<PAGE 212>>>

A First Course in Probability 199
Solution LetXdenote the length of the call made by the person in the booth. Then
the desired probabilities are
(a)
P{X>10}= 1−F(10)
=e−1L.368
(b)
P{10<X<20}= F(20)−F(10)
=e−1−e−2L.233 .
We say that a nonnegative random variable Xismemoryless if
P{X>s+t|X>t}=P{X>s}for all s,tÚ0 (5.1)
If we think of Xas being the lifetime of some instrument, Equation (5.1) states that
the probability that the instrument survives for at least s+thours, given that it has
survived thours, is the same as the initial probability that it survives for at least
shours. In other words, if the instrument is alive at age t, the distribution of the
remaining amount of time that it survives is the same as the original lifetime distri-
bution. (That is, it is as if the instrument does not “remember” that it has already
been in use for a time t.)
Equation (5.1) is equivalent to
P{X>s+t,X>t}
P{X>t}=P{X>s}
or
P{X>s+t}=P{X>s}P{X>t} (5.2)
Since Equation (5.2) is satisﬁed when Xis exponentially distributed (for e−λ(s+t )=
e−λse−λt), it follows that exponentially distributed random variables are memoryless.
Example
5cConsider a post ofﬁce that is staffed by two clerks. Suppose that when Mr. Smithenters the system, he discovers that Ms. Jones is being served by one of the clerks
and Mr. Brown by the other. Suppose also that Mr. Smith is told that his service will
begin as soon as either Ms. Jones or Mr. Brown leaves. If the amount of time thata clerk spends with a customer is exponentially distributed with parameter λ, what
is the probability that of the three customers, Mr. Smith is the last to leave the postofﬁce?
Solution The answer is obtained by reasoning as follows: Consider the time at which
Mr. Smith ﬁrst ﬁnds a free clerk. At this point, either Ms. Jones or Mr. Brown wouldhave just left, and the other one would still be in service. However, because theexponential is memoryless, it follows that the additional amount of time that this
other person (either Ms. Jones or Mr. Brown) would still have to spend in the post
ofﬁce is exponentially distributed with parameter λ. That is, it is the same as if service
for that person were just starting at this point. Hence, by symmetry, the probability
that the remaining person ﬁnishes before Smith leaves must equal
1
2. .
It turns out that not only is the exponential distribution memoryless, but it is
also the unique distribution possessing this property. To see this, suppose that Xis
memoryless and let F(x)=P{X>x}. Then, by Equation (5.2),
F(s+t)=F(s)F(t)

<<<PAGE 213>>>

200 Chapter 5 Continuous Random Variables
That is, F(·)satisﬁes the functional equation
g(s+t)=g(s)g(t)
However, it turns out that the only right continuous solution of this functional
equation is†
g(x)=e−λx(5.3)
and, since a distribution function is always right continuous, we must have
F(x)=e−λxorF(x)=P{X…x}=1 −e−λx
which shows that Xis exponentially distributed.
Example
5dSuppose that the number of miles that a car can run before its battery wears out is
exponentially distributed with an average value of 10,000 miles. If a person desires
to take a 5000-mile trip, what is the probability that he or she will be able to com-
plete the trip without having to replace the car battery? What can be said when the
distribution is not exponential?
Solution It follows by the memoryless property of the exponential distribution that
the remaining lifetime (in thousands of miles) of the battery is exponential with
parameter λ=1
10. Hence, the desired probability is
P{remaining lifetime >5}=1 −F(5)=e−5λ=e−1/2L.607
However, if the lifetime distribution Fis not exponential, then the relevant proba-
bility is
P{lifetime >t+5|lifetime >t}=1−F(t+5)
1−F(t)
where tis the number of miles that the battery had been in use prior to the start of
the trip. Therefore, if the distribution is not exponential, additional information is
needed (namely, the value of t) before the desired probability can be calculated. .
A variation of the exponential distribution is the distribution of a random vari-
able that is equally likely to be either positive or negative and whose absolute value
is exponentially distributed with parameter λ,#λÚ0. Such a random variable is said
to have a Laplace distribution,‡and its density is given by
f(x)=1
2λe−λ|x|−q<x<q
†One can prove Equation (5.3) as follows: If g(s+t)=g(s)g(t),t h e n
g/parenleftbigg2
n/parenrightbigg
=g/parenleftbigg1
n+1
n/parenrightbigg
=g2/parenleftbigg1
n/parenrightbigg
and repeating this yields g(m/n) =gm(1/n).A l s o ,
g(1)=g/parenleftbigg1
n+1
n+ ··· +1
n/parenrightbigg
=gn/parenleftbigg1
n/parenrightbigg
org/parenleftbigg1
n/parenrightbigg
=(g(1))1/n
Hence, g(m/n) =(g(1))m/n, which, since gis right continuous, implies that g(x)=(g(1))x. Because g(1)=/parenleftbigg
g/parenleftBig
1
2/parenrightBig/parenrightbigg2
Ú0, we obtain g(x)=e−λx,w h e r eλ =−log(g(1)).
‡It also is sometimes called the double exponential random variable.

<<<PAGE 214>>>

A First Course in Probability 201
Its distribution function is given by
F(x)=⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩1
2/integraldisplayx
−qλeλydy x<0
1
2/integraldisplay0
−qλeλydy+1
2/integraldisplayx
0λe−λydy x >0
=⎧
⎪⎪⎨
⎪⎪⎩1
2eλxx<0
1−1
2e−λxx>0
Example
5eConsider again Example 4e, which supposes that a binary message is to be transmit-
ted from AtoB, with the value 2 being sent when the message is 1 and −2 when it
is 0. However, suppose now that rather than being a standard normal random vari-
able, the channel noise Nis a Laplacian random variable with parameter λ=1.
Suppose again that if Ris the value received at location B, then the message is
decoded as follows:
IfRÚ.5, then 1 is concluded.
IfR<.5, then 0 is concluded.
In this case, where the noise is Laplacian with parameter λ=1, the two types of
errors will have probabilities given by
P{error|message 1 is sent }=P{N<−1.5}
=1
2e−1.5
L.1116
P{error|message 0 is sent }=P{NÚ2.5}
=1
2e−2.5
L.041
On comparing this with the results of Example 4e, we see that the error probabilities
are higher when the noise is Laplacian with λ=1 than when it is a standard normal
variable.
5.5.1 Hazard Rate Functions
Consider a positive continuous random variable Xthat we interpret as being the
lifetime of some item. Let Xhave distribution function Fand density f.T h e hazard
rate(sometimes called the failure rate ) function λ(t)ofFis deﬁned by
λ(t)=f(t)
F(t), where F=1−F
To interpret λ(t), suppose that the item has survived for a time tand we desire the
probability that it will not survive for an additional time dt. That is, consider P{X∈
(t,t+dt)|X>t}.N o w ,

<<<PAGE 215>>>

202 Chapter 5 Continuous Random Variables
P{X∈(t,t+dt)|X>t}=P{X∈(t,t+dt),X>t}
P{X>t}
=P{X∈(t,t+dt)}
P{X>t}
Lf(t)
F(t)dt
Thus, λ(t)represents the conditional probability intensity that a t-unit-old item will
fail.
Suppose now that the lifetime distribution is exponential. Then, by the memory-
less property, it follows that the distribution of remaining life for a t-year-old item is
the same as that for a new item. Hence, λ(t)should be constant. In fact, this checks
out, since
λ(t)=f(t)
F(t)
=λe−λt
e−λt
=λ
Thus, the failure rate function for the exponential distribution is constant. The param-
eterλis often referred to as the rateof the distribution.
It turns out that the failure rate function λ(s),sÚ0, uniquely determines the
distribution function F. To prove this, we integrate λ(s)from 0 to tto obtain
/integraldisplayt
0λ(s)ds=/integraldisplayt
0f(s)
1−F(s)ds
=−log(1−F(s))|t
0
=−log(1−F(t))+log(1−F(0))
=−log(1−F(t))
where the second equality used that f(s)=d
dsF(s)and the ﬁnal equality used that
F(0)=0.Solving the preceding equation for F(t)gives
F(t)=1−exp/braceleftBigg
−/integraldisplayt
0λ(s)ds/bracerightBigg
(5.4)
Hence, a distribution function of a positive continuous random variable can be
speciﬁed by giving its hazard rate function. For instance, if a random variable has a
linear hazard rate function—that is, if
λ(t)=a+bt
then its distribution function is given by
F(t)=1−e−at−bt2/2
and differentiation yields its density, namely,
f(t)=(a+bt)e−(at+bt2/2)tÚ0
When a=0, the preceding equation is known as the Rayleigh density function .
Example
5fOne often hears that the death rate of a person who smokes is, at each age, twice that
of a nonsmoker. What does this mean? Does it mean that a nonsmoker has twice the
probability of surviving a given number of years as does a smoker of the same age?

<<<PAGE 216>>>

A First Course in Probability 203
Solution Ifλs(t)denotes the hazard rate of a smoker of age tandλn(t)that of a
nonsmoker of age t, then the statement at issue is equivalent to the statement that
λs(t)=2λn(t)
The probability that an A-year-old nonsmoker will survive until age B,A<B,i s
P{A-year-old nonsmoker reaches age B}
=P{nonsmoker’s lifetime >B|nonsmoker’s lifetime >A}
=1−Fnon(B)
1−Fnon(A)
=exp/braceleftBigg
−/integraldisplayB
0λn(t)dt/bracerightBigg
exp/braceleftBigg
−/integraldisplayA
0λn(t)dt/bracerightBigg from(5.4)
=exp/braceleftBigg
−/integraldisplayB
Aλn(t)dt/bracerightBigg
whereas the corresponding probability for a smoker is, by the same reasoning,
P{A-year-old smoker reaches age B}=exp/braceleftBigg
−/integraldisplayB
Aλs(t)dt/bracerightBigg
=exp/braceleftBigg
−2/integraldisplayB
Aλn(t)dt/bracerightBigg
=⎡
⎣exp/braceleftBigg
−/integraldisplayB
Aλn(t)dt/bracerightBigg⎤⎦2
In other words, for two people of the same age, one of whom is a smoker and
the other a nonsmoker, the probability that the smoker survives to any given age
is the square (not one-half) of the corresponding probability for a nonsmoker. For
instance, if λn(t)=1
30,5 0…t…60, then the probability that a 50-year-old nonsmoker
reaches age 60 is e−1/3L.7165, whereas the corresponding probability for a smoker
ise−2/3L.5134. .
5.6 Other Continuous Distributions
5.6.1 The Gamma Distribution
A random variable is said to have a gamma distribution with parameters (α,λ),λ>0,
α> 0, if its density function is given by
f(x)=⎧
⎪⎨
⎪⎩λe−λx(λx)α−1
/Gamma1(α)xÚ0
0 x<0
where /Gamma1(α), called the gamma function, is deﬁned as
/Gamma1(α)=/integraldisplayq
0e−yyα−1dy

<<<PAGE 217>>>

204 Chapter 5 Continuous Random Variables
Integration of /Gamma1(α) by parts yields
/Gamma1(α)=−e−yyα−1/vextendsingle/vextendsingle/vextendsingleq
0+/integraldisplayq
0e−y(α−1)yα−2dy
=(α−1)/integraldisplayq
0e−yyα−2dy (6.1)
=(α−1)/Gamma1(α −1)
For integral values of α,s a y ,α =n, we obtain, by applying Equation (6.1) repeatedly,
/Gamma1(n)=(n−1)/Gamma1(n −1)
=(n−1)(n−2)/Gamma1(n −2)
=···
=(n−1)(n−2)···3·2/Gamma1(1)
Since /Gamma1(1)=/integraltextq
0e−xdx=1, it follows that, for integral values of n,
/Gamma1(n)=(n−1)!
When αis a positive integer, say, α=n, the gamma distribution with parameters
(α,λ)often arises, in practice as the distribution of the amount of time one has to
wait until a total of nevents has occurred. More speciﬁcally, if events are occurring
randomly and in accordance with the three axioms of Section 4.7, then it turns out
that the amount of time one has to wait until a total of nevents has occurred will be
a gamma random variable with parameters (n,λ). To prove this, let Tndenote the
time at which the nth event occurs, and note that Tnis less than or equal to tif and
only if the number of events that have occurred by time tis at least n. That is, with
N(t)equal to the number of events in [0, t],
P{Tn…t}=P{N(t)Ún}
=q/summationdisplay
j=nP{N(t)=j}
=q/summationdisplay
j=ne−λt(λt)j
j!
where the ﬁnal identity follows because the number of events in [0, t] has a
Poisson distribution with parameter λt. Differentiation of the preceding now yields
the density function of Tn:
f(t)=q/summationdisplay
j=ne−λtj(λt)j−1λ
j!−q/summationdisplay
j=nλe−λt(λt)j
j!
=q/summationdisplay
j=nλe−λt(λt)j−1
(j−1)!−q/summationdisplay
j=nλe−λt(λt)j
j!
=λe−λt(λt)n−1
(n−1)!
Hence, Tnhas the gamma distribution with parameters (n,λ). (This distribution is
often referred to in the literature as the n-Erlang distribution .) Note that when n=1,
this distribution reduces to the exponential distribution.
The gamma distribution with λ=1
2andα=n/2,na positive integer, is called
theχ2
n(read “chi-squared”) distribution with ndegrees of freedom. The chi-squared

<<<PAGE 218>>>

A First Course in Probability 205
distribution often arises in practice as the distribution of the error involved in
attempting to hit a target in n-dimensional space when each coordinate error is nor-
mally distributed. This distribution will be studied in Chapter 6, where its relation tothe normal distribution is detailed.
Example
6aLetXbe a gamma random variable with parameters αandλ. Calculate (a) E[X]
and (b) Var (X).
Solution (a)
E[X]=1
/Gamma1(α)/integraldisplayq
0λxe−λx(λx)α−1dx
=1
λ/Gamma1(α)/integraldisplayq
0λe−λx(λx)αdx
=/Gamma1(α+1)
λ/Gamma1(α)
=α
λby Equation (6.1)
(b) By ﬁrst calculating E[X2], we can show that
Var(X)=α
λ2
The details are left as an exercise. .
5.6.2 The Weibull Distribution
The Weibull distribution is widely used in engineering practice due to its versatil-
ity. It was originally proposed for the interpretation of fatigue data, but now its use
has been extended to many other engineering problems. In particular, it is widelyused in the ﬁeld of life phenomena as the distribution of the lifetime of some object,
especially when the “weakest link” model is appropriate for the object. That is, con-
sider an object consisting of many parts, and suppose that the object experiencesdeath (failure) when any of its parts fails. It has been shown (both theoretically
and empirically) that under these conditions, a Weibull distribution provides a close
approximation to the distribution of the lifetime of the item.
The Weibull distribution function has the form
F(x)=⎧
⎪⎪⎪⎨
⎪⎪⎪⎩0 x…ν
1−exp/braceleftBigg
−/parenleftbiggx−ν
α/parenrightbiggβ/bracerightBigg
x>ν(6.2)
A random variable whose cumulative distribution function is given by Equation (6.2)
is said to be a Weibull random variable with parameters ν,α, andβ. Differentiation
yields the density:
f(x)=⎧
⎪⎪⎪⎨
⎪⎪⎪⎩0 x…ν
β
α/parenleftbiggx−ν
α/parenrightbiggβ−1
exp/braceleftBigg
−/parenleftbiggx−ν
α/parenrightbiggβ/bracerightBigg
x>ν

<<<PAGE 219>>>

206 Chapter 5 Continuous Random Variables
5.6.3 The Cauchy Distribution
A random variable is said to have a Cauchy distribution with parameter θ,
−q<θ<q, if its density is given by
f(x)=1
π1
1+(x−θ)2−q<x<q
Example
6bSuppose that a narrow-beam ﬂashlight is spun around its center, which is located a
unit distance from the x-axis. (See Figure 5.7.) Consider the point Xat which the
beam intersects the x-axis when the ﬂashlight has stopped spinning. (If the beam is
not pointing toward the x-axis, repeat the experiment.)
/H9258
1
0 Xx -axis
Figure 5.7
As indicated in Figure 5.7, the point Xis determined by the angle θbetween the
ﬂashlight and the y-axis, which, from the physical situation, appears to be uniformly
distributed between −π/2 and π/2. The distribution function of Xis thus given by
F(x)=P{X…x}
=P{tanθ…x}
=P{θ…tan−1x}
=1
2+1
πtan−1x
where the last equality follows since θ, being uniform over (−π/ 2,π/2), has
distribution
P{θ…a}=a−(−π/ 2)
π=1
2+a
π−π
2<a<π
2
Hence, the density function of Xis given by
f(x)=d
dxF(x)=1
π(1+x2)−q<x<q
and we see that Xhas the Cauchy distribution.†.
†Thatd
dx(tan−1x)=1/(1+x2)can be seen as follows: If y=tan−1x,t h e nt a n y=x,s o
1=d
dx(tany)=d
dy(tany)dy
dx=d
dy/parenleftbiggsiny
cosy/parenrightbiggdy
dx=/parenleftBigg
cos2y+sin2y
cos2y/parenrightBigg
dy
dx
or
dy
dx=cos2y
sin2y+cos2y=1
tan2y+1=1
x2+1

<<<PAGE 220>>>

A First Course in Probability 207
5.6.4 The Beta Distribution
A random variable is said to have a beta distribution if its density is given by
f(x)=⎧
⎪⎨
⎪⎩1
B(a,b)xa−1(1−x)b−10<x<1
0 otherwise
where
B(a,b)=/integraldisplay1
0xa−1(1−x)b−1dx
The beta distribution can be used to model a random phenomenon whose set
of possible values is some ﬁnite interval [ c,d]—which, by letting cdenote the origin
and taking d−cas a unit measurement, can be transformed into the interval [0, 1].
When a=b, the beta density is symmetric about1
2, giving more and more
weight to regions about1
2as the common value aincreases. When a=b=1, the
beta distribution reduces to the uniform (0, 1) distribution. (See Figure 5.8.) When
b>a, the density is skewed to the left (in the sense that smaller values become more
likely), and it is skewed to the right when a>b. (See Figure 5.9.)
f(x)
x
1–2011–4a =a = 3a = 10
a = 1
Figure 5.8 Beta densities with parameters ( a,b)w h e n a=b.
f(x)
x3–2a =a = 6
Figure 5.9 Beta densities with parameters ( a,b)w h e n a/(a+b)=1/20.

<<<PAGE 221>>>

208 Chapter 5 Continuous Random Variables
The relationship
B(a,b)=/Gamma1(a)/Gamma1(b)
/Gamma1(a+b)(6.3)
can be shown to exist between
B(a,b)=/integraldisplay1
0xa−1(1−x)b−1dx
and the gamma function.
Upon using Equation (6.1) along with the identity (6.3), it is an easy matter to
show that if Xis a beta random variable with parameters aandb, then
E[X]=a
a+b
Var(X)=ab
(a+b)2(a+b+1)
Remark A veriﬁcation of Equation (6.3) appears in Example 7c of Chapter 6. .
5.7 The Distribution of a Function of a Random Variable
Often, we know the probability distribution of a random variable and are interested
in determining the distribution of some function of it. For instance, suppose that we
know the distribution of Xand want to ﬁnd the distribution of g(X).T od os o ,i ti s
necessary to express the event that g(X)…yin terms of Xbeing in some set. We
illustrate with the following examples.
Example
7aLetXbe uniformly distributed over (0, 1). We obtain the distribution of the random
variable Y, deﬁned by Y=Xn, as follows: For 0 …y…1,
FY(y)=P{Y…y}
=P{Xn…y}
=P{X…y1/n}
=FX(y1/n)
=y1/n
For instance, the density function of Yis given by
fY(y)=⎧
⎪⎨
⎪⎩1
ny1/n−10…y…1
0 otherwise.
Example
7bIfXis a continuous random variable with probability density fX, then the distribu-
tion of Y=X2is obtained as follows: For yÚ0,
FY(y)=P{Y…y}
=P{X2…y}
=P{−√y…X…√y}
=FX(√y)−FX(−√y)

<<<PAGE 222>>>

A First Course in Probability 209
Differentiation yields
fY(y)=1
2√y[fX(√y)+fX(−√y)] .
Example
7cIfXhas a probability density fX, then Y=|X|has a density function that is obtained
as follows: For yÚ0,
FY(y)=P{Y…y}
=P{|X|…y}
=P{−y…X…y}
=FX(y)−FX(−y)
Hence, on differentiation, we obtain
fY(y)=fX(y)+fX(−y) yÚ0 .
The method employed in Examples 7a through 7c can be used to prove
Theorem 7.1.
Theorem
7.1LetXbe a continuous random variable having probability density function fX. Sup-
pose that g(x)is a strictly monotonic (increasing or decreasing), differentiable (and
thus continuous) function of x. Then the random variable Ydeﬁned by Y=g(X)
has a probability density function given by
fY(y)=⎧
⎪⎨
⎪⎩fX[g−1(y)]/vextendsingle/vextendsingle/vextendsingle/vextendsingled
dyg−1(y)/vextendsingle/vextendsingle/vextendsingle/vextendsingleif y=g(x)for some x
0 if yZg(x)for all x
where g
−1(y)is deﬁned to equal that value of xsuch that g(x)=y.
We shall prove Theorem 7.1 when g(x)is an increasing function.
Proof Suppose that y=g(x)for some x. Then, with Y=g(X),
FY(y)=P{g(X)…y}
=P{X…g−1(y)}
=FX(g−1(y))
Differentiation gives
fY(y)=fX(g−1(y))d
dyg−1(y) .
which agrees with Theorem 7.1, since g−1(y)is nondecreasing, so its derivative is
nonnegative.
When yZg(x)for any x, then FY(y)is either 0 or 1, and in either case fY(y)=0.
Example
7dLetXbe a continuous nonnegative random variable with density function f, and let
Y=Xn.F i n d fY, the probability density function of Y.
Solution Ifg(x)=xn, then
g−1(y)=y1/n
and
d
dy{g−1(y)}=1
ny1/n−1

<<<PAGE 223>>>

210 Chapter 5 Continuous Random Variables
Hence, from Theorem 7.1, we obtain, for y Ú0,
fY(y)=1
ny1/n−1f(y1/n)
Forn=2, this gives
fY(y)=1
2√yf(√y)
which (since XÚ0) is in agreement with the result of Example 7b. .
Example
7eThe Lognormal Distribution IfXis a normal random variable with mean μand vari-
anceσ2, then the random variable
Y=eX
is said to be a lognormal random variable with parameters μandσ2. Thus, a random
variable Yis lognormal if log (Y)is a normal random variable. The lognormal is often
used as the distribution of the ratio of the price of a security at the end of one day
to its price at the end of the prior day. That is, if Snis the price of some security at
the end of day n, then it is often supposed thatSn
Sn−1is a lognormal random variable,
implying that XKlog/parenleftBig
Sn
Sn−1/parenrightBig
is normal. Thus, to assume thatSn
Sn−1is lognormal is to
assume that
Sn=Sn−1eX
where Xis normal.
Let us now use Theorem 7.1 to derive the density of a lognormal random vari-
ableYwith parameters μandσ2. Because Y=eX, where Xis normal with mean μ
and variance σ2, we need to determine the inverse of the function g(x)=ex.Because
y=g(g−1(y))=eg−1(y)
we obtain upon taking logarithms that
g−1(y)=log(y)
Using thatd
dyg−1(y)=1/y, Theorem 7.1 yields the density:
fY(y)=1√
2πσyexp{−(log(y) −μ)2/2σ2},y>0 .
Summary
A random variable Xiscontinuous if there is a nonnega-
tive function f, called the probability density function ofX,
such that, for any set B,
P{X∈B}=/integraldisplay
Bf(x)dx
IfXis continuous, then its distribution function Fwill be
differentiable and
d
dxF(x)=f(x)The expected value of a continuous random variable Xis
deﬁned by
E[X]=/integraldisplayq
−qxf(x)dx
A useful identity is that for any function g,
E[g(X)]=/integraldisplayq
−qg(x)f(x)dx

<<<PAGE 224>>>

A First Course in Probability 211
As in the case of a discrete random variable, the variance
ofXis deﬁned by
Var(X)=E[(X−E[X])2]
A random variable Xis said to be uniform over the inter-
val (a, b) if its probability density function is given by
f(x)=⎧
⎪⎨
⎪⎩1
b−aa…x…b
0 otherwise
Its expected value and variance are
E[X]=a+b
2Var(X)=(b−a)2
12
A random variable Xis said to be normal with parameters
μandσ2if its probability density function is given by
f(x)=1√
2πσe−(x−μ)2/2σ2−q<x<q
It can be shown that
μ=E[X]σ2=Var(X)
IfXis normal with mean μand variance σ2,t h e nZ ,
deﬁned by
Z=X−μ
σ
is normal with mean 0 and variance 1. Such a random
variable is said to be a standard normal random variable.
Probabilities about Xcan be expressed in terms of proba-
bilities about the standard normal variable Z, whose prob-
ability distribution function can be obtained either from
Table 5.1, the normal calculator on StatCrunch, or a web-
site.
When nis large, the probability distribution function
of a binomial random variable with parameters nandp
can be approximated by that of a normal random variable
having mean npand variance np(1−p).
A random variable whose probability density function
is of the form
f(x)=/braceleftbigg
λe−λxxÚ0
0 otherwise
is said to be an exponential random variable with parame-
terλ. Its expected value and variance are, respectively,
E[X]=1
λVar(X)=1
λ2
A key property possessed only by exponential randomvariables is that they are memoryless, in the sense that, for
positive sandt,
P{X>s+t|X>t}=P{X>s}IfXrepresents the life of an item, then the memoryless
property states that for any t, the remaining life of a t-year-
old item has the same probability distribution as the life ofa new item. Thus, one need not remember the age of an
item to know its distribution of remaining life.
LetXbe a nonnegative continuous random variable
with distribution function Fand density function f.T h e
function
λ(t)=f(t)
1−F(t)tÚ0
is called the hazard rate,o r failure rate, function of F.I f
we interpret Xas being the life of an item, then for small
values of dt,λ(t)dtis approximately the probability that a
t-unit-old item will fail within an additional time dt.I fFis
the exponential distribution with parameter λ,t h e n
λ(t)=λtÚ0
In addition, the exponential is the unique distribution hav-ing a constant failure rate.
A random variable is said to have a gamma distri-
bution with parameters αandλif its probability density
function is equal to
f(x)=λe
−λx(λx)α−1
/Gamma1(α)xÚ0
and is 0 otherwise. The quantity /Gamma1(α) is called the gamma
function and is deﬁned by
/Gamma1(α)=/integraldisplayq
0e−xxα−1dx
The expected value and variance of a gamma random vari-able are, respectively,
E[X]=α
λVar(X)=α
λ2
A random variable is said to have a beta distribution
with parameters ( a,b) if its probability density function
is equal to
f(x)=1
B(a,b)xa−1(1−x)b−10…x…1
and is equal to 0 otherwise. The constant B(a,b) is given by
B(a,b)=/integraldisplay1
0xa−1(1−x)b−1dx
The mean and variance of such a random variable are,respectively,
E[X]=a
a+bVar(X)=ab
(a+b)2(a+b+1)

<<<PAGE 225>>>

212 Chapter 5 Continuous Random Variables
Problems
5.1.LetXbe a random variable with probability density
function
f(x)=/braceleftBigg
c(1−x2)−1<x<1
0 otherwise
(a)What is the value of c?
(b)What is the cumulative distribution function of X?
5.2.A system consisting of one original unit plus a spare
can function for a random amount of time X. If the density
ofXis given (in units of months) by
f(x)=/braceleftBigg
Cxe−x/2x>0
0 x…0
what is the probability that the system functions for at least
5 months?
5.3.Consider the function
f(x)=/braceleftBigg
C(2x−x3)0<x<5
2
0 otherwise
Could fbe a probability density function? If so, determine
C. Repeat if f(x)were given by
f(x)=/braceleftBigg
C(2x−x2)0<x<5
2
0 otherwise
5.4.The probability density function of X, the lifetime of
a certain type of electronic device (measured in hours), is
given by
f(x)=⎧
⎪⎨
⎪⎩10
x2x>10
0 x…10
(a)Find P{X>20}.
(b)What is the cumulative distribution function of X?
(c)What is the probability that of 6 such types of devices,
at least 3 will function for at least 15 hours? What assump-
tions are you making?
5.5.A ﬁlling station is supplied with gasoline once a week.
If its weekly volume of sales in thousands of gallons is a
random variable with probability density function
f(x)=/braceleftBigg
5(1−x)40<x<1
0 otherwise
what must the capacity of the tank be so that the prob-ability of the supply being exhausted in a given week
is .01?5.6.Compute E[X]i fXhas a density function given by
(a)f(x)=⎧
⎪⎨
⎪⎩1
4xe−x/2x>0
0 otherwise;
(b)f(x)=/braceleftBigg
c(1−x2)−1<x<1
0 otherwise;
(c)f(x)=⎧
⎪⎨
⎪⎩5
x2x>5
0 x…5.
5.7.The density function of Xis given by
f(x)=/braceleftBigg
a+bx20…x…1
0 otherwise
IfE[X]=3
5, ﬁnd aandb.
5.8.The lifetime in hours of an electronic tube is a random
variable having a probability density function given by
f(x)=xe−xxÚ0
Compute the expected lifetime of such a tube.
5.9.Consider Example 4b of Chapter 4, but now suppose
that the seasonal demand is a continuous random variable
having probability density function f. Show that the opti-
mal amount to stock is the value s∗that satisﬁes
F(s∗)=b
b+/lscript
where bis net proﬁt per unit sale, /lscriptis the net loss per unit
unsold, and Fis the cumulative distribution function of the
seasonal demand.
5.10. Trains headed for destination Aarrive at the train
station at 15-minute intervals starting at 7 a.m., whereas
trains headed for destination Barrive at 15-minute inter-
vals starting at 7:05 a.m.
(a)If a certain passenger arrives at the station at a time
uniformly distributed between 7 and 8 a.m.and then gets
on the ﬁrst train that arrives, what proportion of time does
he or she go to destination A?
(b)What if the passenger arrives at a time uniformly dis-
tributed between 7:10 and 8:10 a.m.?
5.11. A point is chosen at random on a line segment of
length L. Interpret this statement, and ﬁnd the probabil-
ity that the ratio of the shorter to the longer segment is
less than1
4.
5.12. A bus travels between the two cities AandB,w h i c h
are 100 miles apart. If the bus has a breakdown, the dis-
tance from the breakdown to city Ahas a uniform distri-
bution over (0, 100). There is a bus service station in city A,

<<<PAGE 226>>>

A First Course in Probability 213
inB, and in the center of the route between AandB.I ti s
suggested that it would be more efﬁcient to have the three
stations located 25, 50, and 75 miles, respectively, from A.
Do you agree? Why?
5.13. You arrive at a bus stop at 10 a.m., knowing that the
bus will arrive at some time uniformly distributed between
10 and 10:30.
(a)What is the probability that you will have to wait
longer than 10 minutes?
(b)If, at 10:15, the bus has not yet arrived, what is the
probability that you will have to wait at least an additional
10 minutes?
5.14. LetXbe a uniform (0, 1) random variable. Compute
E[Xn] by using Proposition 2.1, and then check the result
by using the deﬁnition of expectation.5.15. IfXis a normal random variable with parameters
μ=10 and σ
2=36, compute
(a)P{X>5};
(b)P{4<X<16};
(c)P{X<8};
(d)P{X<20};
(e)P{X>16}.
5.16. The annual rainfall (in inches) in a certain region is
normally distributed with μ=40 and σ=4. What is the
probability that starting with this year, it will take more
than 10 years before a year occurs having a rainfall of more
than 50 inches? What assumptions are you making?
5.17. The salaries of physicians in a certain speciality are
approximately normally distributed. If 25 percent of these
physicians earn less than $180,000 and 25 percent earn
more than $320,000, approximately what fraction earn
(a)less than $200,000?
(b)between $280,000 and $320,000?
5.18. Suppose that Xis a normal random variable with
mean 5. If P{X>9}=.2, approximately what is Var (X)?
5.19. LetXbe a normal random variable with mean 12 and
variance 4. Find the value of csuch that P{X>c}=.10.
5.20. If 65 percent of the population of a large community
is in favor of a proposed rise in school taxes, approximate
the probability that a random sample of 100 people will
contain
(a)at least 50 who are in favor of the proposition;
(b)between 60 and 70 inclusive who are in favor;
(c)f e w e rt h a n7 5i nf a v o r .
5.21. Suppose that the height, in inches, of a 25-year-old
man is a normal random variable with parameters μ=71
andσ2=6.25. What percentage of 25-year-old men aretaller than 6 feet, 2 inches? What percentage of men in the
6-footer club are taller than 6 feet, 5 inches?
5.22. Every day Jo practices her tennis serve by continu-
ally serving until she has had a total of 50 successful serves.
If each of her serves is, independently of previous ones,
successful with probability .4, approximately what is the
probability that she will need more than 100 serves to
accomplish her goal?
Hint: Imagine even if Jo is successful that she continues to
serve until she has served exactly 100 times. What must be
true about her ﬁrst 100 serves if she is to reach her goal?
5.23. One thousand independent rolls of a fair die will be
made. Compute an approximation to the probability that
the number 6 will appear between 150 and 200 times inclu-sively. If the number 6 appears exactly 200 times, ﬁnd the
probability that the number 5 will appear less than 150
times.
5.24. The lifetimes of interactive computer chips produced
by a certain semiconductor manufacturer are normally dis-
tributed with parameters μ=1.4*10
6hours and σ=
3*105hours. What is the approximate probability that a
batch of 100 chips will contain at least 20 whose lifetimesare less than 1.8 *10
6?
5.25. Each item produced by a certain manufacturer is,
independently, of acceptable quality with probability .95.
Approximate the probability that at most 10 of the next
150 items produced are unacceptable.
5.26. Two types of coins are produced at a factory: a fair
coin and a biased one that comes up heads 55 percent of
the time. We have one of these coins but do not know
whether it is a fair coin or a biased one. In order to ascer-tain which type of coin we have, we shall perform the fol-
lowing statistical test: We shall toss the coin 1000 times. If
the coin lands on heads 525 or more times, then we shallconclude that it is a biased coin, whereas if it lands on
heads fewer than 525 times, then we shall conclude that
it is a fair coin. If the coin is actually fair, what is the prob-ability that we shall reach a false conclusion? What would
it be if the coin were biased?
5.27. In 10,000 independent tosses of a coin, the coin
landed on heads 5800 times. Is it reasonable to assume that
the coin is not fair? Explain.
5.28. Twelve percent of the population is left handed.
Approximate the probability that there are at least 20 left-
handers in a school of 200 students. State your assump-
tions.
5.29. A model for the movement of a stock supposes that
if the present price of the stock is s, then after one period,
it will be either uswith probability pordswith probability
1−p. Assuming that successive movements are indepen-
dent, approximate the probability that the stock’s price

<<<PAGE 227>>>

214 Chapter 5 Continuous Random Variables
will be up at least 30 percent after the next 1000 periods
ifu=1.012, d=0.990, and p=.52.
5.30. An image is partitioned into two regions, one white
and the other black. A reading taken from a randomly cho-sen point in the white section will be normally distributed
withμ=4a n d σ
2=4, whereas one taken from a ran-
domly chosen point in the black region will have a nor-
mally distributed reading with parameters (6, 9). A point
is randomly chosen on the image and has a reading of 5. If
the fraction of the image that is black is α, for what value
ofαwould the probability of making an error be the same,
regardless of whether one concluded that the point was inthe black region or in the white region?
5.31. (a)A ﬁre station is to be located along a road of
length A,A<q. If ﬁres occur at points uniformly cho-
s e no n( 0 , A), where should the station be located so as
to minimize the expected distance from the ﬁre? That is,
choose aso as to
minimize E[|X−a|]
when Xis uniformly distributed over (0, A).
(b)Now suppose that the road is of inﬁnite length—
stretching from point 0 outward to q. If the distance of
a ﬁre from point 0 is exponentially distributed with rate λ,
where should the ﬁre station now be located? That is, we
want to minimize E[|X−a|], where Xis now exponential
with rate λ.
5.32. The time (in hours) required to repair a machine is
an exponentially distributed random variable with param-
eterλ=
1
2.W h a ti s
(a)the probability that a repair time exceeds 2 hours?
(b)the conditional probability that a repair takes at least
10 hours, given that its duration exceeds 9 hours?
5.33. The number of years a radio functions is exponen-
tially distributed with parameter λ=1
8. If Jones buys a
used radio, what is the probability that it will be working
after an additional 8 years?
5.34. Jones ﬁgures that the total number of thousands of
miles that an auto can be driven before it would need to
be junked is an exponential random variable with parame-
ter1
20. Smith has a used car that he claims has been driven
only 10,000 miles. If Jones purchases the car, what is the
probability that she would get at least 20,000 additionalmiles out of it? Repeat under the assumption that the life-
time mileage of the car is not exponentially distributed,but rather is (in thousands of miles) uniformly distributedover (0, 40).
5.35. The lung cancer hazard rate λ(t)of at-year-old male
smoker is such that
λ(t)=.027+.00025(t −40)
2tÚ40
Assuming that a 40-year-old male smoker survives all
other hazards, what is the probability that he survives to
(a) age 50 and (b) age 60 without contracting lung cancer?
5.36. Suppose that the life distribution of an item has the
hazard rate function λ(t)=t3,t>0. What is the probabil-
ity that
(a)the item survives to age 2?
(b)the item’s lifetime is between .4 and 1.4?
(c)a 1-year-old item will survive to age 2?
5.37. IfXis uniformly distributed over (−1, 1), ﬁnd
(a)P{|X|>1
2};
(b)the density function of the random variable |X|.
5.38. IfYis uniformly distributed over (0, 5), what is the
probability that the roots of the equation 4 x2+4xY+
Y+2=0 are both real?
5.39. IfXis an exponential random variable with parame-
terλ=1, compute the probability density function of the
random variable Ydeﬁned by Y=logX.
5.40. IfXis uniformly distributed over (0, 1), ﬁnd the den-
sity function of Y=eX.
5.41. Find the distribution of R=Asinθ,w h e r e A
is a ﬁxed constant and θis uniformly distributed on
(−π/ 2,π/2). Such a random variable Rarises in the the-
ory of ballistics. If a projectile is ﬁred from the origin at
an angle αfrom the earth with a speed ν, then the point
Rat which it returns to the earth can be expressed as
R=(v2/g)sin 2α ,w h e r e gis the gravitational constant,
equal to 980 centimeters per second squared.
5.42. LetYbe a lognormal random variable (see Example
7e for its deﬁnition) and let c>0 be a constant. Answer
true or false to the following, and then give an explanation
for your answer.
(a)cYis lognormal;
(b)c+Yis lognormal.
Theoretical Exercises
5.1.The speed of a molecule in a uniform gas at equilib-
rium is a random variable whose probability density func-
tion is given byf(x)=/braceleftBigg
ax2e−bx2xÚ0
0 x<0

<<<PAGE 228>>>

A First Course in Probability 215
where b=m/2kT andk,T,a n d mdenote, respectively,
Boltzmann’s constant, the absolute temperature of the gas,
and the mass of the molecule. Evaluate ain terms of b.
5.2.Show that
E[Y]=/integraldisplayq
0P{Y>y}dy−/integraldisplayq
0P{Y<−y}dy
Hint: Show that
/integraldisplayq
0P{Y<−y}dy=−/integraldisplay0
−qxfY(x)dx
/integraldisplayq
0P{Y>y}dy=/integraldisplayq
0xfY(x)dx
5.3.Show that if Xhas density function f,t h e n
E[g(X)]=/integraldisplayq
−qg(x)f(x)dx
Hint: Using Theoretical Exercise 5.2, start with
E[g(X)]=/integraldisplayq
0P{g(X)>y}dy−/integraldisplayq
0P{g(X)<−y} dy
and then proceed as in the proof given in the text wheng(X)Ú0.
5.4.Prove Corollary 2.1.
5.5.Use the result that for a nonnegative random vari-
ableY,
E[Y]=/integraldisplay
q
0P{Y>t}dt
to show that for a nonnegative random variable X,
E[Xn]=/integraldisplayq
0nxn−1P{X>x}dx
Hint: Start with
E[Xn]=/integraldisplayq
0P{Xn>t}dt
and make the change of variables t=xn.
5.6.Deﬁne a collection of events Ea,0<a<1, having
the property that P(Ea)=1f o ra l l abutP/parenleftBigg
/intersectiontext
aEa/parenrightBigg
=0.
Hint:L e tX be uniform over (0, 1) and deﬁne each Eain
terms of X.
5.7.The standard deviation of X, denoted SD(X ),i s
given by
SD(X )=/radicalbig
Var(X)
Find SD(aX +b)ifXhas variance σ2.5.8.LetXbe a random variable that takes on values
between 0 and c.T h a ti s ,P {0…X…c}=1. Show that
Var(X)…c2
4
Hint: One approach is to ﬁrst argue that
E[X2]…cE[X]
and then use this inequality to show that
Var(X)…c2[α(1−α)]w h e r e α=E[X]
c
5.9.Show that Zis a standard normal random variable;
then, for x>0,
(a)P{Z>x}= P{Z<−x};
(b)P{|Z|>x}=2P {Z>x};
(c)P{|Z|<x}=2P {Z<x}− 1.
5.10. Letf(x)denote the probability density function of
a normal random variable with mean μand variance σ2.
Show that μ−σandμ+σare points of inﬂection of this
function. That is, show that f/prime/prime(x)=0w h e n x=μ−σor
x=μ+σ.
5.11. LetZbe a standard normal random variable Z,a n d
letgbe a differentiable function with derivative g/prime.
(a)Show that E[g/prime(Z)]=E[Zg(Z)];
(b)Show that E[Zn+1]=nE[Zn−1].
(c)Find E[Z4].
5.12. Use the identity of Theoretical Exercise 5.5 to derive
E[X2]w h e n Xis an exponential random variable with
parameter λ.
5.13. The median of a continuous random variable having
distribution function Fis that value msuch that F(m)=1
2.
That is, a random variable is just as likely to be larger than
its median as it is to be smaller. Find the median of Xif
Xis
(a)uniformly distributed over (a, b);
(b)normal with parameters μ,σ2;
(c)exponential with rate λ.
5.14. The mode of a continuous random variable having
density fis the value of xfor which f(x)attains its maxi-
mum. Compute the mode of Xin cases (a), (b), and (c) of
Theoretical Exercise 5.13.
5.15. IfXis an exponential random variable with parame-
terλ,a n d c>0, show that cXis exponential with param-
eterλ/c.
5.16. Compute the hazard rate function of Xwhen Xis
uniformly distributed over (0, a).

<<<PAGE 229>>>

216 Chapter 5 Continuous Random Variables
5.17. IfXhas hazard rate function λX(t), compute the haz-
ard rate function of aXwhere ais a positive constant.
5.18. Verify that the gamma density function integrates
to 1.
5.19. IfXis an exponential random variable with mean
1/λ, show that
E[Xk]=k!
λkk=1, 2,...
Hint: Make use of the gamma density function to evaluate
the preceding.
5.20. Verify that
Var(X)=α
λ2
when Xis a gamma random variable with parameters α
andλ.
5.21. Show that /Gamma1/parenleftBig
1
2/parenrightBig
=√π.
Hint:/Gamma1/parenleftBig
1
2/parenrightBig
=/integraltextq
0e−xx−1/2dx. Make the change of vari-
ables y=√
2xand then relate the resulting expression to
the normal distribution.
5.22. Compute the hazard rate function of a gamma ran-
dom variable with parameters (α,λ)and show it is increas-
ing when αÚ1 and decreasing when α…1.
5.23. Compute the hazard rate function of a Weibull ran-
dom variable and show it is increasing when βÚ1a n d
decreasing when β…1.
5.24. Show that a plot of log (log(1 −F(x))−1)against log
xwill be a straight line with slope βwhen F(·)is a Weibull
distribution function. Show also that approximately 63.2
percent of all observations from such a distribution will beless than α. Assume that v=0.
5.25. Let
Y=/parenleftbiggX−ν
α/parenrightbiggβ
Show that if Xis a Weibull random variable with parame-
tersν,α,a n dβ,t h e nY is an exponential random variable
with parameter λ=1 and vice versa.
5.26. IfXis a beta random variable with parameters aand
b, show that
E[X]=a
a+b
Var(X)=ab
(a+b)2(a+b+1)
5.27. IfXis uniformly distributed over (a, b), what ran-
dom variable, having a linear relation with X, is uniformly
distributed over (0, 1)?5.28. Consider the beta distribution with parameters
(a,b). Show that
(a)when a>1a n d b>1, the density is unimodal (that
is, it has a unique mode) with mode equal to (a−1)/(a +
b−2);
(b)when a…1,b…1, and a+b<2, the density is either
unimodal with mode at 0 or 1 or U-shaped with modes atb o t h0a n d1 ;
(c)when a=1=b, all points in [0, 1] are modes.
5.29. LetXbe a continuous random variable having
cumulative distribution function F. Deﬁne the random
variable YbyY=F(X). Show that Yis uniformly dis-
tributed over (0, 1).
5.30. LetXhave probability density f
X. Find the proba-
bility density function of the random variable Ydeﬁned
byY=aX+b.
5.31. Find the probability density function of Y=eXwhen
Xis normally distributed with parameters μandσ2.T h e
random variable Yis said to have a lognormal distribution
(since log Yhas a normal distribution) with parameters μ
andσ2.
5.32. LetXandYbe independent random variables that
are both equally likely to be either 1, 2, ...,(10)N,w h e r e
Nis very large. Let Ddenote the greatest common divisor
ofXandY,a n dl e tQ k=P{D=k}.
(a)Give a heuristic argument that Qk=1
k2Q1.
Hint: Note that in order for Dto equal k,kmust divide
both XandYand also X/k,a n d Y/kmust be relatively
prime. (That is, X/k,a n d Y/kmust have a greatest com-
mon divisor equal to 1.)
(b)Use part (a) to show that
Q1=P{XandYare relatively prime }
=1
q/summationdisplay
k=11/k2
It is a well-known identity thatq/summationtext
11/k2=π2/6, so Q1=
6/π2. (In number theory, this is known as the Legendre
theorem.)(c)Now argue that
Q
1=q/productdisplay
i=1/parenleftBigg
P2
i−1
P2
i/parenrightBigg
where Piis the ith-smallest prime greater than 1.
Hint:XandYwill be relatively prime if they have no com-
mon prime factors. Hence, from part (b), we see that

<<<PAGE 230>>>

A First Course in Probability 217
q/productdisplay
i=1/parenleftBigg
P2
i−1
P2
i/parenrightBigg
=6
π2
which was noted without explanation in Problem 11 of
Chapter 4. (The relationship between this problem andProblem 11 of Chapter 4 is that XandYare relatively
prime if XYhas no multiple prime factors.)
5.33. Prove Theorem 7.1 when g(x)is a decreasing
function.
Self-Test Problems and Exercises
5.1.The number of minutes of playing time of a certain
high school basketball player in a randomly chosen gameis a random variable whose probability density function is
given in the following ﬁgure:
.025.050
10 20 30 40
Find the probability that the player plays
(a)more than 15 minutes;
(b)between 20 and 35 minutes;
(c)less than 30 minutes;
(d)more than 36 minutes.
5.2.For some constant c, the random variable Xhas the
probability density function
f(x)=/braceleftbigg
cxn0<x<1
0 otherwise
Find (a) cand (b) P{X>x},0<x<1.
5.3.For some constant c, the random variable Xhas the
probability density function
f(x)=/braceleftBigg
cx40<x<2
0 otherwise
Find (a) E[X]a n d( b )V a r (X).
5.4.The random variable Xhas the probability density
function
f(x)=/braceleftBigg
ax+bx20<x<1
0 otherwise
IfE[X]=.6, ﬁnd (a) P{X<1
2}and (b) Var (X).
5.5.The random variable Xis said to be a discrete uniform
random variable on the integers 1, 2, ...,nif
P{X=i}=1
ni=1, 2,...,n
For any nonnegative real number x,l e tI n t (x)(sometimes
written as [x]) be the largest integer that is less than orequal to x. Show that if Uis a uniform random variable on
(0, 1), then X=Int(nU )+1 is a discrete uniform random
variable on 1, ...,n.
5.6.Your company must make a sealed bid for a construc-
tion project. If you succeed in winning the contract (by
having the lowest bid), then you plan to pay another ﬁrm
$100,000 to do the work. If you believe that the minimumbid (in thousands of dollars) of the other participating
companies can be modeled as the value of a random vari-
able that is uniformly distributed on (70, 140), how much
should you bid to maximize your expected proﬁt?
5.7.To be a winner in a certain game, you must be success-
ful in three successive rounds. The game depends on the
value of U, a uniform random variable on (0, 1). If U>.1,
then you are successful in round 1; if U>.2, then you are
successful in round 2; and if U>.3, then you are successful
in round 3.
(a)Find the probability that you are successful in round 1.
(b)Find the conditional probability that you are successful
in round 2 given that you were successful in round 1.
(c)Find the conditional probability that you are success-
ful in round 3 given that you were successful in rounds 1
and 2.
(d)Find the probability that you are a winner.
5.8.A randomly chosen IQ test taker obtains a score that
is approximately a normal random variable with mean 100
and standard deviation 15. What is the probability that thescore of such a person is (a) more than 125; (b) between
90 and 110?
5.9.Suppose that the travel time from your home to your
ofﬁce is normally distributed with mean 40 minutes and
standard deviation 7 minutes. If you want to be 95 percentcertain that you will not be late for an ofﬁce appointment
at 1 p.m., what is the latest time that you should leave
home?
5.10. The life of a certain type of automobile tire is nor-
mally distributed with mean 34,000 miles and standard
deviation 4000 miles.
(a)What is the probability that such a tire lasts more than
40,000 miles?
(b)What is the probability that it lasts between 30,000 and
35,000 miles?

<<<PAGE 231>>>

218 Chapter 5 Continuous Random Variables
(c)Given that it has survived 30,000 miles, what is the con-
ditional probability that the tire survives another 10,000
miles?
5.11. The annual rainfall in Cleveland, Ohio, is approxi-
mately a normal random variable with mean 40.2 inches
and standard deviation 8.4 inches. What is the probabil-
ity that
(a)next year’s rainfall will exceed 44 inches?
(b)the yearly rainfalls in exactly 3 of the next 7 years will
exceed 44 inches?Assume that if A
iis the event that the rainfall exceeds 44
inches in year i(from now), then the events Ai,iÚ1, are
independent.
5.12. The following table uses 1992 data concerning the
percentages of male and female full-time workers whose
annual salaries fall into different ranges:
Percentage Percentage
Earnings range of females of males
…9999 8.6 4.4
10,000–19,999 38.0 21.1
20,000–24,999 19.4 15.8
25,000–49,999 29.2 41.5
Ú50,000 4.8 17.2
Suppose that random samples of 200 male and 200 female
full-time workers are chosen. Approximate the probabil-
ity that
(a)at least 70 of the women earn $25,000 or more;
(b)at most 60 percent of the men earn $25,000 or more;
(c)at least three-fourths of the men and at least half the
women earn $20,000 or more.
5.13. At a certain bank, the amount of time that a cus-
tomer spends being served by a teller is an exponential
random variable with mean 5 minutes. If there is a cus-tomer in service when you enter the bank, what is the
probability that he or she will still be with the teller after
an additional 4 minutes?
5.14. Suppose that the cumulative distribution function of
the random variable Xis given by
F(x)=1−e
−x2x>0
Evaluate (a) P{X>2};( b ) P{1<X<3}; (c) the hazard
rate function of F;( d )E[X]; (e) Var(X ).
Hint: For parts (d) and (e), you might want to make use of
the results of Theoretical Exercise 5.5.5.15. The number of years that a washing machine func-
tions is a random variable whose hazard rate function is
given byλ(t)=⎧
⎨
⎩.20 <t<2
.2+.3(t−2)2…t<5
1.1 t>5
(a)What is the probability that the machine will still be
working 6 years after being purchased?
(b)If it is still working 6 years after being purchased, what
is the conditional probability that it will fail within the next
2 years?
5.16. A standard Cauchy random variable has density
function
f(x)=1
π(1+x2)−q<x<q
Show that if Xis a standard Cauchy random
variable, then 1/ Xis also a standard Cauchy random vari-
able.5.17. A roulette wheel has 38 slots, numbered 0, 00, and
1 through 36. If you bet 1 on a speciﬁed number, then
you either win 35 if the roulette ball lands on that num-
ber or lose 1 if it does not. If you continually make suchbets, approximate the probability that
(a)you are winning after 34 bets;
(b)you are winning after 1000 bets;
(c)you are winning after 100,000 bets.
Assume that each roll of the roulette ball is equally likely
to land on any of the 38 numbers.
5.18. There are two types of batteries in a bin. When in use,
typeibatteries last (in hours) an exponentially distributed
time with rate λ
i,i=1, 2. A battery that is randomly cho-
sen from the bin will be a type ibattery with probability
pi,2/summationtext
i=1pi=1. If a randomly chosen battery is still operat-
ing after thours of use, what is the probability that it will
still be operating after an additional shours?
5.19. Evidence concerning the guilt or innocence of a
defendant in a criminal investigation can be summarized
by the value of an exponential random variable Xwhose
mean μdepends on whether the defendant is guilty. If
innocent, μ=1; if guilty, μ=2. The deciding judge will
rule the defendant guilty if X>cfor some suitably chosen
value of c.
(a)If the judge wants to be 95 percent certain that an inno-
cent man will not be convicted, what should be the value
ofc?
(b)Using the value of c found in part (a), what is the
probability that a guilty defendant will be convicted?
5.20. For any real number y, deﬁne y+by
y+=y,i f yÚ0
0, if y<0

<<<PAGE 232>>>

A First Course in Probability 219
Letcbe a constant.
(a)Show that
E[(Z−c)+]=1√
2πe−c2/2−c(1−/Phi1(c))
when Zis a standard normal random variable.
(b)Find E[(X−c)+]w h e n Xis normal with mean μand
variance σ2.
5.21. With/Phi1(x)being the probability that a normal ran-
dom variable with mean 0 and variance 1 is less than x,
which of the following are true:
(a)/Phi1(−x)=/Phi1(x)(b)/Phi1(x)+/Phi1(−x)=1
(c)/Phi1(−x)=1//Phi1( x)
5.22. LetUbe a uniform (0, 1) random variable, and let
a<bbe constants.
(a)Show that if b>0, then bUis uniformly distributed
on(0,b),a n di f b<0, then bUis uniformly distributed on
(b,0).
(b)Show that a+Uis uniformly distributed on (a,1+a).
(c)What function of Uis uniformly distributed on (a,b)?
(d)Show that min(U ,1−U)is a uniform (0, 1/2) random
variable.
(e)Show that max (U,1−U)is a uniform (1/2, 1) random
variable.

<<<PAGE 233>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 220
Chapter
Jointly Distributed Random
Variables 6
Contents
6.1 Joint Distribution Functions
6.2 Independent Random Variables
6.3 Sums of Independent Random
Variables
6.4 Conditional Distributions: Discrete Case6.5 Conditional Distributions: Continuous Case
6.6 Order Statistics
6.7 Joint Probability Distribution of Functionsof Random Variables
6.8 Exchangeable Random Variables
6.1 Joint Distribution Functions
Thus far, we have concerned ourselves only with probability distributions for single
random variables. However, we are often interested in probability statements con-
cerning two or more random variables. In order to deal with such probabilities, we
deﬁne, for any two random variables XandY,t h e joint cumulative probability dis-
tribution function ofXandYby
F(a,b)=P{X…a,Y…b}− q<a,b<q
The distribution of Xcan be obtained from the joint distribution of XandYas
follows:
FX(a)=P{X…a}
=P{X…a,Y<q}
=P/parenleftbigg
lim
b→q{X…a,Y…b}/parenrightbigg
=lim
b→qP{X…a,Y…b}
=lim
b→qF(a,b)
KF(a,q)
Note that in the preceding set of equalities, we have once again made use of the fact
that probability is a continuous set (that is, event) function. Similarly, the cumulative
distribution function of Yis given by
FY(b)=P{Y…b}
=lima→qF(a,b)
KF(q,b)
220

<<<PAGE 234>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 221
A First Course in Probability 221
The distribution functions FXandFYare sometimes referred to as the marginal
distributions of XandY.
All joint probability statements about XandYcan, in theory, be answered in
terms of their joint distribution function. For instance, suppose we wanted to com-
pute the joint probability that Xis greater than aandYis greater than b. This could
be done as follows:
P{X>a,Y>b}= 1−P({X>a,Y>b}c)
=1−P({X>a}c∪{Y>b}c)
=1−P({X…a}∪{ Y…b}) (1.1)
=1−[P{X…a}+ P{Y…b}− P{X…a,Y…b}]
=1−FX(a)−FY(b)+F(a,b)
Equation (1.1) is a special case of the following equation, whose veriﬁcation is leftas an exercise:
P{a
1<X…a2,b1<Y…b2}
=F(a2,b2)+F(a1,b1)−F(a1,b2)−F(a2,b1) (1.2)
whenever a1<a2,b1<b2.
In the case when XandYare both discrete random variables, it is convenient
to deﬁne the joint probability mass function ofXandYby
p(x,y)=P{X=x,Y=y}
The probability mass function of Xcan be obtained from p(x,y)by
pX(x)=P{X=x}
=/summationdisplay
y:p(x,y)>0p(x,y)
Similarly,
pY(y)=/summationdisplay
x:p(x,y)>0p(x,y)
Example
1aSuppose that 3 balls are randomly selected from an urn containing 3 red, 4 white, and5 blue balls. If we let XandYdenote, respectively, the number of red and white balls
chosen, then the joint probability mass function of XandY,p(i,j)=P{X=i,Y=j},
is given by
p(0, 0) =/parenleftBigg
53/parenrightBigg/slashBig/parenleftBigg
12
3/parenrightBigg
=10
220
p(0, 1) =/parenleftBigg
4
1/parenrightBigg/parenleftBigg
52/parenrightBigg/slashBig/parenleftBigg
12
3/parenrightBigg
=40
220
p(0, 2) =/parenleftBigg
4
2/parenrightBigg/parenleftBigg
51/parenrightBigg/slashBig/parenleftBigg
12
3/parenrightBigg
=30
220

<<<PAGE 235>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 222
222 Chapter 6 Jointly Distributed Random Variables
p(0, 3) =/parenleftBigg
4
3/parenrightBigg/slashBig/parenleftBigg
12
3/parenrightBigg
=4
220
p(1, 0) =/parenleftBigg
3
1/parenrightBigg/parenleftBigg
52/parenrightBigg/slashBig/parenleftBigg
12
3/parenrightBigg
=30
220
p(1, 1) =/parenleftBigg
3
1/parenrightBigg/parenleftBigg
41/parenrightBigg/parenleftBigg
51/parenrightBigg/slashBig/parenleftBigg
12
3/parenrightBigg
=60
220
p(1, 2) =/parenleftBigg
31/parenrightBigg/parenleftBigg
42/parenrightBigg/slashBig/parenleftBigg
12
3/parenrightBigg
=18
220
p(2, 0) =/parenleftBigg
3
2/parenrightBigg/parenleftBigg
51/parenrightBigg/slashBig/parenleftBigg
12
3/parenrightBigg
=15
220
p(2, 1) =/parenleftBigg
3
2/parenrightBigg/parenleftBigg
41/parenrightBigg/slashBig/parenleftBigg
12
3/parenrightBigg
=12
220
p(3, 0) =/parenleftBigg
33/parenrightBigg/slashBig/parenleftBigg
12
3/parenrightBigg
=1
220
These probabilities can most easily be expressed in tabular form, as in Table 6.1. Thereader should note that the probability mass function of Xis obtained by computing
the row sums, whereas the probability mass function of Yis obtained by computing
the column sums. Because the individual probability mass functions of XandY
thus appear in the margin of such a table, they are often referred to as the marginal
probability mass functions ofXandY, respectively. .
Table 6.1 P{X=i,Y=j}.
ij
0123 R o w s u m =P{X=i}
010
22040
22030
2204
22084
220
130
22060
22018
2200108
220
215
22012
2200027
220
31
2200001
220
Column sum =P{Y=j }56
220112
22048
2204
220
Example
1bSuppose that 15 percent of the families in a certain community have no children, 20
percent have 1 child, 35 percent have 2 children, and 30 percent have 3. Suppose
further that in each family each child is equally likely (independently) to be a boy ora girl. If a family is chosen at random from this community, then B, the number of
boys, and G, the number of girls, in this family will have the joint probability mass
function shown in Table 6.2.

<<<PAGE 236>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 223
A First Course in Probability 223
Table 6.2 P{B=i,G=j}.
ij
01 2 3 R o w s u m =P{B=i}
0 .15 .10 .0875 .0375 .3750
1 .10 .175 .1125 0 .3875
2 .0875 .1125 0 0 .2000
3 .0375 0 0 0 .0375
Column sum =P{G=j } .3750 .3875 .2000 .0375
The probabilities shown in Table 6.2 are obtained as follows:
P{B=0,G=0}=P {no children }=.15
P{B=0,G=1}=P {1 girl and total of 1 child }
=P{1 child}P {1g i r l |1 child}=(. 20)/parenleftbigg1
2/parenrightbigg
P{B=0,G=2}=P {2 girls and total of 2 children }
=P{2 children }P{2 girls |2 children}=(. 35)/parenleftbigg1
2/parenrightbigg2
We leave the veriﬁcation of the remaining probabilities in the table to the reader. .
We say that XandYarejointly continuous if there exists a function f(x,y),
deﬁned for all real xandy, having the property that for every set Cof pairs of real
numbers (that is, Cis a set in the two-dimensional plane),
P{(X,Y)∈C}=/integraldisplay/integraldisplay
(x,y)∈Cf(x,y)dx dy (1.3)
The function f(x,y)is called the joint probability density function ofXandY.I fA
andBare any sets of real numbers, then by deﬁning C={(x,y):x∈A,y∈B},w e
see from Equation (1.3) that
P{X∈A,Y∈B}=/integraldisplay
B/integraldisplay
Af(x,y)dx dy (1.4)
Because
F(a,b)=P{X∈(−q, a],Y∈(−q, b]}
=/integraldisplayb
−q/integraldisplaya
−qf(x,y)dx dy
it follows, upon differentiation, that
f(a,b)=∂2
∂a∂bF(a,b)

<<<PAGE 237>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 224
224 Chapter 6 Jointly Distributed Random Variables
wherever the partial derivatives are deﬁned. Another interpretation of the joint den-
sity function, obtained from Equation (1.4), is
P{a<X<a+da,b<Y<b+db}=/integraldisplayb+db
b/integraldisplaya+da
af(x,y)dx dy
Lf(a,b)da db
when daanddbare small and f(x,y)is continuous at a,b. Hence, f(a,b)is a measure
of how likely it is that the random vector ( X,Y) will be near (a, b).
IfXandYare jointly continuous, they are individually continuous, and their
probability density functions can be obtained as follows:
P{X∈A}=P {X∈A,Y∈(−q,q)}
=/integraldisplay
A/integraldisplayq
−qf(x,y)dy dx
=/integraldisplay
AfX(x)dx
where
fX(x)=/integraldisplayq
−qf(x,y)dy
is thus the probability density function of X. Similarly, the probability density func-
tion of Yis given by
fY(y)=/integraldisplayq
−qf(x,y)dx
Example
1cThe joint density function of XandYis given by
f(x,y)=/braceleftBigg
2e−xe−2y0<x<q,0 <y<q
0 otherwise
Compute (a) P{X>1,Y<1},( b ) P{X<Y}, and (c) P{X<a}.
Solution
(a) P{X>1,Y<1}=/integraldisplay1
0/integraldisplayq
12e−xe−2ydx dy
=/integraldisplay1
02e−2y/parenleftBig
−e−x/vextendsingle/vextendsingleq
1/parenrightBig
dy
=e−1/integraldisplay1
02e−2ydy
=e−1(1−e−2)
(b) P{X<Y}=/integraldisplay/integraldisplay
(x,y):x<y2e−xe−2ydx dy
=/integraldisplayq
0/integraldisplayy
02e−xe−2ydx dy
=/integraldisplayq
02e−2y(1−e−y)dy

<<<PAGE 238>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 225
A First Course in Probability 225
=/integraldisplayq
02e−2ydy−/integraldisplayq
02e−3ydy
=1−2
3
=1
3
(c)P{X<a}=/integraldisplaya
0/integraldisplayq
02e−2ye−xdy dx
=/integraldisplaya
0e−xdx
=1−e−a.
Example
1dConsider a circle of radius R, and suppose that a point within the circle is randomly
chosen in such a manner that all regions within the circle of equal area are equally
likely to contain the point. (In other words, the point is uniformly distributed within
the circle.) If we let the center of the circle denote the origin and deﬁne XandY
to be the coordinates of the point chosen (Figure 6.1), then, since ( X,Y) is equally
likely to be near each point in the circle, it follows that the joint density function ofXandYis given by
f(x,y)=/braceleftBigg
cifx
2+y2…R2
0i fx2+y2>R2
for some value of c.
(a) Determine c.
(b) Find the marginal density functions of XandY.
(c)Compute the probability that D, the distance from the origin of the point
selected, is less than or equal to a.
(d) Find E[D].
(0, 0)(X, Y )R
xy
Figure 6.1 Joint probability distribution.
Solution
(a) Because
/integraldisplayq
−q/integraldisplayq
−qf(x,y)dy dx =1

<<<PAGE 239>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 226
226 Chapter 6 Jointly Distributed Random Variables
it follows that
c/integraldisplay/integraldisplay
x2+y2…R2dy dx =1
We can evaluate/integraltext/integraltext
x2+y2…R2dy dx either by using polar coordinates or,
more simply, by noting that it represents the area of the circle and is thus equal
toπR2. Hence,
c=1
πR2
(b) fX(x)=/integraldisplayq
−qf(x,y)dy
=1
πR2/integraldisplay
x2+y2…R2dy
=1
πR2/integraldisplaya
−ady, where a=/radicalbig
R2−x2
=2
πR2/radicalbig
R2−x2x2…R2
and it equals 0 when x2>R2. By symmetry, the marginal density of Yis
given by
fY(y)=2
πR2/radicalBig
R2−y2y2…R2
=0 y2>R2
(c) The distribution function of D=/radicalbig
X2+Y2, the distance from the origin, is
obtained as follows: For 0 …a…R,
FD(a)=P{/radicalbig
X2+Y2…a}
=P{X2+Y2…a2}
=/integraldisplay/integraldisplay
x2+y2…a2f(x,y)dy dx
=1
πR2/integraldisplay/integraldisplay
x2+y2…a2dy dx
=πa2
πR2
=a2
R2
where we have used the fact that/integraltext/integraltext
x2+y2…a2dy dx is the area of a circle of
radius aand thus is equal to πa2.
(d) From part (c), the density function of Dis
fD(a)=2a
R20…a…R

<<<PAGE 240>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 227
A First Course in Probability 227
Hence,
E[D]=2
R2/integraldisplayR
0a2da=2R
3.
Example
1eThe joint density of XandYis given by
f(x,y)=/braceleftBigg
e−(x+y)0<x<q,0 <y<q
0 otherwise
Find the density function of the random variable X/Y.
Solution We start by computing the distribution function of X/Y.F o r a>0,
FX/Y(a)=P/braceleftbiggX
Y…a/bracerightbigg
=/integraldisplay/integraldisplay
x/y…ae−(x+y)dx dy
=/integraldisplayq
0/integraldisplayay
0e−(x+y)dx dy
=/integraldisplayq
0(1−e−ay)e−ydy
=/braceleftBigg
−e−y+e−(a+1)y
a+1/bracerightBigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleq
0
=1−1
a+1
Differentiation shows that the density function of X/Yis given by fX/Y(a)=1/
(a+1)2,0<a<q. .
We can also deﬁne joint probability distributions for nrandom variables in
exactly the same manner as we did for n=2. For instance, the joint cumulative prob-
ability distribution function F(a1,a2,...,an)of the nrandom variables X1,X2,...,Xn
is deﬁned by
F(a1,a2,...,an)=P{X1…a1,X2…a2,...,Xn…an}
Further, the nrandom variables are said to be jointly continuous if there exists a
function f(x1,x2,...,xn), called the joint probability density function, such that, for
any set Cinn-space,
P{(X 1,X2,...,Xn)∈C}=/integraldisplay/integraldisplay
···/integraldisplay
(x1,...,xn)∈Cf(x1,...,xn)dx 1dx2···dx n
In particular, for any nsets of real numbers A1,A2,...,An,
P{X1∈A1,X2,∈A2,...,Xn∈An}
=/integraldisplay
An/integraldisplay
An−1···/integraldisplay
A1f(x1,...,xn)dx1dx2···dxn

<<<PAGE 241>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 228
228 Chapter 6 Jointly Distributed Random Variables
Example
1fThe multinomial distribution
One of the most important joint distributions is the multinomial distribution, which
arises when a sequence of nindependent and identical experiments is performed.
Suppose that each experiment can result in any one of rpossible outcomes, with
respective probabilities p1,p2,...,pr,r/summationtext
i=1pi=1. If we let Xidenote the number of
thenexperiments that result in outcome number i, then
P{X1=n1,X2=n2,...,Xr=nr}=n!
n1!n2!···nr!pn1
1pn2
2···pnrr (1.5)
wheneverr/summationtext
i=1ni=n.
Equation (1.5) is veriﬁed by noting that any sequence of outcomes for the n
experiments that leads to outcome ioccurring nitimes for i=1, 2,...,rwill, by
the assumed independence of experiments, have probability pn1
1pn2
2...pnrrof occur-
ring. Because there are n!/(n 1!n2!...nr!)such sequences of outcomes (there are
n!/n 1!...nr! different permutations of nthings of which n1are alike, n2are alike,
...,nrare alike), Equation (1.5) is established. The joint distribution whose joint
probability mass function is speciﬁed by Equation (1.5) is called the multinomial
distribution. Note that when r=2, the multinomial reduces to the binomial distri-
bution.
Note also that any sum of a ﬁxed set of the X/prime
iswill have a binomial distribu-
tion. That is, if N({1, 2,...,r}, then/summationtext
i∈NXiwill be a binomial random variable
with parameters nandp=/summationtext
i∈Npi.This follows because/summationtext
i∈NXirepresents the
number of the nexperiments whose outcome is in N, and each experiment will inde-
pendently have such an outcome with probability/summationtext
i∈Npi.
As an application of the multinomial distribution, suppose that a fair die is rolled
9 times. The probability that 1 appears three times, 2 and 3 twice each, 4 and 5 once
each, and 6 not at all is
9!
3!2!2!1!1!0!/parenleftbigg1
6/parenrightbigg3/parenleftbigg1
6/parenrightbigg2/parenleftbigg1
6/parenrightbigg2/parenleftbigg1
6/parenrightbigg1/parenleftbigg1
6/parenrightbigg1/parenleftbigg1
6/parenrightbigg0
=9!
3!2!2!/parenleftbigg1
6/parenrightbigg9
.
6.2 Independent Random Variables
The random variables XandYare said to be independent if, for any two sets of real
numbers AandB,
P{X∈A,Y∈B}=P {X∈A}P{Y∈B} (2.1)
In other words, XandYare independent if, for all AandB, the events EA=
{X∈A}andFB={Y∈B}are independent.
It can be shown by using the three axioms of probability that Equation (2.1) will
follow if and only if, for all a,b,
P{X…a,Y…b}= P{X…a}P{Y…b}
Hence, in terms of the joint distribution function FofXandY,XandYare inde-
pendent if
F(a,b)=FX(a)F Y(b) for all a,b

<<<PAGE 242>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 229
A First Course in Probability 229
When XandYare discrete random variables, the condition of independence (2.1)
is equivalent to
p(x,y)=pX(x)p Y(y) for all x,y (2.2)
The equivalence follows because, if Equation (2.1) is satisﬁed, then we obtain Equa-
tion (2.2) by letting AandBbe, respectively, the one-point sets A={x}andB={y}.
Furthermore, if Equation (2.2) is valid, then for any sets A,B,
P{X∈A,Y∈B}=/summationdisplay
y∈B/summationdisplay
x∈Ap(x,y)
=/summationdisplay
y∈B/summationdisplay
x∈ApX(x)p Y(y)
=/summationdisplay
y∈BpY(y)/summationdisplay
x∈ApX(x)
=P{Y∈B}P{X∈A}
and Equation (2.1) is established.
In the jointly continuous case, the condition of independence is equivalent to
f(x,y)=fX(x)fY(y) for all x,y
Thus, loosely speaking, XandYare independent if knowing the value of one
does not change the distribution of the other. Random variables that are not inde-
pendent are said to be dependent.
Example
2aSuppose that n+mindependent trials having a common probability of success pare
performed. If Xis the number of successes in the ﬁrst ntrials, and Yis the number
of successes in the ﬁnal mtrials, then XandYare independent, since knowing the
number of successes in the ﬁrst ntrials does not affect the distribution of the number
of successes in the ﬁnal mtrials (by the assumption of independent trials). In fact,
for integral xandy,
P{X=x,Y=y}=/parenleftBigg
n
x/parenrightBigg
px(1−p)n−x/parenleftBigg
m
y/parenrightBigg
py(1−p)m−y 0…x…n,
0…y…m
=P{X=x}P{Y=y}
In contrast, XandZwill be dependent, where Zis the total number of successes in
then+mtrials. (Why?) .
Example
2bSuppose that the number of people who enter a post ofﬁce on a given day is a Pois-son random variable with parameter λ. Show that if each person who enters the post
ofﬁce is a male with probability pand a female with probability 1 −p, then the num-
ber of males and females entering the post ofﬁce are independent Poisson random
variables with respective parameters λpandλ(1−p).
Solution LetXandYdenote, respectively, the number of males and females that
enter the post ofﬁce. We shall show the independence of XandYby establishing
Equation (2.2). To obtain an expression for P{X=i,Y=j}, we condition on X+Y
as follows:
P{X=i,Y=j}=P{X=i,Y=j|X+Y=i+j}P{X+Y=i+j}
+P{X=i,Y=j|X+YZi+j}P{X+YZi+j}

<<<PAGE 243>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 230
230 Chapter 6 Jointly Distributed Random Variables
[Note that this equation is merely a special case of the formula P(E)=P(E|F)P(F)+
P(E|Fc)P(Fc).]
Since P{X=i,Y=j|X+YZi+j}is clearly 0, we obtain
P{X=i,Y=j}=P{X=i,Y=j|X+Y=i+j}P{X+Y=i+j} (2.3)
Now, because X+Yis the total number of people who enter the post ofﬁce, it
follows, by assumption, that
P{X+Y=i+j}=e−λλi+j
(i+j)!(2.4)
Furthermore, given that i+jpeople do enter the post ofﬁce, since each person
entering will be male with probability p, it follows that the probability that exactly
iof them will be male (and thus jof them female) is just the binomial probability/parenleftBigg
i+j
i/parenrightBigg
pi(1−p)j. That is,
P{X=i,Y=j|X+Y=i+j}=/parenleftBigg
i+j
i/parenrightBigg
pi(1−p)j(2.5)
Substituting Equations (2.4) and (2.5) into Equation (2.3) yields
P{X=i,Y=j}=/parenleftBigg
i+j
i/parenrightBigg
pi(1−p)je−λλi+j
(i+j)!
=e−λ(λp)i
i!j![λ(1−p)]j
=e−λp(λp)i
i!e−λ(1−p) [λ(1−p)]j
j!(2.6)
Hence,
P{X=i}=e−λp(λp)i
i!/summationdisplay
je−λ(1−p) [λ(1−p)]j
j!=e−λp(λp)i
i!(2.7)
and similarly,
P{Y=j}=e−λ(1−p) [λ(1−p)]j
j!(2.8)
Equations (2.6), (2.7), and (2.8) establish the desired result. .
Example
2cA man and a woman decide to meet at a certain location. If each of them indepen-
dently arrives at a time uniformly distributed between 12 noon and 1 p.m., ﬁnd the
probability that the ﬁrst to arrive has to wait longer than 10 minutes.
Solution If we let XandYdenote, respectively, the time past 12 that the man and
the woman arrive, then XandYare independent random variables, each of which is

<<<PAGE 244>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 231
A First Course in Probability 231
uniformly distributed over (0, 60). The desired probability, P{X+10<Y}+P{Y+
10<X}, which, by symmetry, equals 2P {X+10<Y}, is obtained as follows:
2P{X+10<Y}=2/integraldisplay/integraldisplay
x+10<yf(x,y)dx dy
=2/integraldisplay/integraldisplay
x+10<yfX(x)fY(y)dx dy
=2/integraldisplay60
10/integraldisplayy−10
0/parenleftbigg1
60/parenrightbigg2
dx dy
=2
(60)2/integraldisplay60
10(y−10)dy
=25
36.
Our next example presents the oldest problem dealing with geometrical prob-
abilities. It was ﬁrst considered and solved by Buffon, a French naturalist of the
eighteenth century, and is usually referred to as Buffon’s needle problem.
Example
2dBuffon’s needle problem
A table is ruled with equidistant parallel lines a distance Dapart. A needle of
length L, where L…D, is randomly thrown on the table. What is the probability
that the needle will intersect one of the lines (the other possibility being that the
needle will be completely contained in the strip between two lines)?
Solution Let us determine the position of the needle by specifying (1) the distance X
from the middle point of the needle to the nearest parallel line and (2) the angle θ
between the needle and the projected line of length X. (See Figure 6.2.) The needle
will intersect a line if the hypotenuse of the right triangle in Figure 6.2 is less thanL/2—that is, if
X
cosθ<L
2orX<L
2cosθ
/H9258X
Figure 6.2
AsXvaries between 0 and D/2 and θbetween 0 and π/2, it is reasonable to assume
that they are independent, uniformly distributed random variables over these respec-
tive ranges. Hence,

<<<PAGE 245>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 232
232 Chapter 6 Jointly Distributed Random Variables
P/braceleftbigg
X<L
2cosθ/bracerightbigg
=/integraldisplay/integraldisplay
x<L/2c o syfX(x)fθ(y)dx dy
=4
πD/integraldisplayπ/2
0/integraldisplayL/2c o sy
0dx dy
=4
πD/integraldisplayπ/2
0L
2cosyd y
=2L
πD.
∗Example
2eCharacterization of the normal distribution
LetXandYdenote the horizontal and vertical miss distances when a bullet is ﬁred
at a target, and assume that
1.XandYare independent continuous random variables having differentiable
density functions.
2. The joint density f(x,y)=fX(x)fY(y)ofXandYdepends on ( x,y) only
through x2+y2.
Loosely put, assumption 2 states that the probability of the bullet landing on any
point of the x–yplane depends only on the distance of the point from the target and
not on its angle of orientation. An equivalent way of phrasing this assumption is tosay that the joint density function is rotation invariant.
It is a rather interesting fact that assumptions 1 and 2 imply that XandYare
normally distributed random variables. To prove this, note ﬁrst that the assumptionsyield the relation
f(x,y)=f
X(x)fY(y)=g(x2+y2) (2.9)
for some function g. Differentiating Equation (2.9) with respect to xyields
f/prime
X(x)fY(y)=2xg/prime(x2+y2) (2.10)
Dividing Equation (2.10) by Equation (2.9) gives
f/prime
X(x)
fX(x)=2xg/prime(x2+y2)
g(x2+y2)
or
f/prime
X(x)
2xfX(x)=g/prime(x2+y2)
g(x2+y2)(2.11)
Because the value of the left-hand side of Equation (2.11) depends only on x,
whereas the value of the right-hand side depends on x2+y2, it follows that the left-
hand side must be the same for all x. To see this, consider any x1,x2and let y1,y2be
such that x2
1+y21=x2
2+y22. Then, from Equation (2.11), we obtain
f/prime
X(x1)
2x1fX(x1)=g/prime(x2
1+y21)
g(x21+y21)=g/prime(x2
2+y22)
g(x22+y22)=f/prime
X(x2)
2x2fX(x2)
Hence,
f/prime
X(x)
xfX(x)=cord
dx(logfX(x))=cx

<<<PAGE 246>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 233
A First Course in Probability 233
which implies, upon integration of both sides, that
logfX(x)=a+cx2
2orfX(x)=kecx2/2
Since/integraltextq
−qfX(x)dx=1, it follows that cis necessarily negative, and we may write
c=− 1/σ2. Thus,
fX(x)=ke−x2/2σ2
That is, Xis a normal random variable with parameters μ=0 and σ2. A similar
argument can be applied to fY(y)to show that
fY(y)=1√
2πσe−y2/2σ2
Furthermore, it follows from assumption 2 that σ2=σ2and that XandYare thus
independent, identically distributed normal random variables with parameters μ=0
andσ2. .
A necessary and sufﬁcient condition for the random variables XandYto be
independent is for their joint probability density function (or joint probability mass
function in the discrete case) f(x,y) to factor into two terms, one depending only on
xand the other depending only on y.
Proposition
2.1The continuous (discrete) random variables XandYare independent if and only if
their joint probability density (mass) function can be expressed as
fX,Y(x,y)=h(x)g(y) −q<x<q,−q<y<q
Proof Let us give the proof in the continuous case. First, note that independence
implies that the joint density is the product of the marginal densities of XandY,s o
the preceding factorization will hold when the random variables are independent.Now, suppose that
f
X,Y(x,y)=h(x)g(y)
Then
1=/integraldisplayq
−q/integraldisplayq
−qfX,Y(x,y)dx dy
=/integraldisplayq
−qh(x)dx/integraldisplayq
−qg(y)dy
=C1C2
where C1=/integraltextq
−qh(x)dxandC2=/integraltextq
−qg(y)dy.A l s o ,
fX(x)=/integraldisplayq
−qfX,Y(x,y)dy=C2h(x)
fY(y)=/integraldisplayq
−qfX,Y(x,y)dx=C1g(y)
Since C1C2=1, it follows that
fX,Y(x,y)=fX(x)fY(y)
and the proof is complete.

<<<PAGE 247>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 234
234 Chapter 6 Jointly Distributed Random Variables
Example
2fIf the joint density function of XandYis
f(x,y)=6e−2xe−3y0<x<q,0 <y<q
and is equal to 0 outside this region, are the random variables independent? What if
the joint density function is
f(x,y)=24xy 0<x<1, 0<y<1, 0<x+y<1
and is equal to 0 otherwise?
Solution In the ﬁrst instance, the joint density function factors, and thus the random
variables, are independent (with one being exponential with rate 2 and the otherexponential with rate 3). In the second instance, because the region in which thejoint density is nonzero cannot be expressed in the form x∈A,y∈B, the joint
density does not factor, so the random variables are not independent. This can beseen clearly by letting
I(x,y)=/braceleftBigg
1i f 0 <x<1, 0<y<1, 0<x+y<1
0 otherwise
and writing
f(x,y)=24xy I (x,y)
which clearly does not factor into a part depending only on xand another depending
only on y. .
The concept of independence may, of course, be deﬁned for more than two
random variables. In general, the nrandom variables X
1,X2,...,Xna r es a i dt ob e
independent if, for all sets of real numbers A1,A2,...,An,
P{X1∈A1,X2∈A2,...,Xn∈An}=n/productdisplay
i=1P{Xi∈Ai}
As before, it can be shown that this condition is equivalent to
P{X1…a1,X2…a2,...,Xn…an}
=n/productdisplay
i=1P{Xi…ai}for all a1,a2,...,an
Finally, we say that an inﬁnite collection of random variables is independent if every
ﬁnite subcollection of them is independent.
Example
2gHow can a computer choose a random subset?
Most computers are able to generate the value of, or simulate, a uniform (0, 1)
random variable by means of a built-in subroutine that (to a high degree of approxi-
mation) produces such “random numbers.” As a result, it is quite easy for a computer
to simulate an indicator (that is, a Bernoulli) random variable. Suppose Iis an indi-
cator variable such that
P{I=1}= p=1−P{I=0}
The computer can simulate Iby choosing a uniform (0, 1) random number Uand
then letting
I=1ifU<p
0ifUÚp

<<<PAGE 248>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 235
A First Course in Probability 235
Suppose that we are interested in having the computer select k,k…n, of the num-
bers 1, 2, ...,nin such a way that each of the/parenleftBigg
n
k/parenrightBigg
subsets of size kis equally likely
to be chosen. We now present a method that will enable the computer to solve this
task. To generate such a subset, we will ﬁrst simulate, in sequence, nindicator vari-
ables I1,I2,...,In, of which exactly kwill equal 1. Those ifor which Ii=1 will then
constitute the desired subset.
To generate the random variables I1,...,In, start by simulating nindependent
uniform (0, 1) random variables U1,U2,...,Un. Now deﬁne
I1=⎧
⎪⎨
⎪⎩1i f U1<k
n
0 otherwise
and then, once I1,...,Iiare determined, recursively set
Ii+1=⎧
⎪⎨
⎪⎩1i f Ui+1<k−(I1+ ··· + Ii)
n−i
0 otherwise
In words, at the (i +1)th stage, we set Ii+1equal to 1 (and thus put i+1i n t o
the desired subset) with a probability equal to the remaining number of places in
the subset⎛
⎝namely, k−i/summationtext
j=1Ij⎞⎠, divided by the remaining number of possibilities
(namely, n−i). Hence, the joint distribution of I
1,I2,...,Inis determined from
P{I1=1}=k
n
P{Ii+1=1|I1,...,Ii}=k−i/summationdisplay
j=1Ij
n−i1<i<n
The proof that the preceding formula results in all subsets of size kbeing equally
likely to be chosen is by induction on k+n. It is immediate when k+n=2 (that
is, when k=1,n=1), so assume it to be true whenever k+n…l. Now, suppose
thatk+n=l+1, and consider any subset of size k—say, i1…i2…···…ik—and
consider the following two cases.
Case 1: i1=1
P{I1=Ii2=···= Iik=1,Ij=0 otherwise}
=P{I1=1}P{Ii2=···= Iik=1,Ij=0 otherwise|I 1=1}
Now given that I1=1, the remaining elements of the subset are chosen as if a
subset of size k−1 were to be chosen from the n−1 elements 2, 3, ...,n. Hence, by
the induction hypothesis, the conditional probability that this will result in a given
subset of size k−1 being selected is 1 //parenleftBigg
n−1
k−1/parenrightBigg
. Hence,
P{I1=Ii2=···= Iik=1,Ij=0 otherwise}
=k
n1/parenleftBigg
n−1
k−1/parenrightBigg=1/parenleftBigg
n
k/parenrightBigg

<<<PAGE 249>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 236
236 Chapter 6 Jointly Distributed Random Variables
Case 2: i1Z1
P{Ii1=Ii2=···= Iik=1,Ij=0 otherwise}
=P{Ii1=···= Iik=1,Ij=0 otherwise |I1=0}P{I1=0}
=1/parenleftBigg
n−1
k/parenrightBigg/parenleftbigg
1−k
n/parenrightbigg
=1/parenleftBigg
n
k/parenrightBigg
where the induction hypothesis was used to evaluate the preceding conditional prob-ability.
Thus, in all cases, the probability that a given subset of size kwill be the subset
chosen is 1/slashbig/parenleftBigg
nk/parenrightBigg
. .
Remark The foregoing method for generating a random subset has a very low
memory requirement. A faster algorithm that requires somewhat more memory is
presented in Section 10.1. (The latter algorithm uses the last kelements of a random
permutation of 1, 2, ...,n.) .
Example
2hLetX,Y,Zbe independent and uniformly distributed over (0, 1). Compute
P{XÚYZ}.
Solution Since
fX,Y,Z(x,y,z)=fX(x)fY(y)fZ(z)=10…x…1, 0…y…1, 0…z…1
we have
P{XÚYZ}=/integraldisplay/integraldisplay/integraldisplay
xÚyzfX,Y,Z(x,y,z)dx dy dz
=/integraldisplay1
0/integraldisplay1
0/integraldisplay1
yzdx dy dz
=/integraldisplay1
0/integraldisplay1
0(1−yz)dy dz
=/integraldisplay1
0/parenleftbigg
1−z
2/parenrightbigg
dz
=3
4.
Example
2iProbabilistic interpretation of half-life
LetN(t)denote the number of nuclei contained in a radioactive mass of material at
time t. The concept of half-life is often deﬁned in a deterministic fashion by stating
this it is an empirical fact that, for some value h, called the half-life ,
N(t)=2−t/hN(0) t>0
[Note that N(h)=N(0)/2.] Since the preceding implies that, for any nonnegative s
andt,
N(t+s)=2−(s+t )/hN(0)=2−t/hN(s)

<<<PAGE 250>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 237
A First Course in Probability 237
it follows that no matter how much time shas already elapsed, in an additional time
t, the number of existing nuclei will decrease by the factor 2−t/h.
Because the deterministic relationship just given results from observations of
radioactive masses containing huge numbers of nuclei, it would seem that it might
be consistent with a probabilistic interpretation. The clue to deriving the appropriate
probability model for half-life resides in the empirical observation that the propor-
tion of decay in any time interval depends neither on the total number of nuclei atthe beginning of the interval nor on the location of this interval [since N(t+s)/N(s)
depends neither on N(s)nor on s]. Thus, it appears that the individual nuclei act inde-
pendently and with a memoryless life distribution. Consequently, since the unique
life distribution that is memoryless is the exponential distribution, and since exactly
one-half of a given amount of mass decays every htime units, we propose the fol-
lowing probabilistic model for radioactive decay.
Probabilistic interpretation of the half-life h:The lifetimes of the individual nuclei
are independent random variables having a life distribution that is exponential with
median equal to h. That is, if Lrepresents the lifetime of a given nucleus, then
P{L<t}=1 −2
−t/h
(Because P{L<h}=1
2and the preceding can be written as
P{L<t}=1 −exp/braceleftbigg
−tlog 2
h/bracerightbigg
it can be seen that Lindeed has an exponential distribution with median h.)
Note that under the probabilistic interpretation of half-life just given, if one
starts with N(0) nuclei at time 0, then N(t), the number of nuclei that remain at
time twill have a binomial distribution with parameters n=N(0)andp=2−t/h.
Results of Chapter 8 will show that this interpretation of half-life is consistent withthe deterministic model when considering the proportion of a large number of nuclei
that decay over a given time frame. However, the difference between the determinis-tic and probabilistic interpretation becomes apparent when one considers the actual
number of decayed nuclei. We will now indicate this with regard to the question of
whether protons decay.
There is some controversy over whether or not protons decay. Indeed, one the-
ory predicts that protons should decay with a half-life of about h=10
30years. To
check this prediction empirically, it has been suggested that one follow a large num-ber of protons for, say, one or two years and determine whether any of them decay
within that period. (Clearly, it would not be feasible to follow a mass of protons for
10
30years to see whether one-half of it decays.) Let us suppose that we are able to
keep track of N(0)=1030protons for cyears. The number of decays predicted by
the deterministic model would then be given by
N(0)−N(c)=h(1−2−c/h)
=1−2−c/h
1/h
Llim
x→01−2−cx
xsince1
h=10−30L0
=lim
x→0(c2−cxlog 2) by L’H ˆopital’s rule
=clog 2L.6931c

<<<PAGE 251>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 238
238 Chapter 6 Jointly Distributed Random Variables
For instance, the deterministic model predicts that in 2 years there should be 1.3863
decays, and it would thus appear to be a serious blow to the hypothesis that protons
decay with a half-life of 1030years if no decays are observed over those 2 years.
Let us now contrast the conclusions just drawn with those obtained from the
probabilistic model. Again, let us consider the hypothesis that the half-life ofprotons is h=10
30years, and suppose that we follow hprotons for cyears. Since
there is a huge number of independent protons, each of which will have a very small
probability of decaying within this time period, it follows that the number of protons
that decay will have (to a very strong approximation) a Poisson distribution with
parameter equal to h(1−2−c/h)Lclog 2. Thus,
P{0 decays }=e−clog 2
=e−log(2c)=1
2c
and, in general,
P{ndecays }=2−c[clog 2]n
n!nÚ0
Thus, we see that even though the average number of decays over 2 years is (as
predicted by the deterministic model) 1.3863, there is 1 chance in 4 that there will
not be any decays, thereby indicating that such a result in no way invalidates theoriginal hypothesis of proton decay. .
Remark Independence is a symmetric relation. The random variables XandYare
independent if their joint density function (or mass function in the discrete case) is
the product of their individual density (or mass) functions. Therefore, to say that
Xis independent of Yis equivalent to saying that Yis independent of X—or just
thatXandYare independent. As a result, in considering whether Xis independent
ofYin situations where it is not at all intuitive that knowing the value of Ywill
not change the probabilities concerning X, it can be beneﬁcial to interchange the
roles of XandYand ask instead whether Yis independent of X. The next example
illustrates this point. .
Example
2jIf the initial throw of the dice in the game of craps results in the sum of the diceequaling 4, then the player will continue to throw the dice until the sum is either 4or 7. If this sum is 4, then the player wins, and if it is 7, then the player loses. Let N
denote the number of throws needed until either 4 or 7 appears, and let Xdenote the
value (either 4 or 7) of the ﬁnal throw. Is Nindependent of X? That is, does knowing
which of 4 or 7 occurs ﬁrst affect the distribution of the number of throws needed
until that number appears? Most people do not ﬁnd the answer to this question to
be intuitively obvious. However, suppose that we turn it around and ask whether X
is independent of N. That is, does knowing how many throws it takes to obtain a
sum of either 4 or 7 affect the probability that that sum is equal to 4? For instance,suppose we know that it takes nthrows of the dice to obtain a sum of either 4 or
7. Does this affect the probability distribution of the ﬁnal sum? Clearly not, sinceall that is important is that its value is either 4 or 7, and the fact that none of theﬁrstn−1 throws were either 4 or 7 does not change the probabilities for the nth
throw. Thus, we can conclude that Xis independent of N, or equivalently, that Nis
independent of X.
As another example, let X
1,X2,...be a sequence of independent and identically
distributed continuous random variables, and suppose that we observe these randomvariables in sequence. If X
n>Xifor each i=1,...,n−1, then we say that Xnis
arecord value. That is, each random variable that is larger than all those preceding

<<<PAGE 252>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 239
A First Course in Probability 239
it is called a record value. Let Andenote the event that Xnis a record value. Is An+1
independent of An? That is, does knowing that the nth random variable is the largest
of the ﬁrst nchange the probability that the (n+1)random variable is the largest
of the ﬁrst n+1? While it is true that An+1is independent of An, this may not be
intuitively obvious. However, if we turn the question around and ask whether Anis
independent of An+1, then the result is more easily understood. For knowing that
the(n+1)value is larger than X1,...,Xnclearly gives us no information about
t h er e l a t i v es i z eo fX namong the ﬁrst nrandom variables. Indeed, by symmetry, it is
clear that each of these nrandom variables is equally likely to be the largest of this
set, so P(An|An+1)=P(An)=1/n. Hence, we can conclude that AnandAn+1are
independent events. .
Remark It follows from the identity
P{X1…a1,...,Xn…an}
=P{X1…a1}P{X2…a2|X1…a1}···P{Xn…an|X1…a1,...,Xn−1…an−1}
that the independence of X1,...,Xncan be established sequentially. That is, we can
show that these random variables are independent by showing that
X2is independent of X1
X3is independent of X1,X2
X4is independent of X1,X2,X3
···
X
nis independent of X1,...,Xn−1
6.3 Sums of Independent Random Variables
It is often important to be able to calculate the distribution of X+Yfrom the
distributions of XandYwhen XandYare independent. Suppose that XandYare
independent, continuous random variables having probability density functions fX
andfY. The cumulative distribution function of X+Yis obtained as follows:
FX+Y(a)=P{X+Y…a}
=/integraldisplay/integraldisplay
x+y…afX(x)fY(y)dx dy
=/integraldisplayq
−q/integraldisplaya−y
−qfX(x)fY(y)dx dy
=/integraldisplayq
−q/integraldisplaya−y
−qfX(x)dxfY(y)dy
=/integraldisplayq
−qFX(a−y)fY(y)dy (3.1)
The cumulative distribution function FX+Yis called the convolution of the distribu-
tions FXandFY(the cumulative distribution functions of XandY, respectively).
By differentiating Equation (3.1), we ﬁnd that the probability density function
fX+YofX+Yis given by

<<<PAGE 253>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 240
240 Chapter 6 Jointly Distributed Random Variables
fX+Y(a)=d
da/integraldisplayq
−qFX(a−y)fY(y)dy
=/integraldisplayq
−qd
daFX(a−y)fY(y)dy
=/integraldisplayq
−qfX(a−y)fY(y)dy (3.2)
6.3.1 Identically Distributed Uniform Random Variables
It is not difﬁcult to determine the density function of the sum of two independent
uniform (0, 1) random variables.
Example
3aSum of two independent uniform random variables
IfXandYare independent random variables, both uniformly distributed on (0, 1),
calculate the probability density of X+Y.
Solution From Equation (3.2), since
fX(a)=fY(a)=/braceleftBigg
10<a<1
0 otherwise
we obtain
fX+Y(a)=/integraldisplay1
0fX(a−y)dy
For 0 …a…1, this yields
fX+Y(a)=/integraldisplaya
0dy=a
For 1 <a<2, we get
fX+Y(a)=/integraldisplay1
a−1dy=2−a
Hence,
fX+Y(a)=⎧
⎪⎨
⎪⎩a 0…a…1
2−a 1<a<2
0 otherwise
Because of the shape of its density function (see Figure 6.3), the random variable
X+Yis said to have a triangular distribution. .
1
12af(a)
0
Figure 6.3 Triangular density function.

<<<PAGE 254>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 241
A First Course in Probability 241
Now, suppose that X1,X2,...,Xnare independent uniform (0, 1) random variables,
and let
Fn(x)=P{X1+...+Xn…x}
Whereas a general formula for Fn(x)is messy, it has a particularly nice form when
x…1.Indeed, we now use mathematical induction to prove that
Fn(x)=xn/n!, 0 …x…1
Because the proceeding equation is true for n=1, assume that
Fn−1(x)=xn−1/(n−1)!, 0 …x…1
Now, writing
n/summationdisplay
i=1Xi=n−1/summationdisplay
i=1Xi+Xn
and using the fact that the Xiare all nonnegative, we see from Equation 3.1 that, for
0…x…1,
Fn(x)=/integraldisplay1
0Fn−1(x−y)fXn(y)dy
=1
(n−1)!/integraldisplayx
0(x−y)n−1dy by the induction hypothesis
=xn/n!
which completes the proof.
For an interesting application of the preceding formula, let us use it to determine
the expected number of independent uniform (0, 1) random variables that need to
be summed to exceed 1. That is, with X1,X2,...being independent uniform (0, 1)
random variables, we want to determine E[N], where
N=min{n:X1+...+Xn>1}
Noting that Nis greater than n>0 if and only if X1+...+Xn…1, we see that
P{N>n}= Fn(1)=1/n!, n>0
Because
P{N>0}= 1=1/0!
we see that, for n>0,
P{N=n}=P {N>n−1}−P {N>n}=1
(n−1)!−1
n!=n−1
n!
Therefore,
E[N]=q/summationdisplay
n=1n(n−1)
n!
=q/summationdisplay
n=21
(n−2)!
=e
That is, the mean number of independent uniform (0, 1) random variables that must
be summed for the sum to exceed 1 is equal to e.

<<<PAGE 255>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 242
242 Chapter 6 Jointly Distributed Random Variables
6.3.2 Gamma Random Variables
Recall that a gamma random variable has a density of the form
f(y)=λe−λy(λy)t−1
/Gamma1(t)0<y<q
An important property of this family of distributions is that for a ﬁxed value of λ,i t
is closed under convolutions.
Proposition
3.1IfXandYare independent gamma random variables with respective parameters
(s,λ)and(t,λ), then X+Yis a gamma random variable with parameters (s+t,λ).
Proof Using Equation (3.2), we obtain
fX+Y(a)=1
/Gamma1(s)/Gamma1(t)/integraldisplaya
0λe−λ(a−y)[λ(a−y)]s−1λe−λy(λy)t−1dy
=Ke−λa/integraldisplaya
0(a−y)s−1yt−1dy
=Ke−λaas+t−1/integraldisplay1
0(1−x)s−1xt−1dx by letting x=y
a
=Ce−λaas+t−1
where Cis a constant that does not depend on a. But, as the preceding is a density
function and thus must integrate to 1, the value of Cis determined, and we have
fX+Y(a)=λe−λa(λa)s+t−1
/Gamma1(s+t)
Hence, the result is proved.
It is now a simple matter to establish, by using Proposition 3.1 and induction,
that if Xi,i=1,...,nare independent gamma random variables with respective
parameters (ti,λ),i=1,...,n, thenn/summationtext
i=1Xiis gamma with parameters/parenleftBigg
n/summationtext
i=1ti,λ/parenrightBigg
.W e
leave the proof of this statement as an exercise.
Example
3bLetX1,X2,...,Xnbenindependent exponential random variables, each having
parameter λ. Then, since an exponential random variable with parameter λis the
same as a gamma random variable with parameters (1,λ), it follows from Proposi-
tion 3.1 that X1+X2+ ··· + Xnis a gamma random variable with parameters
(n,λ). .
IfZ1,Z2,...,Znare independent standard normal random variables, then YK
n/summationtext
i=1Z2
iis said to have the chi-squared (sometimes seen as χ2) distribution with n
degrees of freedom. Let us compute the density function of Y. When n=1,Y=Z2
1,
and from Example 7b of Chapter 5, we see that its probability density function is
given by
fZ2(y)=1
2√y[fZ(√y)+fZ(−√y)]
=1
2√y2√
2πe−y/2
=1
2e−y/2(y/2)1/2−1
√π

<<<PAGE 256>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 243
A First Course in Probability 243
But we recognize the preceding as the gamma distribution with parameters/parenleftBig
1
2,1
2/parenrightBig
. [A by-product of this analysis is that /Gamma1/parenleftBig
1
2/parenrightBig
=√π.] But since each Z2
iis
gamma/parenleftBig
1
2,1
2/parenrightBig
, it follows from Proposition 3.1 that the chi-squared distribution with
ndegrees of freedom is just the gamma distribution with parameters/parenleftBig
n/2,1
2/parenrightBig
and
hence has a probability density function given by
fY(y)=1
2e−y/2/parenleftbiggy
2/parenrightbiggn/2−1
/Gamma1/parenleftbiggn
2/parenrightbigg y>0
=e−y/2yn/2−1
2n/2/Gamma1/parenleftbiggn
2/parenrightbigg y>0
When nis an even integer, /Gamma1(n/2)=[(n/2) −1]!, whereas when nis odd, /Gamma1(n/2) can
be obtained from iterating the relationship /Gamma1(t)=(t−1)/Gamma1(t −1)and then using
the previously obtained result that /Gamma1/parenleftBig
1
2/parenrightBig
=√π. [For instance, /Gamma1/parenleftBig
5
2/parenrightBig
=3
2/Gamma1/parenleftBig
3
2/parenrightBig
=
3
21
2/Gamma1/parenleftBig
1
2/parenrightBig
=3
4√π.]
In practice, the chi-squared distribution often arises as the distribution of the
square of the error involved when one attempts to hit a target in n-dimensional space
when the coordinate errors are taken to be independent standard normal random
variables. It is also important in statistical analysis.
6.3.3 Normal Random Variables
We can also use Equation (3.2) to prove the following important result about normalrandom variables.
Proposition
3.2IfX
i,i=1,...,n, are independent random variables that are normally distributed
with respective parameters μi,σ2
i,i=1,...,n, thenn/summationtext
i=1Xiis normally distributed
with parametersn/summationtext
i=1μiandn/summationtext
i=1σ2
i.
Proof of Proposition 3.2: To begin, let XandYbe independent normal random
variables with Xhaving mean 0 and variance σ2andYhaving mean 0 and variance
1. We will determine the density function of X+Yby utilizing Equation (3.2).
Now, with
c=1
2σ2+1
2=1+σ2
2σ2
we have
fX(a−y)fY(y)=1√
2πσexp/braceleftBigg
−(a−y)2
2σ2/bracerightBigg
1√
2πexp/braceleftBigg
−y2
2/bracerightBigg
=1
2πσexp/braceleftBigg
−a2
2σ2/bracerightBigg
exp/braceleftBigg
−c/parenleftbigg
y2−2ya
1+σ2/parenrightbigg/bracerightBigg

<<<PAGE 257>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 244
244 Chapter 6 Jointly Distributed Random Variables
Hence, from Equation (3.2),
fX+Y(a)=1
2πσexp/braceleftBigg
−a2
2σ2/bracerightBigg
exp/braceleftBigg
a2
2σ2(1+σ2)/bracerightBigg
*/integraldisplayq
−qexp/braceleftBigg
−c/parenleftbigg
y−a
1+σ2/parenrightbigg2/bracerightBigg
dy
=1
2πσexp/braceleftBigg
−a2
2(1+σ2)/bracerightBigg/integraldisplayq
−qexp{−cx2}dx
=Cexp/braceleftBigg
−a2
2(1+σ2)/bracerightBigg
where Cdoes not depend on a. But this implies that X+Yis normal with mean 0
and variance 1 +σ2.
Now, suppose that X1andX2are independent normal random variables with Xi
having mean μiand variance σ2
i,i=1, 2. Then
X1+X2=σ2/parenleftbiggX1−μ1
σ2+X2−μ2
σ2/parenrightbigg
+μ1+μ2
But since (X1−μ1)/σ2is normal with mean 0 and variance σ2
1/σ2
2, and(X2−μ2)/σ2
is normal with mean 0 and variance 1, it follows from our previous result that (X1−
μ1)/σ2+(X2−μ2)/σ2is normal with mean 0 and variance 1 +σ2
1/σ2
2, implying
thatX1+X2is normal with mean μ1+μ2and variance σ2
2(1+σ2
1/σ2
2)=σ2
1+σ2
2.
Thus, Proposition 3.2 is established when n=2. The general case now follows by
induction. That is, assume that Proposition 3.2 is true when there are n−1 random
variables. Now consider the case of n, and write
n/summationdisplay
i=1Xi=n−1/summationdisplay
i=1Xi+Xn
By the induction hypothesis,n−1/summationtext
i=1Xiis normal with meann−1/summationtext
i=1μiand variancen−1/summationtext
i=1σ2
i.
Therefore, by the result for n=2,n/summationtext
i=1Xiis normal with meann/summationtext
i=1μiand variance
n/summationtext
i=1σ2
i.
Example
3cA basketball team will play a 44-game season. Twenty-six of these games are against
class A teams and 18 are against class B teams. Suppose that the team will win each
game against a class A team with probability .4 and will win each game against aclass B team with probability .7. Suppose also that the results of the different games
are independent. Approximate the probability that
(a) the team wins 25 games or more;
(b) the team wins more games against class A teams than it does against class B
teams.
Solution (a) Let XAandXBrespectively denote the number of games the team wins
against class A and against class B teams. Note that XAandXBare independent
binomial random variables and
E[XA]=26(.4)=10.4V a r (XA)=26(.4)(.6)=6.24
E[XB]=18(.7)=12.6V a r (XB)=18(.7)(.3)=3.78

<<<PAGE 258>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 245
A First Course in Probability 245
By the normal approximation to the binomial, XAandXBwill have approximately
the same distribution as would independent normal random variables with the pre-
ceding expected values and variances. Hence, by Proposition 3.2, XA+XBwill have
approximately a normal distribution with mean 23 and variance 10.02. Therefore,
letting Zdenote a standard normal random variable, we have
P{XA+XBÚ25}= P{XA+XBÚ24.5}
=P/braceleftBigg
XA+XB−23√
10.02Ú24.5−23√
10.02/bracerightBigg
LP/braceleftBigg
ZÚ1.5√
10.02/bracerightBigg
L1−P{Z<.4739}
L.3178
(b) We note that XA−XBwill have approximately a normal distribution with
mean −2.2 and variance 10.02. Hence,
P{XA−XBÚ1}=P {XA−XBÚ.5}
=P/braceleftBigg
XA−XB+2.2√
10.02Ú.5+2.2√
10.02/bracerightBigg
LP/braceleftBigg
ZÚ2.7√
10.02/bracerightBigg
L1−P{Z<.8530}
L.1968
Therefore, there is approximately a 31.78 percent chance that the team will win atleast 25 games and approximately a 19.68 percent chance that it will win more gamesagainst class A teams than against class B teams. .
The random variable Yis said to be a lognormal random variable with param-
etersμandσif log (Y)is a normal random variable with mean μand variance σ
2.
That is, Yis lognormal if it can be expressed as
Y=eX
where Xis a normal random variable.
Example
3dStarting at some ﬁxed time, let S(n) denote the price of a certain security at the
end of nadditional weeks, nÚ1. A popular model for the evolution of these prices
assumes that the price ratios S(n)/S(n −1),nÚ1, are independent and identically
distributed lognormal random variables. Assuming this model, with parameters μ=
.0165, σ=.0730, what is the probability that
(a) the price of the security increases over each of the next two weeks?
(b) the price at the end of two weeks is higher than it is today?
Solution LetZbe a standard normal random variable. To solve part (a), we use the
fact that log(x) increases in xto conclude that x>1 if and only if log (x)> log(1)=0.

<<<PAGE 259>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 246
246 Chapter 6 Jointly Distributed Random Variables
As a result, we have
P/braceleftbiggS(1)
S(0)>1/bracerightbigg
=P/braceleftBigg
log/parenleftbiggS(1)
S(0)/parenrightbigg
>0/bracerightBigg
=P/braceleftbigg
Z>−.0165
.0730/bracerightbigg
=P{Z<.2260}
=.5894
In other words, the probability that the price is up after one week is .5894. Since the
successive price ratios are independent, the probability that the price increases over
each of the next two weeks is (.5894)2=.3474.
To solve part (b), we reason as follows:
P/braceleftbiggS(2)
S(0)>1/bracerightbigg
=P/braceleftbiggS(2)
S(1)S(1)
S(0)>1/bracerightbigg
=P/braceleftBigg
log/parenleftbiggS(2)
S(1)/parenrightbigg
+log/parenleftbiggS(1)
S(0)/parenrightbigg
>0/bracerightBigg
However, log/parenleftBig
S(2)
S(1)/parenrightBig
+log/parenleftBig
S(1)
S(0)/parenrightBig
, being the sum of two independent normal random
variables with a common mean .0165 and a common standard deviation .0730, is anormal random variable with mean .0330 and variance 2 (.0730)
2. Consequently,
P/braceleftbiggS(2)
S(0)>1/bracerightbigg
=P/braceleftBigg
Z>−.0330
.0730√
2/bracerightBigg
=P{Z<.31965}
=.6254 .
6.3.4 Poisson and Binomial Random Variables
Rather than attempt to derive a general expression for the distribution of X+Yin
the discrete case, we shall consider some examples.
Example
3eSums of independent Poisson random variables
IfXandYare independent Poisson random variables with respective parameters
λ1andλ2, compute the distribution of X+Y.
Solution Because the event {X+Y=n}may be written as the union of the disjoint
events {X=k,Y=n−k},0…k…n, we have
P{X+Y=n}=n/summationdisplay
k=0P{X=k,Y=n−k}
=n/summationdisplay
k=0P{X=k}P{Y=n−k}
=n/summationdisplay
k=0e−λ1λk
1
k!e−λ2λn−k
2
(n−k)!

<<<PAGE 260>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 247
A First Course in Probability 247
=e−(λ 1+λ2)n/summationdisplay
k=0λk
1λn−k
2
k!(n−k)!
=e−(λ 1+λ2)
n!n/summationdisplay
k=0n!
k!(n−k)!λk
1λn−k
2
=e−(λ 1+λ2)
n!(λ1+λ2)n
Thus, X+Yhas a Poisson distribution with parameter λ1+λ2. .
Example
3fSums of independent binomial random variables
LetXandYbe independent binomial random variables with respective parameters
(n,p) and ( m,p). Calculate the distribution of X+Y.
Solution Recalling the interpretation of a binomial random variable, and without
any computation at all, we can immediately conclude that X+Yis binomial with
parameters (n+m,p). This follows because Xrepresents the number of successes in
nindependent trials, each of which results in a success with probability p; similarly, Y
represents the number of successes in mindependent trials, each of which results in
a success with probability p. Hence, given that XandYare assumed independent,
it follows that X+Yrepresents the number of successes in n+mindependent
trials when each trial has a probability pof resulting in a success. Therefore, X+Y
is a binomial random variable with parameters (n+m,p). To check this conclusion
analytically, note that
P{X+Y=k}=n/summationdisplay
i=0P{X=i,Y=k−i}
=n/summationdisplay
i=0P{X=i}P{Y=k−i}
=n/summationdisplay
i=0/parenleftBigg
n
i/parenrightBigg
piqn−i/parenleftBigg
m
k−i/parenrightBigg
pk−iqm−k+i
where q=1−pand where/parenleftBigg
r
j/parenrightBigg
=0 when j<0. Thus,
P{X+Y=k}=pkqn+m−kn/summationdisplay
i=0/parenleftBigg
n
i/parenrightBigg/parenleftBigg
m
k−i/parenrightBigg
and the conclusion follows upon application of the combinatorial identity
/parenleftBigg
n+m
k/parenrightBigg
=n/summationdisplay
i=0/parenleftBigg
n
i/parenrightBigg/parenleftBigg
m
k−i/parenrightBigg
.

<<<PAGE 261>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 248
248 Chapter 6 Jointly Distributed Random Variables
6.4 Conditional Distributions: Discrete Case
Recall that for any two events EandF, the conditional probability of Egiven Fis
deﬁned, provided that P(F)> 0, by
P(E|F)=P(EF)
P(F)
Hence, if XandYare discrete random variables, it is natural to deﬁne the condi-
tional probability mass function of Xgiven that Y=y,b y
pX|Y(x|y)=P{X=x|Y=y}
=P{X=x,Y=y}
P{Y=y}
=p(x,y)
pY(y)
for all values of ysuch that pY(y)> 0. Similarly, the conditional probability distri-
bution function of Xgiven that Y=yis deﬁned, for all ysuch that pY(y)>0, by
FX|Y(x|y)=P{X…x|Y=y}
=/summationdisplay
a…xpX|Y(a|y)
In other words, the deﬁnitions are exactly the same as in the unconditional case,
except that everything is now conditional on the event that Y=y.I fXis indepen-
dent of Y, then the conditional mass function and the distribution function are the
same as the respective unconditional ones. This follows because if Xis independent
ofY, then
pX|Y(x|y)=P{X=x|Y=y}
=P{X=x,Y=y}
P{Y=y}
=P{X=x}P{Y=y}
P{Y=y}
=P{X=x}
Example
4aSuppose that p(x,y), the joint probability mass function of XandY, is given by
p(0, 0) =.4p(0, 1) =.2p(1, 0) =.1p(1, 1) =.3
Calculate the conditional probability mass function of Xgiven that Y=1.
Solution We ﬁrst note that
pY(1)=/summationdisplay
xp(x,1)=p(0, 1) +p(1, 1) =.5
Hence,
pX|Y(0|1)=p(0, 1)
pY(1)=2
5
and
pX|Y(1|1)=p(1, 1)
pY(1)=3
5.

<<<PAGE 262>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 249
A First Course in Probability 249
Example
4bIfXandYare independent Poisson random variables with respective parameters
λ1andλ2, calculate the conditional distribution of Xgiven that X+Y=n.
Solution We calculate the conditional probability mass function of Xgiven that X+
Y=nas follows:
P{X=k|X+Y=n}=P{X=k,X+Y=n}
P{X+Y=n}
=P{X=k,Y=n−k}
P{X+Y=n}
=P{X=k}P{Y=n−k}
P{X+Y=n}
where the last equality follows from the assumed independence of XandY. Recall-
ing (Example 3e) that X+Yhas a Poisson distribution with parameter λ1+λ2,
we see that the preceding equals
P{X=k|X+Y=n}=e−λ1λk
1
k!e−λ2λn−k
2
(n−k)!/bracketleftBigg
e−(λ 1+λ2)(λ1+λ2)n
n!/bracketrightBigg−1
=n!
(n−k)!k!λk
1λn−k
2
(λ1+λ2)n
=/parenleftBigg
n
k/parenrightBigg/parenleftbiggλ1
λ1+λ2/parenrightbiggk/parenleftbiggλ2
λ1+λ2/parenrightbiggn−k
In other words, the conditional distribution of Xgiven that X+Y=nis the bino-
mial distribution with parameters nandλ1/(λ1+λ2). .
We can also talk about joint conditional distributions, as is indicated in the next
two examples.
Example
4cConsider the multinomial distribution with joint probability mass function
P{Xi=ni,i=1,...,k}=n!
n1!···nk!pn1
1···pnk
k,niÚ0,k/summationdisplay
i=1ni=n
Such a mass function results when nindependent trials are performed, with each
trial resulting in outcome iwith probability pi,/summationtextk
i=1pi=1. The random variables
Xi,i=1,...,k, represent, respectively, the number of trials that result in outcome i,
i=1,...,k. Suppose we are given that njof the trials resulted in outcome j,f o r
j=r+1,...,k, where/summationtextk
j=r+1nj=m…n. Then, because each of the other n−
mtrials must have resulted in one of the outcomes 1, ...,r, it would seem that the
conditional distribution of X1,...,Xris the multinomial distribution on n−mtrials
with respective trial outcome probabilities
P{outcome i|outcome is not any of r+1,...,k}=pi
Fr,i=1,...,r
where Fr=/summationtextri=1piis the probability that a trial results in one of the outcomes
1,...,r.

<<<PAGE 263>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 250
250 Chapter 6 Jointly Distributed Random Variables
Solution To verify this intuition, let n1,...,nr, be such that/summationtextr
i=1ni=n−m. Then
P{X1=n1,...,Xr=nr|Xr+1=nr+1,...Xk=nk}
=P{X1=n1,...,Xk=nk}
P{Xr+1=nr+1,...Xk=nk}
=n!
n1!···n k!pn1
1···pnrrpnr+1
r+1···pnk
k
n!
(n−m)!n r+1!···n k!Fn−mrpnr+1
r+1···pnk
k
where the probability in the denominator was obtained by regarding outcomes
1,...,ras a single outcome having probability Fr, thus showing that the probability
is a multinomial probability on ntrials with outcome probabilities Fr,pr+1,...,pk.
Because/summationtextr
i=1ni=n−m, the preceding can be written as
P{X1=n1,...,Xr=nr|Xr+1=nr+1,...Xk=nk}
=(n−m)!
n1!···nr!/parenleftbiggp1
Fr/parenrightbiggn1
···/parenleftbiggpr
Fr/parenrightbiggnr
and our intuition is upheld. .
Example
4dConsider nindependent trials, with each trial being a success with probability p.
Given a total of ksuccesses, show that all possible orderings of the ksuccesses and
n−kfailures are equally likely.
Solution We want to show that given a total of ksuccesses, each of the/parenleftbign
k/parenrightbig
possible
orderings of ksuccesses and n−kfailures is equally likely. Let Xdenote the number
of successes, and consider any ordering of ksuccesses and n−kfailures, say, o=
(s,...,s,f,...,f).Then
P(o|X=k)=P(o,X=k)
P(X=k)
=P(o)
P(X=k)
=pk(1−p)n−k
/parenleftbign
k/parenrightbig
pk(1−p)n−k
=1/parenleftbignk/parenrightbig .
6.5 Conditional Distributions: Continuous Case
IfXandYhave a joint probability density function f(x,y), then the conditional
probability density function of Xgiven that Y=yis deﬁned, for all values of ysuch
thatfY(y)>0, by
fX|Y(x|y)=f(x,y)
fY(y)
To motivate this deﬁnition, multiply the left-hand side by dxand the right-hand side
by(dx dy)/dy to obtain

<<<PAGE 264>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 251
A First Course in Probability 251
fX|Y(x|y) dx=f(x,y)dx dy
fY(y)dy
LP{x…X…x+dx,y…Y…y+dy}
P{y…Y…y+dy}
=P{x…X…x+dx|y…Y…y+dy}
In other words, for small values of dxanddy,fX|Y(x|y)dx represents the conditional
probability that Xis between xandx+dxgiven that Yis between yandy+dy.
The use of conditional densities allows us to deﬁne conditional probabilities of
events associated with one random variable when we are given the value of a second
random variable. That is, if XandYare jointly continuous, then, for any set A,
P{X∈A|Y=y}=/integraldisplay
AfX|Y(x|y) dx
In particular, by letting A=(−q, a)we can deﬁne the conditional cumulative dis-
tribution function of Xgiven that Y=yby
FX|Y(a|y)KP{X…a|Y=y}=/integraldisplaya
−qfX|Y(x|y) dx
The reader should note that by using the ideas presented in the preceding discussion,we have been able to give workable expressions for conditional probabilities, even
though the event on which we are conditioning (namely, the event {Y=y}) has
probability 0.
IfXandYare independent continuous random variables, the conditional den-
sity of Xgiven that Y=yis just the unconditional density of X. This is so because,
in the independent case,
f
X|Y(x|y)=f(x,y)
fY(y)=fX(x)fY(y)
fY(y)=fX(x)
Example
5aThe joint density of XandYis given by
f(x,y)=/braceleftBigg12
5x(2−x−y) 0<x<1, 0<y<1
0 otherwise
Compute the conditional density of Xgiven that Y=y, where 0 <y<1.
Solution For 0 <x<1, 0<y<1, we have
fX|Y(x|y)=f(x,y)
fY(y)
=f(x,y)/integraltextq
−qf(x,y)dx
=x(2−x−y)
/integraltext1
0x(2−x−y)dx
=x(2−x−y)
2
3−y/2
=6x(2−x−y)
4−3y.

<<<PAGE 265>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 252
252 Chapter 6 Jointly Distributed Random Variables
Example
5bSuppose that the joint density of XandYis given by
f(x,y)=⎧
⎪⎨
⎪⎩e−x/ye−y
y0<x<q,0<y<q
0 otherwise
Find P{X>1|Y=y}.
Solution We ﬁrst obtain the conditional density of Xgiven that Y=y.
fX|Y(x|y)=f(x,y)
fY(y)
=e−x/ye−y/y
e−y/integraltextq
0(1/y)e−x/ydx
=1
ye−x/y
Hence,
P{X>1|Y=y}=/integraldisplayq
11
ye−x/ydx
=−e−x/y/vextendsingle/vextendsingle/vextendsingleq
1
=e−1/y.
Example
5cThe t-distribution
IfZandYare independent, with Zhaving a standard normal distribution and Yhav-
ing a chi-squared distribution with ndegrees of freedom, then the random variable
Tdeﬁned by
T=Z√Y/n=√nZ√
Y
is said to have a t-distribution withndegrees of freedom. As will be seen in Section
7.8, the t-distribution has important applications in statistical inference. At present,
we will content ourselves with computing its density function. This will be accom-
plished by using the conditional density of Tgiven Yto obtain the joint density
function of TandY, from which we will then obtain the marginal density of T.T o
begin, note that because of the independence of ZandY, it follows that the con-
ditional distribution of Tgiven that Y=yis the distribution of√n/yZ , which is
normal with mean 0 and variance n/y. Hence, the conditional density of Tgiven
thatY=yis
fT|Y(t|y)=1/radicalbig
2πn/ye−t2y/2n,−q<t<q
Using the preceding, along with the following formula for the chi-squared densitygiven in Example 3b of this chapter,
f
Y(y)=e−y/2yn/2−1
2n/2/Gamma1(n/2),y>0

<<<PAGE 266>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 253
A First Course in Probability 253
we obtain that the joint density of T,Yis
fT,Y(t,y)=1√
2πn2n/2/Gamma1(n/2)e−t2y/2ne−y/2y(n−1)/ 2
=1√πn2(n+1)/ 2/Gamma1(n/2)e−t2+n
2nyy(n−1)/ 2,y>0,−q<t<q
Letting c=t2+n
2n, and integrating the preceding over all y, gives
fT(t)=/integraldisplayq
0fT,Y(t,y)dy
=1√πn2(n+1)/ 2/Gamma1(n/2)/integraldisplayq
0e−cyy(n−1)/ 2dy
=c−(n+1)/ 2
√πn2(n+1)/ 2/Gamma1(n/2)/integraldisplayq
0e−xx(n−1)/ 2dx (by letting x=cy)
=n(n+1)/ 2/Gamma1/parenleftBig
n+1
2/parenrightBig
√πn(t2+n)(n+1)/ 2/Gamma1/parenleftBig
n
2/parenrightBig/parenleftbigg
because1
c=2n
t2+n/parenrightbigg
=/Gamma1/parenleftBig
n+1
2/parenrightBig
√πn/Gamma1/parenleftBig
n
2/parenrightBig/parenleftBigg
1+t2
n/parenrightBigg−(n+1)/ 2
,−q<t<q
Example
5dThe bivariate normal distribution
One of the most important joint distributions is the bivariate normal distribution.
We say that the random variables X,Yhave a bivariate normal distribution if, for
constants μx,μy,σx>0,σy>0,−1<ρ<1, their joint density function is given,
for all −q<x,y<q,b y
f(x,y)=1
2πσ xσy/radicalbig
1−ρ2exp⎧
⎨
⎩−1
2(1−ρ2)/bracketleftBigg/parenleftbiggx−μx
σx/parenrightbigg2
+/parenleftBigg
y−μy
σy/parenrightBigg2
−2ρ(x−μx)(y−μy)
σxσy⎤⎦⎫
⎪⎬
⎪⎭
We now determine the conditional density of Xgiven that Y=y. In doing so, we
will continually collect all factors that do not depend on xand represent them by the
constants C
i. The ﬁnal constant will then be found by using that/integraltextq
−qfX|Y(x|y) dx=1.
We have
fX|Y(x|y)=f(x,y)
fY(y)
=C1f(x,y)
=C2exp⎧
⎨
⎩−1
2(1−ρ2)/bracketleftBigg/parenleftbiggx−μx
σx/parenrightbigg2
−2ρx(y−μy)
σxσy/bracketrightBigg⎫
⎬
⎭

<<<PAGE 267>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 254
254 Chapter 6 Jointly Distributed Random Variables
=C3exp⎧
⎪⎨
⎪⎩−1
2σ2x(1−ρ2)⎡
⎣x2−2x/parenleftBigg
μx+ρσx
σy(y−μy)/parenrightBigg⎤⎦⎫
⎪⎬
⎪⎭
=C
4exp⎧
⎪⎨
⎪⎩−1
2σ2x(1−ρ2)⎡
⎣x−/parenleftBigg
μx+ρσx
σy(y−μy)/parenrightBigg⎤⎦2⎫
⎪⎬
⎪⎭
Recognizing the preceding equation as a normal density, we can conclude that given
Y=y, the random variable Xis normally distributed with mean μx+ρσx
σy(y−μy)
and variance σ2
x(1−ρ2).Also, because the joint density of Y,Xis exactly the same
as that of X,Y, except that μx,σxare interchanged with μy,σy, it similarly follows
that the conditional distribution of Ygiven X=xis the normal distribution with
mean μy+ρσy
σx(x−μx)and variance σ2
y(1−ρ2).It follows from these results that
the necessary and sufﬁcient condition for the bivariate normal random variables X
andYto be independent is that ρ=0 (a result that also follows directly from their
joint density, because it is only when ρ=0 that the joint density factors into two
terms, one depending only on xand the other only on y).
With C=1
2πσ xσy√
1−ρ2, the marginal density of Xcan be obtained from
fX(x)=/integraldisplayq
−qf(x,y)dy
=C/integraldisplayq
−qexp⎧
⎪⎨
⎪⎩−1
2(1−ρ2)⎡
⎣/parenleftbiggx−μx
σx/parenrightbigg2
+/parenleftBigg
y−μy
σy/parenrightBigg2
−2ρ(x−μx)(y−μy)
σxσy/bracketrightBigg⎫
⎬
⎭dy
Making the change of variables w=y−μ y
σygives
fX(x)=Cσyexp/braceleftBigg
−1
2(1−ρ2)/parenleftbiggx−μx
σx/parenrightbigg2/bracerightBigg
*/integraldisplayq
−qexp/braceleftBigg
−1
2(1−ρ2)/bracketleftbigg
w2−2ρx−μx
σxw/bracketrightbigg/bracerightBigg
dw
=Cσyexp/braceleftBigg
−1
2(1−ρ2)/parenleftbiggx−μx
σx/parenrightbigg2
(1−ρ2)/bracerightBigg
*/integraldisplayq
−qexp/braceleftBigg
−1
2(1−ρ2)/bracketleftbigg
w−ρx−μx
σx/bracketrightbigg2/bracerightBigg
dw
Because
1/radicalbig
2π(1−ρ2)/integraldisplayq
−qexp/braceleftBigg
−1
2(1−ρ2)/bracketleftbigg
w−ρ
σx(x−μx)/bracketrightbigg2/bracerightBigg
dw=1

<<<PAGE 268>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 255
A First Course in Probability 255
we see that
fX(x)=Cσy/radicalBig
2π(1−ρ2)e−(x−μ x)2/2σ2
x
=1√
2πσ xe−(x−μ x)2/2σ2
x
That is, Xis normal with mean μxand variance σ2
x. Similarly, Yis normal with
mean μyand variance σ2
y. .
We can also talk about conditional distributions when the random variables are
neither jointly continuous nor jointly discrete. For example, suppose that Xis a con-
tinuous random variable having probability density function fandNis a discrete
random variable, and consider the conditional distribution of Xgiven that N=n.
Then
P{x<X<x+dx|N=n}
dx
=P{N=n|x<X<x+dx}
P{N=n}P{x<X<x+dx}
dx
and letting dxapproach 0 gives
lim
dx→0P{x<X<x+dx|N=n}
dx=P{N=n|X=x}
P{N=n}f(x)
thus showing that the conditional density of Xgiven that N=nis given by
fX|N(x|n)=P{N=n|X=x}
P{N=n}f(x)
Example
5eConsider n+mtrials having a common probability of success. Suppose, however,
that this success probability is not ﬁxed in advance but is chosen from a uniform
(0, 1) population. What is the conditional distribution of the success probabilitygiven that the n+mtrials result in nsuccesses?
Solution If we let Xdenote the probability that a given trial is a success, then X
is a uniform (0, 1) random variable. Also, given that X=x,t h e n+mtrials are
independent with common probability of success x,s o N, the number of successes,
is a binomial random variable with parameters (n+m,x). Hence, the conditional
density of Xgiven that N=nis
fX|N(x|n)=P{N=n|X=x}fX(x)
P{N=n}
=/parenleftBigg
n+m
n/parenrightBigg
xn(1−x)m
P{N=n}0<x<1
=cxn(1−x)m
where cdoes not depend on x. Thus, the conditional density is that of a beta random
variable with parameters n+1,m+1.
The preceding result is quite interesting, for it states that if the original or prior
(to the collection of data) distribution of a trial success probability is uniformly
distributed over (0, 1) [or, equivalently, is beta with parameters (1, 1)], then the

<<<PAGE 269>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 256
256 Chapter 6 Jointly Distributed Random Variables
posterior (or conditional) distribution given a total of nsuccesses in n+mtrials is
beta with parameters (1+n,1+m). This is valuable, for it enhances our intuition
as to what it means to assume that a random variable has a beta distribution. .
*6.6 Order Statistics
LetX1,X2,...,Xnbenindependent and identically distributed continuous random
variables having a common density fand distribution function F. Deﬁne
X(1)=smallest of X1,X2,...,Xn
X(2)=second smallest of X1,X2,...,Xn
#
##
X
(j)=jth smallest of X1,X2,...,Xn
###
X
(n)=largest of X1,X2,...,Xn
The ordered values X(1)…X(2)…···…X(n)are known as the order statistics corre-
sponding to the random variables X1,X2,...,Xn. In other words, X(1),...,X(n)are
the ordered values of X1,...,Xn.
The joint density function of the order statistics is obtained by noting that the
order statistics X(1),...,X(n)will take on the values x1…x2…···…xnif and only
if, for some permutation (i1,i2,...,in)of(1, 2,...,n),
X1=xi1,X2=xi2,...,Xn=xin
Since, for any permutation (i1,...,in)of(1, 2,...,n),
P/braceleftbigg
xi1−ε
2<X1<xi1+ε
2,...,xin−ε
2<Xn<xin+ε
2/bracerightbigg
LεnfX1,···,Xn(xi1,...,xin)
=εnf(xi1)···f(xin)
=εnf(x1)···f(xn)
it follows that, for x1<x2<···<xn,
P/braceleftbigg
x1−ε
2<X(1)<x1+ε
2,...,xn−ε
2<X(n)<xn+ε
2/bracerightbigg
Ln!εnf(x1)···f(xn)
Dividing by εnand letting ε→0 yields
fX(1),...,X(n)(x1,x2,...,xn)=n!f(x1)···f(xn)x1<x2<···<xn (6.1)
Equation (6.1) is most simply explained by arguing that, in order for the vector
/angbracketleftX(1),...,X(n)/angbracketrightto equal /angbracketleftx1,...,xn/angbracketright, it is necessary and sufﬁcient for /angbracketleftX1,...,Xn/angbracketright

<<<PAGE 270>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 257
A First Course in Probability 257
to equal one of the n! permutations of /angbracketleftx1,...,xn/angbracketright. Since the probability (density)
that/angbracketleftX1,...,Xn/angbracketrightequals any given permutation of /angbracketleftx1,...,xn/angbracketrightis just f(x1)···f(xn),
Equation (6.1) follows.
Example
6aAlong a road 1 mile long are 3 people “distributed at random.” Find the probability
that no 2 people are less than a distance of dmiles apart when d…1
2.
Solution Let us assume that “distributed at random” means that the positions of the
3 people are independent and uniformly distributed over the road. If Xidenotes the
position of the ith person, then the desired probability is P{X(i)>X(i−1)+d,i=
2, 3}. Because
fX(1),X(2),X(3)(x1,x2,x3)=3! 0 <x1<x2<x3<1
it follows that
P{X(i)>X(i−1)+d,i=2, 3}=/integraldisplay/integraldisplay/integraldisplay
xi>xj−1+dfX(1),X(2),X(3)(x1,x2,x3)dx1dx2dx3
=3!/integraldisplay1−2d
0/integraldisplay1−d
x1+d/integraldisplay1
x2+ddx3dx2dx1
=6/integraldisplay1−2d
0/integraldisplay1−d
x1+d(1−d−x2)dx2dx1
=6/integraldisplay1−2d
0/integraldisplay1−2d−x 1
0y2dy2dx1
where we have made the change of variables y2=1−d−x2. Continuing the string
of equalities yields
=3/integraldisplay1−2d
0(1−2d−x1)2dx1
=3/integraldisplay1−2d
0y2
1dy1
=(1−2d)3
Hence, the desired probability that no 2 people are within a distance dof each
other when 3 people are uniformly and independently distributed over an interval
of size 1 is (1−2d)3when d…1
2. In fact, the same method can be used to prove
that when npeople are distributed at random over the unit interval, the desired
probability is
[1−(n−1)d]nwhen d…1
n−1
The proof is left as an exercise. .
The density function of the jth-order statistic X(j)can be obtained either by inte-
grating the joint density function (6.1) or by direct reasoning as follows: In order for
X(j)to equal x, it is necessary for j−1o ft h en values X1,...,Xnto be less than
x,n−jof them to be greater than x, and 1 of them to equal x. Now, the probability
density that any given set of j−1o ft h eX i’s are less than x, another given set of
n−jare all greater than x, and the remaining value is equal to xequals
[F(x)]j−1[1−F(x)]n−jf(x)

<<<PAGE 271>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 258
258 Chapter 6 Jointly Distributed Random Variables
Hence, since there are
/parenleftBigg
n
j−1,n−j,1/parenrightBigg
=n!
(n−j)!(j−1)!
different partitions of the nrandom variables X1,...,Xninto the preceding three
groups, it follows that the density function of X(j)is given by
fX(j)(x)=n!
(n−j)!(j−1)![F(x)]j−1[1−F(x)]n−jf(x) (6.2)
Example
6bWhen a sample of 2n +1 random variables (that is, when 2n +1 independent and
identically distributed random variables) is observed, the (n+1)smallest is called
thesample median . If a sample of size 3 from a uniform distribution over (0, 1) is
observed, ﬁnd the probability that the sample median is between1
4and3
4.
Solution From Equation (6.2), the density of X(2)is given by
fX(2)(x)=3!
1!1!x(1−x) 0<x<1
Hence,
P/braceleftbigg1
4<X(2)<3
4/bracerightbigg
=6/integraldisplay3/4
1/4x(1−x)dx
=6/braceleftBigg
x2
2−x3
3/bracerightBigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglex=3/ 4
x=1/ 4=11
16.
The cumulative distribution function of X(j)can be found by integrating Equa-
tion (6.2). That is,
FX(j)(y)=n!
(n−j)!(j−1)!/integraldisplayy
−q[F(x)]j−1[1−F(x)]n−jf(x)dx (6.3)
However, FX(j)(y)could also have been derived directly by noting that the jth order
statistic is less than or equal to yif and only if there are jor more of the Xi’s that are
less than or equal to y. Thus, because the number of Xi’s that are less than or equal
toyis a binomial random variable with parameters n,p=F(y), it follows that
FX(j)(y)=P{X(j)…y}= P{jor more of the Xi’s are …y}
=n/summationdisplay
k=j/parenleftBigg
n
k/parenrightBigg
[F(y)]k[1−F(y)]n−k(6.4)
If, in Equations (6.3) and (6.4), we take Fto be the uniform (0, 1) distribution
[that is, f(x)=1, 0<x<1], then we obtain the interesting analytical identity
n/summationdisplay
k=j/parenleftBigg
nk/parenrightBigg
y
k(1−y)n−k=n!
(n−j)!(j−1)!/integraldisplayy
0xj−1(1−x)n−jdx 0…y…1 (6.5)

<<<PAGE 272>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 259
A First Course in Probability 259
By employing the same type of argument that we used in establishing
Equation (6.2), we can show that the joint density function of the order statistics
X(i)andX(j)when i<jis
fX(i),X(j)(xi,xj)=n!
(i−1)!(j−i−1)!(n −j)![F(xi)]i−1(6.6)
*[F(xj)−F(xi)]j−i−1[1−F(xj)]n−jf(xi)f(xj)
for all xi<xj.
Example
6cDistribution of the range of a random sample
Suppose that nindependent and identically distributed random variables X1,X2,...,
Xnare observed. The random variable Rdeﬁned by R=X(n)−X(1)is called the
range of the observed random variables. If the random variables Xihave distribution
function Fand density function f, then the distribution of Rcan be obtained from
Equation (6.6) as follows: For aÚ0,
P{R…a}=P {X(n)−X(1)…a}
=/integraldisplay/integraldisplay
xn−x1…afX(1),X(n)(x1,xn)dx1dxn
=/integraldisplayq
−q/integraldisplayx1+a
x1n!
(n−2)![F(xn)−F(x1)]n−2f(x1)f(xn)dxndx1
Making the change of variable y=F(xn)−F(x1),dy=f(xn)dxnyields
/integraldisplayx1+a
x1[F(xn)−F(x1)]n−2f(xn)dxn=/integraldisplayF(x1+a)−F (x1)
0yn−2dy
=1
n−1[F(x1+a)−F(x1)]n−1
Thus,
P{R…a}=n/integraldisplayq
−q[F(x1+a)−F(x1)]n−1f(x1)dx1 (6.7)
Equation (6.7) can be evaluated explicitly only in a few special cases. One such case
is when the Xi’s are all uniformly distributed on (0, 1). In this case, we obtain, from
Equation (6.7), that for 0 <a<1,
P{R<a}= n/integraldisplay1
0[F(x1+a)−F(x1)]n−1f(x1)dx1
=n/integraldisplay1−a
0an−1dx1+n/integraldisplay1
1−a(1−x1)n−1dx1
=n(1−a)an−1+an
Differentiation yields the density function of the range: given in this case by
fR(a)=/braceleftBigg
n(n−1)an−2(1−a) 0…a…1
0 otherwise
That is, the range of nindependent uniform (0, 1) random variables is a beta random
variable with parameters n−1, 2. .

<<<PAGE 273>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 260
260 Chapter 6 Jointly Distributed Random Variables
6.7 Joint Probability Distribution of Functions of Random Variables
LetX1andX2be jointly continuous random variables with joint probability density
function fX1,X2. It is sometimes necessary to obtain the joint distribution of the ran-
dom variables Y1andY2, which arise as functions of X1andX2. Speciﬁcally, suppose
thatY1=g1(X1,X2)andY2=g2(X1,X2)for some functions g1andg2.
Assume that the functions g1andg2satisfy the following conditions:
1. The equations y1=g1(x1,x2)andy2=g2(x1,x2)can be uniquely solved for x1
andx2in terms of y1andy2, with solutions given by, say, x1=h1(y1,y2),x2=
h2(y1,y2).
2. The functions g1andg2have continuous partial derivatives at all points (x1,x2)
and are such that the 2 *2 determinant
J(x1,x2)=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle∂g
1
∂x1∂g1
∂x2
∂g2
∂x1∂g2
∂x2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleK∂g
1
∂x1∂g2
∂x2−∂g1
∂x2∂g2
∂x1Z0
at all points (x1,x2).
Under these two conditions, it can be shown that the random variables Y1and
Y2are jointly continuous with joint density function given by
fY1Y2(y1,y2)=fX1,X2(x1,x2)|J(x1,x2)|−1(7.1)
where x1=h1(y1,y2),x2=h2(y1,y2).
A proof of Equation (7.1) would proceed along the following lines:
P{Y1…y1,Y2…y2}=/integraldisplay/integraldisplay
(x1,x2):
g1(x1,x2)…y1
g2(x1,x2)…y2fX1,X2(x1,x2)dx1dx2 (7.2)
The joint density function can now be obtained by differentiating Equation (7.2)
with respect to y1andy2. That the result of this differentiation will be equal to the
right-hand side of Equation (7.1) is an exercise in advanced calculus whose proof
will not be presented in this book.
Example
7aLetX1andX2be jointly continuous random variables with probability density func-
tionfX1,X2.L e tY 1=X1+X2,Y2=X1−X2. Find the joint density function of Y1
andY2in terms of fX1,X2.
Solution Letg1(x1,x2)=x1+x2andg2(x1,x2)=x1−x2. Then
J(x1,x2)=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle11
1−1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle=− 2
Also, since the equations y
1=x1+x2andy2=x1−x2have x1=(y1+y2)/2,x2=
(y1−y2)/2 as their solution, it follows from Equation (7.1) that the desired density is
fY1,Y2(y1,y2)=1
2fX1,X2/parenleftbiggy1+y2
2,y1−y2
2/parenrightbigg

<<<PAGE 274>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 261
A First Course in Probability 261
For instance, if X1andX2are independent uniform (0, 1) random variables, then
fY1,Y2(y1,y2)=/braceleftBigg1
20…y1+y2…2, 0…y1−y2…2
0 otherwise
or ifX1andX2are independent exponential random variables with respective param-
etersλ1andλ2, then
fY1,Y2(y1,y2)
=⎧
⎪⎪⎪⎨
⎪⎪⎪⎩λ
1λ2
2exp/braceleftBigg
−λ1/parenleftbiggy1+y2
2/parenrightbigg
−λ2/parenleftbiggy1−y2
2/parenrightbigg/bracerightBigg
y1+y2Ú0,y1−y2Ú0
0 otherwise
Finally, if X1andX2are independent standard normal random variables, then
fY1,Y2(y1,y2)=1
4πe−[(y 1+y2)2/8+(y 1−y2)2/8]
=1
4πe−(y2
1+y2
2)/4
=1√
4πe−y2
1/41√
4πe−y2
2/4
Thus, not only do we obtain (in agreement with Proposition 3.2) that both X1+X2
andX1−X2are normal with mean 0 and variance 2, but we also conclude that these
two random variables are independent. (In fact, it can be shown that if X1andX2
are independent random variables having a common distribution function F, then
X1+X2will be independent of X1−X2if and only if Fis a normal distribution
function.) .
Example
7bLet (X ,Y) denote a random point in the plane, and assume that the rectangular
coordinates XandYare independent standard normal random variables. We are
interested in the joint distribution of R,Θ, the polar coordinate representation of
(x,y). (See Figure 6.4.)
Y
XR
/H9052
Figure 6.4 •=Random point. (X,Y)=(R,Θ).

<<<PAGE 275>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 262
262 Chapter 6 Jointly Distributed Random Variables
Suppose ﬁrst that XandYare both positive. For xandypositive, letting r=
g1(x,y)=/radicalbig
x2+y2andθ=g2(x,y)=tan−1y/x, we see that
∂g1
∂x=x/radicalbig
x2+y2
∂g1
∂y=y/radicalbig
x2+y2
∂g2
∂x=1
1+(y/x)2/parenleftbigg−y
x2/parenrightbigg
=−y
x2+y2
∂g2
∂y=1
x[1+(y/x)2]=x
x2+y2
Hence,
J(x,y)=x2
(x2+y2)3/2+y2
(x2+y2)3/2=1/radicalbig
x2+y2=1
r
Because the conditional joint density function of X,Ygiven that they are both
positive is
f(x,y|X>0,Y>0)=f(x,y)
P(X>0,Y>0)=2
πe−(x2+y2)/2,x>0,y>0
we see that the conditional joint density function of R=/radicalbig
X2+Y2and/Theta1=
tan−1(Y/X), given that XandYare both positive, is
f(r,θ|X>0,Y>0)=2
πre−r2/2,0<θ<π / 2, 0 <r<q
Similarly, we can show that
f(r,θ|X<0,Y>0)=2
πre−r2/2,π/2<θ<π ,0<r<q
f(r,θ|X<0,Y<0)=2
πre−r2/2,π<θ< 3π/2, 0 <r<q
f(r,θ|X>0,Y<0)=2
πre−r2/2,3π/2<θ<2π ,0<r<q
As the joint density is an equally weighted average of these four conditional joint
densities, we obtain that the joint density of R,Θis given by
f(r,θ)=1
2πre−r2/20<θ<2π ,0 <r<q
Now, this joint density factors into the marginal densities for RandΘ,s oRandΘ
are independent random variables, with Θbeing uniformly distributed over (0, 2π)
andRhaving the Rayleigh distribution with density
f(r)=re−r2/20<r<q
(For instance, when one is aiming at a target in the plane, if the horizontal and verti-
cal miss distances are independent standard normals, then the absolute value of the
error has the preceding Rayleigh distribution.)
This result is quite interesting, for it certainly is not evident a priori that a ran-
dom vector whose coordinates are independent standard normal random variableswill have an angle of orientation that not only is uniformly distributed, but also is
independent of the vector’s distance from the origin.

<<<PAGE 276>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 263
A First Course in Probability 263
If we wanted the joint distribution of R2andΘ, then, since the transformation
d=g1(x,y)=x2+y2andθ=g2(x,y)=tan−1y/xhas the Jacobian
J=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2x 2y
−y
x2+y2x
x2+y2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle=2
it follows that
f(d,θ)=1
2e−d/21
2π0<d<q,0 <θ<2π
Therefore, R2andΘare independent, with R2having an exponential distribution
with parameter1
2. But because R2=X2+Y2, it follows by deﬁnition that R2has
a chi-squared distribution with 2 degrees of freedom. Hence, we have a veriﬁcation
of the result that the exponential distribution with parameter1
2is the same as the
chi-squared distribution with 2 degrees of freedom.
The preceding result can be used to simulate (or generate) normal random vari-
ables by making a suitable transformation on uniform random variables. Let U1and
U2be independent random variables, each uniformly distributed over (0, 1). We
will transform U1,U2into two independent unit normal random variables X1and
X2by ﬁrst considering the polar coordinate representation (R,Θ)of the random
vector (X1,X2). From the preceding, R2andΘwill be independent, and, in addi-
tion, R2=X2
1+X2
2will have an exponential distribution with parameter λ=1
2.
But−2l o g U1has such a distribution, since, for x>0,
P{−2l o gU1<x}= P/braceleftbigg
logU1>−x
2/bracerightbigg
=P{U1>e−x/2}
=1−e−x/2
Also, because 2 πU2is a uniform (0, 2π) random variable, we can use it to gener-
ateΘ. That is, if we let
R2=− 2l o gU1
Θ=2πU2
then R2can be taken to be the square of the distance from the origin and θcan be
taken to be the angle of orientation of (X1,X2). Now, since X1=RcosΘ,X2=
RsinΘ, it follows that
X1=/radicalbig
−2l o g U1cos(2πU2)
X2=/radicalbig
−2l o g U1sin(2πU2)
are independent standard normal random variables. .
Example
7cIfXandYare independent gamma random variables with parameters (α,λ)and
(β,λ), respectively, compute the joint density of U=X+YandV=X/(X+Y).
Solution The joint density of XandYis given by
fX,Y(x,y)=λe−λx(λx)α−1
/Gamma1(α)λe−λy(λy)β−1
/Gamma1(β)
=λα+β
/Gamma1(α)/Gamma1(β)e−λ(x+y)xα−1yβ−1

<<<PAGE 277>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 264
264 Chapter 6 Jointly Distributed Random Variables
Now, if g1(x,y)=x+y,g2(x,y)=x/(x+y), then
∂g1
∂x=∂g1
∂y=1∂g2
∂x=y
(x+y)2∂g2
∂y=−x
(x+y)2
so
J(x,y)=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle11
y
(x+y)2−x
(x+y)2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle=−1
x+y
Finally, as the equations u=x+y,v=x/(x+y)have as their solutions x=uv,
y=u(1−v), we see that
fU,V(u,v)=fX,Y[uv,u(1−v)]u
=λe−λu(λu)α+β−1
/Gamma1(α+β)vα−1(1−v)β−1/Gamma1(α+β)
/Gamma1(α)/Gamma1(β)
Hence, X+YandX/(X+Y)are independent, with X+Yhaving a gamma dis-
tribution with parameters (α+β,λ)andX/(X+Y)having a beta distribution with
parameters (α,β). The preceding reasoning also shows that B(α,β), the normalizing
factor in the beta density, is such that
B(α,β)K/integraldisplay1
0vα−1(1−v)β−1dv
=/Gamma1(α)/Gamma1(β)
/Gamma1(α+β)
This entire result is quite interesting. For suppose there are n+mjobs to be per-
formed, each (independently) taking an exponential amount of time with rate λto
be completed and suppose that we have two workers to perform these jobs. Worker
I will do jobs 1, 2, ...,n, and worker II will do the remaining mjobs. If we let Xand
Ydenote the total working times of workers I and II, respectively, then (either from
the foregoing result or from Example 3b) XandYwill be independent gamma ran-
dom variables having parameters (n,λ)and(m,λ), respectively. It then follows that
independently of the working time needed to complete all n+mjobs (that is, of
X+Y), the proportion of this work that will be performed by worker I has a beta
distribution with parameters (n, m). .
When the joint density function of the nrandom variables X1,X2,...,Xnis given
and we want to compute the joint density function of Y1,Y2,...,Yn, where
Y1=g1(X1,...,Xn)Y2=g2(X1,...,Xn),... Yn=gn(X1,...,Xn)
the approach is the same—namely, we assume that the functions gihave continuous
partial derivatives and that the Jacobian determinant
J(x1,...,xn)=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle∂g
1
∂x1∂g1
∂x2···∂g1
∂xn
∂g2
∂x1∂g2
∂x2···∂g2
∂xn
∂gn
∂x1∂gn
∂x2···∂gn
∂xn/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleZ0
at all points (x
1,...,xn). Furthermore, we suppose that the equations y1=
g1(x1,...,xn),y2=g2(x1,...,xn),...,yn=gn(x1,...,xn)have a unique solution,

<<<PAGE 278>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 265
A First Course in Probability 265
say,x1=h1(y1,...,yn),...,xn=hn(y1,...,yn). Under these assumptions, the joint
density function of the random variables Yiis given by
fY1,...,Yn(y1,...,yn)=fX1,...,Xn(x1,...,xn)|J(x1,...,xn)|−1(7.3)
where xi=hi(y1,...,yn),i=1, 2,...,n.
Example
7dLetX1,X2, and X3be independent standard normal random variables. If Y1=X1+
X2+X3,Y2=X1−X2, and Y3=X1−X3, compute the joint density function
ofY1,Y2,Y3.
Solution Letting Y1=X1+X2+X3,Y2=X1−X2,Y3=X1−X3, the Jacobian
of these transformations is given by
J=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle111
1−10
10 −1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle=3
As the preceding transformations yield that
X
1=Y1+Y2+Y3
3X2=Y1−2Y2+Y3
3X3=Y1+Y2−2Y3
3
we see from Equation (7.3) that
fY1,Y2,Y3(y1,y2,y3)
=1
3fX1,X2,X3/parenleftbiggy1+y2+y3
3,y1−2y2+y3
3,y1+y2−2y3
3/parenrightbigg
Hence, as
fX1,X2,X3(x1,x2,x3)=1
(2π)3/2e−/summationtext3
i=1x2
i/2
we see that
fY1,Y2,Y3(y1,y2,y3)=1
3(2π)3/2e−Q(y 1,y2,y3)/2
where
Q(y 1,y2,y3)
=/parenleftbiggy1+y2+y3
3/parenrightbigg2
+/parenleftbiggy1−2y2+y3
3/parenrightbigg2
+/parenleftbiggy1+y2−2y3
3/parenrightbigg2
=y2
1
3+2
3y2
2+2
3y23−2
3y2y3 .
Example
7eLetX1,X2,...,Xnbe independent and identically distributed exponential random
variables with rate λ.L e t
Yi=X1+ ··· + Xii=1,...,n
(a) Find the joint density function of Y1,...,Yn.
(b) Use the result of part (a) to ﬁnd the density of Yn.

<<<PAGE 279>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 266
266 Chapter 6 Jointly Distributed Random Variables
Solution (a) The Jacobian of the transformations Y1=X1,Y2=X1+X2,...,
Yn=X1+ ··· + Xnis
J=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1000 ··· 0
1100 ··· 0
1110 ··· 0
··· ···
··· ···
1111 ··· 1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
Since only the ﬁrst term of the determinant will be nonzero, we have J=1.
Now, the joint density function of X
1,...,Xnis given by
fX1,...,Xn(x1,...,xn)=n/productdisplay
i=1λe−λx i0<xi<q,i=1,...,n
Hence, because the preceding transformations yield
X1=Y1,X2=Y2−Y1,...,Xi=Yi−Yi−1,...,Xn=Yn−Yn−1
it follows from Equation (7.3) that the joint density function of Y1,...,Ynis
fY1,...,Yn(y1,y2,...,yn)
=fX1,...,Xn(y1,y2−y1,...,yi−yi−1,...,yn−yn−1)
=λnexp⎧
⎪⎨
⎪⎩−λ⎡
⎣y1+n/summationdisplay
i=2(yi−yi−1)⎤⎦⎫
⎪⎬
⎪⎭
=λ
ne−λy n0<y1,0<yi−yi−1,i=2,...,n
=λne−λy n0<y1<y2<···<yn
(b) To obtain the marginal density of Yn, let us integrate out the other variables
one at a time. Doing this gives
fY2,...,Yn(y2,...,yn)=/integraldisplayy2
0λne−λy ndy1
=λny2e−λy n0<y2<y3<···<yn
Continuing, we obtain
fY3,...,Yn(y3,...,yn)=/integraldisplayy3
0λny2e−λy ndy2
=λny2
3
2e−λy n0<y3<y4<···<yn
The next integration yields
fY4,...,Yn(y4,...,yn)=λny3
4
3!e−λy n0<y4<···<yn
Continuing in this fashion gives
fYn(yn)=λnyn−1
n
(n−1)!e−λy n0<yn
which, in agreement with the result obtained in Example 3b, shows that X1+
··· + Xnis a gamma random variable with parameters nandλ. .

<<<PAGE 280>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 267
A First Course in Probability 267
*6.8 Exchangeable Random Variables
The random variables X1,X2,...,Xnare said to be exchangeable if, for every per-
mutation i1,...,inof the integers 1, ...,n,
P{Xi1…x1,Xi2…x2,...,Xin…xn}=P{X1…x1,X2…x2,...,Xn…xn}
for all x1,...,xn. That is, the nrandom variables are exchangeable if their joint dis-
tribution is the same no matter in which order the variables are observed.
Discrete random variables will be exchangeable if
P{Xi1=x1,Xi2=x2,...,Xin=xn}=P{X1=x1,X2=x2,...,Xn=xn}
for all permutations i1,...,in, and all values x1,...,xn. This is equivalent to stating
thatp(x 1,x2,...,xn)=P{X1=x1,...,Xn=xn}is a symmetric function of the
vector (x1,...,xn), which means that its value does not change when the values of
the vector are permuted.
Example
8aSuppose that balls are withdrawn one at a time and without replacement from an
urn that initially contains nballs, of which kare considered special, in such a man-
ner that each withdrawal is equally likely to be any of the balls that remains in theurn at the time. Let X
i=1i ft h e ith ball withdrawn is special and let Xi=0o t h -
erwise. We will show that the random variables X1,...,Xnare exchangeable. To
do so, let (x1,...,xn)be a vector consisting of kones and n−kzeros. However,
before considering the joint mass function evaluated at (x1,...,xn), let us try to gain
some insight by considering a ﬁxed such vector—for instance, consider the vector(1, 1, 0, 1, 0, ...,0 ,1), which is assumed to have kones and n−kzeros. Then
p(1, 1, 0, 1, 0, ...,0 ,1)=k
nk−1
n−1n−k
n−2k−2
n−3n−k−1
n−4···1
21
1
which follows because the probability that the ﬁrst ball is special is k/n, the con-
ditional probability that the next one is special is (k−1)/(n −1), the conditional
probability that the next one is not special is (n−k)/(n −2), and so on. By the same
argument, it follows that p(x 1,...,xn)can be expressed as the product of nfractions.
The successive denominator terms of these fractions will go from ndown to 1. The
numerator term at the location where the vector (x1,...,xn)is 1 for the ith time is
k−(i−1), and where it is 0 for the ith time it is n−k−(i−1). Hence, since
the vector (x1,...,xn)consists of kones and n−kzeros, we obtain
p(x 1,...,xn)=k!(n−k)!
n!xi=0, 1,n/summationdisplay
i=1xi=k
Since this is a symmetric function of (x1,...,xn), it follows that the random variables
are exchangeable. .
Remark Another way to obtain the preceding formula for the joint probability
mass function is to regard all the nballs as distinguishable from one another. Then,
since the outcome of the experiment is an ordering of these balls, it follows thatthere are n! equally likely outcomes. Finally, because the number of outcomes having
special and nonspecial balls in speciﬁed places is equal to the number of ways of
permuting the special and the nonspecial balls among themselves, namely k!(n−k)!,
we obtain the preceding mass function. .

<<<PAGE 281>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 268
268 Chapter 6 Jointly Distributed Random Variables
It is easily seen that if X1,X2,...,Xnare exchangeable, then each Xihas the
same probability distribution. For instance, if XandYare exchangeable discrete
random variables, then
P{X=x}=/summationdisplay
yP{X=x,Y=y}=/summationdisplay
yP{X=y,Y=x}= P{Y=x}
For example, it follows from Example 8a that the ith ball withdrawn will be special
with probability k/n, which is intuitively clear, since each of the nballs is equally
likely to be the ith one selected.
Example
8bIn Example 8a, let Y1denote the selection number of the ﬁrst special ball withdrawn,
letY2denote the additional number of balls that are then withdrawn until the second
special ball appears, and, in general, let Yidenote the additional number of balls
withdrawn after the (i−1)special ball is selected until the ith is selected, i=1,...,k.
For instance, if n=4,k=2 and X1=1,X2=0,X3=0,X4=1, then Y1=1,Y2=
3. Now, Y1=i1,Y2=i2,...,Yk=ik3Xi1=Xi1+i2=···= Xi1+···+i k=1,Xj=0,
otherwise; thus, from the joint mass function of the Xi, we obtain
P{Y1=i1,Y2=i2,...,Yk=ik}=k!(n−k)!
n!i1+ ··· + ik…n
Hence, the random variables Y1,...,Ykare exchangeable. Note that it follows from
this result that the number of cards one must select from a well-shufﬂed deck until
an ace appears has the same distribution as the number of additional cards one must
select after the ﬁrst ace appears until the next one does, and so on. .
Example
8cThe following is known as Polya’s urn model: Suppose that an urn initially contains
nred and mblue balls. At each stage, a ball is randomly chosen, its color is noted,
and it is then replaced along with another ball of the same color. Let Xi=1i ft h eith
ball selected is red and let it equal 0 if the ith ball is blue, iÚ1. To obtain a feeling
for the joint probabilities of these Xi, note the following special cases:
P{X1=1,X2=1,X3=0,X4=1,X5=0}
=n
n+mn+1
n+m+1m
n+m+2n+2
n+m+3m+1
n+m+4
=n(n+1)(n+2)m(m +1)
(n+m)(n +m+1)(n+m+2)(n+m+3)(n+m+4)
and
P{X1=0,X2=1,X3=0,X4=1,X5=1}
=m
n+mn
n+m+1m+1
n+m+2n+1
n+m+3n+2
n+m+4
=n(n+1)(n+2)m(m +1)
(n+m)(n +m+1)(n+m+2)(n+m+3)(n+m+4)
By the same reasoning, for any sequence x1,...,xkthat contains rones and k−r
zeros, we have
P{X1=x1,...,Xk=xk}
=n(n+1)···(n+r−1)m(m +1)···(m+k−r−1)
(n+m)···(n+m+k−1)

<<<PAGE 282>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 269
A First Course in Probability 269
Therefore, for any value of k, the random variables X1,...,Xkare exchangeable.
An interesting corollary of the exchangeability in this model is that the proba-
bility that the ith ball selected is red is the same as the probability that the ﬁrst ball
selected is red, namely,n
n+m. (For an intuitive argument for this initially nonintuitive
result, imagine that all the n+mballs initially in the urn are of different types. That
is, one is a red ball of type 1, one is a red ball of type 2, ..., one is a red ball type of
n, one is a blue ball of type 1, and so on, down to the blue ball of type m. Suppose
that when a ball is selected it is replaced along with another of its type. Then, by
symmetry, the ith ball selected is equally likely to be of any of the n+mdistinct
types. Because nof these n+mtypes are red, the probability isn
n+m.) .
Our ﬁnal example deals with continuous random variables that are exchangeable.
Example
8dLetX1,X2,...,Xnbe independent uniform (0, 1) random variables, and denote their
order statistics by X(1),...,X(n). That is, X(j)is the jth smallest of X1,X2,...,Xn.
Also, let
Y1=X(1),
Yi=X(i)−X(i−1),i=2,...n
Show that Y1,...,Ynare exchangeable.
Solution The transformations
y1=x1,yi=xi−xi−1,i=2,...,n
yield
xi=y1+ ··· + yii=1,...,n
As it is easy to see that the Jacobian of the preceding transformations is equal to 1,so, from Equation (7.3), we obtain
f
Y1,...,Yn(y1,y2,...,yn)=f(y1,y1+y2,...,y1+ ··· + yn)
where fis the joint density function of the order statistics. Hence, from Equation (6.1),
we obtain that
fY1,...,Yn(y1,y2,...,yn)=n!0 <y1<y1+y2<···<y1+ ··· + yn<1
or, equivalently,
fY1,...,Yn(y1,y2,...,yn)=n!0 <yi<1,i=1,...,n,y1+ ··· + yn<1
Because the preceding joint density is a symmetric function of y1,...,yn, we see that
the random variables Y1,...,Ynare exchangeable. .

<<<PAGE 283>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 270
270 Chapter 6 Jointly Distributed Random Variables
Summary
Thejoint cumulative probability distribution function of
the pair of random variables XandYis deﬁned by
F(x,y)=P{X…x,Y…y}− q<x,y<q
All probabilities regarding the pair can be obtained from
F. To ﬁnd the individual probability distribution functions
ofXandY,u s e
FX(x)=limy→qF(x,y)FY(y)=limx→qF(x,y)
IfXandYare both discrete random variables, then
their joint probability mass function is deﬁned by
p(i,j)=P{X=i,Y=j}
The individual mass functions are
P{X=i}=/summationdisplay
jp(i,j) P{Y=j}=/summationdisplay
ip(i,j)
The random variables Xand Yare said to be
jointly continuous if there is a function f(x,y), called the
joint probability density function, such that for any two-
dimensional set C,
P{(X,Y)∈C}=/integraldisplay/integraldisplay
Cf(x,y)dx dy
It follows from the preceding formula that
P{x<X<x+dx,y<Y<y+dy}Lf(x,y)dx dy
IfXandYare jointly continuous, then they are individu-
ally continuous with density functions
fX(x)=/integraldisplayq
−qf(x,y)dy f Y(y)=/integraldisplayq
−qf(x,y)dx
The random variables XandYareindependent if, for
all sets AandB,
P{X∈A,Y∈B}=P {X∈A}P{Y∈B}
If the joint distribution function (or the joint probability
mass function in the discrete case, or the joint density func-
tion in the continuous case) factors into a part depending
only on xand a part depending only on y,t h e n XandY
are independent.In general, the random variables X1,...,Xnare inde-
pendent if, for all sets of real numbers A1,...,An,
P{X1∈A1,...,Xn∈An}=P{X1∈A1}···P{Xn∈An}
IfXandYare independent continuous random vari-
ables, then the distribution function of their sum can beobtained from the identity
F
X+Y(a)=/integraldisplayq
−qFX(a−y)fY(y)dy
IfXi,i=1,...,n, are independent normal ran-
dom variables with respective parameters μiandσ2
i,i=
1,...,n,t h e nn/summationtext
i=1Xiis normal with parametersn/summationtext
i=1μiand
n/summationtext
i=1σ2
i.
IfXi,i=1,...,n, are independent Poisson random
variables with respective parameters λi,i=1,...,n,t h e n
n/summationtext
i=1Xiis Poisson with parametern/summationtext
i=1λi.
IfXandYare discrete random variables, then the
conditional probability mass function ofXgiven that Y=
yis deﬁned by
P{X=x|Y=y}=p(x,y)
pY(y)
where pis their joint probability mass function. Also, if X
andYare jointly continuous with joint density function f,
then the conditional probability density function ofXgiven
thatY=yis given by
fX|Y(x|y)=f(x,y)
fY(y)
The ordered values X(1)…X(2)…···…X(n)of a set of
independent and identically distributed random variables
are called the order statistics of that set. If the random vari-
ables are continuous and have density function f, then the
joint density function of the order statistics is
f(x1,...,xn)=n!f(x1)···f(xn) x1…x2…···…xn
The random variables X1,...,Xnare called exchangeable
if the joint distribution of Xi1,...,Xinis the same for every
permutation i1,...,inof 1,...,n.

<<<PAGE 284>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 271
A First Course in Probability 271
Problems
6.1.Two fair dice are rolled. Find the joint probability mass
function of XandYwhen
(a)Xis the largest value obtained on any die and Yis the
sum of the values;
(b)Xis the value on the ﬁrst die and Yis the larger of the
two values;(c)Xis the smallest and Yis the largest value obtained on
the dice.
6.2. Suppose that 3 balls are chosen without replacement
from an urn consisting of 5 white and 8 red balls. Let X
i
equal 1 if the ith ball selected is white, and let it equal 0
otherwise. Give the joint probability mass function of
(a)X1,X2;
(b)X1,X2,X3.
6.3. In Problem 6.2, suppose that the white balls are num-
bered, and let Yiequal 1 if the ith white ball is selected and
0 otherwise. Find the joint probability mass function of
(a)Y1,Y2;
(b)Y1,Y2,Y3.
6.4. Repeat Problem 6.2 when the ball selected is replaced
in the urn before the next selection.
6.5. Repeat Problem 6.3a when the ball selected is
replaced in the urn before the next selection.6.6. A bin of 5 transistors is known to contain 2 that are
defective. The transistors are to be tested, one at a time,
until the defective ones are identiﬁed. Denote by N
1the
number of tests made until the ﬁrst defective is identiﬁedand by N
2the number of additional tests until the second
defective is identiﬁed. Find the joint probability mass func-
tion of N1andN2.
6.7.Consider a sequence of independent Bernoulli trials,
each of which is a success with probability p.L e tX 1be the
number of failures preceding the ﬁrst success, and let X2
be the number of failures between the ﬁrst two successes.
Find the joint mass function of X1andX2.
6.8. The joint probability density function of XandYis
given by
f(x,y)=c(y2−x2)e−y−y…x…y,0<y<q
(a)Find c.
(b)Find the marginal densities of XandY.
(c)Find E[X].
6.9. The joint probability density function of XandYis
given by
f(x,y)=6
7/parenleftbigg
x2+xy
2/parenrightbigg
0<x<1, 0<y<2(a)Verify that this is indeed a joint density function.
(b)Compute the density function of X.
(c)Find P{X>Y}.
(d)Find P{Y>1
2|X<1
2}.
(e)Find E[X].
(f)Find E[Y].
6.10. The joint probability density function of XandYis
given by
f(x,y)=e−(x+y)0…x<q,0…y<q
Find (a) P{X<Y}and (b) P{X<a}.
6.11. A television store owner ﬁgures that 45 percent of
the customers entering his store will purchase an ordinary
television set, 15 percent will purchase a plasma television
set, and 40 percent will just be browsing. If 5 customersenter his store on a given day, what is the probability that
he will sell exactly 2 ordinary sets and 1 plasma set on
that day?
6.12. The number of people who enter a drugstore in a
given hour is a Poisson random variable with parameter
λ=10. Compute the conditional probability that at most
3 men entered the drugstore, given that 10 women enteredin that hour. What assumptions have you made?
6.13. A man and a woman agree to meet at a certain loca-
tion about 12:30 p.m. If the man arrives at a time uni-
formly distributed between 12:15 and 12:45, and if the
woman independently arrives at a time uniformly dis-
tributed between 12:00 and 1 p.m., ﬁnd the probability that
the ﬁrst to arrive waits no longer than 5 minutes. What is
the probability that the man arrives ﬁrst?
6.14. An ambulance travels back and forth at a constant
speed along a road of length L. At a certain moment of
time, an accident occurs at a point uniformly distributed on
the road. [That is, the distance of the point from one of the
ﬁxed ends of the road is uniformly distributed over (0, L).]
Assuming that the ambulance’s location at the moment of
the accident is also uniformly distributed, and assuming
independence of the variables, compute the distribution of
the distance of the ambulance from the accident.
6.15. The random vector ( X,Y) is said to be uniformly dis-
tributed over a region Rin the plane if, for some constant
c, its joint density is
f(x,y)=/braceleftbigg
c if(x,y)∈R
0 otherwise
(a)Show that 1/c =area of region R.
Suppose that (X ,Y) is uniformly distributed over the
square centered at (0, 0) and with sides of length 2.

<<<PAGE 285>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 272
272 Chapter 6 Jointly Distributed Random Variables
(b)Show that XandYare independent, with each being
distributed uniformly over (−1, 1).
(c)What is the probability that (X ,Y) lies in the cir-
cle of radius 1 centered at the origin? That is, ﬁnd
P{X2+Y2…1}.
6.16. Suppose that npoints are independently chosen at
random on the circumference of a circle, and we want the
probability that they all lie in some semicircle. That is, wewant the probability that there is a line passing through
the center of the circle such that all the points are on one
side of that line, as shown in the following diagram:
LetP1,...,Pndenote the npoints. Let Adenote the event
that all the points are contained in some semicircle, and
letAibe the event that all the points lie in the semi-
circle beginning at the point Piand going clockwise for
180◦,i=1,...,n.
(a)Express Ain terms of the Ai.
(b)Are the Aimutually exclusive?
(c)Find P(A).
6.17. Three points X1,X2,X3are selected at random on a
lineL. What is the probability that X2lies between X1and
X3?
6.18. Two points are selected randomly on a line of length
Lso as to be on opposite sides of the midpoint of the line.
[In other words, the two points XandYare independent
random variables such that Xis uniformly distributed over
(0,L/2) and Yis uniformly distributed over (L/2, L).] Find
the probability that the distance between the two points is
greater than L/3.
6.19. Show that f(x,y)=1/x,0 <y<x<1, is a joint den-
sity function. Assuming that fis the joint density function
ofX,Y, ﬁnd
(a)the marginal density of Y;
(b)the marginal density of X;
(c)E[X];
(d)E[Y].
6.20. The joint density of XandYis given by
f(x,y)=/braceleftBigg
xe−(x+y)x>0,y>0
0 otherwiseAre Xand Yindependent? If, instead, f(x,y)w e r e
given by
f(x,y)=/braceleftbigg
20 <x<y,0<y<1
0 otherwise
would XandYbe independent?
6.21. Let
f(x,y)=24xy 0…x…1, 0…y…1, 0…x+y…1
and let it equal 0 otherwise.
(a)Show that f(x,y) is a joint probability density function.
(b)Find E[X].
(c)Find E[Y].
6.22. The joint density function of XandYis
f(x,y)=/braceleftbigg
x+y 0<x<1, 0<y<1
0 otherwise
(a)AreXandYindependent?
(b)Find the density function of X.
(c)Find P{X+Y<1}.
6.23. The random variables XandYhave joint density
function
f(x,y)=12xy(1 −x) 0<x<1, 0<y<1
and equal to 0 otherwise.
(a)AreXandYindependent?
(b)Find E[X].
(c)Find E[Y].
(d)Find Var (X).
(e)Find Var (Y).
6.24. Consider independent trials, each of which results in
outcome i,i=0, 1,...,k, with probability pi,k/summationtext
i=0pi=1.
LetNdenote the number of trials needed to obtain an
outcome that is not equal to 0, and let Xbe that outcome.
(a)Find P{N=n},nÚ1.
(b)Find P{X=j},j=1,...,k.
(c)Show that P{N=n,X=j}=P{N=n}P{X=j}.
(d)Is it intuitive to you that Nis independent of X?
(e)Is it intuitive to you that Xis independent of N?
6.25. Suppose that 106people arrive at a service station
at times that are independent random variables, each of
which is uniformly distributed over (0, 106).L e tN denote
the number that arrive in the ﬁrst hour. Find an approxi-
mation for P{N=i}.

<<<PAGE 286>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 273
A First Course in Probability 273
6.26. Suppose that A,B,C, are independent random vari-
ables, each being uniformly distributed over (0, 1).
(a)What is the joint cumulative distribution function of A,
B,C?
(b)What is the probability that all of the roots of the equa-
tionAx2+Bx+C=0 are real?
6.27. IfX1andX2are independent exponential random
variables with respective parameters λ1andλ2, ﬁnd the
distribution of Z=X1/X2. Also compute P{X1<X2}.
6.28. The time that it takes to service a car is an exponen-
tial random variable with rate 1.(a)If A. J. brings his car in at time 0 and M. J. brings her
car in at time t, what is the probability that M. J.’s car is
ready before A. J.’s car? (Assume that service times are
independent and service begins upon arrival of the car.)
(b)If both cars are brought in at time 0, with work start-
ing on M. J.’s car only when A. J.’s car has been completely
serviced, what is the probability that M. J.’s car is ready
before time 2?
6.29. The gross weekly sales at a certain restaurant are a
normal random variable with mean $2200 and standard
deviation $230. What is the probability that
(a)the total gross sales over the next 2 weeks exceeds
$5000;
(b)weekly sales exceed $2000 in at least 2 of the next 3
weeks?
What independence assumptions have you made?
6.30. Jill’s bowling scores are approximately normally dis-
tributed with mean 170 and standard deviation 20, while
Jack’s scores are approximately normally distributed with
mean 160 and standard deviation 15. If Jack and Jill
each bowl one game, then assuming that their scores areindependent random variables, approximate the probabil-
ity that
(a)Jack’s score is higher;
(b)the total of their scores is above 350.
6.31. According to the U.S. National Center for Health
Statistics, 25.2 percent of males and 23.6 percent of females
never eat breakfast. Suppose that random samples of 200men and 200 women are chosen. Approximate the proba-
bility that
(a)at least 110 of these 400 people never eat breakfast;
(b)the number of the women who never eat breakfast is
at least as large as the number of the men who never eat
breakfast.
6.32. Monthly sales are independent normal random vari-
ables with mean 100 and standard deviation 5.
(a)Find the probability that exactly 3 of the next 6 months
have sales greater than 100.
(b)Find the probability that the total of the sales in the
next 4 months is greater than 420.6.33. The expected number of typographical errors on a
page of a certain magazine is .2. What is the probability
that an article of 10 pages contains (a) 0 and (b) 2 or more
typographical errors? Explain your reasoning!
6.34. The monthly worldwide average number of airplane
crashes of commercial airlines is 2.2. What is the probabil-
ity that there will be
(a)more than 2 such accidents in the next month?
(b)more than 4 such accidents in the next 2 months?
(c)more than 5 such accidents in the next 3 months?
Explain your reasoning!
6.35. In Problem 6.4, calculate the conditional probability
mass function of X
1given that
(a)X2=1;
(b)X2=0.
6.36. In Problem 6.3, calculate the conditional probability
mass function of Y1given that
(a)Y2=1;
(b)Y2=0.
6.37. In Problem 6.5, calculate the conditional probability
mass function of Y1given that
(a)Y2=1;
(b)Y2=0.
6.38. Choose a number Xat random from the set of num-
bers{1, 2, 3, 4, 5}. Now choose a number at random from
the subset no larger than X, that is, from {1,...,X}.C a l l
this second number Y.
(a)Find the joint mass function of XandY.
(b)Find the conditional mass function of Xgiven that
Y=i.D oi tf o ri =1, 2, 3, 4, 5.
(c)AreXandYindependent? Why?
6.39. Two dice are rolled. Let XandYdenote, respec-
tively, the largest and smallest values obtained. Compute
the conditional mass function of Ygiven X=i,f o r i=
1, 2,...,6 .A r e XandYindependent? Why?
6.40. The joint probability mass function of XandYis
given by
p(1, 1) =1
8p(1, 2) =1
4
p(2, 1) =1
8p(2, 2) =1
2
(a)Compute the conditional mass function of Xgiven
Y=i,i=1, 2.
(b)AreXandYindependent?
(c)Compute P{XY…3},P{X+Y>2},P{X/Y>1}.

<<<PAGE 287>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 274
274 Chapter 6 Jointly Distributed Random Variables
6.41. The joint density function of XandYis given by
f(x,y)=xe−x(y+ 1)x>0,y>0
(a)Find the conditional density of X, given Y=y,a n dt h a t
ofY, given X=x.
(b)Find the density function of Z=XY.
6.42. The joint density of XandYis
f(x,y)=c(x2−y2)e−x0…x<q,−x…y…x
Find the conditional distribution of Y, given X=x.
6.43. An insurance company supposes that each person
has an accident parameter and that the yearly number of
accidents of someone whose accident parameter is λis
Poisson distributed with mean λ. They also suppose that
the parameter value of a newly insured person can beassumed to be the value of a gamma random variable with
parameters sandα. If a newly insured person has nacci-
dents in her ﬁrst year, ﬁnd the conditional density of her
accident parameter. Also, determine the expected number
of accidents that she will have in the following year.
6.44. IfX
1,X2,X3are independent random variables that
are uniformly distributed over (0, 1), compute the proba-
bility that the largest of the three is greater than the sum
of the other two.
6.45. A complex machine is able to operate effectively as
long as at least 3 of its 5 motors are functioning. If each
motor independently functions for a random amount oftime with density function f(x)=xe
−x,x>0, compute
the density function of the length of time that the machinefunctions.
6.46. If 3 trucks break down at points randomly dis-
tributed on a road of length L, ﬁnd the probability that no
2 of the trucks are within a distance dof each other when
d…L/2.
6.47. Consider a sample of size 5 from a uniform distribu-
tion over (0, 1). Compute the probability that the median
is in the interval/parenleftBig
1
4,3
4/parenrightBig
.
6.48. IfX1,X2,X3,X4,X5are independent and identically
distributed exponential random variables with the param-
eterλ, compute
(a)P{min(X1,...,X5)…a};
(b)P{max(X 1,...,X5)…a}.
6.49. Let X(1),X(2),...,X(n)be the order statistics of a
set of nindependent uniform (0, 1) random variables. Find
the conditional distribution of X(n)given that X(1)=
s1,X(2)=s2,...,X(n−1)=sn−1.
6.50. LetZ1andZ2be independent standard normal ran-
dom variables. Show that X,Yhas a bivariate normal dis-
tribution when X=Z1,Y=Z1+Z2.6.51. Derive the distribution of the range of a sample of
size 2 from a distribution having density function f(x)=
2x,0 <x<1.
6.52. LetXandYdenote the coordinates of a point uni-
formly chosen in the circle of radius 1 centered at the
origin. That is, their joint density is
f(x,y)=1
πx2+y2…1
Find the joint density function of the polar coordinates
R=(X2+Y2)1/2and/Theta1=tan−1Y/X.
6.53. IfXandYare independent random variables both
uniformly distributed over (0, 1), ﬁnd the joint density
function of R=/radicalbig
X2+Y2,Θ=tan−1Y/X.
6.54. IfUis uniform on (0, 2π) andZ, independent of U,
is exponential with rate 1, show directly (without using the
results of Example 7b) that XandYdeﬁned by
X=√
2ZcosU
Y=√
2ZsinU
are independent standard normal random variables.
6.55. XandYhave joint density function
f(x,y)=1
x2y2xÚ1,yÚ1
(a)Compute the joint density function of U=XY,V=
X/Y.
(b)What are the marginal densities?
6.56. IfXand Yare independent and identically
distributed uniform random variables on (0, 1),
compute the joint density of
(a)U=X+Y,V=X/Y;
(b)U=X,V=X/Y;
(c)U=X+Y,V=X/(X+Y).
6.57. Repeat Problem 6.56 when XandYare independent
exponential random variables, each with parameter λ=1.
6.58. IfX1andX2are independent exponential random
variables, each having parameter λ, ﬁnd the joint density
function of Y1=X1+X2andY2=eX1.
6.59. IfX,Y,a n d Zare independent random variables
having identical density functions f(x)=e−x,0<x<q,
derive the joint distribution of U=X+Y,V=X+
Z,W=Y+Z.

<<<PAGE 288>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 275
A First Course in Probability 275
6.60. In Example 8b, let Yk+1=n+1−k/summationtext
i=1Yi. Show
thatY1,...,Yk,Yk+1are exchangeable. Note that Yk+1is
the number of balls one must observe to obtain a special
ball if one considers the balls in their reverse order of with-
drawal.6.61. Consider an urn containing nballs numbered
1,...,n, and suppose that kof them are randomly with-
drawn. Let Xiequal 1 if ball number iis removed and let
Xibe 0 otherwise. Show that X1,...,Xnare exchangeable.
Theoretical Exercises
6.1.Verify Equation (1.2).
6.2. Suppose that the number of events occurring in a
given time period is a Poisson random variable withparameter λ. If each event is classiﬁed as a type ievent
with probability p
i,i=1,...,n,/summationtextpi=1, independently
of other events, show that the numbers of type ievents
that occur, i=1,...,n, are independent Poisson random
variables with respective parameters λpi,i=1,...,n.
6.3. Suggest a procedure for using Buffon’s needle prob-
lem to estimate π. Surprisingly enough, this was once a
common method of evaluating π.
6.4. Solve Buffon’s needle problem when L>D.
answer:2L
πD(1−sinθ)+2θ/π ,w h e r ec o s θ=D/L.
6.5. IfXandYare independent continuous positive ran-
dom variables, express the density function of (a) Z=
X/Yand (b) Z=XY in terms of the density functions of
XandY. Evaluate the density functions in the special case
where XandYare both exponential random variables.
6.6. IfXandYare jointly continuous with joint density
function fX,Y(x,y), show that X+Yis continuous with
density function
fX+Y(t)=/integraldisplayq
−qfX,Y(x,t−x)dx
6.7. (a)IfXhas a gamma distribution with parameters
(t,λ), what is the distribution of cX,c>0?
(b)Show that
1
2λχ2
2n
has a gamma distribution with parameters n,λwhen nis a
positive integer and χ2
2nis a chi-squared random variable
with 2 ndegrees of freedom.
6.8. LetXandYbe independent continuous random
variables with respective hazard rate functions λX(t)and
λY(t),a n ds e t W=min(X ,Y).
(a)Determine the distribution function of Win terms of
those of XandY.(b)Show that λW(t), the hazard rate function of W,i s
given by
λW(t)=λX(t)+λY(t)
6.9. LetX1,...,Xnbe independent exponential random
variables having a common parameter λ. Determine the
distribution of min (X1,...,Xn).
6.10. The lifetimes of batteries are independent exponen-
tial random variables, each having parameter λ.Aﬂ a s h -
light needs 2 batteries to work. If one has a ﬂashlight anda stockpile of nbatteries, what is the distribution of time
that the ﬂashlight can operate?
6.11. LetX
1,X2,X3,X4,X5be independent continuous
random variables having a common distribution function
Fand density function f,a n ds e t
I=P{X1<X2<X3<X4<X5}
(a)Show that Idoes not depend on F.
Hint :W r i t e Ias a ﬁve-dimensional integral and make the
change of variables ui=F(xi),i=1,...,5 .
(b)Evaluate I.
(c)Give an intuitive explanation for your answer to (b).
6.12. Show that the jointly continuous (discrete) random
variables X1,...,Xnare independent if and only if their
joint probability density (mass) function f(x1,...,xn)can
be written as
f(x1,...,xn)=n/productdisplay
i=1gi(xi)
for nonnegative functions gi(x),i=1,...,n.
6.13. In Example 5e, we computed the conditional density
of a success probability for a sequence of trials when the
ﬁrstn+mtrials resulted in nsuccesses. Would the condi-
tional density change if we speciﬁed which nof these trials
resulted in successes?
6.14. Suppose that XandYare independent geometric
random variables with the same parameter p.
(a)Without any computations, what do you think is the
value of
P{X=i|X+Y=n}?

<<<PAGE 289>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 276
276 Chapter 6 Jointly Distributed Random Variables
Hint : Imagine that you continually ﬂip a coin having prob-
ability pof coming up heads. If the second head occurs on
thenth ﬂip, what is the probability mass function of the
time of the ﬁrst head?
(b)Verify your conjecture in part (a).
6.15. Consider a sequence of independent trials, with each
trial being a success with probability p. Given that the kth
success occurs on trial n, show that all possible outcomes
of the ﬁrst n−1 trials that consist of k−1 successes and
n−kfailures are equally likely.
6.16. IfXandYare independent binomial random vari-
ables with identical parameters nandp, show analytically
that the conditional distribution of Xgiven that X+Y=
mis the hypergeometric distribution. Also, give a second
argument that yields the same result without any compu-
tations.
Hint : Suppose that 2n coins are ﬂipped. Let Xdenote the
number of heads in the ﬁrst nﬂips and Ythe number in
the second nﬂips. Argue that given a total of mheads, the
number of heads in the ﬁrst nﬂips has the same distribu-
tion as the number of white balls selected when a sample
of size mis chosen from nwhite and nblack balls.
6.17. Suppose that Xi,i=1, 2, 3 are independent Pois-
son random variables with respective means λi,i=1, 2, 3.
LetX=X1+X2andY=X2+X3. The random
vector X,Yis said to have a bivariate Poisson distribu-
tion. Find its joint probability mass function. That is, ﬁndP{X=n,Y=m}.
6.18. Suppose XandYare both integer-valued random
variables. Let
p(i|j)=P(X=i|Y=j)
and
q(j|i)=P(Y=j|X=i)
Show that
P(X=i,Y=j)=p(i|j)
/summationtext
ip(i|j)
q(j|i)
6.19. LetX1,X2,X3be independent and identically dis-
tributed continuous random variables. Compute
(a)P{X1>X2|X1>X3};
(b)P{X1>X2|X1<X3};
(c)P{X1>X2|X2>X3};
(d)P{X1>X2|X2<X3}.
6.20. LetUdenote a random variable uniformly dis-
tributed over (0, 1). Compute the conditional distribution
ofUgiven that
(a)U>a;
(b)U<a;
where 0 <a<1.6.21. Suppose that W, the amount of moisture in the air
on a given day, is a gamma random variable with parame-
ters(t,β). That is, its density is f(w)=βe−βw(βw)t−1//Gamma1(t),
w>0. Suppose also that given that W=w, the num-
ber of accidents during that day — call it N— has a Poisson
distribution with mean w. Show that the conditional distri-
bution of Wgiven that N=nis the gamma distribution
with parameters (t+n,β+1).
6.22. LetWbe a gamma random variable with param-
eters (t,β), and suppose that conditional on W=w,
X1,X2,...,Xnare independent exponential random vari-
ables with rate w. Show that the conditional distribution
ofWgiven that X1=x1,X2=x2,...,Xn=xnis gamma
with parameters/parenleftBigg
t+n,β+n/summationtext
i=1xi/parenrightBigg
.
6.23. A rectangular array of mnnumbers arranged in n
rows, each consisting of mcolumns, is said to contain a
saddlepoint if there is a number that is both the minimum
of its row and the maximum of its column. For instance, in
the array
13 20−26
.51 2 3
the number 1 in the ﬁrst row, ﬁrst column is a saddlepoint.
The existence of a saddlepoint is of signiﬁcance in the the-
ory of games. Consider a rectangular array of numbers asdescribed previously and suppose that there are two indi-
viduals — AandB— who are playing the following game: A
is to choose one of the numbers 1, 2, ...,nandBone of the
numbers 1, 2, ...,m. These choices are announced simulta-
neously, and if Achose iandBchose j,t h e n Awins from
Bthe amount speciﬁed by the number in the ith row, jth
column of the array. Now suppose that the array contains
a saddlepoint — say the number in the row rand column
k— call this number x
rk. Now if player Achooses row r,
then that player can guarantee herself a win of at leastx
rk(since xrkis the minimum number in the row r). On
the other hand, if player Bchooses column k,t h e nh ec a n
guarantee that he will lose no more than xrk(since xrkis
the maximum number in the column k). Hence, as Ahas
a way of playing that guarantees her a win of xrkand as B
has a way of playing that guarantees he will lose no morethan x
rk, it seems reasonable to take these two strategies
as being optimal and declare that the value of the game to
player Aisxrk.
If the nmnumbers in the rectangular array described are
independently chosen from an arbitrary continuous distri-bution, what is the probability that the resulting array will
contain a saddlepoint?
6.24. IfXis exponential with rate λ, ﬁnd P{[X]=n,X−
[X]…x},w h e r e[ x] is deﬁned as the largest integer less
than or equal to x. Can you conclude that [ X]a n d X−[X]
are independent?

<<<PAGE 290>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 277
A First Course in Probability 277
6.25. Suppose that F(x)is a cumulative distribution func-
tion. Show that (a) Fn(x)and (b) 1 −[1−F(x)]nare
also cumulative distribution functions when nis a positive
integer.
Hint :L e tX 1,...,Xnbe independent random variables
having the common distribution function F. Deﬁne ran-
dom variables YandZin terms of the Xiso that P{Y…
x}= Fn(x)andP{Z…x}= 1−[1−F(x)]n.
6.26. Show that if npeople are distributed at random
along a road Lmiles long, then the probability that no 2
people are less than a distance Dmiles apart is when D…
L/(n −1),[ 1 −(n−1)D/L]n.W h a ti f D>L/(n −1)?
6.27. Establish Equation (6.2) by differentiating Equa-
tion (6.4).
6.28. Show that the median of a sample of size 2 n+1f r o m
a uniform distribution on (0, 1) has a beta distribution with
parameters (n+1,n+1).
6.29. Verify Equation (6.6), which gives the joint density
ofX(i)andX(j).
6.30. Compute the density of the range of a sample of size
nfrom a continuous distribution having density function f.
6.31. LetX(1)…X(2)…···…X(n)be the ordered values
ofnindependent uniform (0, 1) random variables. Prove
that for 1 …k…n+1,
P{X(k)−X(k−1)>t}=(1−t)n
where X(0)K0,X(n+1)Kt.6.32. LetX1,...,Xnbe a set of independent and identi-
cally distributed continuous random variables having dis-tribution function F,a n dl e t X
(i),i=1,...,ndenote their
ordered values. If X, independent of the Xi,i=1,...,n,
also has distribution F, determine
(a)P{X>X(n)};
(b)P{X>X(1)};
(c)P{X(i)<X<X(j)},1…i<j…n.
6.33. LetX1,...,Xnbe independent and identically dis-
tributed random variables having distribution function F
and density f. The quantity MK[X(1)+X(n)]/2, deﬁned
to be the average of the smallest and largest values in
X1,...,Xn, is called the midrange of the sequence. Show
that its distribution function is
FM(m)=n/integraldisplaym
−q[F(2m−x)−F(x)]n−1f(x)dx
6.34. LetX1,...,Xnbe independent uniform (0, 1) ran-
dom variables. Let R=X(n)−X(1)denote the range and
M=[X(n)+X(1)]/2 the midrange of X1,...,Xn. Compute
the joint density function of RandM.
6.35. IfXandYare independent standard normal random
variables, determine the joint density function of
U=XV =X
Y
Then use your result to show that X/Yhas a Cauchy
distribution.
Self-Test Problems and Exercises
6.1.Each throw of an unfair die lands on each of the odd
numbers 1, 3, 5 with probability Cand on each of the even
numbers with probability 2C .
(a)Find C.
(b)Suppose that the die is tossed. Let Xequal 1 if the
result is an even number, and let it be 0 otherwise. Also,letYequal 1 if the result is a number greater than three
and let it be 0 otherwise. Find the joint probability mass
function of XandY. Suppose now that 12 independent
tosses of the die are made.
(c)Find the probability that each of the six outcomes
occurs exactly twice.(d)Find the probability that 4 of the outcomes are either
one or two, 4 are either three or four, and 4 are either ﬁve
or six.
(e)Find the probability that at least 8 of the tosses land on
even numbers.
6.2. The joint probability mass function of the random
variables X,Y,Zis
p(1, 2, 3) =p(2, 1, 1) =p(2, 2, 1) =p(2, 3, 2) =1
4Find (a) E[XYZ ], and (b) E[XY+XZ+YZ].
6.3. The joint density of XandYis given by
f(x,y)=C(y−x)e−y−y<x<y,0 <y<q
(a)Find C.
(b)Find the density function of X.
(c)Find the density function of Y.
(d)Find E[X].
(e)Find E[Y].
6.4. Letr=r1+...+rk,w h e r ea l lr iare positive integers.
Argue that if X1,...,Xrhas a multinomial distribution,
then so does Y1,...,Ykwhere, with r0=0,
Yi=ri−1+ri/summationdisplay
j=ri−1+1Xj,i…k
That is, Y1is the sum of the ﬁrst r1of the X/primes,Y2is the
sum of the next r2, and so on.

<<<PAGE 291>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 278
278 Chapter 6 Jointly Distributed Random Variables
6.5. Suppose that X,Y,a n d Zare independent random
variables that are each equally likely to be either 1 or
2. Find the probability mass function of (a) XYZ ,( b )
XY+XZ+YZ,a n d( c ) X2+YZ.
6.6. LetXandYbe continuous random variables with
joint density function
f(x,y)=⎧
⎨
⎩x
5+cy 0<x<1, 1<y<5
0 otherwise
where cis a constant.
(a)What is the value of c?
(b)AreXandYindependent?
(c)Find P{X+Y>3}.
6.7.The joint density function of XandYis
f(x,y)=/braceleftbigg
xy 0<x<1, 0<y<2
0 otherwise
(a)AreXandYindependent?
(b)Find the density function of X.
(c)Find the density function of Y.
(d)Find the joint distribution function.
(e)Find E[Y].
(f)Find P{X+Y<1}.
6.8. Consider two components and three types of shocks.
A type 1 shock causes component 1 to fail, a type 2 shock
causes component 2 to fail, and a type 3 shock causes both
components 1 and 2 to fail. The times until shocks 1, 2,and 3 occur are independent exponential random vari-
ables with respective rates λ
1,λ2,a n dλ3.L e tXidenote the
time at which component ifails, i=1, 2. The random vari-
ables X1,X2are said to have a joint bivariate exponential
distribution. Find P{X1>s,X2>t}.
6.9. Consider a directory of classiﬁed advertisements that
consists of mpages, where mis very large. Suppose that
the number of advertisements per page varies and that
your only method of ﬁnding out how many advertisements
there are on a speciﬁed page is to count them. In addition,
suppose that there are too many pages for it to be feasibleto make a complete count of the total number of adver-
tisements and that your objective is to choose a directory
advertisement in such a way that each of them has an equalchance of being selected.
(a)If you randomly choose a page and then randomly
choose an advertisement from that page, would that satisfy
your objective? Why or why not?
Letn(i) denote the number of advertisements on page
i,i=1,...,m, and suppose that whereas these quantitiesare unknown, we can assume that they are all less thanor equal to some speciﬁed value n. Consider the following
algorithm for choosing an advertisement.
Step 1. Choose a page at random. Suppose it is page X.
Determine n(X)by counting the number of adver-
tisements on page X.
Step 2. “Accept” page Xwith probability n(X)/n. If page
Xis accepted, go to step 3. Otherwise, return to
step 1.
Step 3. Randomly choose one of the advertisements on
page X.
Call each pass of the algorithm through step 1 an iter-
ation. For instance, if the ﬁrst randomly chosen page
is rejected and the second accepted, then we wouldhave needed 2 iterations of the algorithm to obtain an
advertisement.
(b)What is the probability that a single iteration of the
algorithm results in the acceptance of an advertisement on
page i?
(c)What is the probability that a single iteration of the
algorithm results in the acceptance of an advertisement?
(d)What is the probability that the algorithm goes through
kiterations, accepting the jth advertisement on page ion
the ﬁnal iteration?
(e)What is the probability that the jth advertisement on
page iis the advertisement obtained from the algorithm?
(f)What is the expected number of iterations taken by the
algorithm?
6.10. The “random” parts of the algorithm in Self-Test
Problem 6.8 can be written in terms of the generated val-
ues of a sequence of independent uniform (0, 1) randomvariables, known as random numbers. With [ x] deﬁned as
the largest integer less than or equal to x, the ﬁrst step can
be written as follows:
Step 1. Generate a uniform (0, 1) random variable U.L e t
X=[mU ]+1, and determine the value of n(X).
(a)Explain why the above is equivalent to step 1 of Prob-
lem 6.8.
Hint : What is the probability mass function of X?
(b)Write the remaining steps of the algorithm in a similar
style.
6.11. LetX
1,X2,...be a sequence of independent uniform
(0, 1) random variables. For a ﬁxed constant c, deﬁne the
random variable Nby
N=min{n :Xn>c}
IsNindependent of XN? That is, does knowing the
value of the ﬁrst random variable that is greater than c
affect the probability distribution of when this random
variable occurs? Give an intuitive explanation for your
answer.

<<<PAGE 292>>>

July 14, 2014 M06_ROSSS4772_09_SE_C06 page 279
A First Course in Probability 279
6.12. The accompanying dartboard is a square whose sides
are of length 6:
10
20
30
The three circles are all centered at the center of the board
and are of radii 1, 2, and 3, respectively. Darts landingwithin the circle of radius 1 score 30 points, those land-
ing outside this circle, but within the circle of radius 2,
are worth 20 points, and those landing outside the circleof radius 2, but within the circle of radius 3, are worth 10
points. Darts that do not land within the circle of radius 3
do not score any points. Assuming that each dart that youthrow will, independently of what occurred on your pre-
vious throws, land on a point uniformly distributed in the
square, ﬁnd the probabilities of the accompanying events:
(a)You score 20 on a throw of the dart.
(b)You score at least 20 on a throw of the dart.
(c)You score 0 on a throw of the dart.
(d)The expected value of your score on a throw of
the dart.
(e)Both of your ﬁrst two throws score at least 10.
(f)Your total score after two throws is 30.
6.13. A model proposed for NBA basketball supposes that
when two teams with roughly the same record play each
other, the number of points scored in a quarter by the
home team minus the number scored by the visiting team
is approximately a normal random variable with mean 1.5and variance 6. In addition, the model supposes that the
point differentials for the four quarters are independent.
Assume that this model is correct.
(a)What is the probability that the home team wins?
(b)What is the conditional probability that the home team
wins, given that it is behind by 5 points at halftime?
(c)What is the conditional probability that the home team
wins, given that it is ahead by 5 points at the end of the ﬁrst
quarter?
6.14. LetNbe a geometric random variable with parame-
terp. Suppose that the conditional distribution of Xgiven
thatN=nis the gamma distribution with parameters n
andλ. Find the conditional probability mass function of N
given that X=x.
6.15. LetXandYbe independent uniform (0, 1) random
variables.
(a)Find the joint density of U=X,V=X+Y.
(b)Use the result obtained in part (a) to compute the den-
sity function of V.6.16. You and three other people are to place bids for an
object, with the high bid winning. If you win, you plan to
sell the object immediately for $10,000. How much should
you bid to maximize your expected proﬁt if you believe
that the bids of the others can be regarded as being inde-pendent and uniformly distributed between $7,000 and
$10,000 thousand dollars?
6.17. Find the probability that X
1,X2,...,Xnis a permu-
tation of 1, 2, ...,n,w h e nX 1,X2,...,Xnare independent
and
(a)each is equally likely to be any of the values 1, ...,n;
(b)each has the probability mass function P{Xi=j}=
pj,j=1,...,n.
6.18. LetX1,...,XnandY1,...,Ynbe independent ran-
dom vectors, with each vector being a random ordering of
kones and n−kzeros. That is, their joint probability mass
functions are
P{X1=i1,...,Xn=in}=P{Y1=i1,...,Yn=in}
=1/parenleftbigg
nk/parenrightbigg,i
j=0, 1,n/summationdisplay
j=1ij=k
Let
N=n/summationdisplay
i=1|Xi−Yi|
denote the number of coordinates at which the two vec-
tors have different values. Also, let Mdenote the number
of values of ifor which Xi=1,Yi=0.
(a)Relate NtoM.
(b)What is the distribution of M?
(c)Find E[N].
(d)Find Var (N).
*6.19. LetZ1,Z2,...,Znbe independent standard normal
random variables, and let
Sj=j/summationdisplay
i=1Zi
(a)What is the conditional distribution of Sngiven that
Sk=y,f o r k=1,...,n?
(b)Show that, for 1 …k…n, the conditional distribution
ofSkgiven that Sn=xis normal with mean xk/n and
variance k(n−k)/n.
6.20. LetX1,X2,...be a sequence of independent and
identically distributed continuous random variables. Find
(a)P{X6>X1|X1=max(X 1,...,X5)}
(b)P{X6>X2|X1=max(X 1,...,X5)}
6.21. Prove the identity
P{X…s,Y…t}=P{X…s}+P{Y…t}+P{X>s,Y>t}−1
Hint: Derive an expression for P{X>s,Y>t}by taking
the probability of the complementary event.

<<<PAGE 293>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 280
Chapter
Properties of Expectation7
Contents
7.1 Introduction
7.2 Expectation of Sums of Random
Variables
7.3 Moments of the Number of Events that
Occur
7.4 Covariance, Variance of Sums, and
Correlations7.5 Conditional Expectation
7.6 Conditional Expectation andPrediction
7.7 Moment Generating Functions
7.8 Additional Properties of Normal RandomVariables
7.9 General Deﬁnition of Expectation
7.1 Introduction
In this chapter, we develop and exploit additional properties of expected values. Tobegin, recall that the expected value of the random variable Xis deﬁned by
E[X]=/summationdisplay
xxp(x)
where Xis a discrete random variable with probability mass function p(x), and by
E[X]=/integraldisplayq
−qxf(x)dx
when Xis a continuous random variable with probability density function f(x).
Since E[X] is a weighted average of the possible values of X, it follows that if X
must lie between aandb, then so must its expected value. That is, if
P{a…X…b}= 1
then
a…E[X]…b
To verify the preceding statement, suppose that Xis a discrete random variable for
which P{a…X…b}= 1. Since this implies that p(x)=0 for all xoutside of the
interval [a, b], it follows that
E[X]=/summationdisplay
x:p(x)>0xp(x)
Ú/summationdisplay
x:p(x)>0ap(x)
=a/summationdisplay
x:p(x)>0p(x)
=a
280

<<<PAGE 294>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 281
A First Course in Probability 281
In the same manner, it can be shown that E[X]…b, so the result follows for discrete
random variables. As the proof in the continuous case is similar, the result follows.
7.2 Expectation of Sums of Random Variables
For a two-dimensional analog of Propositions 4.1 of Chapter 4 and 2.1 of Chapter 5,
which give the computational formulas for the expected value of a function of a
random variable, suppose that XandYare random variables and gis a function of
two variables. Then we have the following result.
Proposition
2.1IfXandYhave a joint probability mass function p(x,y) , then
E[g(X,Y)]=/summationdisplay
y/summationdisplay
xg(x,y)p(x, y)
IfXandYhave a joint probability density function f(x,y) , then
E[g(X,Y)]=/integraldisplayq
−q/integraldisplayq
−qg(x,y)f(x,y)dx dy
Let us give a proof of Proposition 2.1 when the random variables XandYare
jointly continuous with joint density function f(x,y)and when g(X,Y)is a nonneg-
ative random variable. Because g(X,Y)Ú0, we have, by Lemma 2.1 of Chapter 5,
that
E[g(X,Y)]=/integraldisplayq
0P{g(X,Y)>t}dt
Writing
P{g(X,Y)>t}=/integraldisplay/integraldisplay
(x,y):g (x,y)>tf(x,y)dy dx
shows that
E[g(X,Y)]=/integraldisplayq
0/integraldisplay/integraldisplay
(x,y):g (x,y)>tf(x,y)dy dx dt
Interchanging the order of integration gives
E[g(X,Y)]=/integraldisplay
x/integraldisplay
y/integraldisplayg(x,y)
t=0f(x,y)dt dy dx
=/integraldisplay
x/integraldisplay
yg(x,y)f(x,y)dy dx
Thus, the result is proven when g(X,Y)is a nonnegative random variable. The gen-
eral case then follows as in the one-dimensional case. (See Theoretical Exercises 2
and 3 of Chapter 5.)
Example
2aAn accident occurs at a point Xthat is uniformly distributed on a road of length L.
At the time of the accident, an ambulance is at a location Ythat is also uniformly
distributed on the road. Assuming that XandYare independent, ﬁnd the expected
distance between the ambulance and the point of the accident.
Solution We need to compute E[|X−Y|]. Since the joint density function of Xand
Yis
f(x,y)=1
L2,0<x<L,0 <y<L

<<<PAGE 295>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 282
282 Chapter 7 Properties of Expectation
it follows from Proposition 2.1 that
E[|X−Y|]=1
L2/integraldisplayL
0/integraldisplayL
0|x−y|dy dx
Now,
/integraldisplayL
0|x−y|dy=/integraldisplayx
0(x−y)dy+/integraldisplayL
x(y−x)dy
=x2
2+L2
2−x2
2−x(L−x)
=L2
2+x2−xL
Therefore,
E[|X−Y|]=1
L2/integraldisplayL
0/parenleftBigg
L2
2+x2−xL/parenrightBigg
dx
=L
3.
For an important application of Proposition 2.1, suppose that E[X] and E[Y]a r e
both ﬁnite and let g(X,Y)=X+Y. Then, in the continuous case,
E[X+Y]=/integraldisplayq
−q/integraldisplayq
−q(x+y)f(x,y)dx dy
=/integraldisplayq
−q/integraldisplayq
−qxf(x,y)dy dx +/integraldisplayq
−q/integraldisplayq
−qyf(x,y)dx dy
=/integraldisplayq
−qxfX(x)dx+/integraldisplayq
−qyfY(y)dy
=E[X]+E[Y]
The same result holds in general; thus, whenever E[X] and E[Y] are ﬁnite,
E[X+Y]=E[X]+E[Y] (2.1)
Example
2bSuppose that for random variables XandY,
XÚY
That is, for any outcome of the probability experiment, the value of the random
variable Xis greater than or equal to the value of the random variable Y. Since
XÚYis equivalent to the inequality X−YÚ0, it follows that E[X−Y]Ú0,
or, equivalently,
E[X]ÚE[Y] .
Using Equation (2.1), we may show by a simple induction proof that if E[Xi]i s
ﬁnite for all i=1,...,n, then
E[X1+ ··· + Xn]=E[X1]+ ··· + E[Xn] (2.2)
Equation (2.2) is an extremely useful formula whose utility will now be illustratedby a series of examples.

<<<PAGE 296>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 283
A First Course in Probability 283
Example
2cThe sample mean
LetX1,...,Xnbe independent and identically distributed random variables having
distribution function Fand expected value μ. Such a sequence of random variables
is said to constitute a sample from the distribution F. The quantity
X=n/summationdisplay
i=1Xi
n
is called the sample mean . Compute E[X].
Solution
E[X]=E⎡
⎣n/summationdisplay
i=1Xi
n⎤⎦
=1
nE⎡
⎣n/summationdisplay
i=1Xi⎤⎦
=1
nn/summationdisplay
i=1E[Xi]
=μsince E[Xi]Kμ
That is, the expected value of the sample mean is μ, the mean of the distribution.
When the distribution mean μis unknown, the sample mean is often used in statistics
to estimate it. .
Example
2dBoole’s inequalityLetA
1,...,Andenote events, and deﬁne the indicator variables Xi,i=1,...,n,b y
Xi=/braceleftBigg
1i f Aioccurs
0 otherwise
Let
X=n/summationdisplay
i=1Xi
soXdenotes the number of the events Aithat occur. Finally, let
Y=/braceleftBigg
1i f XÚ1
0 otherwise
soYis equal to 1 if at least one of the Aioccurs and is 0 otherwise. Now, it is imme-
diate that
XÚY
so
E[X]ÚE[Y]
But since
E[X]=n/summationdisplay
i=1E[Xi]=n/summationdisplay
i=1P(Ai)

<<<PAGE 297>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 284
284 Chapter 7 Properties of Expectation
and
E[Y]=P{at least one of the Aioccur}=P⎛
⎝n/uniondisplay
i=1Ai⎞⎠
we obtain Boole’s inequality, namely,
P⎛⎝
n/uniondisplay
i=1Ai⎞⎠…n/summationdisplay
i=1P(Ai) .
The next three examples show how Equation (2.2) can be used to calculate the
expected value of binomial, negative binomial, and hypergeometric random vari-
ables. These derivations should be compared with those presented in Chapter 4.
Example
2eExpectation of a binomial random variable
LetXbe a binomial random variable with parameters nandp. Recalling that such
a random variable represents the number of successes in nindependent trials when
each trial has probability pof being a success, we have that
X=X1+X2+ ··· + Xn
where
Xi=/braceleftBigg
1i f t h e ith trial is a success
0i f t h e ith trial is a failure
Hence, Xiis a Bernoulli random variable having expectation E[Xi]=1(p)+
0(1−p). Thus,
E[X]=E[X1]+E[X2]+ ··· + E[Xn]=np .
Example
2fMean of a negative binomial random variable
If independent trials having a constant probability pof being successes are per-
formed, determine the expected number of trials required to amass a total of r
successes.
Solution IfXdenotes the number of trials needed to amass a total of rsuccesses,
then Xis a negative binomial random variable that can be represented by
X=X1+X2+ ··· + Xr
where X1is the number of trials required to obtain the ﬁrst success, X2the number
of additional trials until the second success is obtained, X3the number of additional
trials until the third success is obtained, and so on. That is, Xirepresents the num-
ber of additional trials required after the ( i−1) success until a total of isuccesses
is amassed. A little thought reveals that each of the random variables Xiis a geo-
metric random variable with parameter p. Hence, from the results of Example 8b of
Chapter 4, E[Xi]=1/p,i=1, 2,...,r; thus,
E[X]=E[X1]+ ··· + E[Xr]=r
p.

<<<PAGE 298>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 285
A First Course in Probability 285
Example
2gMean of a hypergeometric random variable
Ifnballs are randomly selected from an urn containing Nballs of which mare white,
ﬁnd the expected number of white balls selected.
Solution LetXdenote the number of white balls selected, and represent Xas
X=X1+ ··· + Xm
where
Xi=/braceleftBigg
1i f t h e ith white ball is selected
0 otherwise
Now
E[Xi]=P{Xi=1}
=P{ith white ball is selected }
=/parenleftBigg
1
1/parenrightBigg/parenleftBigg
N−1
n−1/parenrightBigg
/parenleftBigg
N
n/parenrightBigg
=n
N
Hence,
E[X]=E[X1]+ ··· + E[Xm]=mn
N
We could also have obtained the preceding result by using the alternative represen-tation
X=Y
1+ ··· + Yn
where
Yi=/braceleftBigg
1i f t h e ith ball selected is white
0 otherwise
Since the ith ball selected is equally likely to be any of the Nballs, it follows that
E[Yi]=m
N
so
E[X]=E[Y1]+ ··· + E[Yn]=nm
N.
Example
2hExpected number of matches
Suppose that Npeople throw their hats into the center of a room. The hats are mixed
up, and each person randomly selects one. Find the expected number of people who
select their own hat.
Solution Letting Xdenote the number of matches, we can compute E[X] most eas-
ily by writing
X=X1+X2+ ··· + XN

<<<PAGE 299>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 286
286 Chapter 7 Properties of Expectation
where
Xi=/braceleftBigg
1i f t h e ith person selects his own hat
0 otherwise
Since, for each i,t h e ith person is equally likely to select any of the Nhats,
E[Xi]=P{Xi=1}=1
N
Thus,
E[X]=E[X1]+ ··· + E[XN]=/parenleftbigg1
N/parenrightbigg
N=1
Hence, on the average, exactly one person selects his own hat. .
Example
2iCoupon-collecting problems
Suppose that there are Ntypes of coupons, and each time one obtains a coupon, it
is equally likely to be any one of the Ntypes. Find the expected number of coupons
one needs to amass before obtaining a complete set of at least one of each type.
Solution LetXdenote the number of coupons collected before a complete set is
attained. We compute E[X] by using the same technique we used in computing the
mean of a negative binomial random variable (Example 2f). That is, we deﬁne Xi,i=
0, 1,...,N−1 to be the number of additional coupons that need be obtained after
idistinct types have been collected in order to obtain another distinct type, and we
note that
X=X0+X1+ ··· + XN−1
When idistinct types of coupons have already been collected, a new coupon obtained
will be of a distinct type with probability (N−i)/N . Therefore,
P{Xi=k}=N−i
N/parenleftbiggi
N/parenrightbiggk−1
kÚ1
or, in other words, Xiis a geometric random variable with parameter (N−i)/N .
Hence,
E[Xi]=N
N−i
implying that
E[X]=1+N
N−1+N
N−2+ ··· +N
1
=N/bracketleftbigg
1+ ··· +1
N−1+1
N/bracketrightbigg
.
Example
2jTen hunters are waiting for ducks to ﬂy by. When a ﬂock of ducks ﬂies overhead, the
hunters ﬁre at the same time, but each chooses his target at random, independentlyof the others. If each hunter independently hits his target with probability p, com-
pute the expected number of ducks that escape unhurt when a ﬂock of size 10 ﬂiesoverhead.

<<<PAGE 300>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 287
A First Course in Probability 287
Solution LetXiequal 1 if the ith duck escapes unhurt and 0 otherwise, for i=1,
2,..., 10. The expected number of ducks to escape can be expressed as
E[X1+ ··· + X10]=E[X1]+ ··· + E[X10]
To compute E[Xi]=P{Xi=1}, we note that each of the hunters will, independently,
hit the ith duck with probability p/10, so
P{Xi=1}=/parenleftbigg
1−p
10/parenrightbigg10
Hence,
E[X]=10/parenleftbigg
1−p
10/parenrightbigg10
.
Example
2kExpected number of runs
Suppose that a sequence of n1’s and m0’s is randomly permuted so that each of the
(n+m)!/(n!m!) possible arrangements is equally likely. Any consecutive string of
1’s is said to constitute a run of 1’s—for instance, if n=6,m=4, and the ordering
is 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, then there are 3 runs of 1’s—and we are interested in
computing the mean number of such runs. To compute this quantity, let
Ii=/braceleftBigg
1 i far u no f1 ’ ss t a r t sa tt h e ith position
0 otherwise
Therefore, R(1), the number of runs of 1, can be expressed as
R(1)=n+m/summationdisplay
i=1Ii
and it follows that
E[R(1)] =n+m/summationdisplay
i=1E[Ii]
Now,
E[I1]=P{“1” in position 1 }
=n
n+m
and for 1 <i…n+m,
E[Ii]=P{“0” in position i−1, “1” in position i}
=m
n+mn
n+m−1
Hence,
E[R(1)] =n
n+m+(n+m−1)nm
(n+m)(n +m−1)
Similarly, E[R(0)], the expected number of runs of 0’s, is
E[R(0)] =m
n+m+nm
n+m

<<<PAGE 301>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 288
288 Chapter 7 Properties of Expectation
and the expected number of runs of either type is
E[R(1) +R(0)] =1+2nm
n+m.
Example
2lA random walk in the plane
Consider a particle initially located at a given point in the plane, and suppose that it
undergoes a sequence of steps of ﬁxed length, but in a completely random direction.
Speciﬁcally, suppose that the new position after each step is one unit of distance fromthe previous position and at an angle of orientation from the previous position that
is uniformly distributed over (0, 2 π). (See Figure 7.1.) Compute the expected square
of the distance from the origin after nsteps.
1
/H92582
011/H925821
0 = initial position
1 = position after first step
2 = position after second step
Figure 7.1
Solution Letting (X i,Yi) denote the change in position at the ith step, i=1,...,n,
in rectangular coordinates, we have
Xi=cosθi
Yi=sinθi
where θi,i=1,...,n, are, by assumption, independent uniform (0, 2 π) random vari-
ables. Because the position after nsteps has rectangular coordinates/parenleftBigg
n/summationtext
i=1Xi,n/summationtext
i=1Yi/parenrightBigg
,
it follows that D2, the square of the distance from the origin, is given by
D2=⎛
⎝n/summationdisplay
i=1Xi⎞⎠2
+⎛⎝n/summationdisplay
i=1Yi⎞⎠2
=n/summationdisplay
i=1(X2
i+Y2
i)+/summationdisplay/summationdisplay
iZj(XiXj+YiYj)
=n+/summationdisplay/summationdisplay
iZj(cosθicosθj+sinθisinθj)

<<<PAGE 302>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 289
A First Course in Probability 289
where cos2θi+sin2θi=1. Taking expectations and using the independence of θi
andθjwhen iZjand the fact that
2πE[cosθi]=/integraldisplay2π
0cosud u=sin 2π−sin 0=0
2πE[sinθi]=/integraldisplay2π
0sinud u=cos 0 −cos 2π =0
we arrive at
E[D2]=n .
Example
2mAnalyzing the quick-sort algorithm
Suppose that we are presented with a set of ndistinct values x1,x2,...,xnand that
we desire to put them in increasing order, or as it is commonly stated, to sort them.
An efﬁcient procedure for accomplishing this task is the quick-sort algorithm, which
is deﬁned as follows: When n=2, the algorithm compares the two values and then
puts them in the appropriate order. When n>2, one of the elements is randomly
chosen—say it is xi—and then all of the other values are compared with xi. Those
smaller than xiare put in a bracket to the left of xiand those larger than xiare put in
a bracket to the right of xi. The algorithm then repeats itself on these brackets and
continues until all values have been sorted. For instance, suppose that we desire tosort the following 10 distinct values:
5, 9, 3, 10, 11, 14, 8, 4, 17, 6
We start by choosing one of them at random (that is, each value has probability
1
10of
being chosen). Suppose, for instance, that the value 10 is chosen. We then compare
each of the others to this value, putting in a bracket to the left of 10 all those values
smaller than 10 and to the right all those larger. This gives
{5, 9, 3, 8, 4, 6}, 10, {11, 14, 17}
We now focus on a bracketed set that contains more than a single value—say the oneon the left of the preceding—and randomly choose one of its values—say that 6 is
chosen. Comparing each of the values in the bracket with 6 and putting the smallerones in a new bracket to the left of 6 and the larger ones in a bracket to the right
of 6 gives
{5, 3, 4},6 ,{9, 8}, 10, {11, 14, 17}
If we now consider the leftmost bracket, and randomly choose the value 4 for com-parison, then the next iteration yields
{3},4 ,{5},6 ,{9, 8}, 10, {11, 14, 17}
This continues until there is no bracketed set that contains more than a single value.
If we let Xdenote the number of comparisons that it takes the quick-sort algo-
rithm to sort ndistinct numbers, then E[X] is a measure of the effectiveness of this
algorithm. To compute E[X], we will ﬁrst express Xas a sum of other random vari-
ables as follows. To begin, give the following names to the values that are to be
sorted: Let 1 stand for the smallest, let 2 stand for the next smallest, and so on. Then,
for 1 …i<j…n,l e t I(i,j)equal 1 if iandjare ever directly compared, and let it
equal 0 otherwise. With this deﬁnition, it follows that

<<<PAGE 303>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 290
290 Chapter 7 Properties of Expectation
X=n−1/summationdisplay
i=1n/summationdisplay
j=i+1I(i,j)
implying that
E[X]=E⎡
⎢⎣n−1/summationdisplay
i=1n/summationdisplay
j=i+1I(i,j)⎤
⎥⎦
=n−1/summationdisplay
i=1n/summationdisplay
j=i+1E[I(i,j)]
=n−1/summationdisplay
i=1n/summationdisplay
j=i+1P{iandjare ever compared }
To determine the probability that iandjare ever compared, note that the values
i,i+1,...,j−1,jwill initially be in the same bracket (since all values are initially
in the same bracket) and will remain in the same bracket if the number chosen for
the ﬁrst comparison is not between iandj. For instance, if the comparison number is
larger than j, then all the values i,i+1,...,j−1,jwill go in a bracket to the left of
the comparison number, and if it is smaller than i, then they will all go in a bracket
to the right. Thus all the values i,i+1,...,j−1,jwill remain in the same bracket
until the ﬁrst time that one of them is chosen as a comparison value. At that point allthe other values between iandjwill be compared with this comparison value. Now,
if this comparison value is neither inorj, then upon comparison with it, iwill go into
a left bracket and jinto a right bracket, and thus iandjwill be in different brackets
and so will never be compared. On the other hand, if the comparison value of theseti,i+1,...,j−1,jis either iorj, then there will be a direct comparison between
iandj. Now, given that the comparison value is one of the values between iandj,
it follows that it is equally likely to be any of these j−i+1 values, and thus the
probability that it is either iorjis 2/(j −i+1). Therefore, we can conclude that
P{iandjare ever compared }=2
j−i+1
and
E[X]=n−1/summationdisplay
i=1n/summationdisplay
j=i+12
j−i+1
To obtain a rough approximation of the magnitude of E[X] when nis large, we can
approximate the sums by integrals. Now
n/summationdisplay
j=i+12
j−i+1L/integraldisplayn
i+12
x−i+1dx
=2l o g(x−i+1)/vextendsingle/vextendsinglen
i+1
=2l o g(n−i+1)−2l o g(2)
L2l o g(n−i+1)

<<<PAGE 304>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 291
A First Course in Probability 291
Thus
E[X]Ln−1/summationdisplay
i=12l o g(n−i+1)
L2/integraldisplayn−1
1log(n−x+1)dx
=2/integraldisplayn
2log(y)dy
=2(ylog(y)−y)|n
2
L2nlog(n)
Thus we see that when nis large, the quick-sort algorithm requires, on average,
approximately 2 nlog(n) comparisons to sort ndistinct values. .
Example
2nThe probability of a union of events
LetA1,...Andenote events, and deﬁne the indicator variables Xi,i=1,...,n,b y
Xi=/braceleftBigg
1i f Aioccurs
0 otherwise
Now, note that
1−n/productdisplay
i=1(1−Xi)=/braceleftBigg
1i f ∪Aioccurs
0 otherwise
Hence,
E⎡⎣1−n/productdisplay
i=1(1−Xi)⎤⎦=P⎛⎝
n/uniondisplay
i=1Ai⎞⎠
Expanding the left side of the preceding formula yields
P⎛⎝
n/uniondisplay
i=1Ai⎞⎠=E⎡
⎢⎣n/summationdisplay
i=1Xi−/summationdisplay/summationdisplay
i<jXiXj+/summationdisplay/summationdisplay/summationdisplay
i<j<kXiXjXk
−··· + (−1)n+1X1···X n⎤
⎦ (2.3)
However,
Xi1Xi2···X ik=/braceleftBigg
1i f Ai1Ai2···Aikoccurs
0 otherwise
so
E[Xi1···Xik]=P(Ai1···Aik)
Thus, Equation (2.3) is just a statement of the well-known formula for the union of
events:
P(∪A i)=/summationdisplay
iP(Ai)−/summationdisplay/summationdisplay
i<jP(AiAj)+/summationdisplay/summationdisplay/summationdisplay
i<j<kP(AiAjAk)
− ··· + (−1)n+1P(A1···An) .

<<<PAGE 305>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 292
292 Chapter 7 Properties of Expectation
When one is dealing with an inﬁnite collection of random variables Xi,iÚ1,
each having a ﬁnite expectation, it is not necessarily true that
E⎡
⎣q/summationdisplay
i=1Xi⎤⎦=q/summationdisplay
i=1E[Xi] (2.4)
To determine when (2.4) is valid, we note thatq/summationtext
i=1Xi=limn→qn/summationtext
i=1Xi. Thus,
E⎡
⎣q/summationdisplay
i=1Xi⎤⎦=E⎡⎣lim
n→qn/summationdisplay
i=1Xi⎤⎦
?=limn→qE⎡
⎣n/summationdisplay
i=1Xi⎤⎦
=lim
n→qn/summationdisplay
i=1E[Xi]
=q/summationdisplay
i=1E[Xi] (2.5)
Hence, Equation (2.4) is valid whenever we are justiﬁed in interchanging the expec-
tation and limit operations in Equation (2.5). Although, in general, this interchangeisnotjustiﬁed, it can be shown to be valid in two important special cases:
1. The X
iare all nonnegative random variables. (That is, P{XiÚ0}= 1 for all i.)
2.q/summationtext
i=1E[|Xi|]<q.
Example
2oConsider any nonnegative, integer-valued random variable X. If, for each iÚ1, we
deﬁne
Xi=/braceleftBigg
1i f XÚi
0i f X<i
then
q/summationdisplay
i=1Xi=X/summationdisplay
i=1Xi+q/summationdisplay
i=X+1Xi
=X/summationdisplay
i=11+q/summationdisplay
i=X+10
=X
Hence, since the Xiare all nonnegative, we obtain
E[X]=q/summationdisplay
i=1E(Xi)
=q/summationdisplay
i=1P{XÚi} (2.6)
a useful identity. .

<<<PAGE 306>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 293
A First Course in Probability 293
Example
2pSuppose that nelements—call them 1, 2, ...,n—must be stored in a computer in the
form of an ordered list. Each unit of time, a request will be made for one of these
elements— ibeing requested, independently of the past, with probability P(i),iÚ1,/summationtext
iP(i)=1. Assuming that these probabilities are known, what ordering minimizes
the average position in the line of the element requested?
Solution Suppose that the elements are numbered so that P(1)ÚP(2)Ú···ÚP(n).
To show that 1, 2, ...,nis the optimal ordering, let Xdenote the position of the
requested element. Now, under any ordering—say, O=i1,i2,...,in,
PO{XÚk}=n/summationdisplay
j=kP(ij)
Ún/summationdisplay
j=kP(j)
=P1,2,...,n{XÚk}
Summing over kand using Equation (2.6) yields
Eo[X]ÚE1,2,...,n[X]
thus showing that ordering the elements in decreasing order of the probability thatthey are requested minimizes the expected position of the element requested. .
*7.2.1 Obtaining Bounds from Expectations via the Probabilistic
Method
The probabilistic method is a technique for analyzing the properties of the elements
of a set by introducing probabilities on the set and then studying an element chosen
according to those probabilities. The technique was previously seen in Example 4l of
Chapter 3, where it was used to show that a set contained an element that satisﬁed a
certain property. In this subsection, we show how it can sometimes be used to boundcomplicated functions.
Letfbe a function on the elements of a ﬁnite set A, and suppose that we are
interested in
m=max
s∈Af(s)
A useful lower bound for mcan often be obtained by letting Sbe a random element
ofAfor which the expected value of f(S)is computable and then noting that mÚ
f(S)implies that
mÚE[f(S)]
with strict inequality if f(S)is not a constant random variable. That is, E[f(S)]i sa
lower bound on the maximum value.
Example
2qThe maximum number of Hamiltonian paths in a tournament
A round-robin tournament of n>2 contestants is a tournament in which each of
the/parenleftBigg
n
2/parenrightBigg
pairs of contestants play each other exactly once. Suppose that the players

<<<PAGE 307>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 294
294 Chapter 7 Properties of Expectation
are numbered 1, 2, 3, ...,n. The permutation i1,i2,...inis said to be a Hamiltonian
path ifi1beats i2,i2beats i3,..., and in−1beats in. A problem of some interest is to
determine the largest possible number of Hamiltonian paths.
As an illustration, suppose that there are 3 players. On the one hand, one of
them wins twice, then there is a single Hamiltonian path. (For instance, if 1 wins
twice and 2 beats 3, then the only Hamiltonian path is 1, 2, 3.) On the other hand, if
each of the players wins once, then there are 3 Hamiltonian paths. (For instance, if 1beats 2, 2 beats 3, and 3 beats 1, then 1, 2, 3; 2, 3, 1; and 3, 1, 2, are all Hamiltonians.)
Hence, when n=3, there is a maximum of 3 Hamiltonian paths.
We now show that there is an outcome of the tournament that results in more
thann!/2
n−1Hamiltonian paths. To begin, let the outcome of the tournament specify
the result of each of the/parenleftBigg
n
2/parenrightBigg
games played, and let Adenote the set of all 2/parenleftbigg
n
2/parenrightbigg
pos-
sible tournament outcomes. Then, with f(s)deﬁned as the number of Hamiltonian
paths that result when the outcome is s∈A, we are asked to show that
max
sf(s)Ún!
2n−1
To show this, consider the randomly chosen outcome Sthat is obtained when the
results of the/parenleftBigg
n
2/parenrightBigg
games are independent, with each contestant being equally likely
to win each encounter. To determine E[f(S)], the expected number of Hamiltonian
paths that result from the outcome S, number the n! permutations, and, for i=
1,...,n!, let
Xi=/braceleftBigg
1, if permutation iis a Hamiltonian
0, otherwise
Since
f(S)=/summationdisplay
iXi
it follows that
E[f(S)]=/summationdisplay
iE[Xi]
Because, by the assumed independence of the outcomes of the games, the probabil-
ity that any speciﬁed permutation is a Hamiltonian is (1/2)n−1, it follows that
E[Xi]=P{Xi=1}=(1/2)n−1
Therefore,
E[f(S)]=n!(1/2)n−1
Since f(S)is not a constant random variable, the preceding equation implies that
there is an outcome of the tournament having more than n!/2n−1Hamiltonian
paths. .
Example
2rA grove of 52 trees is arranged in a circular fashion. If 15 chipmunks live in thesetrees, show that there is a group of 7 consecutive trees that together house at least 3chipmunks.
Solution Let the neighborhood of a tree consist of that tree along with the next six
trees visited by moving in the clockwise direction. We want to show that for anychoice of living accommodations of the 15 chipmunks, there is a tree that has at least

<<<PAGE 308>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 295
A First Course in Probability 295
3 chipmunks living in its neighborhood. To show this, choose a tree at random and
letXdenote the number of chipmunks that live in its neighborhood. To determine
E[X], arbitrarily number the 15 chipmunks and for i=1,..., 15, let
Xi=/braceleftBigg
1, if chipmunk ilives in the neighborhood of the randomly chosen tree
0, otherwise
Because
X=15/summationdisplay
i=1Xi
we obtain that
E[X]=15/summationdisplay
i=1E[Xi]
However, because Xiwill equal 1 if the randomly chosen tree is any of the 7 trees
consisting of the tree in which chipmunk ilives along with its 6 neighboring trees
when moving in the counterclockwise direction,
E[Xi]=P{Xi=1}=7
52
Consequently,
E[X]=105
52>2
showing that there exists a tree with more than 2 chipmunks living in its neigh-borhood. .
*7.2.2 The Maximum–Minimums Identity
We start with an identity relating the maximum of a set of numbers to the minimumsof the subsets of these numbers.
Proposition
2.2For arbitrary numbers x
i,i=1,...,n,
max
ixi=/summationdisplay
ixi−/summationdisplay
i<jmin(x i,xj)+/summationdisplay
i<j<kmin(x i,xj,xk)
+...+(−1)n+1min(x1,...,xn)
Proof We will give a probabilistic proof of the proposition. To begin, assume that all
thexiare in the interval [0, 1]. Let Ube a uniform (0, 1) random variable, and deﬁne
the events Ai,i=1,...,n,b yA i={U<xi}. That is, Aiis the event that the uniform
random variable is less than xi. Because at least one of these events Aiwill occur if
Uis less than at least one of the values xi, we have that
∪iAi=/braceleftbigg
U<max
ixi/bracerightbigg
Therefore,
P(∪iAi)=P/braceleftbigg
U<max
ixi/bracerightbigg
=max
ixi
Also,
P(Ai)=P/braceleftbig
U<xi/bracerightbig
=xi

<<<PAGE 309>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 296
296 Chapter 7 Properties of Expectation
In addition, because all of the events Ai1,...,Airwill occur if Uis less than all the
values xi1,...,xir, we see that the intersection of these events is
Ai1...Air=/braceleftBigg
U<min
j=1,...rxij/bracerightBigg
implying that
P(Ai1...Air)=P/braceleftBigg
U<min
j=1,...rxij/bracerightBigg
=min
j=1,...rxij
Thus, the proposition follows from the inclusion–exclusion formula for the probabil-
ity of the union of events:
P(∪iAi)=/summationdisplay
iP(Ai)−/summationdisplay
i<jP(AiAj)+/summationdisplay
i<j<kP(AiAjAk)
+...+(−1)n+1P(A1...An)
When the xiare nonnegative, but not restricted to the unit interval, let cbe such
that all the xiare less than c. Then the identity holds for the values yi=xi/c, and the
desired result follows by multiplying through by c. When the xican be negative, let
bbe such that xi+b>0 for all i. Therefore, by the preceding,
max
i(xi+b)=/summationdisplay
i(xi+b)−/summationdisplay
i<jmin(xi+b,xj+b)
+ ··· + (−1)n+1min(x1+b,...,xn+b)
Letting
M=/summationdisplay
ixi−/summationdisplay
i<jmin(xi,xj)+ ··· + (−1)n+1min(x1,...,xn)
we can rewrite the foregoing identity as
max
ixi+b=M+b⎛
⎝n−/parenleftBigg
n
2/parenrightBigg
+ ··· + (−1)n+1/parenleftBigg
nn/parenrightBigg⎞
⎠
But
0=(1−1)
n=1−n+/parenleftBigg
n
2/parenrightBigg
+ ··· + (−1)n/parenleftBigg
nn/parenrightBigg
The preceding two equations show that
max
ixi=M
and the proposition is proven.
It follows from Proposition 2.2 that for any random variables X1,...,Xn,
max
iXi=/summationdisplay
iXi−/summationdisplay
i<jmin(X i,Xj)+ ··· + (−1)n+1min(X1,...,Xn)

<<<PAGE 310>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 297
A First Course in Probability 297
Taking expectations of both sides of this equality yields the following relationship
between the expected value of the maximum and those of the partial minimums:
E/bracketleftbigg
max
iXi/bracketrightbigg
=/summationdisplay
iE[Xi]−/summationdisplay
i<jE[min(X i,Xj)]
+ ··· + (−1)n+1E[min(X 1,...,Xn)] (2.7)
Example
2sCoupon collecting with unequal probabilities
Suppose there are ntypes of coupons and that each time one collects a coupon, it
is, independently of previous coupons collected, a type icoupon with probability pi,
n/summationtext
i=1pi=1. Find the expected number of coupons one needs to collect to obtain a
complete set of at least one of each type.
Solution If we let Xidenote the number of coupons one needs to collect to obtain
a type i, then we can express Xas
X=max
i=1,...,nXi
Because each new coupon obtained is a type iwith probability pi,Xiis a geometric
random variable with parameter pi. Also, because the minimum of XiandXjis the
number of coupons needed to obtain either a type ior a type j, it follows that for
iZj,m i n( Xi,Xj) is a geometric random variable with parameter pi+pj. Similarly,
min (X i,Xj,Xk), the number needed to obtain any of types i,j, and k, is a geometric
random variable with parameter pi+pj+pk, and so on. Therefore, the identity (2.7)
yields
E[X]=/summationdisplay
i1
pi−/summationdisplay
i<j1
pi+pj+/summationdisplay
i<j<k1
pi+pj+pk
+ ··· + (−1)n+1 1
p1+ ··· + pn
Noting that
/integraldisplayq
0e−pxdx=1
p
and using the identity
1−n/productdisplay
i=1(1−e−pix)=/summationdisplay
ie−pix−/summationdisplay
i<je−(p i+pj)x+ ··· + (−1)n+1e−(p 1+···+ pn)x
shows, upon integrating the identity, that
E[X]=/integraldisplayq
0⎛⎝1−n/productdisplay
i=1(1−e−pix)⎞⎠dx
is a more useful computational form. .

<<<PAGE 311>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 298
298 Chapter 7 Properties of Expectation
7.3 Moments of the Number of Events that Occur
Many of the examples solved in the previous section were of the following form: For
given events A1,...,An, ﬁnd E[X], where Xis the number of these events that occur.
The solution then involved deﬁning an indicator variable Iifor event Aisuch that
Ii=/braceleftBigg
1, if Aioccurs
0, otherwise
Because
X=n/summationdisplay
i=1Ii
we obtained the result
E[X]=E⎡
⎣n/summationdisplay
i=1Ii⎤⎦=n/summationdisplay
i=1E[Ii]=n/summationdisplay
i=1P(Ai) (3.1)
Now suppose we are interested in the number of pairs of events that occur.
Because IiIjwill equal 1 if both AiandAjoccur and will equal 0 otherwise, it fol-
lows that the number of pairs is equal to/summationtext
i<jIiIj.But because Xis the number of
events that occur, it also follows that the number of pairs of events that occur is/parenleftbigX
2/parenrightbig
.
Consequently,/parenleftbiggX
2/parenrightbigg
=/summationdisplay
i<jIiIj
where there are/parenleftbign
2/parenrightbig
terms in the summation. Taking expectations yields
E/bracketleftBigg/parenleftbiggX
2/parenrightbigg/bracketrightBigg
=/summationdisplay
i<jE[IiIj]=/summationdisplay
i<jP(AiAj) (3.2)
or
E/bracketleftbiggX(X−1)
2/bracketrightbigg
=/summationdisplay
i<jP(AiAj)
giving that
E[X2]−E[X]=2/summationdisplay
i<jP(AiAj) (3.3)
which yields E[X2], and thus Var (X)=E[X2]−(E[X])2.
Moreover, by considering the number of distinct subsets of kevents that all
occur, we see that/parenleftbiggX
k/parenrightbigg
=/summationdisplay
i1<i2<...< ikIi1Ii2···Iik
Taking expectations gives the identity
E/bracketleftBigg/parenleftbiggX
k/parenrightbigg/bracketrightBigg
=/summationdisplay
i1<i2<...< ikE[Ii1Ii2···Iik]=/summationdisplay
i1<i2<...< ikP(Ai1Ai2···Aik) (3.4)

<<<PAGE 312>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 299
A First Course in Probability 299
Example
3aMoments of binomial random variables
Consider nindependent trials, with each trial being a success with probability p.L e t
Aibe the event that trial iis a success. When iZj,P(AiAj)=p2. Consequently,
Equation (3.2) yields
E/bracketleftBigg/parenleftbiggX
2/parenrightbigg/bracketrightBigg
=/summationdisplay
i<jp2=/parenleftbiggn
2/parenrightbigg
p2
or
E[X(X−1)]=n(n−1)p2
or
E[X2]−E[X]=n(n−1)p2
Now, E[X]=/summationtextn
i=1P(Ai)=np, so, from the preceding equation
Var(X)=E[X2]−(E[X])2=n(n−1)p2+np−(np)2=np(1−p)
which is in agreement with the result obtained in Section 4.6.1.
In general, because P(Ai1Ai2···Aik)=pk, we obtain from Equation (3.4) that
E/bracketleftBigg/parenleftbiggX
k/parenrightbigg/bracketrightBigg
=/summationdisplay
i1<i2<...< ikpk=/parenleftbiggn
k/parenrightbigg
pk
or, equivalently,
E[X(X−1)···(X−k+1)]=n(n−1)···(n−k+1)pk
The successive values E[Xk],kÚ3, can be recursively obtained from this identity.
For instance, with k=3, it yields
E[X(X−1)(X −2)]=n(n−1)(n−2)p3
or
E[X3−3X2+2X]=n(n−1)(n−2)p3
or
E[X3]=3E[X2]−2E[X]+n(n−1)(n−2)p3
=3n(n−1)p2+np+n(n−1)(n−2)p3.
Example
3bMoments of hypergeometric random variables
Suppose nballs are randomly selected from an urn containing Nballs, of which m
are white. Let Aibe the event that the ith ball selected is white. Then X, the number
of white balls selected, is equal to the number of the events A1,...,Anthat occur.
Because the ith ball selected is equally likely to be any of the Nballs, of which mare
white, P(Ai)=m/N.Consequently, Equation (3.1) gives that E[X]=/summationtextn
i=1P(Ai)=
nm/N . Also, since
P(AiAj)=P(Ai)P(Aj|Ai)=m
Nm−1
N−1

<<<PAGE 313>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 300
300 Chapter 7 Properties of Expectation
we obtain, from Equation (3.2), that
E/bracketleftBigg/parenleftbiggX
2/parenrightbigg/bracketrightBigg
=/summationdisplay
i<jm(m−1)
N(N−1)=/parenleftbiggn
2/parenrightbiggm(m−1)
N(N−1)
or
E[X(X−1)]=n(n−1)m(m−1)
N(N−1)
showing that
E[X2]=n(n−1)m(m−1)
N(N−1)+E[X]
This formula yields the variance of the hypergeometric, namely,
Var(X)=E[X2]−(E[X])2
=n(n−1)m(m−1)
N(N−1)+nm
N−n2m2
N2
=mn
N/bracketleftbigg(n−1)(m −1)
N−1+1−mn
N/bracketrightbigg
which agrees with the result obtained in Example 8j of Chapter 4.
Higher moments of Xare obtained by using Equation (3.4). Because
P(Ai1Ai2···Aik)=m(m−1)···(m−k+1)
N(N−1)···(N−k+1)
Equation (3.4) yields
E/bracketleftBigg/parenleftbiggX
k/parenrightbigg/bracketrightBigg
=/parenleftbiggn
k/parenrightbiggm(m−1)···(m−k+1)
N(N−1)···(N−k+1)
or
E[X(X−1)···(X−k+1)]
=n(n−1)···(n−k+1)m(m−1)···(m−k+1)
N(N−1)···(N−k+1).
Example
3cMoments in the match problem
Fori=1,...,N,l e tAibe the event that person iselects his or her own hat in the
match problem. Then
P(AiAj)=P(Ai)P(Aj|Ai)=1
N1
N−1
which follows because, conditional on person iselecting her own hat, the hat selected
by person jis equally likely to be any of the other N−1 hats, of which one is his
own. Consequently, with Xequal to the number of people who select their own hat,
it follows from Equation (3.2) that
E/bracketleftBigg/parenleftbiggX
2/parenrightbigg/bracketrightBigg
=/summationdisplay
i<j1
N(N−1)=/parenleftbiggN
2/parenrightbigg1
N(N−1)
thus showing that
E[X(X−1)]=1

<<<PAGE 314>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 301
A First Course in Probability 301
Therefore, E[X2]=1+E[X]. Because E[X]=/summationtextN
i=1P(Ai)=1, we obtain that
Var(X)=E[X2]−(E[X])2=1.
Hence, both the mean and variance of the number of matches is 1. For higher
moments, we use Equation (3.4), along with the fact that P(Ai1Ai2···Aik)=
1
N(N−1)···(N−k+1), to obtain
E/bracketleftBigg/parenleftbiggX
k/parenrightbigg/bracketrightBigg
=/parenleftbiggN
k/parenrightbigg1
N(N−1)···(N−k+1)
or
E[X(X−1)···(X−k+1)]=1 .
Example
3dAnother coupon-collecting problem
Suppose that there are Ndistinct types of coupons and that, independently of past
types collected, each new one obtained is type jwith probability pj,/summationtextN
j=1pj=1.
Find the expected value and variance of the number of different types of coupons
that appear among the ﬁrst ncollected.
Solution We will ﬁnd it more convenient to work with the number of uncollected
types. So, let Yequal the number of types of coupons collected, and let X=N−Y
denote the number of uncollected types. With Aideﬁned as the event that there are
no type icoupons in the collection, Xis equal to the number of the events A1,...,AN
that occur. Because the types of the successive coupons collected are independent,
and, with probability 1 −pieach new coupon is not type i, we have
P(Ai)=(1−pi)n
Hence, E[X]=/summationtextN
i=1(1−pi)n, from which it follows that
E[Y]=N−E[X]=N−N/summationdisplay
i=1(1−pi)n
Similarly, because each of the ncoupons collected is neither a type inor a type j
coupon, with probability 1 −pi−pj, we have
P(AiAj)=(1−pi−pj)n,iZj
Thus,
E[X(X−1)]=2/summationdisplay
i<jP(AiAj)=2/summationdisplay
i<j(1−pi−pj)n
or
E[X2]=2/summationdisplay
i<j(1−pi−pj)n+E[X]
Hence, we obtain
Var(Y)=Var(X)
=E[X2]−(E[X])2
=2/summationdisplay
i<j(1−pi−pj)n+N/summationdisplay
i=1(1−pi)n−⎛
⎝N/summationdisplay
i=1(1−pi)n⎞⎠2

<<<PAGE 315>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 302
302 Chapter 7 Properties of Expectation
In the special case where pi=1/N,i=1,...,N, the preceding formulas give
E[Y]=N/bracketleftBigg
1−/parenleftbigg
1−1
N/parenrightbiggn/bracketrightBigg
and
Var(Y)=N(N−1)/parenleftbigg
1−2
N/parenrightbiggn
+N/parenleftbigg
1−1
N/parenrightbiggn
−N2/parenleftbigg
1−1
N/parenrightbigg2n
.
Example
3eThe negative hypergeometric random variables
Suppose an urn contains n+mballs, of which nare special and mare ordinary. These
items are removed one at a time, with each new removal being equally likely to be
any of the balls that remain in the urn. The random variable Y, equal to the number
of balls that need be withdrawn until a total of rspecial balls have been removed,
is said to have a negative hypergeometric distribution. The negative hypergeometric
distribution bears the same relationship to the hypergeometric distribution as thenegative binomial does to the binomial. That is, in both cases, rather than considering
a random variable equal to the number of successes in a ﬁxed number of trials (as
are the binomial and hypergeometric variables), they refer to the number of trialsneeded to obtain a ﬁxed number of successes.
To obtain the probability mass function of a negative hypergeometric random
variable Y, note that Ywill equal kif both
1. the ﬁrst k−1 withdrawals consist of r−1 special and k−rordinary balls
and
2. the kth ball withdrawn is special.
Consequently,
P{Y=k}=/parenleftbig
n
r−1/parenrightbig/parenleftbigm
k−r/parenrightbig
/parenleftbign+m
k−1/parenrightbign−r+1
n+m−k+1
We will not, however, utilize the preceding probability mass function to obtain themean and variance of Y. Rather, let us number the mordinary balls as o
1,...,om,
and then, for each i=1,...,n,l e tA ibe the event that oiis withdrawn before r
special balls have been removed. Then, if Xis the number of the events A1,...,Am
that occur, it follows that Xis the number of ordinary balls that are withdrawn before
a total of rspecial balls have been removed. Consequently,
Y=r+X
showing that
E[Y]=r+E[X]=r+m/summationdisplay
i=1P(Ai)
To determine P(Ai), consider the n+1 balls consisting of oialong with the nspecial
balls. Of these n+1 balls, oiis equally likely to be the ﬁrst one withdrawn, or the
second one withdrawn, ..., or the ﬁnal one withdrawn. Hence, the probability that
it is among the ﬁrst rof these to be selected (and so is removed before a total or r
special balls have been withdrawn) isr
n+1. Consequently,
P(Ai)=r
n+1

<<<PAGE 316>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 303
A First Course in Probability 303
and
E[Y]=r+mr
n+1=r(n+m+1)
n+1
Thus, for instance, the expected number of cards of a well-shufﬂed deck that would
need to be turned over until a spade appears is 1 +39
14=3.786, and the expected
number of cards that would need to be turned over until an ace appears is1+
48
5=10.6.
To determine Var(Y) =Var(X), we use the identity
E[X(X−1)]=2/summationdisplay
i<jP(AiAj)
Now, P(AiAj)is the probability that both oiandojare removed before there have
been a total of rspecial balls removed. So consider the n+2 balls consisting of oi,oj,
and the nspecial balls. Because all withdrawal orderings of these balls are equally
likely, the probability that oiandojare both among the ﬁrst r+1 of them to be
removed (and so are both removed before rspecial balls have been withdrawn) is
P(AiAj)=/parenleftbig2
2/parenrightbig/parenleftbign
r−1/parenrightbig
/parenleftbign+2
r+1/parenrightbig=r(r+1)
(n+1)(n+2)
Consequently,
E[X(X−1)]=2/parenleftbiggm
2/parenrightbiggr(r+1)
(n+1)(n+2)
so
E[X2]=m(m−1)r(r+1)
(n+1)(n+2)+E[X]
Because E[X]=mr
n+1, this yields
Var(Y) =Var(X) =m(m−1)r(r+1)
(n+1)(n+2)+mr
n+1−/parenleftbigg
mr
n+1/parenrightbigg2
A little algebra now shows that
Var(Y) =mr(n+1−r)(n+m+1)
(n+1)2(n+2).
Example
3fSingletons in the coupon collector’s problem
Suppose that there are ndistinct types of coupons and that, independently of past
types collected, each new one obtained is equally likely to be any of the ntypes.
Suppose also that one continues to collect coupons until a complete set of at least
one of each type has been obtained. Find the expected value and variance of thenumber of types for which exactly one coupon of that type is collected.
Solution LetXequal the number of types for which exactly one of that type is
collected. Also, let Tidenote the ith type of coupon to be collected, and let Aibe
the event that there is only a single type Ticoupon in the complete set. Because X
is equal to the number of the events A1,...,Anthat occur, we have
E[X]=n/summationdisplay
i=1P(Ai)
Now, at the moment when the ﬁrst type Ticoupon is collected, there remain n−i
types that need to be collected to have a complete set. Because, starting at this

<<<PAGE 317>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 304
304 Chapter 7 Properties of Expectation
moment, each of these n−i+1 types (the n−inot yet collected and type Ti)
is equally likely to be the last of these types to be collected, it follows that the type
Tiwill be the last of these types (and so will be a singleton) with probability1
n−i+1.
Consequently, P(Ai)=1
n−i+1, yielding
E[X]=n/summationdisplay
i=11
n−i+1=n/summationdisplay
i=11
i
To determine the variance of the number of singletons, let Si,j,f o ri <j, be the event
that the ﬁrst type Ticoupon to be collected is still the only one of its type to have
been collected at the moment that the ﬁrst type Tjcoupon has been collected. Then
P(AiAj)=P(AiAj|Si,j)P(Si,j)
Now, P(Si,j)is the probability that when a type Tihas just been collected, of the
n−i+1 types consisting of type Tiand the n−ias yet uncollected types, a type Ti
is not among the ﬁrst j−iof these types to be collected. Because type Tiis equally
likely to be the ﬁrst, or second, or ...,n−i+1 of these types to be collected,
we have
P(Si,j)=1−j−i
n−i+1=n+1−j
n+1−i
Now, conditional on the event Si,j, both AiandAjwill occur if, at the time the ﬁrst
typeTjcoupon is collected, of the n−j+2 types consisting of types Ti,Tj, and the
n−jas yet uncollected types, TiandTjare both collected after the other n−j.B u t
this implies that
P(AiAj|Si,j)=21
n−j+21
n−j+1
Therefore,
P(AiAj)=2
(n+1−i)(n+2−j),i<j
yielding
E[X(X−1)]=4/summationdisplay
i<j1
(n+1−i)(n+2−j)
Consequently, using the previous result for E[X], we obtain
Var(X)=4/summationdisplay
i<j1
(n+1−i)(n+2−j)+n/summationdisplay
i=11
i−⎛
⎝n/summationdisplay
i=11
i⎞⎠2
.
7.4 Covariance, Variance of Sums, and Correlations
The following proposition shows that the expectation of a product of independent
random variables is equal to the product of their expectations.
Proposition
4.1IfXandYare independent, then, for any functions handg,
E[g(X)h(Y)]=E[g(X)]E[h(Y)]

<<<PAGE 318>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 305
A First Course in Probability 305
Proof Suppose that XandYare jointly continuous with joint density f(x,y). Then
E[g(X)h(Y)]=/integraldisplayq
−q/integraldisplayq
−qg(x)h(y)f (x,y)dx dy
=/integraldisplayq
−q/integraldisplayq
−qg(x)h(y)f X(x)fY(y)dx dy
=/integraldisplayq
−qh(y)f Y(y)dy/integraldisplayq
−qg(x)fX(x)dx
=E[h(Y)]E[g(X)]
The proof in the discrete case is similar.
Just as the expected value and the variance of a single random variable give
us information about that random variable, so does the covariance between two
random variables give us information about the relationship between the random
variables.
Deﬁnition
The covariance between XandY, denoted by Cov ( X,Y), is deﬁned by
Cov(X,Y)=E[(X −E[X])(Y −E[Y])]
Upon expanding the right side of the preceding deﬁnition, we see that
Cov(X,Y)=E[XY−E[X]Y−XE[Y]+E[Y]E[X]]
=E[XY ]−E[X]E[Y]−E[X]E[Y]+E[X]E[Y]
=E[XY ]−E[X]E[Y]
Note that if XandYare independent, then, by Proposition 4.1, Cov (X,Y)=0.
However, the converse is not true. A simple example of two dependent randomvariables XandYhaving zero covariance is obtained by letting Xbe a random
variable such that
P{X=0}=P {X=1}= P{X=−1}=1
3
and deﬁning
Y=/braceleftBigg
0i f XZ0
1i f X=0
Now, XY=0, so E[XY ]=0. Also, E[X]=0. Thus,
Cov(X,Y)=E[XY ]−E[X]E[Y]=0
However, XandYare clearly not independent.
The following proposition lists some of the properties of covariance.
Proposition
4.2(i) Cov (X,Y)=Cov(Y,X)
(ii) Cov(X ,X)=Var(X)
(iii) Cov(aX ,Y)=aCov(X,Y)

<<<PAGE 319>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 306
306 Chapter 7 Properties of Expectation
(iv) Cov⎛
⎜⎝n/summationdisplay
i=1Xi,m/summationdisplay
j=1Yj⎞
⎟⎠=n/summationdisplay
i=1m/summationdisplay
j=1Cov(Xi,Yj)
Proof of Proposition 4.2 Parts (i) and (ii) follow immediately from the deﬁnition
of covariance, and part (iii) is left as an exercise for the reader. To prove part (iv),
which states that the covariance operation is additive (as is the operation of taking
expectations), let μi=E[Xi] and vj=E[Yj]. Then
E⎡
⎣n/summationdisplay
i=1Xi⎤⎦=n/summationdisplay
i=1μi,E⎡
⎢⎣m/summationdisplay
j=1Yj⎤
⎥⎦=m/summationdisplay
j=1vj
and
Cov⎛
⎜⎝n/summationdisplay
i=1Xi,m/summationdisplay
j=1Yj⎞
⎟⎠=E⎡
⎢⎢⎣⎛
⎝n/summationdisplay
i=1Xi−n/summationdisplay
i=1μi⎞⎠⎛
⎜⎝m/summationdisplay
j=1Yj−m/summationdisplay
j=1vj⎞
⎟⎠⎤
⎥⎥⎦
=E⎡
⎢⎣n/summationdisplay
i=1(Xi−μi)m/summationdisplay
j=1(Yj−vj)⎤
⎥⎦
=E⎡
⎢⎣n/summationdisplay
i=1m/summationdisplay
j=1(Xi−μi)(Yj−vj)⎤
⎥⎦
=n/summationdisplay
i=1m/summationdisplay
j=1E[(Xi−μi)(Yj−vj)]
where the last equality follows because the expected value of a sum of random vari-
ables is equal to the sum of the expected values.
It follows from parts (ii) and (iv) of Proposition 4.2, upon taking Yj=Xj,j=
1,...,n, that
Var⎛
⎝n/summationdisplay
i=1Xi⎞⎠=Cov⎛
⎜⎝n/summationdisplay
i=1Xi,n/summationdisplay
j=1Xj⎞
⎟⎠
=n/summationdisplay
i=1n/summationdisplay
j=1Cov(Xi,Xj)
=n/summationdisplay
i=1Var(Xi)+/summationdisplay/summationdisplay
iZjCov(Xi,Xj)
Since each pair of indices i,j,iZj, appears twice in the double summation, the pre-
ceding formula is equivalent to
Var⎛
⎝n/summationdisplay
i=1Xi⎞⎠=n/summationdisplay
i=1Var(Xi)+2/summationdisplay/summationdisplay
i<jCov(Xi,Xj) (4.1)

<<<PAGE 320>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 307
A First Course in Probability 307
IfX1,...,Xnare pairwise independent, in that XiandXjare independent for
iZj, then Equation (4.1) reduces to
Var⎛
⎝n/summationdisplay
i=1Xi⎞⎠=n/summationdisplay
i=1Var(Xi)
The following examples illustrate the use of Equation (4.1).
Example
4aLetX1,...,Xnbe independent and identically distributed random variables having
expected value μand variance σ2, and as in Example 2c, let X=n/summationtext
i=1Xi/nbe the
sample mean. The quantities Xi−X,i=1,...,n, are called deviations, as they
equal the differences between the individual data and the sample mean. The random
variable
S2=n/summationdisplay
i=1(Xi−X)2
n−1
is called the sample variance .F i n d( a )V a r ( X) and (b) E[S2].
Solution
(a) Var (X)=/parenleftbigg1
n/parenrightbigg2
Var⎛
⎝n/summationdisplay
i=1Xi⎞⎠
=/parenleftbigg1
n/parenrightbigg2n/summationdisplay
i=1Var(Xi)by independence
=σ2
n
(b) We start with the following algebraic identity:
(n−1)S2=n/summationdisplay
i=1(Xi−μ+μ−X)2
=n/summationdisplay
i=1(Xi−μ)2+n/summationdisplay
i=1(X−μ)2−2(X−μ)n/summationdisplay
i=1(Xi−μ)
=n/summationdisplay
i=1(Xi−μ)2+n(X−μ)2−2(X−μ)n(X−μ)
=n/summationdisplay
i=1(Xi−μ)2−n(X−μ)2
Taking expectations of the preceding yields
(n−1)E[S2]=n/summationdisplay
i=1E[(Xi−μ)2]−nE[(X−μ)2]
=nσ2−nVar(X)
=(n−1)σ2

<<<PAGE 321>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 308
308 Chapter 7 Properties of Expectation
where the ﬁnal equality made use of part (a) of this example and the one preceding
it made use of the result of Example 2c, namely, that E[X]=μ. Dividing through
byn−1 shows that the expected value of the sample variance is the distribution
variance σ2. .
Our next example presents another method for obtaining the variance of a bino-
mial random variable.
Example
4bVariance of a binomial random variable
Compute the variance of a binomial random variable Xwith parameters nandp.
Solution Since such a random variable represents the number of successes in ninde-
pendent trials when each trial has the common probability pof being a success, we
may write
X=X1+ ··· + Xn
where the Xiare independent Bernoulli random variables such that
Xi=/braceleftBigg
1i f t h e ith trial is a success
0 otherwise
Hence, from Equation (4.1), we obtain
Var(X)=Var(X1)+ ··· + Var(Xn)
But
Var(Xi)=E[X2
i]−(E[Xi])2
=E[Xi]−(E[Xi])2since X2
i=Xi
=p−p2
Thus,
Var(X)=np(1−p) .
Example
4cSampling from a ﬁnite populationConsider a set of Npeople, each of whom has an opinion about a certain sub-
ject that is measured by a real number vthat represents the person’s “strength of
feeling” about the subject. Let v
irepresent the strength of feeling of person i,
i=1,...N.
Suppose that the quantities vi,i=1,...,N, are unknown and, to gather infor-
mation, a group of nof the Npeople is “randomly chosen” in the sense that all of the/parenleftBigg
N
n/parenrightBigg
subsets of size nare equally likely to be chosen. These npeople are then ques-
tioned and their feelings determined. If Sdenotes the sum of the nsampled values,
determine its mean and variance.
An important application of the preceding problem is to a forthcoming election
in which each person in the population is either for or against a certain candidate or
proposition. If we take vito equal 1 if person iis in favor and 0 if he or she is against,
then v=N/summationtext
i=1vi/Nrepresents the proportion of the population that is in favor. To
estimate v, a random sample of npeople is chosen, and these people are polled.

<<<PAGE 322>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 309
A First Course in Probability 309
The proportion of those polled who are in favor—that is, S/n—is often used as an
estimate of v.
Solution For each person i,i=1,...,N, deﬁne an indicator variable Iito indicate
whether or not that person is included in the sample. That is,
Ii=/braceleftBigg
1 if person iis in the random sample
0 otherwise
Now, Scan be expressed by
S=N/summationdisplay
i=1viIi
so
E[S]=N/summationdisplay
i=1viE[Ii]
Var(S)=N/summationdisplay
i=1Var(viIi)+2/summationdisplay/summationdisplay
i<jCov(viIi,vjIj)
=N/summationdisplay
i=1v2
iVar(Ii)+2/summationdisplay/summationdisplay
i<jvivjCov(Ii,Ij)
Because
E[Ii]=n
N
E[IiIj]=n
Nn−1
N−1
it follows that
Var(Ii)=n
N/parenleftbigg
1−n
N/parenrightbigg
Cov(Ii,Ij)=n(n−1)
N(N−1)−/parenleftbiggn
N/parenrightbigg2
=−n(N −n)
N2(N−1)
Hence,
E[S]=nN/summationdisplay
i=1vi
N=nv
Var(S)=n
N/parenleftbiggN−n
N/parenrightbiggN/summationdisplay
i=1v2i−2n(N −n)
N2(N−1)/summationdisplay/summationdisplay
i<jvivj

<<<PAGE 323>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 310
310 Chapter 7 Properties of Expectation
The expression for Var(S) can be simpliﬁed somewhat by using the identity
(v1+ ··· + vN)2=N/summationtext
i=1v2
i+2/summationtext/summationtext
i<jvivj. After some simpliﬁcation, we obtain
Var(S)=n(N−n)
N−1⎛
⎜⎜⎝N/summationdisplay
i=1v2
i
N−v2⎞
⎟⎟⎠
Consider now the special case in which Npof the v’s are equal to 1 and the
remainder equal to 0. Then, in this case, Sis a hypergeometric random variable and
has mean and variance given, respectively, by
E[S]=nv=np since v=Np
N=p
and
Var(S)=n(N−n)
N−1/parenleftbiggNp
N−p2/parenrightbigg
=n(N−n)
N−1p(1−p)
The quantity S/n, equal to the proportion of those sampled that have values equal to
1, is such that
E/bracketleftbiggS
n/bracketrightbigg
=p
Var/parenleftbiggS
n/parenrightbigg
=N−n
n(N−1)p(1−p) .
The correlation of two random variables XandY, denoted by ρ(X,Y), is deﬁned,
as long as Var (X)Var(Y)is positive, by
ρ(X,Y)=Cov(X,Y)√Var(X)Var(Y)
It can be shown that
−1…ρ(X,Y)…1 (4.2)
To prove Equation (4.2), suppose that XandYhave variances given by σ2
xandσ2
y,
respectively. Then, on the one hand,
0…Var/parenleftBigg
X
σx+Y
σy/parenrightBigg
=Var(X)
σ2x+Var(Y)
σ2y+2Cov(X,Y)
σxσy
=2[1+ρ(X,Y)]
implying that
−1…ρ(X,Y)

<<<PAGE 324>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 311
A First Course in Probability 311
On the other hand,
0…Var/parenleftBigg
X
σx−Y
σy/parenrightBigg
=Var(X)
σ2x+Var(Y)
(−σ y)2−2Cov(X,Y)
σxσy
=2[1−ρ(X,Y)]
implying that
ρ(X,Y)…1
which completes the proof of Equation (4.2).
In fact, since Var(Z )=0 implies that Zis constant with probability 1 (this intu-
itive relationship will be rigorously proven in Chapter 8), it follows from the proof
of Equation (4.2) that ρ(X,Y)=1 implies that Y=a+bX, where b=σy/σx>0
andρ(X,Y)=−1 implies that Y=a+bX, where b=−σy/σx<0. We leave it as
an exercise for the reader to show that the reverse is also true: that if Y=a+bX,
thenρ(X,Y)is either +1o r −1, depending on the sign of b.
The correlation coefﬁcient is a measure of the degree of linearity between X
andY. A value of ρ(X,Y)near+1o r −1 indicates a high degree of linearity between
XandY, whereas a value near 0 indicates that such linearity is absent. A positive
value of ρ(X,Y)indicates that Ytends to increase when Xdoes, whereas a negative
value indicates that Ytends to decrease when Xincreases. If ρ(X,Y)=0, then X
andYa r es a i dt ob e uncorrelated .
Example
4dLetIAandIBbe indicator variables for the events AandB. That is,
IA=/braceleftBigg
1i f Aoccurs
0 otherwise
IB=/braceleftBigg
1i f Boccurs
0 otherwise
Then
E[IA]=P(A)
E[IB]=P(B)
E[IAIB]=P(AB)
so
Cov(IA,IB)=P(AB) −P(A)P(B)
=P(B)[P(A|B) −P(A)]
Thus, we obtain the quite intuitive result that the indicator variables for AandB
are either positively correlated, uncorrelated, or negatively correlated, depending
on whether P(A|B) is, respectively, greater than, equal to, or less than P(A). .
Our next example shows that the sample mean and a deviation from the sample
mean are uncorrelated.
Example
4eLetX1,...,Xnbe independent and identically distributed random variables having
variance σ2. Show that
Cov(Xi−X,X)=0

<<<PAGE 325>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 312
312 Chapter 7 Properties of Expectation
Solution We have
Cov(Xi−X,X)=Cov(Xi,X)−Cov(X,X)
=Cov⎛
⎜⎝Xi,1
nn/summationdisplay
j=1Xj⎞
⎟⎠−Var(X)
=1
nn/summationdisplay
j=1Cov(Xi,Xj)−σ2
n
=σ2
n−σ2
n=0
where the next-to-last equality uses the result of Example 4a and the ﬁnal equality
follows because
Cov(Xi,Xj)=/braceleftBigg
0i f jZiby independence
σ2ifj=isince Var (Xi)=σ2
Although Xand the deviation Xi−Xare uncorrelated, they are not, in gen-
eral, independent. However, in the special case where the Xiare normal random
variables, it turns out that not only is Xindependent of a single deviation, but it is
independent of the entire sequence of deviations Xj−X,j=1,...,n. This result
will be established in Section 7.8, where we will also show that, in this case, the sam-
ple mean Xand the sample variance S2are independent, with (n−1)S2/σ2having
a chi-squared distribution with n−1 degrees of freedom. (See Example 4a for the
deﬁnition of S2.) .
Example
4fConsider mindependent trials, each of which results in any of rpossible outcomes
with probabilities p1,...,pr,/summationtextr
i=1pi=1. If we let Ni,i=1,...,r, denote the number
of the mtrials that result in outcome i, then N1,N2,...,Nrhave the multinomial
distribution
P{N1=n1,...,Nr=nr}=m!
n1!...nr!pn1
1···pnrr,r/summationdisplay
i=1ni=m
ForiZj, it seems likely that when Niis large, Njwould tend to be small; hence, it is
intuitive that they should be negatively correlated. Let us compute their covariance
by using Proposition 4.2(iv) and the representation
Ni=m/summationdisplay
k=1Ii(k) and Nj=m/summationdisplay
k=1Ij(k)
where
Ii(k)=/braceleftBigg
1i f t r i a l kresults in outcome i
0 otherwise
Ij(k)=/braceleftBigg
1i f t r i a l kresults in outcome j
0 otherwise

<<<PAGE 326>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 313
A First Course in Probability 313
From Proposition 4.2(iv), we have
Cov(Ni,Nj)=m/summationdisplay
/lscript=1m/summationdisplay
k=1Cov(Ii(k),Ij(/lscript))
Now, on the one hand, when kZ/lscript,
Cov(Ii(k),Ij(/lscript))=0
since the outcome of trial kis independent of the outcome of trial /lscript. On the other hand,
Cov(Ii(/lscript),Ij(/lscript))=E[Ii(/lscript)Ij(/lscript)]−E[Ii(/lscript)]E [Ij(/lscript)]
=0−pipj=−pipj
where the equation uses the fact that Ii(/lscript)Ij(/lscript)=0, since trial /lscriptcannot result in both
outcome iand outcome j. Hence, we obtain
Cov(Ni,Nj)=−mpipj
which is in accord with our intuition that NiandNjare negatively correlated. .
7.5 Conditional Expectation
7.5.1 Deﬁnitions
Recall that if XandYare jointly discrete random variables, then the conditional
probability mass function of X, given that Y=y, is deﬁned for all ysuch that
P{Y=y}>0, by
pX|Y(x|y)=P{X=x|Y=y}=p(x,y)
pY(y)
It is therefore natural to deﬁne, in this case, the conditional expectation of Xgiven
thatY=y, for all values of ysuch that pY(y)>0, by
E[X|Y=y]=/summationdisplay
xxP{X=x|Y=y}
=/summationdisplay
xxpX|Y(x|y)
Example
5aIfXandYare independent binomial random variables with identical parameters n
andp, calculate the conditional expected value of Xgiven that X+Y=m.
Solution Let us ﬁrst calculate the conditional probability mass function of Xgiven
thatX+Y=m.F o rk …min(n, m),
P{X=k|X+Y=m}=P{X=k,X+Y=m}
P{X+Y=m}
=P{X=k,Y=m−k}
P{X+Y=m}
=P{X=k}P{Y=m−k}
P{X+Y=m}

<<<PAGE 327>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 314
314 Chapter 7 Properties of Expectation
=/parenleftBigg
n
k/parenrightBigg
pk(1−p)n−k/parenleftBigg
n
m−k/parenrightBigg
pm−k(1−p)n−m+k
/parenleftBigg
2n
m/parenrightBigg
pm(1−p)2n−m
=/parenleftBigg
nk/parenrightBigg/parenleftBigg
n
m−k/parenrightBigg
/parenleftBigg
2n
m/parenrightBigg
where we have used the fact (see Example 3f of Chapter 6) that X+Yis a binomial
random variable with parameters 2 nandp. Hence, the conditional distribution of X,
given that X+Y=m, is the hypergeometric distribution, and from Example 2g,
we obtain
E[X|X+Y=m]=m
2.
Similarly, let us recall that if XandYare jointly continuous with a joint probabil-
ity density function f(x,y), then the conditional probability density of X, given that
Y=y, is deﬁned for all values of ysuch that fY(y)>0b y
fX|Y(x|y)=f(x,y)
fY(y)
It is natural, in this case, to deﬁne the conditional expectation of X, given that
Y=y,b y
E[X|Y=y]=/integraldisplayq
−qxfX|Y(x|y) dx
provided that fY(y)>0.
Example
5bSuppose that the joint density of XandYis given by
f(x,y)=e−x/ye−y
y0<x<q,0 <y<q
Compute E[X|Y=y].
Solution We start by computing the conditional density
fX|Y(x|y)=f(x,y)
fY(y)
=f(x,y)/integraldisplayq
−qf(x,y)dx
=(1/y)e−x/ye−y
/integraldisplayq
0(1/y)e−x/ye−ydx
=(1/y)e−x/y
/integraldisplayq
0(1/y)e−x/ydx
=1
ye−x/y

<<<PAGE 328>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 315
A First Course in Probability 315
Hence, the conditional distribution of X, given that Y=y, is just the exponential
distribution with mean y. Thus,
E[X|Y=y]=/integraldisplayq
0x
ye−x/ydx=y .
Remark Just as conditional probabilities satisfy all of the properties of ordinary
probabilities, so do conditional expectations satisfy the properties of ordinary expec-
tations. For instance, such formulas as
E[g(X)|Y=y]=⎧
⎪⎪⎪⎨
⎪⎪⎪⎩/summationdisplay
xg(x)p X|Y(x|y) in the discrete case
/integraldisplayq
−qg(x)fX|Y(x|y) dx in the continuous case
and
E⎡
⎣n/summationdisplay
i=1Xi|Y=y⎤⎦=n/summationdisplay
i=1E[Xi|Y=y]
remain valid. As a matter of fact, conditional expectation given that Y=ycan be
thought of as being an ordinary expectation on a reduced sample space consisting
only of outcomes for which Y=y. .
7.5.2 Computing Expectations by Conditioning
Let us denote by E[X|Y] that function of the random variable Ywhose value at
Y=yisE[X|Y=y]. Note that E[X|Y] is itself a random variable. An extremely
important property of conditional expectations is given by the following proposition.
Proposition
5.1E[X]=E[E[X|Y]] (5.1)
IfYis a discrete random variable, then Equation (5.1) states that
E[X]=/summationdisplay
yE[X|Y=y]P{Y=y} (5.1a)
whereas if Yis continuous with density fY(y), then Equation (5.1) states
E[X]=/integraldisplayq
−qE[X|Y=y]fY(y)dy (5.1b)
We now give a proof of Equation (5.1) in the case where XandYare both discrete
random variables.
Proof of Equation (5.1) When XandYAre Discrete: We must show that
E[X]=/summationdisplay
yE[X|Y=y]P{Y=y} (5.2)

<<<PAGE 329>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 316
316 Chapter 7 Properties of Expectation
Now, the right-hand side of Equation (5.2) can be written as
/summationdisplay
yE[X|Y=y]P{Y=y}=/summationdisplay
y/summationdisplay
xxP{X=x|Y=y}P{Y=y}
=/summationdisplay
y/summationdisplay
xxP{X=x,Y=y}
P{Y=y}P{Y=y}
=/summationdisplay
y/summationdisplay
xxP{X=x,Y=y}
=/summationdisplay
xx/summationdisplay
yP{X=x,Y=y}
=/summationdisplay
xxP{X=x}
=E[X]
and the result is proved.
One way to understand Equation (5.2) is to interpret it as follows: To calcu-
lateE[X], we may take a weighted average of the conditional expected value of X
given that Y=y, each of the terms E[X|Y=y] being weighted by the probabil-
ity of the event on which it is conditioned. (Of what does this remind you?) This
is an extremely useful result that often enables us to compute expectations easily
by ﬁrst conditioning on some appropriate random variable. The following examplesillustrate its use.
Example
5cA miner is trapped in a mine containing 3 doors. The ﬁrst door leads to a tunnel thatwill take him to safety after 3 hours of travel. The second door leads to a tunnel that
will return him to the mine after 5 hours of travel. The third door leads to a tunnel
that will return him to the mine after 7 hours. If we assume that the miner is at alltimes equally likely to choose any one of the doors, what is the expected length of
time until he reaches safety?
Solution LetXdenote the amount of time (in hours) until the miner reaches safety,
and let Ydenote the door he initially chooses. Now,
E[X]=E[X|Y=1]P{Y=1}+ E[X|Y=2]P{Y=2}
+E[X|Y=3]P{Y=3}
=1
3(E[X|Y=1]+E[X|Y=2]+E[X|Y=3])
However,
E[X|Y=1]=3
E[X|Y=2]=5+E[X] (5.3)
E[X|Y=3]=7+E[X]
To understand why Equation (5.3) is correct, consider, for instance, E[X|Y=2]
and reason as follows: If the miner chooses the second door, he spends 5 hours in
the tunnel and then returns to his cell. But once he returns to his cell, the prob-lem is as before; thus, his expected additional time until safety is just E[X]. Hence,
E[X|Y=2]=5+E[X]. The argument behind the other equalities in Equation (5.3)
is similar. Hence,

<<<PAGE 330>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 317
A First Course in Probability 317
E[X]=1
3(3+5+E[X]+7+E[X])
or
E[X]=15 .
Example
5dExpectation of a sum of a random number of random variables
Suppose that the number of people entering a department store on a given day is
a random variable with mean 50. Suppose further that the amounts of money spent
by these customers are independent random variables having a common mean of$8. Finally, suppose also that the amount of money spent by a customer is also inde-
pendent of the total number of customers who enter the store. What is the expected
amount of money spent in the store on a given day?
Solution If we let Ndenote the number of customers who enter the store and Xi
the amount spent by the ith such customer, then the total amount of money spent
can be expressed asN/summationtext
i=1Xi.N o w ,
E⎡
⎣N/summationdisplay
1Xi⎤⎦=E⎡
⎢⎣E⎡
⎣N/summationdisplay
1Xi|N⎤⎦⎤
⎥⎦
But
E⎡
⎣N/summationdisplay
1Xi|N=n⎤⎦=E⎡⎣
n/summationdisplay
1Xi|N=n⎤⎦
=E⎡
⎣n/summationdisplay
1Xi⎤⎦ by the independence of the X
iandN
=nE[X] where E[X]=E[Xi]
which implies that
E⎡⎣N/summationdisplay
1Xi|N⎤⎦=NE[X]
Thus,
E⎡⎣
N/summationdisplay
i=1Xi⎤⎦=E[NE[X]]=E[N]E[X]
Hence, in our example, the expected amount of money spent in the store is 50 *$8,
or $400. .
Example
5eThe game of craps is begun by rolling an ordinary pair of dice. If the sum of the
dice is 2, 3, or 12, the player loses. If it is 7 or 11, the player wins. If it is any othernumber i, the player continues to roll the dice until the sum is either 7 or i.I fi ti s
7, the player loses; if it is i, the player wins. Let Rdenote the number of rolls of the
dice in a game of craps. Find

<<<PAGE 331>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 318
318 Chapter 7 Properties of Expectation
(a)E[R];
(b)E[R|player wins];
(c)E[R|player loses].
Solution If we let Pidenote the probability that the sum of the dice is i, then
Pi=P14−i=i−1
36,i=2,...,7
To compute E[R], we condition on S, the initial sum, giving
E[R]=12/summationdisplay
i=2E[R|S=i]Pi
However,
E[R|S=i]=⎧
⎨
⎩1, ifi=2, 3, 7, 11, 12
1+1
Pi+P7, otherwise
The preceding equation follows because if the sum is a value ithat does not end
the game, then the dice will continue to be rolled until the sum is either ior 7, and
the number of rolls until this occurs is a geometric random variable with parameter
Pi+P7. Therefore,
E[R]=1+6/summationdisplay
i=4Pi
Pi+P7+10/summationdisplay
i=8Pi
Pi+P7
=1+2(3/9 +4/10+5/11) =3.376
To determine E[R|win], let us start by determining p, the probability that the player
wins. Conditioning on Syields
p=12/summationdisplay
i=2P{win|S =i}Pi
=P7+P11+6/summationdisplay
i=4Pi
Pi+P7Pi+10/summationdisplay
i=8Pi
Pi+P7Pi
=0.493
where the preceding uses the fact that the probability of obtaining a sum of ibefore
one of 7 is Pi/(Pi+P7). Now, let us determine the conditional probability mass
function of S, given that the player wins. Letting Qi=P{S=i|win}, we have
Q2=Q3=Q12=0,Q7=P7/p, Q11=P11/p
and, for i=4, 5, 6, 8, 9, 10,
Qi=P{S=i,w i n}
P{win}
=PiP{win|S =i}
p
=P2
i
p(Pi+P7)

<<<PAGE 332>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 319
A First Course in Probability 319
Now, conditioning on the initial sum gives
E[R|win] =/summationdisplay
iE[R|win, S=i]Qi
However, as was noted in Example 2j of Chapter 6, given that the initial sum is i,
the number of additional rolls needed and the outcome (whether a win or a loss)
are independent. (This is easily seen by ﬁrst noting that conditional on an initial sumofi, the outcome is independent of the number of additional dice rolls needed and
then using the symmetry property of independence, which states that if event Ais
independent of event B, then event Bis independent of event A.) Therefore,
E[R|win] =/summationdisplay
iE[R|S=i]Qi
=1+6/summationdisplay
i=4Qi
Pi+P7+10/summationdisplay
i=8Qi
Pi+P7
=2.938
Although we could determine E[R|player loses] exactly as we did E[R|player
wins], it is easier to use
E[R]=E[R|win] p+E[R|lose] (1−p)
implying that
E[R|lose] =E[R]−E[R|win] p
1−p=3.801 .
Example
5fAs deﬁned in Example 5d of Chapter 6, the bivariate normal joint density functionof the random variables XandYis
f(x,y)=1
2πσ xσy/radicalbig
1−ρ2exp⎧
⎪⎨
⎪⎩−1
2(1−ρ2)⎡
⎣/parenleftbiggx−μx
σx/parenrightbigg2
+/parenleftBigg
y−μy
σy/parenrightBigg2
−2ρ(x−μx)(y−μy)
σxσy⎤
⎦⎫
⎪⎬
⎪⎭
We will now show that ρis the correlation between XandY. As shown in Exam-
ple 5c, μx=E[X],σ2
x=Var(X), and μy=E[Y],σ2
y=Var(Y). Consequently,
Corr(X ,Y)=Cov(X,Y)
σxσy
=E[XY ]−μxμy
σxσy
To determine E[XY ], we condition on Y. That is, we use the identity
E[XY ]=E/bracketleftbig
E[XY|Y]/bracketrightbig

<<<PAGE 333>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 320
320 Chapter 7 Properties of Expectation
Recalling from Example 5d that the conditional distribution of Xgiven that Y=y
is normal with mean μx+ρσx
σy(y−μy), we see that
E[XY|Y=y]=E[Xy|Y =y]
=yE[X|Y=y]
=y/bracketleftBigg
μx+ρσx
σy(y−μy)/bracketrightBigg
=yμx+ρσx
σy(y2−μyy)
Consequently,
E[XY|Y]=Yμx+ρσx
σy(Y2−μyY)
implying that
E[XY ]=E/bracketleftBigg
Yμx+ρσx
σy(Y2−μyY)/bracketrightBigg
=μxE[Y]+ρσx
σyE[Y2−μyY]
=μxμy+ρσx
σy/parenleftBig
E[Y2]−μ2
y/parenrightBig
=μxμy+ρσx
σyVar(Y)
=μxμy+ρσxσy
Therefore,
Corr(X ,Y)=ρσxσy
σxσy=ρ .
Sometimes E[X] is easy to compute, and we use the conditioning identity to
compute a conditional expected value. This approach is illustrated by our next
example.
Example
5gConsider nindependent trials, each of which results in one of the outcomes 1, ...,k,
with respective probabilities p1,...,pk,/summationtextk
i=1pi=1. Let Nidenote the number of
trials that result in outcome i,i=1,...,k.F o ri Zj, ﬁnd
(a) E[Nj|Ni>0] and (b) E[Nj|Ni>1]
Solution To solve (a), let
I=/braceleftBigg
0, if Ni=0
1, if Ni>0
Then
E[Nj]=E[Nj|I=0]P{I=0}+ E[Nj|I=1]P{I=1}
or, equivalently,
E[Nj]=E[Nj|Ni=0]P{Ni=0}+ E[Nj|Ni>0]P{Ni>0}
Now, the unconditional distribution of Njis binomial with parameters n,pj.A l s o ,
given that Ni=r, each of the n−rtrials that does not result in outcome iwill,
independently, result in outcome jwith probability P(j|noti)=pj
1−p i. Consequently,

<<<PAGE 334>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 321
A First Course in Probability 321
the conditional distribution of Nj, given that Ni=r, is binomial with parameters
n−r,pj
1−p i. (For a more detailed argument for this conclusion, see Example 4c of
Chapter 6.) Because P{Ni=0}=(1−pi)n, the preceding equation yields
npj=npj
1−pi(1−pi)n+E[Nj|Ni>0](1−(1−pi)n)
giving the result
E[Nj|Ni>0]=npj1−(1−pi)n−1
1−(1−pi)n
We can solve part (b) in a similar manner. Let
J=⎧
⎪⎨
⎪⎩0, if Ni=0
1, if Ni=1
2, if Ni>1
Then
E[Nj]=E[Nj|J=0]P{J=0}+ E[Nj|J=1]P{J=1}
+E[Nj|J=2]P{J=2}
or, equivalently,
E[Nj]=E[Nj|Ni=0]P{Ni=0}+ E[Nj|Ni=1]P{Ni=1}
+E[Nj|Ni>1]P{Ni>1}
This equation yields
npj=npj
1−pi(1−pi)n+(n−1)pj
1−pinpi(1−pi)n−1
+E[Nj|Ni>1](1−(1−pi)n−npi(1−pi)n−1)
giving the result
E[Nj|Ni>1]=npj[1−(1−pi)n−1−(n−1)pi(1−pi)n−2]
1−(1−pi)n−npi(1−pi)n−1.
It is also possible to obtain the variance of a random variable by conditioning.
We illustrate this approach by the following example.
Example
5hVariance of the geometric distribution
Independent trials, each resulting in a success with probability p, are successively
performed. Let Nbe the time of the ﬁrst success. Find Var(N ).
Solution LetY=1 if the ﬁrst trial results in a success and Y=0 otherwise. Now,
Var(N)=E[N2]−(E[N])2
To calculate E[N2], we condition on Yas follows:
E[N2]=E[E[N2|Y]]
However,
E[N2|Y=1]=1
E[N2|Y=0]=E[(1+N)2]

<<<PAGE 335>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 322
322 Chapter 7 Properties of Expectation
These two equations follow because, on the one hand, if the ﬁrst trial results in a
success, then, clearly, N=1; thus, N2=1. On the other hand, if the ﬁrst trial results
in a failure, then the total number of trials necessary for the ﬁrst success will have
the same distribution as 1 (the ﬁrst trial that results in failure) plus the necessarynumber of additional trials. Since the latter quantity has the same distribution as N,
we obtain E[N
2|Y=0]=E[(1+N)2]. Hence,
E[N2]=E[N2|Y=1]P{Y=1}+ E[N2|Y=0]P{Y=0}
=p+(1−p)E[(1+N)2]
=1+(1−p)E[2N+N2]
However, as was shown in Example 8b of Chapter 4, E[N]=1/p; therefore,
E[N2]=1+2(1−p)
p+(1−p)E[N2]
or
E[N2]=2−p
p2
Consequently,
Var(N)=E[N2]−(E[N])2
=2−p
p2−/parenleftbigg1
p/parenrightbigg2
=1−p
p2.
Example
5iConsider a gambling situation in which there are rplayers, with player iinitially
having niunits, ni>0,i=1,...,r.At each stage, two of the players are chosen to
play a game, with the winner of the game receiving 1 unit from the loser. Any player
whose fortune drops to 0 is eliminated, and this continues until a single player has
allnK/summationtextr
i=1niunits, with that player designated as the victor. Assuming that the
results of successive games are independent and that each game is equally likely to
be won by either of its two players, ﬁnd the average number of stages until one of
the players has all nunits.
Solution To ﬁnd the expected number of stages played, suppose ﬁrst that there are
only 2 players, with players 1 and 2 initially having jandn−junits, respectively.
LetXjdenote the number of stages that will be played, and let mj=E[Xj].Then,
forj=1,...,n−1,
Xj=1+Aj
where Ajis the additional number of stages needed beyond the ﬁrst stage. Taking
expectations gives
mj=1+E[Aj]
Conditioning on the result of the ﬁrst stage then yields
mj=1+E[Aj|1 wins ﬁrst stage]1 /2+E[Aj|2 wins ﬁrst stage]1 /2
Now, if player 1 wins at the ﬁrst stage, then the situation from that point on is exactly
the same as in a problem that supposes that player 1starts with j+1 and player 2
withn−(j+1)units. Consequently,
E[Aj|1 wins ﬁrst stage] =mj+1

<<<PAGE 336>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 323
A First Course in Probability 323
and, analogously,
E[Aj|2 wins ﬁrst stage] =mj−1
Thus,
mj=1+1
2mj+1+1
2mj−1
or, equivalently,
mj+1=2mj−mj−1−2,j=1,...,n−1 (5.4)
Using that m0=0, the preceding equation yields
m2=2m1−2
m3=2m2−m1−2=3m1−6=3(m 1−2)
m4=2m3−m2−2=4m1−12=4(m 1−3)
suggesting that
mi=i(m 1−i+1), i=1,...,n (5.5)
To prove the preceding equality, we use mathematical induction. Since we’ve already
shown the equation to be true for i=1, 2, we take as the induction hypothesis that
it is true whenever i…j<n. Now we must prove that it is true for j+1. Using
Equation (5.4) yields
mj+1=2mj−mj−1−2
=2j(m1−j+1)−(j−1)(m 1−j+2)−2(by the induction hypothesis )
=(j+1)m 1−2j2+2j+j2−3j+2−2
=(j+1)m 1−j2−j
=(j+1)(m 1−j)
which completes the induction proof of (5.5). Letting i=nin(5.5), and using that
mn=0, now yields that
m1=n−1
which, again using (5.5), gives the result
mi=i(n−i)
Thus, the mean number of games played when there are only 2players with initial
amounts iandn−iis the product of their initial amounts. Because both players
play all stages, this is also the mean number of stages involving player 1.
Now let us return to the problem involving rplayers with initial amounts ni,i=
1,...,r,/summationtextr
i=1ni=n.LetXdenote the number of stages needed to obtain a victor,
and let Xidenote the number of stages involving player i. Now, from the point of
view of player i, starting with ni, he will continue to play stages, independently being
equally likely to win or lose each one, until his fortune is either nor 0. Thus, the
number of stages he plays is exactly the same as when he has a single opponent with
an initial fortune of n−ni. Consequently, by the preceding result, it follows that
E[Xi]=ni(n−ni)

<<<PAGE 337>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 324
324 Chapter 7 Properties of Expectation
so
E⎡
⎣r/summationdisplay
i=1Xi⎤⎦=r/summationdisplay
i=1ni(n−ni)=n2−r/summationdisplay
i=1n2
i
But because each stage involves two players,
X=1
2r/summationdisplay
i=1Xi
Taking expectations now yields
E[X]=1
2⎛
⎝n2−r/summationdisplay
i=1n2
i⎞
⎠
It is interesting to note that while our argument shows that the mean number of
stages does not depend on the manner in which the teams are selected at each stage,
the same is not true for the distribution of the number of stages. To see this, suppose
r=3,n1=n2=1, and n3=2.If players 1and2are chosen in the ﬁrst stage, then
it will take at least three stages to determine a winner, whereas if player 3 is in theﬁrst stage, then it is possible for there to be only two stages. .
In our next example, we use conditioning to verify a result previously noted in
Section 6.3.1: that the expected number of uniform (0, 1) random variables that need
to be added for their sum to exceed 1 is equal to e.
Example
5jLetU1,U2,...be a sequence of independent uniform (0, 1) random variables. Find
E[N] when
N=min⎧
⎨
⎩n:n/summationdisplay
i=1Ui>1⎫
⎬
⎭
Solution We will ﬁnd E[N] by obtaining a more general result. For x∈[0, 1], let
N(x)=min⎧
⎨
⎩n:n/summationdisplay
i=1Ui>x⎫
⎬
⎭
and set
m(x)=E[N(x)]
That is, N(x)is the number of uniform (0, 1) random variables we must add until
their sum exceeds x, and m(x) is its expected value. We will now derive an equation
form(x) by conditioning on U1. This gives, from Equation (5.1b),
m(x)=/integraldisplay1
0E[N(x)|U 1=y]dy (5.6)
Now,
E[N(x)|U 1=y]=/braceleftBigg
1i fy>x
1+m(x−y) ify…x(5.7)
The preceding formula is obviously true when y>x.I ti sa l s ot r u ew h e n y…x,
since, if the ﬁrst uniform value is y, then, at that point, the remaining number of
uniform random variables needed is the same as if we were just starting and were

<<<PAGE 338>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 325
A First Course in Probability 325
going to add uniform random variables until their sum exceeded x−y. Substituting
Equation (5.7) into Equation (5.6) gives
m(x)=1+/integraldisplayx
0m(x−y)dy
=1+/integraldisplayx
0m(u)duby letting
u=x−y
Differentiating the preceding equation yields
m/prime(x)=m(x)
or, equivalently,
m/prime(x)
m(x)=1
Integrating this equation gives
log[m(x)] =x+c
or
m(x)=kex
Since m(0)=1, it follows that k=1, so we obtain
m(x)=ex
Therefore, m(1), the expected number of uniform (0, 1) random variables that need
to be added until their sum exceeds 1, is equal to e. .
7.5.3 Computing Probabilities by Conditioning
Not only can we obtain expectations by ﬁrst conditioning on an appropriate randomvariable, but we can also use this approach to compute probabilities. To see this, let
Edenote an arbitrary event, and deﬁne the indicator random variable Xby
X=/braceleftBigg
1i f Eoccurs
0i f Edoes not occur
It follows from the deﬁnition of Xthat
E[X]=P(E)
E[X|Y=y]=P(E|Y=y)for any random variable Y
Therefore, from Equations (5.1a) and (5.1b), we obtain
P(E)=/summationdisplay
yP(E|Y=y)P(Y=y)ifYis discrete
=/integraldisplayq
−qP(E|Y=y)fY(y)dy ifYis continuous(5.8)
Note that if Yis a discrete random variable taking on one of the values y1,...,yn,
then by deﬁning the events Fi,i=1,...,n,b y Fi={Y=yi}, Equation (5.8) reduces
to the familiar equation
P(E)=n/summationdisplay
i=1P(E|Fi)P(Fi)
where F1,...,Fnare mutually exclusive events whose union is the sample space.

<<<PAGE 339>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 326
326 Chapter 7 Properties of Expectation
Example
5kThe best-prize problem
Suppose that we are to be presented with ndistinct prizes, in sequence. After being
presented with a prize, we must immediately decide whether to accept it or to reject
it and consider the next prize. The only information we are given when decidingwhether to accept a prize is the relative rank of that prize compared to ones already
seen. That is, for instance, when the ﬁfth prize is presented, we learn how it compares
with the four prizes we’ve already seen. Suppose that once a prize is rejected, it islost, and that our objective is to maximize the probability of obtaining the best prize.
Assuming that all n! orderings of the prizes are equally likely, how well can we do?
Solution Rather surprisingly, we can do quite well. To see this, ﬁx a value k,0…
k<n, and consider the strategy that rejects the ﬁrst kprizes and then accepts the
ﬁrst one that is better than all of those ﬁrst k.L e t Pk(best) denote the probability that
the best prize is selected when this strategy is employed. To compute this probability,
condition on X, the position of the best prize. This gives
Pk(best)=n/summationdisplay
i=1Pk(best|X =i)P(X=i)
=1
nn/summationdisplay
i=1Pk(best|X =i)
Now, on the one hand, if the overall best prize is among the ﬁrst k, then no prize is
ever selected under the strategy considered. That is,
Pk(best|X =i)=0i f i…k
On the other hand, if the best prize is in position i, where i>k, then the best prize
will be selected if the best of the ﬁrst i−1 prizes is among the ﬁrst k(for then none
of the prizes in positions k+1,k+2,...,i−1 would be selected). But, conditional
on the best prize being in position i, it is easy to verify that all possible orderings of
the other prizes remain equally likely, which implies that each of the ﬁrst i−1 prizes
is equally likely to be the best of that batch. Hence, we have
Pk(best|X =i)=P{b e s to fﬁ r s t i−1 is among the ﬁrst k|X=i}
=k
i−1ifi>k
From the preceding, we obtain
Pk(best) =k
nn/summationdisplay
i=k+11
i−1
Lk
n/integraldisplayn
k+11
x−1dx
=k
nlog/parenleftbiggn−1
k/parenrightbigg
Lk
nlog/parenleftbiggn
k/parenrightbigg
Now, if we consider the function
g(x)=x
nlog/parenleftbiggn
x/parenrightbigg

<<<PAGE 340>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 327
A First Course in Probability 327
then
g/prime(x)=1
nlog/parenleftbiggn
x/parenrightbigg
−1
n
so
g/prime(x)=0*log/parenleftbiggn
x/parenrightbigg
=1*x=n
e
Thus, since Pk(best) Lg(k), we see that the best strategy of the type considered is to
let the ﬁrst n/eprizes go by and then accept the ﬁrst one to appear that is better than
all of those. In addition, since g(n/e) =1/e, the probability that this strategy selects
the best prize is approximately 1/e L.36788.
Remark Most people are quite surprised by the size of the probability of obtaining
the best prize, thinking that this probability would be close to 0 when nis large. How-
ever, even without going through the calculations, a little thought reveals that the
probability of obtaining the best prize can be made reasonably large. Consider the
strategy of letting half of the prizes go by and then selecting the ﬁrst one to appearthat is better than all of those. The probability that a prize is actually selected is the
probability that the overall best is among the second half, and this is
1
2. In addition,
given that a prize is selected, at the time of selection that prize would have been
the best of more than n/2 prizes to have appeared and would thus have probability
of at least1
2of being the overall best. Hence, the strategy of letting the ﬁrst half of
all prizes go by and then accepting the ﬁrst one that is better than all of those prizes
has a probability greater than1
4of obtaining the best prize. .
Example
5lLetUbe a uniform random variable on (0, 1), and suppose that the conditional
distribution of X, given that U=p, is binomial with parameters nandp.F i n dt h e
probability mass function of X.
Solution Conditioning on the value of Ugives
P{X=i}=/integraldisplay1
0P{X=i|U=p}fU(p)dp
=/integraldisplay1
0P{X=i|U=p}dp
=n!
i!(n−i)!/integraldisplay1
0pi(1−p)n−idp
Now, it can be shown (a probabilistic proof is given in Section 6.6) that
/integraldisplay1
0pi(1−p)n−idp=i!(n−i)!
(n+1)!
Hence, we obtain
P{X=i}=1
n+1i=0,...,n
That is, we obtain the surprising result that if a coin whose probability of coming
up heads is uniformly distributed over (0, 1) is ﬂipped ntimes, then the number of
heads occurring is equally likely to be any of the values 0, ...,n.
Because the preceding conditional distribution has such a nice form, it is worth
trying to ﬁnd another argument to enhance our intuition as to why such a result
is true. To do so, let U,U1,...,Unben+1 independent uniform (0, 1) random

<<<PAGE 341>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 328
328 Chapter 7 Properties of Expectation
variables, and let Xdenote the number of the random variables U1,...,Unthat are
smaller than U. Since all the random variables U,U1,...,Unhave the same distri-
bution, it follows that Uis equally likely to be the smallest, or second smallest, or
largest of them; so Xis equally likely to be any of the values 0, 1, ...,n. However,
given that U=p, the number of the Uithat are less than Uis a binomial random
variable with parameters nandp, thus establishing our previous result. .
Example
5mSuppose that XandYare independent continuous random variables having densi-
tiesfXandfY, respectively. Compute P{X<Y}.
Solution Conditioning on the value of Yyields
P{X<Y}=/integraldisplayq
−qP{X<Y|Y=y}fY(y)dy
=/integraldisplayq
−qP{X<y|Y=y}fY(y)dy
=/integraldisplayq
−qP{X<y}fY(y}dy by independence
=/integraldisplayq
−qFX(y)fY(y)dy
where
FX(y)=/integraldisplayy
−qfX(x)dx .
Example
5nSuppose that XandYare independent continuous random variables. Find the dis-
tribution of X+Y.
Solution By conditioning on the value of Y, we obtain
P{X+Y<a}=/integraldisplayq
−qP{X+Y<a|Y=y}fY(y)dy
=/integraldisplayq
−qP{X+y<a|Y=y}fY(y)dy
=/integraldisplayq
−qP{X<a−y}fY(y)dy
=/integraldisplayq
−qFX(a−y)fY(y)dy .
7.5.4 Conditional Variance
Just as we have deﬁned the conditional expectation of Xgiven the value of Y,w e
can also deﬁne the conditional variance of Xgiven that Y=y:
Var(X|Y)KE[(X−E[X|Y])2|Y]
That is, Var(X |Y)is equal to the (conditional) expected square of the difference
between Xand its (conditional) mean when the value of Yis given. In other words,
Var(X|Y)is exactly analogous to the usual deﬁnition of variance, but now all expec-
tations are conditional on the fact that Yis known.
There is a very useful relationship between Var(X ), the unconditional variance
ofX, and Var (X|Y), the conditional variance of Xgiven Y, that can often be applied

<<<PAGE 342>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 329
A First Course in Probability 329
to compute Var (X). To obtain this relationship, note ﬁrst that by the same reasoning
that yields Var (X)=E[X2]−(E[X])2, we have
Var(X|Y)=E[X2|Y]−(E[X|Y])2
so
E[Var(X|Y)]=E[E[X2|Y]]−E[(E[X|Y])2]
=E[X2]−E[(E[X|Y])2] (5.9)
Also, since E[E[X|Y]]=E[X], we have
Var(E[X|Y])=E[(E[X|Y])2]−(E[X])2(5.10)
Hence, by adding Equations (5.9) and (5.10), we arrive at the following proposition.
Proposition
5.2The conditional variance formula
Var(X)=E[Var(X |Y)]+Var(E[X|Y])
Example
5oSuppose that by any time tthe number of people who have arrived at a train depot
is a Poisson random variable with mean λt. If the initial train arrives at the depot at a
time (independent of when the passengers arrive) that is uniformly distributed over
(0,T), what are the mean and variance of the number of passengers who enter the
train?
Solution For each tÚ0, let N(t)denote the number of arrivals by t, and let Y
denote the time at which the train arrives. The random variable of interest is then
N(Y). Conditioning on Ygives
E[N(Y)|Y=t]=E[N(t)|Y=t]
=E[N(t)] by the independence of YandN(t)
=λt since N(t)is Poisson with mean λt
Hence,
E[N(Y)|Y]=λY
so taking expectations gives
E[N(Y)]=λE[Y]=λT
2
To obtain Var( N(Y)), we use the conditional variance formula:
Var(N(Y)|Y=t)=Var(N(t)|Y=t)
=Var(N(t))by independence
=λt
Thus,
Var(N(Y)|Y)=λY
E[N(Y)|Y]=λY

<<<PAGE 343>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 330
330 Chapter 7 Properties of Expectation
Hence, from the conditional variance formula,
Var(N(Y))=E[λY]+Var(λY)
=λT
2+λ2T2
12
where we have used the fact that Var(Y )=T2/12. .
Example
5pVariance of a sum of a random number of random variables
LetX1,X2,...be a sequence of independent and identically distributed random vari-
ables, and let Nbe a nonnegative integer-valued random variable that is independent
of the sequence Xi,iÚ1. To compute Var/parenleftBigg
N/summationtext
i=1Xi/parenrightBigg
, we condition on N:
E⎡⎣N/summationdisplay
i=1Xi|N⎤⎦=NE[X]
Var⎛
⎝N/summationdisplay
i=1Xi|N⎞⎠=NVar(X)
The preceding result follows because, given N,/summationtext
N
i=1Xiis just the sum of a ﬁxed
number of independent random variables, so its expectation and variance are just
the sums of the individual means and variances, respectively. Hence, from the con-
ditional variance formula,
Var⎛
⎝N/summationdisplay
i=1Xi⎞⎠=E[N]Var(X )+(E[X])
2Var(N) .
7.6 Conditional Expectation and Prediction
Sometimes a situation arises in which the value of a random variable Xis observed
and then, on the basis of the observed value, an attempt is made to predict the
value of a second random variable Y.L e tg (X)denote the predictor; that is, if X
is observed to equal x, then g(x)is our prediction for the value of Y. Clearly, we
would like to choose gso that g(X)tends to be close to Y. One possible criterion for
closeness is to choose gso as to minimize E[(Y−g(X))2]. We now show that, under
this criterion, the best possible predictor of Yisg(X)=E[Y|X].
Proposition
6.1E[(Y−g(X))2]ÚE[(Y−E[Y|X])2]
Proof
E[(Y−g(X))2|X]=E[(Y−E[Y|X]+E[Y|X]−g(X))2|X]
=E[(Y−E[Y|X])2|X]
+E[(E[Y|X]−g(X))2|X]
+2E[(Y−E[Y|X])(E[Y|X]−g(X))|X ] (6.1)

<<<PAGE 344>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 331
A First Course in Probability 331
However, given X,E[Y|X]−g(X), being a function of X, can be treated as a con-
stant. Thus,
E[(Y−E[Y|X])(E[Y|X]−g(X))|X ]
=(E[Y|X]−g(X))E[Y−E[Y|X]|X]
=(E[Y|X]−g(X))(E[Y|X]−E[Y|X])
=0 (6.2)
Hence, from Equations (6.1) and (6.2), we obtain
E[(Y−g(X))2|X]ÚE[(Y−E[Y|X])2|X]
and the desired result follows by taking expectations of both sides of the preceding
expression.
Remark A second, more intuitive, although less rigorous, argument verifying Propo-
sition 6.1 is as follows: It is straightforward to verify that E[(Y−c)2] is minimized
atc=E[Y]. (See Theoretical Exercise 1.) Thus, if we want to predict the value of
Ywhen there are no data available to use, the best possible prediction, in the sense
of minimizing the mean square error, is to predict that Ywill equal its mean. How-
ever, if the value of the random variable Xis observed to be x, then the prediction
problem remains exactly as in the previous (no-data) case, with the exception that allprobabilities and expectations are now conditional on the event that X=x. Hence,
the best prediction in this situation is to predict that Ywill equal its conditional
expected value given that X=x, thus establishing Proposition 6.1. .
Example
6aSuppose that the son of a man of height x(in inches) attains a height that is normally
distributed with mean x+1 and variance 4. What is the best prediction of the height
at full growth of the son of a man who is 6 feet tall?
Solution Formally, this model can be written as
Y=X+1+e
where eis a normal random variable, independent of X, having mean 0 and variance
4. The XandY, of course, represent the heights of the man and his son, respectively.
The best prediction E[Y|X=72] is thus equal to
E[Y|X=72]=E[X+1+e|X=72]
=73+E[e|X=72]
=73+E(e) by independence
=73 .
Example
6bSuppose that if a signal value sis sent from location A, then the signal value received
at location Bis normally distributed with parameters (s, 1). If S, the value of the
signal sent at A, is normally distributed with parameters ( μ,σ2), what is the best
estimate of the signal sent if R, the value received at B, is equal to r?

<<<PAGE 345>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 332
332 Chapter 7 Properties of Expectation
Solution Let us start by computing the conditional density of Sgiven R. We have
fS|R(s|r)=fS,R(s,r)
fR(r)
=fS(s)fR|S(r|s)
fR(r)
=Ke−(s−μ)2/2σ2e−(r−s)2/2
where Kdoes not depend on s.N o w ,
(s−μ)2
2σ2+(r−s)2
2=s2/parenleftbigg1
2σ2+1
2/parenrightbigg
−/parenleftbiggμ
σ2+r/parenrightbigg
s+C1
=1+σ2
2σ2⎡
⎣s2−2/parenleftBigg
μ+rσ2
1+σ2/parenrightBigg
s⎤⎦+C
1
=1+σ2
2σ2/parenleftBigg
s−(μ+rσ2)
1+σ2/parenrightBigg2
+C2
where C1andC2do not depend on s. Hence,
fS|R(s|r)=Cexp⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩−/bracketleftBigg
s−(μ+rσ
2)
1+σ2/bracketrightBigg2
2/parenleftBigg
σ2
1+σ2/parenrightBigg⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
where Cdoes not depend on s. Thus, we may conclude that the conditional distribu-
tion of S, the signal sent, given that ris received, is normal with mean and variance
now given by
E[S|R=r]=μ+rσ
2
1+σ2
Var(S|R=r)=σ2
1+σ2
Consequently, from Proposition 6.1, given that the value received is r, the best esti-
mate, in the sense of minimizing the mean square error, for the signal sent is
E[S|R=r]=1
1+σ2μ+σ2
1+σ2r
Writing the conditional mean as we did previously is informative, for it shows that it
equals a weighted average of μ, the a priori expected value of the signal, and r,t h e
value received. The relative weights given to μandrare in the same proportion to
each other as 1 (the conditional variance of the received signal when si ss e n t )i st o
σ2(the variance of the signal to be sent). .
Example
6cIn digital signal processing, raw continuous analog data Xmust be quantized, or
discretized, in order to obtain a digital representation. In order to quantize the rawdataX, an increasing set of numbers a
i,i=0,;1,;2,..., such that lim
i→+qai=q
and lim
i→−qai=−qis ﬁxed, and the raw data are then quantized according to the

<<<PAGE 346>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 333
A First Course in Probability 333
interval (ai,ai+1] in which Xlies. Let us denote by yithe discretized value when
X∈(ai,ai+1], and let Ydenote the observed discretized value — that is,
Y=yiifai<X…ai+1
The distribution of Yis given by
P{Y=yi}=F X(ai+1)−FX(ai)
Suppose now that we want to choose the values yi,i=0,;1,;2,...so as to
minimize E[(X−Y)2], the expected mean square difference between the raw data
and their quantized version.
(a) Find the optimal values yi,i=0,;1,....
For the optimal quantizer Y, show that
(b)E[Y]=E[X], so the mean square error quantizer preserves the input mean;
(c)V a r (Y)=Var(X)−E[(X−Y)2].
Solution (a) For any quantizer Y, upon conditioning on the value of Y, we obtain
E[(X−Y)2]=/summationdisplay
iE[(X−yi)2|ai<X…ai+1]P{ai<X…ai+1}
Now, if we let
I=iifai<X…ai+1
then
E[(X−yi)2|ai<X…ai+1]=E[(X−yi)2|I=i]
and by Proposition 6.1, this quantity is minimized when
yi=E[X|I=i]
=E[X|ai<X…ai+1]
=/integraldisplayai+1
aixfX(x)dx
FX(ai+1)−FX(ai)
Now, since the optimal quantizer is given by Y=E[X|I], it follows that
(b)E[Y]=E[X]
(c)
Var(X)=E[Var(X|I)]+Var(E[X|I])
=E[E[(X−Y)2|I]]+Var(Y)
=E[(X−Y)2]+Var(Y) .
It sometimes happens that the joint probability distribution of XandYis not
completely known; or if it is known, it is such that the calculation of E[Y|X=x]
is mathematically intractable. If, however, the means and variances of XandYand
the correlation of XandYare known, then we can at least determine the best linear
predictor of Ywith respect to X.
To obtain the best linear predictor of Ywith respect to X, we need to choose a
andbso as to minimize E[(Y−(a+bX))2]. Now,
E[(Y−(a+bX))2]=E[Y2−2aY−2bXY +a2+2abX +b2X2]
=E[Y2]−2aE[Y]−2bE[XY ]+a2
+2abE [X]+b2E[X2]

<<<PAGE 347>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 334
334 Chapter 7 Properties of Expectation
Taking partial derivatives, we obtain
∂
∂aE[(Y−a−bX)2]=−2E[Y]+2a+2bE[X]
∂
∂bE[(Y−a−bX)2]=−2E[XY ]+2aE[X]+2bE[X2](6.3)
Setting Equations (6.3) to 0 and solving for aandbyields the solutions
b=E[XY ]−E[X]E[Y]
E[X2]−(E[X])2=Cov(X,Y)
σ2x=ρσy
σx
a=E[Y]−bE[X]=E[Y]−ρσyE[X]
σx(6.4)
where ρ=Correlation (X,Y),σ2
y=Var(Y), and σ2
x=Var(X). It is easy to ver-
ify that the values of aandbfrom Equation (6.4) minimize E[(Y−a−bX)2];
thus, the best (in the sense of mean square error) linear predictor Ywith respect
toXis
μy+ρσy
σx(X−μx)
where μy=E[Y] andμx=E[X].
The mean square error of this predictor is given by
E/bracketleftBigg/parenleftbigg
Y−μy−ρσy
σx(X−μx)/parenrightbigg2/bracketrightBigg
=E/bracketleftBig
(Y−μy)2/bracketrightBig
+ρ2σ2
y
σ2xE/bracketleftBig
(X−μx)2/bracketrightBig
−2ρσy
σxE/bracketleftbig
(Y−μy)(X−μx)/bracketrightbig
=σ2
y+ρ2σ2
y−2ρ2σ2
y
=σ2
y(1−ρ2) (6.5)
We note from Equation (6.5) that if ρis near +1o r−1, then the mean square error
of the best linear predictor is near zero. .
Example
6dAn example in which the conditional expectation of Ygiven Xis linear in X, and
hence in which the best linear predictor of Ywith respect to Xis the best overall
predictor, is when XandYhave a bivariate normal distribution. For, as shown in
Example 5d of Chapter 6, in that case,
E[Y|X=x]=μy+ρσy
σx(x−μx) .
7.7 Moment Generating Functions
The moment generating function M(t)of the random variable Xis deﬁned for all
real values of tby
M(t)=E[etX]
=⎧
⎪⎪⎪⎨
⎪⎪⎪⎩/summationdisplay
xetxp(x) ifXis discrete with mass function p(x)
/integraldisplayq
−qetxf(x)dx ifXis continuous with density f(x)

<<<PAGE 348>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 335
A First Course in Probability 335
We call M(t)the moment generating function because all of the moments of Xcan
be obtained by successively differentiating M(t)and then evaluating the result at
t=0. For example,
M/prime(t)=d
dtE[etX]
=E/bracketleftbiggd
dt(etX)/bracketrightbigg
(7.1)
=E[XetX]
where we have assumed that the interchange of the differentiation and expectation
operators is legitimate. That is, we have assumed that
d
dt⎡
⎣/summationdisplay
xetxp(x)⎤⎦=/summationdisplay
xd
dt[etxp(x)]
in the discrete case and
d
dt/bracketleftbigg/integraldisplay
etxf(x)dx/bracketrightbigg
=/integraldisplayd
dt[etxf(x)]dx
in the continuous case. This assumption can almost always be justiﬁed and, indeed, is
valid for all of the distributions considered in this book. Hence, from Equation (7.1),
evaluated at t=0, we obtain
M/prime(0)=E[X]
Similarly,
M/prime/prime(t)=d
dtM/prime(t)
=d
dtE[XetX]
=E/bracketleftbiggd
dt(XetX)/bracketrightbigg
=E[X2etX]
Thus,
M/prime/prime(0)=E[X2]
In general, the nth derivative of M(t)is given by
Mn(t)=E[XnetX]nÚ1
implying that
Mn(0)=E[Xn]nÚ1
We now compute M(t)for some common distributions.

<<<PAGE 349>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 336
336 Chapter 7 Properties of Expectation
Example
7aBinomial distribution with parameters nandp
IfXis a binomial random variable with parameters nandp, then
M(t)=E[etX]
=n/summationdisplay
k=0etk/parenleftBigg
n
k/parenrightBigg
pk(1−p)n−k
=n/summationdisplay
k=0/parenleftBigg
n
k/parenrightBigg
(pet)k(1−p)n−k
=(pet+1−p)n
where the last equality follows from the binomial theorem. Differentiation yields
M/prime(t)=n(pet+1−p)n−1pet
Thus,
E[X]=M/prime(0)=np
Differentiating a second time yields
M/prime/prime(t)=n(n−1)(pet+1−p)n−2(pet)2+n(pet+1−p)n−1pet
so
E[X2]=M/prime/prime(0)=n(n−1)p2+np
The variance of Xis given by
Var(X)=E[X2]−(E[X])2
=n(n−1)p2+np−n2p2
=np(1−p)
verifying the result obtained previously. .
Example
7bPoisson distribution with mean λ
IfXis a Poisson random variable with parameter λ, then
M(t)=E[etX]
=q/summationdisplay
n=0etne−λλn
n!
=e−λq/summationdisplay
n=0(λet)n
n!
=e−λeλet
=exp{λ(et−1)}
Differentiation yields
M/prime(t)=λetexp{λ(et−1)}
M/prime/prime(t)=(λet)2exp{λ(et−1)}+λetexp{λ(et−1)}

<<<PAGE 350>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 337
A First Course in Probability 337
Thus,
E[X]=M/prime(0)=λ
E[X2]=M/prime/prime(0)=λ2+λ
Var(X)=E[X2]−(E[X])2
=λ
Hence, both the mean and the variance of the Poisson random variable equal λ..
Example
7cExponential distribution with parameter λ
M(t)=E[etX]
=/integraldisplayq
0etxλe−λxdx
=λ/integraldisplayq
0e−(λ−t )xdx
=λ
λ−tfort<λ
We note from this derivation that for the exponential distribution, M(t)is deﬁned
only for values of tless than λ. Differentiation of M(t)yields
M/prime(t)=λ
(λ−t)2,M/prime/prime(t)=2λ
(λ−t)3
Hence,
E[X]=M/prime(0)=1
λ,E[X2]=M/prime/prime(0)=2
λ2
The variance of Xis given by
Var(X)=E[X2]−(E[X])2
=1
λ2.
Example
7dNormal distribution
We ﬁrst compute the moment generating function of a unit normal random variable
with parameters 0 and 1. Letting Zbe such a random variable, we have
MZ(t)=E[etZ]
=1√
2π/integraldisplayq
−qetxe−x2/2dx
=1√
2π/integraldisplayq
−qexp/braceleftBigg
−(x2−2tx)
2/bracerightBigg
dx
=1√
2π/integraldisplayq
−qexp/braceleftBigg
−(x−t)2
2+t2
2/bracerightBigg
dx
=et2/21√
2π/integraldisplayq
−qe−(x−t )2/2dx
=et2/2

<<<PAGE 351>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 338
338 Chapter 7 Properties of Expectation
Hence, the moment generating function of the unit normal random variable Zis
given by MZ(t)=et2/2. To obtain the moment generating function of an arbitrary
normal random variable, we recall (see Section 5.4) that X=μ+σZwill have
a normal distribution with parameters μandσ2whenever Zis a unit normal ran-
dom variable. Hence, the moment generating function of such a random variable is
given by
MX(t)=E[etX]
=E[et(μ+σ Z)]
=E[etμetσZ]
=etμE[etσZ]
=etμMZ(tσ)
=etμe(tσ)2/2
=exp/braceleftBigg
σ2t2
2+μt/bracerightBigg
By differentiating, we obtain
M/prime
X(t)=(μ+tσ2)exp/braceleftBigg
σ2t2
2+μt/bracerightBigg
M/prime/prime
X(t)=(μ+tσ2)2exp/braceleftBigg
σ2t2
2+μt/bracerightBigg
+σ2exp/braceleftBigg
σ2t2
2+μt/bracerightBigg
Thus,
E[X]=M/prime(0)=μ
E[X2]=M/prime/prime(0)=μ2+σ2
implying that
Var(X)=E[X2]−E([X])2
=σ2.
Tables 7.1 and 7.2 (on page 340) give the moment generating functions for some
common discrete and continuous distributions.
An important property of moment generating functions is that the moment gen-
erating function of the sum of independent random variables equals the product ofthe individual moment generating functions. To prove this, suppose that XandYare
independent and have moment generating functions M
X(t)andMY(t), respectively.
Then MX+Y(t), the moment generating function of X+Y, is given by
MX+Y(t)=E[et(X+Y)]
=E[etXetY]
=E[etX]E[etY]
=MX(t)MY(t)
where the next-to-last equality follows from Proposition 4.1, since XandYare
independent.

<<<PAGE 352>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 339
A First Course in Probability 339
Table 7.1 Discrete Probability Distribution.
Moment
Probability mass generating
function, p(x) function, M(t) Mean Variance
Binomial with
parameters n,p;
0…p…1/parenleftbigg
n
x/parenrightbigg
px(1−p)n−x(pet+1−p)nnp np(1 −p)
x=0, 1,...,n
Poisson with
parameter λ>0e−λλx
x!exp{λ(et−1)} λλ
x=0, 1, 2, ...
Geometric with
parameter0…p…1p(1−p)
x−1pet
1−(1−p)et1
p1−p
p2
x=1, 2,...
Negativebinomial withparameters r,p;
0…p…1/parenleftbigg
n−1
r−1/parenrightbigg
p
r(1−p)n−r/bracketleftBigg
pet
1−(1−p)et/bracketrightBiggrr
pr(1−p)
p2
n=r,r+1,...
Another important result is that the moment generating function uniquely deter-
mines the distribution. That is, if MX(t)exists and is ﬁnite in some region about t=0,
then the distribution of Xis uniquely determined. For instance, if
MX(t)=/parenleftbigg1
2/parenrightbigg10
(et+1)10,
then it follows from Table 7.1 that Xis a binomial random variable with parameters
10 and1
2.
Example
7eSuppose that the moment generating function of a random variable Xis given by
M(t)=e3(et−1). What is P{X=0}?
Solution We see from Table 7.1 that M(t)=e3(et−1)is the moment generating func-
tion of a Poisson random variable with mean 3. Hence, by the one-to-one correspon-
dence between moment generating functions and distribution functions, it follows
thatXmust be a Poisson random variable with mean 3. Thus, P{X=0}= e−3..
Example
7fSums of independent binomial random variables
IfXandYare independent binomial random variables with parameters (n, p) and
(m,p), respectively, what is the distribution of X+Y?
Solution The moment generating function of X+Yis given by
MX+Y(t)=MX(t)MY(t)=(pet+1−p)n(pet+1−p)m
=(pet+1−p)m+n
However, (pet+1−p)m+nis the moment generating function of a binomial ran-
dom variable having parameters m+nandp. Thus, this must be the distribution
ofX+Y. .

<<<PAGE 353>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 340
340 Chapter 7 Properties of Expectation
Table 7.2 Continuous Probability Distribution.
Moment
generating
Probability density function, f(x) function, M(t) Mean Variance
Uniform over (a, b) f(x)=⎧
⎪⎨
⎪⎩1
b−aa<x<b
0 otherwiseetb−eta
t(b−a)a+b
2(b−a)2
12
Exponential with
parameter λ> 0f(x)=/braceleftBigg
λe−λxxÚ0
0 x<0λ
λ−t1
λ1
λ2
Gamma with parameters(s,λ),λ> 0f(x)=⎧
⎪⎪⎨
⎪⎪⎩λe
−λx(λx)s−1
/Gamma1(s)xÚ0
0 x<0/parenleftbiggλ
λ−t/parenrightbiggss
λs
λ2
Normal with parameters
(μ,σ2)f(x)=1√
2πσe−(x−μ)2/2σ2−q<x<q exp/braceleftBigg
μt+σ2t2
2/bracerightBigg
μσ2

<<<PAGE 354>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 341
A First Course in Probability 341
Example
7gSums of independent Poisson random variables
Calculate the distribution of X+Ywhen XandYare independent Poisson random
variables with means respectively λ1andλ2.
Solution
MX+Y(t)=MX(t)MY(t)
=exp{λ1(et−1)}exp{λ2(et−1)}
=exp{(λ1+λ2)(et−1)}
Hence, X+Yis Poisson distributed with mean λ1+λ2, verifying the result given
in Example 3e of Chapter 6. .
Example
7hSums of independent normal random variables
Show that if XandYare independent normal random variables with respective
parameters (μ1,σ2
1)and(μ2,σ2
2), then X+Yis normal with mean μ1+μ2and
variance σ2
1+σ2
2.
Solution
MX+Y(t)=MX(t)MY(t)
=exp/braceleftBigg
σ2
1t2
2+μ1t/bracerightBigg
exp/braceleftBigg
σ2
2t2
2+μ2t/bracerightBigg
=exp/braceleftBigg
(σ2
1+σ2
2)t2
2+(μ1+μ2)t/bracerightBigg
which is the moment generating function of a normal random variable with mean
μ1+μ2and variance σ2
1+σ2
2. The desired result then follows because the moment
generating function uniquely determines the distribution. .
Example
7iCompute the moment generating function of a chi-squared random variable with n
degrees of freedom.
Solution We can represent such a random variable as
Z2
1+ ··· + Z2
n
where Z1,...,Znare independent standard normal random variables. Let M(t)be
its moment generating function. Then, by the preceding,
M(t)=(E[etZ2])n
where Zis a standard normal random variable. Now,
E[etZ2]=1√
2π/integraldisplayq
−qetx2e−x2/2dx
=1√
2π/integraldisplayq
−qe−x2/2σ2dx where σ2=(1−2t)−1
=σ
=(1−2t)−1/2

<<<PAGE 355>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 342
342 Chapter 7 Properties of Expectation
where the next-to-last equality uses the fact that the normal density with mean 0 and
variance σ2integrates to 1. Therefore,
M(t)=(1−2t)−n/2.
Example
7jMoment generating function of the sum of a random number of random variables
LetX1,X2,...be a sequence of independent and identically distributed random
variables, and let Nbe a nonnegative, integer-valued random variable that is inde-
pendent of the sequence X,iÚ1. We want to compute the moment generating
function of
Y=N/summationdisplay
i=1Xi
(In Example 5d, Ywas interpreted as the amount of money spent in a store on a
given day when both the amount spent by a customer and the number of customers
are random variables.)
To compute the moment generating function of Y, we ﬁrst condition on Nas
follows:
E⎡
⎢⎢⎣exp⎧
⎨
⎩tN/summationdisplay
1Xi⎫
⎬
⎭/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN=n⎤
⎥⎥⎦=E⎡
⎢⎢⎣exp⎧
⎨
⎩tn/summationdisplay
1Xi⎫
⎬
⎭/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN=n⎤
⎥⎥⎦
=E⎡
⎢⎣exp⎧
⎨
⎩tn/summationdisplay
1Xi⎫
⎬
⎭⎤
⎥⎦
=[MX(t)]n
where
MX(t)=E[etXi]
Hence,
E[etY|N]=(MX(t))N
Thus,
MY(t)=E[(M X(t))N]
The moments of Ycan now be obtained upon differentiation, as follows:
M/prime
Y(t)=E[N(MX(t))N−1M/prime
X(t)]
So
E[Y]=M/prime
Y(0)
=E[N(MX(0))N−1M/prime
X(0)]
=E[NE[X]] (7.2)
=E[N]E[X]
verifying the result of Example 5d. (In this last set of equalities, we have used the
fact that MX(0)=E[e0X]=1.)

<<<PAGE 356>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 343
A First Course in Probability 343
Also,
M/prime/prime
Y(t)=E[N(N−1)(M X(t))N−2(M/prime
X(t))2+N(MX(t))N−1M/prime/prime
X(t)]
so
E[Y2]=M/prime/prime
Y(0)
=E[N(N−1)(E [X])2+NE[X2]]
=(E[X])2(E[N2]−E[N])+E[N]E[X2] (7.3)
=E[N](E[X2]−(E[X])2)+(E[X])2E[N2]
=E[N]Var(X)+(E[X])2E[N2]
Hence, from Equations (7.2) and (7.3), we have
Var(Y)=E[N]Var(X)+(E[X])2(E[N2]−(E[N])2)
=E[N]Var(X)+(E[X])2Var(N) .
Example
7kLetYdenote a uniform random variable on (0, 1), and suppose that conditional on
Y=p, the random variable Xhas a binomial distribution with parameters nandp.
In Example 5k, we showed that Xis equally likely to take on any of the values
0, 1,...,n. Establish this result by using moment generating functions.
Solution To compute the moment generating function of X, start by conditioning
on the value of Y. Using the formula for the binomial moment generating function
gives
E[etX|Y=p]=(pet+1−p)n
Now, Yis uniform on (0, 1), so, upon taking expectations, we obtain
E[etX]=/integraldisplay1
0(pet+1−p)ndp
=1
et−1/integraldisplayet
1yndy(by the substitution y=pet+1−p)
=1
n+1et(n+1)−1
et−1
=1
n+1(1+et+e2t+ ··· + ent)
Because the preceding is the moment generating function of a random variable that
is equally likely to be any of the values 0, 1, ...,n, the desired result follows from the
fact that the moment generating function of a random variable uniquely determines
its distribution. .
7.7.1 Joint Moment Generating Functions
It is also possible to deﬁne the joint moment generating function of two or more
random variables. This is done as follows: For any nrandom variables X1,...,Xn,
the joint moment generating function, M(t1,...,tn), is deﬁned, for all real values of
t1,...,tn,b y
M(t1,...,tn)=E[et1X1+···+ tnXn]

<<<PAGE 357>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 344
344 Chapter 7 Properties of Expectation
The individual moment generating functions can be obtained from M(t1,...,tn)by
letting all but one of the tj’s be 0. That is,
MXi(t)=E[etXi]=M(0,...,0 ,t,0 ,...,0)
where the tis in the ith place.
It can be proven (although the proof is too advanced for this text) that the joint
moment generating function M(t1,...,tn)uniquely determines the joint distribution
ofX1,...,Xn. This result can then be used to prove that the nrandom variables
X1,...,Xnare independent if and only if
M(t1,...,tn)=MX1(t1)···MXn(tn) (7.4)
For the proof in one direction, if the nrandom variables are independent, then
M(t1,...,tn)=E[e(t1X1+···+ tnXn)]
=E[et1X1···etnXn]
=E[et1X1]···E[etnXn] by independence
=MX1(t1)···MXn(tn)
For the proof in the other direction, if Equation (7.4) is satisﬁed, then the joint
moment generating function M(t1,...,tn)is the same as the joint moment generating
function of nindependent random variables, the ith of which has the same distribu-
tion as Xi. As the joint moment generating function uniquely determines the joint
distribution, this must be the joint distribution; hence, the random variables are
independent.
Example
7lLetXandYbe independent normal random variables, each with mean μand vari-
anceσ2. In Example 7a of Chapter 6, we showed that X+YandX−Yare
independent. Let us now establish that X+YandX−Yare independent by
computing their joint moment generating function:
E[et(X+Y)+s(X −Y)]=E[e(t+s)X+(t−s)Y]
=E[e(t+s)X]E[e(t−s)Y]
=eμ(t+s)+σ2(t+s)2/2eμ(t−s)+σ2(t−s)2/2
=e2μt+σ2t2eσ2s2
But we recognize the preceding as the joint moment generating function of the sum
of a normal random variable with mean 2μ and variance 2σ2and an independent
normal random variable with mean 0 and variance 2 σ2. Because the joint moment
generating function uniquely determines the joint distribution, it follows that X+Y
andX−Yare independent normal random variables. .
In the next example, we use the joint moment generating function to verify a
result that was established in Example 2b of Chapter 6.
Example
7mSuppose that the number of events that occur is a Poisson random variable with
mean λand that each event is independently counted with probability p. Show that
the number of counted events and the number of uncounted events are independent
Poisson random variables with respective means λpandλ(1−p).

<<<PAGE 358>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 345
A First Course in Probability 345
Solution LetXdenote the total number of events, and let Xcdenote the number of
them that are counted. To compute the joint moment generating function of Xc,t h e
number of events that are counted, and X−Xc, the number that are uncounted,
start by conditioning on Xto obtain
E[esXc+t(X−Xc)|X=n]=etnE[e(s−t)Xc|X=n]
=etn(pes−t+1−p)n
=(pes+(1−p)et)n
which follows because, conditional on X=n,Xcis a binomial random variable with
parameters nandp. Hence,
E[esXc+t(X−Xc)|X]=(pes+(1−p)et)X
Taking expectations of both sides of this equation yields
E[esXc+t(X−Xc)]=E[(pes+(1−p)et)X]
Now, since Xis Poisson with mean λ, it follows that E[etX]=eλ(et−1). Therefore, for
any positive value awe see (by letting a=et) that E[aX]=eλ(a−1). Thus,
E[esXc+t(X−Xc)]=eλ(pes+(1−p)et−1)
=eλp(es−1)eλ(1−p)(et−1)
As the preceding is the joint moment generating function of independent Poisson
random variables with respective means λpandλ(1−p), the result is proven. .
7.8 Additional Properties of Normal Random Variables
7.8.1 The Multivariate Normal Distribution
LetZ1,...,Znbe a set of nindependent unit normal random variables. If, for some
constants aij,1…i…m,1…j…n, and μi,1…i…m,
X1=a11Z1+ ··· + a1nZn+μ1
X2=a21Z1+ ··· + a2nZn+μ2
...
X
i=ai1Z1+ ··· + ainZn+μi
...
X
m=am1Z1+ ··· + amnZn+μm
then the random variables X1,...,Xmare said to have a multivariate normal distri-
bution.

<<<PAGE 359>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 346
346 Chapter 7 Properties of Expectation
From the fact that the sum of independent normal random variables is itself a
normal random variable, it follows that each Xiis a normal random variable with
mean and variance given, respectively, by
E[Xi]=μi
Var(Xi)=n/summationdisplay
j=1a2
ij
Let us now consider
M(t1,...,tm)=E[exp{t1X1+ ··· + tmXm}]
the joint moment generating function of X1,...,Xm. The ﬁrst thing to note is that
sincem/summationtext
i=1tiXiis itself a linear combination of the independent normal random vari-
ables Z1,...,Zn, it is also normally distributed. Its mean and variance are
E⎡
⎣m/summationdisplay
i=1tiXi⎤⎦=m/summationdisplay
i=1tiμi
and
Var⎛
⎝m/summationdisplay
i=1tiXi⎞⎠=Cov⎛
⎜⎝m/summationdisplay
i=1tiXi,m/summationdisplay
j=1tjXj⎞
⎟⎠
=m/summationdisplay
i=1m/summationdisplay
j=1titjCov(Xi,Xj)
Now, if Yis a normal random variable with mean μand variance σ2, then
E[eY]=MY(t)|t=1=eμ+σ2/2
Thus,
M(t1,...,tm)=exp⎧
⎪⎨
⎪⎩m/summationdisplay
i=1tiμi+1
2m/summationdisplay
i=1m/summationdisplay
j=1titjCov(Xi,Xj)⎫
⎪⎬
⎪⎭
which shows that the joint distribution of X1,...,Xmis completely determined from
a knowledge of the values of E[Xi] and Cov (Xi,Xj),i,j=1,...,m.
It can be shown that when m=2, the multivariate normal distribution reduces
to the bivariate normal.
Example
8aFind P(X<Y)for bivariate normal random variables XandYhaving parameters
μx=E[X],μy=E[Y],σ2
x=Var(X),σ2
y=Var(Y),ρ=Corr(X ,Y)
Solution Because X−Yis normal with mean
E[X−Y]=μx−μy
and variance
Var(X−Y)=Var(X)+Var(−Y)+2Cov(X,−Y)
=σ2
x+σ2
y−2ρσxσy

<<<PAGE 360>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 347
A First Course in Probability 347
we obtain
P{X<Y}=P{X−Y<0}
=P⎧
⎪⎨
⎪⎩X−Y−(μx−μy)/radicalBig
σ2x+σ2y−2ρσxσy<−(μ x−μy)/radicalBig
σ2x+σ2y−2ρσxσy⎫
⎪⎬
⎪⎭
=/Phi1⎛
⎝μy−μx/radicalBig
σ2x+σ2y−2ρσxσy⎞⎠.
Example
8bSuppose that the conditional distribution of X, given that /Theta1=θ, is normal with
mean θand variance 1. Moreover, suppose that /Theta1itself is a normal random vari-
able with mean μand variance σ2. Find the conditional distribution of /Theta1given that
X=x.
Solution Rather than using and then simplifying Bayes’s formula, we will solve this
problem by ﬁrst showing that X,/Theta1has a bivariate normal distribution. To do so, note
that the joint density function of X,/Theta1can be written as
fX,/Theta1(x,θ)=fX|/Theta1(x|θ)f/Theta1(θ)
where fX|/Theta1(x|θ) is a normal density with mean θand variance 1. However, if we let Z
be a standard normal random variable that is independent of /Theta1, then the conditional
distribution of Z+/Theta1, given that /Theta1=θ, is also normal with mean θand variance 1.
Consequently, the joint density of Z+/Theta1,/Theta1is the same as that of X,/Theta1.Because the
former joint density is clearly bivariate normal (since Z+/Theta1and/Theta1are both linear
combinations of the independent normal random variables Zand/Theta1), it follows that
X,/Theta1has a bivariate normal distribution. Now,
E[X]=E[Z+/Theta1]=μ
Var(X)=Var(Z+/Theta1)=1+σ2
and
ρ=Corr(X ,/Theta1)
=Corr(Z +/Theta1,/Theta1)
=Cov(Z+/Theta1,/Theta1)√Var(Z+/Theta1)Var (/Theta1)
=σ/radicalbig
1+σ2
Because X,/Theta1has a bivariate normal distribution, the conditional distribution of /Theta1,
given that X=x, is normal with mean
E[/Theta1|X =x]=E[/Theta1]+ρ/radicalBigg
Var(/Theta1)
Var(X)(x−E[X])
=μ+σ2
1+σ2(x−μ)
and variance
Var(/Theta1|X =x)=Var(/Theta1)(1 −ρ2)
=σ2
1+σ2.

<<<PAGE 361>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 348
348 Chapter 7 Properties of Expectation
7.8.2 The Joint Distribution of the Sample Mean and Sample
Variance
LetX1,...,Xnbe independent normal random variables, each with mean μand vari-
anceσ2.L e t X=n/summationtext
i=1Xi/ndenote their sample mean. Since the sum of independent
normal random variables is also a normal random variable, it follows that Xis a nor-
mal random variable with (from Examples 2c and 4a) expected value μand variance
σ2/n.
Now, recall from Example 4e that
Cov(X,Xi−X)=0,i=1,...,n (8.1)
Also, note that since X,X1−X,X2−X,...,Xn−Xare all linear combina-
tions of the independent standard normals (Xi−μ)/σ ,i=1,...,n, it follows that
X,Xi−X,i=1,...,nhas a joint distribution that is multivariate normal. If we let
Ybe a normal random variable, with mean μand variance σ2/n, that is independent
of the Xi,i=1,...,n, then Y,Xi−X,i=1,...,nalso has a multivariate normal
distribution and, indeed, because of Equation (8.1), has the same expected values
and covariances as the random variables X,Xi−X,i=1,...,n. But since a mul-
tivariate normal distribution is determined completely by its expected values and
covariances, it follows that Y,Xi−X,i=1,...,nandX,Xi−X,i=1,...,nhave
the same joint distribution, thus showing that Xis independent of the sequence of
deviations Xi−X,i=1,...,n.
Since Xis independent of the sequence of deviations Xi−X,i=1,...,n,i ti s
also independent of the sample variance S2Kn/summationtext
i=1(Xi−X)2/(n−1).
Since we already know that Xis normal with mean μand variance σ2/n,i t
remains only to determine the distribution of S2. To accomplish this, recall, from
Example 4a, the algebraic identity
(n−1)S2=n/summationdisplay
i=1(Xi−X)2
=n/summationdisplay
i=1(Xi−μ)2−n(X−μ)2
Upon dividing the preceding equation by σ2, we obtain
(n−1)S2
σ2+/parenleftBigg
X−μ
σ/√n/parenrightBigg2
=n/summationdisplay
i=1/parenleftbiggXi−μ
σ/parenrightbigg2
(8.2)
Now,
n/summationdisplay
i=1/parenleftbiggXi−μ
σ/parenrightbigg2
is the sum of the squares of nindependent standard normal random variables and
so is a chi-squared random variable with ndegrees of freedom. Hence, from Exam-
ple 7i, its moment generating function is (1−2t)−n/2. Also, because
/parenleftBigg
X−μ
σ/√n/parenrightBigg2

<<<PAGE 362>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 349
A First Course in Probability 349
is the square of a standard normal variable, it is a chi-squared random variable with
1 degree of freedom, and so has moment generating function (1−2t)−1/2.N o w ,w e
have seen previously that the two random variables on the left side of Equation (8.2)are independent. Hence, as the moment generating function of the sum of indepen-dent random variables is equal to the product of their individual moment generating
functions, we have
E[e
t(n−1)S2/σ2](1−2t)−1/2=(1−2t)−n/2
or
E[et(n−1)S2/σ2]=(1−2t)−(n−1)/ 2
But as (1−2t)−(n−1)/ 2is the moment generating function of a chi-squared random
variable with n−1 degrees of freedom, we can conclude, since the moment gener-
ating function uniquely determines the distribution of the random variable, that thatis the distribution of (n−1)S
2/σ2.
Summing up, we have shown the following.
Proposition
8.1IfX1,...,Xnare independent and identically distributed normal random variables
with mean μand variance σ2, then the sample mean Xand the sample variance S2
are independent. Xis a normal random variable with mean μand variance σ2/n;
(n−1)S2/σ2is a chi-squared random variable with n−1 degrees of freedom.
7.9 General Deﬁnition of Expectation
Up to this point, we have deﬁned expectations only for discrete and continuous ran-
dom variables. However, there also exist random variables that are neither discretenor continuous, and they, too, may possess an expectation. As an example of such a
random variable, let Xbe a Bernoulli random variable with parameter p=
1
2, and
letYbe a uniformly distributed random variable over the interval [0, 1]. Further-
more, suppose that XandYare independent, and deﬁne the new random variable
Wby
W=/braceleftBigg
X ifX=1
Y ifXZ1
Clearly, Wis neither a discrete (since its set of possible values, [0, 1], is uncountable)
nor a continuous (since P{W=1}=1
2) random variable.
In order to deﬁne the expectation of an arbitrary random variable, we require
the notion of a Stieltjes integral. Before deﬁning this integral, let us recall that for
any function g,/integraltextb
ag(x)dxis deﬁned by
/integraldisplayb
ag(x)dx=limn/summationdisplay
i=1g(xi)(xi−xi−1)
where the limit is taken over all a=x0<x1<x2···<xn=basn→q and where
max
i=1,...,n(xi−xi−1)→0.
For any distribution function F, we deﬁne the Stieltjes integral of the nonnega-
tive function gover the interval [a, b]b y
/integraldisplayb
ag(x)dF(x)=limn/summationdisplay
i=1g(xi)[F(xi)−F(xi−1)]

<<<PAGE 363>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 350
350 Chapter 7 Properties of Expectation
where, as before, the limit is taken over all a=x0<x1<···<xn=basn→q and
where max
i=1,...,n(xi−xi−1)→0. Further, we deﬁne the Stieltjes integral over the whole
real line by/integraldisplayq
−qg(x)dF(x)= lim
a→−q
b→+q/integraldisplayb
ag(x)dF(x)
Finally, if gis not a nonnegative function, we deﬁne g+andg−by
g+(x)=/braceleftBigg
g(x) ifg(x)Ú0
0i f g(x)< 0
g−(x)=/braceleftBigg
0i f g(x)Ú0
−g(x) ifg(x)< 0
Because g(x)=g+(x)−g−(x)andg+andg−are both nonnegative functions, it is
natural to deﬁne
/integraldisplayq
−qg(x)dF(x)=/integraldisplayq
−qg+(x)dF(x)−/integraldisplayq
−qg−(x)dF(x)
and we say that/integraltextq
−qg(x)dF(x)exists as long as/integraltextq
−qg+(x)dF(x)and/integraltextq
−qg−(x)dF(x)
are not both equal to +q.
IfXis an arbitrary random variable having cumulative distribution F, we deﬁne
the expected value of Xby
E[X]=/integraldisplayq
−qxd F(x) (9.1)
It can be shown that if Xis a discrete random variable with mass function p(x), then
/integraldisplayq
−qxdF(x)=/summationdisplay
x:p(x)>0xp(x)
whereas if Xis a continuous random variable with density function f(x), then
/integraldisplayq
−qxdF(x)=/integraldisplayq
−qxf(x)dx
The reader should note that Equation (9.1) yields an intuitive deﬁnition of E[X];
consider the approximating sum
n/summationdisplay
i=1xi[F(xi)−F(xi−1)]
ofE[X]. Because F(xi)−F(xi−1)is just the probability that Xwill be in the interval
(xi−1,xi], the approximating sum multiplies the approximate value of Xwhen it is in
the interval (x i−1,xi] by the probability that it will be in that interval and then sums
over all the intervals. Clearly, as these intervals get smaller and smaller in length, we
obtain the “expected value” of X.
Stieltjes integrals are mainly of theoretical interest because they yield a compact
way of deﬁning and dealing with the properties of expectation. For instance, theuse of Stieltjes integrals avoids the necessity of having to give separate statements
and proofs of theorems for the continuous and the discrete cases. However, their
properties are very much the same as those of ordinary integrals, and all of the proofspresented in this chapter can easily be translated into proofs in the general case.

<<<PAGE 364>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 351
A First Course in Probability 351
Summary
IfXandYhave a joint probability mass function p(x,y),
then
E[g(X,Y)]=/summationdisplay
y/summationdisplay
xg(x,y)p(x, y)
whereas if they have a joint density function f(x,y), then
E[g(X,Y)]=/integraldisplayq
−q/integraldisplayq
−qg(x,y)f(x,y)dx dy
A consequence of the preceding equations is that
E[X+Y]=E[X]+E[Y]
which generalizes to
E⎡
⎣n/summationdisplay
i=1Xi⎤⎦=n/summationdisplay
i=1E[Xi]
The covariance between random variables XandYis
given by
Cov(X ,Y)=E[(X−E[X])(Y−E[Y])]
=E[XY ]−E[X]E[Y]
Au s e f u li d e n t i t yi s
Cov⎛
⎜⎝n/summationdisplay
i=1Xi,m/summationdisplay
j=1Yj⎞
⎟⎠=n/summationdisplay
i=1m/summationdisplay
j=1Cov(Xi,Yj)
When n=mandYi=Xi,i=1,...,n, the preceding
formula gives
Var⎛
⎝n/summationdisplay
i=1Xi⎞⎠=n/summationdisplay
i=1Var(Xi)+2/summationdisplay/summationdisplay
i<jCov(X i,Yj)
The correlation between XandY, denoted by ρ(X,Y),i s
deﬁned by
ρ(X,Y)=Cov(X,Y)√Var(X)Var(Y)
IfXandYare jointly discrete random variables, then the
conditional expected value of X, given that Y=y,i s
deﬁned by
E[X|Y=y]=/summationdisplay
xxP{X=x|Y=y]
IfXandYare jointly continuous random variables, then
E[X|Y=y]=/integraldisplayq
−qxfX|Y(x|y) dx
wherefX|Y(x|y)=f(x,y)
fY(y)is the conditional probability density of Xgiven that
Y=y. Conditional expectations, which are similar to
ordinary expectations except that all probabilities are now
computed conditional on the event that Y=y, satisfy all
the properties of ordinary expectations.
LetE[X|Y] denote that function of Ywhose value at
Y=yisE[X|Y=y]. A very useful identity is
E[X]=E[E[X|Y]]
In the case of discrete random variables, this equationreduces to the identity
E[X]=/summationdisplay
yE[X|Y=y]P{Y=y}
and, in the continuous case, to
E[X]=/integraldisplayq
−qE[X|Y=y]fY(y)dy
The preceding equations can often be applied to obtainE[X] by ﬁrst “conditioning” on the value of some other
random variable Y. In addition, since, for any event A,
P(A)=E[I
A], where IAis 1 if Aoccurs and is 0 otherwise,
we can use the same equations to compute probabilities.
The conditional variance of X, given that Y=y,i s
deﬁned by
Var(X|Y=y)=E[(X−E[X|Y=y])2|Y=y]
Let Var(X |Y)be that function of Ywhose value at Y=y
is Var(X|Y=y). The following is known as the conditional
variance formula :
Var(X)=E[Var(X|Y)]+Var(E[X|Y])
Suppose that the random variable Xis to be observed and,
on the basis of its value, one must then predict the value ofthe random variable Y. In such a situation, it turns out that
among all predictors, E[Y|X] has the smallest expectation
of the square of the difference between it and Y.
Themoment generating function of the random vari-
ableXis deﬁned by
M(t)=E[e
tX]
The moments of Xcan be obtained by successively differ-
entiating M(t)and then evaluating the resulting quantity
att=0. Speciﬁcally, we have
E[Xn]=dn
dtnM(t)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
t=0n=1, 2,...
Two useful results concerning moment generating func-
tions are, ﬁrst, that the moment generating function
uniquely determines the distribution function of the

<<<PAGE 365>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 352
352 Chapter 7 Properties of Expectation
random variable and, second, that the moment generat-
ing function of the sum of independent random variablesis equal to the product of their moment generating func-
tions. These results lead to simple proofs that the sum of
independent normal (Poisson, gamma) random variablesremains a normal (Poisson, gamma) random variable.
IfX
1,...,Xmare all linear combinations of a ﬁnite
set of independent standard normal random variables,then they are said to have a multivariate normal distribu-
tion. Their joint distribution is speciﬁed by the values ofE[X
i], Cov (Xi,Xj),i,j=1,...,m.
IfX1,...,Xnare independent and identically dis-
tributed normal random variables, then their sample meanX=n/summationdisplay
i=1Xi
n
and their sample variance
S2=n/summationdisplay
i=1(Xi−X)2
n−1
are independent. The sample mean Xis a normal random
variable with mean μand variance σ2/n; the random vari-
able(n−1)S2/σ2is a chi-squared random variable with
n−1 degrees of freedom.
Problems
7.1.A player throws a fair die and simultaneously ﬂips a
fair coin. If the coin lands heads, then she wins twice, andif tails, then she wins one-half of the value that appears on
the die. Determine her expected winnings.
7.2.The game of Clue involves 6 suspects, 6 weapons, and
9 rooms. One of each is randomly chosen and the object of
the game is to guess the chosen three.
(a)How many solutions are possible?
In one version of the game, the selection is made and then
each of the players is randomly given three of the remain-
ing cards. Let S,W,a n dR be, respectively, the numbers
of suspects, weapons, and rooms in the set of three cardsgiven to a speciﬁed player. Also, let Xdenote the number
of solutions that are possible after that player observes his
or her three cards.
(b)Express Xin terms of S,W,a n d R.
(c)Find E[X].
7.3.Gambles are independent, and each one results in
the player being equally likely to win or lose 1 unit. Let
Wdenote the net winnings of a gambler whose strat-
egy is to stop gambling immediately after his ﬁrst win.
Find
(a)P{W>0}
(b)P{W<0}
(c)E[W]
7.4.IfXandYhave joint density function
f
X,Y(x,y)=/braceleftbigg
1/y,i f 0 <y<1, 0<x<y
0, otherwise
ﬁnd
(a)E[XY ]
(b)E[X]
(c)E[Y]
7.5.The county hospital is located at the center of a square
whose sides are 3 miles wide. If an accident occurs withinthis square, then the hospital sends out an ambulance. The
road network is rectangular, so the travel distance fromthe hospital, whose coordinates are (0, 0), to the point
(x,y)i s|x|+| y|. If an accident occurs at a point that is uni-
formly distributed in the square, ﬁnd the expected travel
distance of the ambulance.
7.6.A fair die is rolled 10 times. Calculate the expected
sum of the 10 rolls.7.7.Suppose that AandBeach randomly and indepen-
dently choose 3 of 10 objects. Find the expected number
of objects
(a)chosen by both AandB;
(b)not chosen by either AorB;
(c)chosen by exactly one of AandB.
7.8.Npeople arrive separately to a professional dinner.
Upon arrival, each person looks to see if he or she has
any friends among those present. That person then sits
either at the table of a friend or at an unoccupied tableif none of those present is a friend. Assuming that each
of the/parenleftbigg
N
2/parenrightbigg
pairs of people is, independently, a pair of
friends with probability p, ﬁnd the expected number of
occupied tables.
Hint :L e tX
iequal 1 or 0, depending on whether the ith
arrival sits at a previously unoccupied table.
7.9.A total of nballs, numbered 1 through n, are put into
nurns, also numbered 1 through nin such a way that ball i
is equally likely to go into any of the urns 1, 2, ...,i.F i n d
(a)the expected number of urns that are empty;
(b)the probability that none of the urns is empty.
7.10. Consider 3 trials, each having the same probability
of success. Let Xdenote the total number of successes in
these trials. If E[X]=1.8, what is
(a)the largest possible value of P{X=3}?
(b)the smallest possible value of P{X=3}?

<<<PAGE 366>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 353
A First Course in Probability 353
In both cases, construct a probability scenario that results
inP{X=3}having the stated value.
Hint : For part (b), you might start by letting Ube a uni-
form random variable on (0, 1) and then deﬁning the trialsin terms of the value of U.
7.11. Consider nindependent ﬂips of a coin having proba-
bility pof landing on heads. Say that a changeover occurs
whenever an outcome differs from the one preceding it.For instance, if n=5 and the outcome is HHTHT ,t h e n
there are 3 changeovers. Find the expected number of
changeovers.
Hint : Express the number of changeovers as the sum of
n−1 Bernoulli random variables.
7.12. A group of nmen and nwomen is lined up at
random.
(a)Find the expected number of men who have a woman
next to them.
(b)Repeat part (a), but now assuming that the group is
randomly seated at a round table.
7.13. A set of 1000 cards numbered 1 through 1000 is ran-
domly distributed among 1000 people with each receiving
one card. Compute the expected number of cards that
are given to people whose age matches the number onthe card.
7.14. An urn has mblack balls. At each stage, a black ball
is removed and a new ball that is black with probability p
and white with probability 1 −pis put in its place. Find
the expected number of stages needed until there are no
more black balls in the urn.note: The preceding has possible applications to under-
standing the AIDS disease. Part of the body’s immune
system consists of a certain class of cells known as T-cells.There are 2 types of T-cells, called CD4 and CD8. Now,
while the total number of T-cells in AIDS sufferers is (at
least in the early stages of the disease) the same as thatin healthy individuals, it has recently been discovered that
the mix of CD4 and CD8 T-cells is different. Roughly 60
percent of the T-cells of a healthy person are of the CD4type, whereas the percentage of the T-cells that are of
CD4 type appears to decrease continually in AIDS suf-
ferers. A recent model proposes that the HIV virus (thevirus that causes AIDS) attacks CD4 cells and that the
body’s mechanism for replacing killed T-cells does not dif-
ferentiate between whether the killed T-cell was CD4 orCD8. Instead, it just produces a new T-cell that is CD4
with probability .6 and CD8 with probability .4. However,
although this would seem to be a very efﬁcient way ofreplacing killed T-cells when each one killed is equally
likely to be any of the body’s T-cells (and thus has prob-
ability .6 of being CD4), it has dangerous consequences
when facing a virus that targets only the CD4 T-cells.
7.15. In Example 2h, say that iandj,iZj, form a matched
pair if ichooses the hat belonging to jandjchooses thehat belonging to i. Find the expected number of matched
pairs.
7.16. LetZbe a standard normal random variable, and, for
a ﬁxed x,s e t
X=/braceleftbigg
ZifZ>x
0 otherwise
Show that E[X]=1
√
2πe−x2/2.
7.17. Ad e c ko fn cards numbered 1 through nis thor-
oughly shufﬂed so that all possible n! orderings can be
assumed to be equally likely. Suppose you are to make n
guesses sequentially, where the ith one is a guess of the
card in position i.L e t Ndenote the number of correct
guesses.
(a)If you are not given any information about your earlier
guesses, show that for any strategy, E[N]=1.
(b)Suppose that after each guess you are shown the card
that was in the position in question. What do you think is
the best strategy? Show that under this strategy,
E[N]=1
n+1
n−1+ ··· + 1
L/integraldisplayn
11
xdx=logn
(c)Suppose that you are told after each guess whether you
are right or wrong. In this case, it can be shown that thestrategy that maximizes E[N] is one that keeps on guess-
ing the same card until you are told you are correct andthen changes to a new card. For this strategy, show that
E[N]=1+1
2!+1
3!+ ··· +1
n!
Le−1
Hint : For all parts, express Nas the sum of indicator (that
is, Bernoulli) random variables.
7.18. Cards from an ordinary deck of 52 playing cards are
turned face up one at a time. If the 1st card is an ace, or the
2nd a deuce, or the 3rd a three, or ..., or the 13th a king, or
the 14 an ace, and so on, we say that a match occurs. Note
that we do not require that the (13 n+1) card be any par-
ticular ace for a match to occur but only that it be an ace.
Compute the expected number of matches that occur.
7.19. A certain region is inhabited by rdistinct types of a
certain species of insect. Each insect caught will, indepen-
dently of the types of the previous catches, be of type iwith
probability
Pi,i=1,...,rr/summationdisplay
1Pi=1
(a)Compute the mean number of insects that are caught
before the ﬁrst type 1 catch.
(b)Compute the mean number of types of insects that are
caught before the ﬁrst type 1 catch.

<<<PAGE 367>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 354
354 Chapter 7 Properties of Expectation
7.20. In an urn containing nballs, the ith ball has weight
W(i),i=1,...,n. The balls are removed without replace-
ment, one at a time, according to the following rule: At
each selection, the probability that a given ball in the urn
is chosen is equal to its weight divided by the sum of theweights remaining in the urn. For instance, if at some time
i
1,...,iris the set of balls remaining in the urn, then the
next selection will be ijwith probability W(ij)/slashBigr/summationtext
k=1W(ik),
j=1,...,r. Compute the expected number of balls that
are withdrawn before ball number 1 is removed.
7.21. For a group of 100 people, compute
(a)the expected number of days of the year that are birth-
days of exactly 3 people;
(b)the expected number of distinct birthdays.
7.22. How many times would you expect to roll a fair die
before all 6 sides appeared at least once?
7.23. Urn 1 contains 5 white and 6 black balls, while urn 2
contains 8 white and 10 black balls. Two balls are randomly
selected from urn 1 and are put into urn 2. If 3 balls are
then randomly selected from urn 2, compute the expectednumber of white balls in the trio.
Hint :L e tX
i=1i ft h e ith white ball initially in urn 1 is
one of the three selected, and let Xi=0 otherwise. Simi-
larly, let Yi=1i ft h eith white ball from urn 2 is one of the
three selected, and let Yi=0 otherwise. The number of
white balls in the trio can now be written as5/summationtext
1Xi+8/summationtext
1Yi.
7.24. A bottle initially contains mlarge pills and nsmall
pills. Each day, a patient randomly chooses one of the pills.
If a small pill is chosen, then that pill is eaten. If a large
pill is chosen, then the pill is broken in two; one part is
returned to the bottle (and is now considered a small pill)and the other part is then eaten.
(a)LetXdenote the number of small pills in the bottle
after the last large pill has been chosen and its smaller half
returned. Find E[X].
Hint : Deﬁne n+mindicator variables, one for each of the
small pills initially present and one for each of the msmall
pills created when a large one is split in two. Now use the
argument of Example 2m.
(b)LetYdenote the day on which the last large pill is cho-
sen. Find E[Y].
Hint : What is the relationship between XandY?
7.25. LetX
1,X2,...be a sequence of independent and
identically distributed continuous random variables. Let
NÚ2 be such that
X1ÚX2Ú···ÚXN−1<XN
That is, Nis the point at which the sequence stops decreas-
ing. Show that E[N]=e.
Hint : First ﬁnd P{NÚn}.7.26. IfX1,X2,...,Xnare independent and identically dis-
tributed random variables having uniform distributionsover (0, 1), ﬁnd
(a)E[max(X
1,...,Xn)];
(b)E[min(X 1,...,Xn)].
*7.27. If 101 items are distributed among 10 boxes, then at
least one of the boxes must contain more than 10 items.
Use the probabilistic method to prove this result.
*7.28. Thek-of-r -out-of-n circular reliability system, k…r…
n, consists of ncomponents that are arranged in a circu-
lar fashion. Each component is either functional or failed,and the system functions if there is no block of rcon-
secutive components of which at least kare failed. Show
that there is no way to arrange 47 components, 8 of whichare failed, to make a functional 3-of-12-out-of-47 circular
system.
*7.29. There are 4 different types of coupons, the ﬁrst 2 of
which comprise one group and the second 2 another group.Each new coupon obtained is type iwith probability p
i,
where p1=p2=1/8,p3=p4=3/8. Find the expected
number of coupons that one must obtain to have at leastone of
(a)all 4 types;
(b)all the types of the ﬁrst group;
(c)all the types of the second group;
(d)all the types of either group.
7.30. IfXandYare independent and identically dis-
tributed with mean μand variance σ
2, ﬁnd
E[(X−Y)2]
7.31. In Problem 7.6, calculate the variance of the sum of
the rolls.
7.32. In Problem 7.9, compute the variance of the number
of empty urns.
7.33. IfE[X]=1a n dV a r (X)=5, ﬁnd
(a)E[(2+X)2];
(b)Var(4+3X).
7.34. If 10 married couples are randomly seated at a round
table, compute (a) the expected number and (b) the vari-
ance of the number of wives who are seated next to their
husbands.
7.35. Cards from an ordinary deck are turned face up one
at a time. Compute the expected number of cards that
need to be turned face up in order to obtain
(a)2a c e s ;
(b)5 spades;
(c)all 13 hearts.

<<<PAGE 368>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 355
A First Course in Probability 355
7.36. LetXbe the number of 1’s and Ythe number of 2’s
that occur in nrolls of a fair die. Compute Cov(X ,Y).
7.37. A die is rolled twice. Let Xequal the sum of the out-
comes, and let Yequal the ﬁrst outcome minus the second.
Compute Cov(X ,Y).
7.38. The random variables XandYhave a joint density
function given by
f(x,y)=/braceleftBigg
2e−2x/x0…x<q,0…y…x
0 otherwise
Compute Cov(X ,Y).
7.39. LetX1,...be independent with common mean μand
common variance σ2,a n ds e tY n=Xn+Xn+1+Xn+2.
ForjÚ0, ﬁnd Cov(Y n,Yn+j).
7.40. The joint density function of XandYis given by
f(x,y)=1
ye−(y+x/y),x>0,y>0
Find E[X],E[Y], and show that Cov (X,Y)=1.
7.41. A pond contains 100 ﬁsh, of which 30 are carp. If 20
ﬁsh are caught, what are the mean and variance of the
number of carp among the 20? What assumptions are you
making?
7.42. A group of 20 people consisting of 10 men and 10
women is randomly arranged into 10 pairs of 2 each. Com-
pute the expectation and variance of the number of pairs
that consist of a man and a woman. Now suppose the 20people consist of 10 married couples. Compute the mean
and variance of the number of married couples that are
paired together.
7.43. LetX
1,X2,...,Xnbe independent random variables
having an unknown continuous distribution function F,
and let Y1,Y2,...,Ymbe independent random variables
having an unknown continuous distribution function G.
Now order those n+mvariables, and let
Ii=⎧
⎨
⎩1i f t h e ith smallest of the n+m
variables is from the Xsample
0 otherwise
The random variable R=n+m/summationtext
i=1iIiis the sum of the ranks
of the Xsample and is the basis of a standard statisti-
cal procedure (called the Wilcoxon sum-of-ranks test) for
testing whether FandGare identical distributions. This
test accepts the hypothesis that F=Gwhen Ris neither
too large nor too small. Assuming that the hypothesis ofequality is in fact correct, compute the mean and variance
ofR.
Hint : Use the results of Example 3e.7.44. Between two distinct methods for manufacturing cer-
tain goods, the quality of goods produced by method iis
a continuous random variable having distribution F
i,i=
1, 2. Suppose that ngoods are produced by method 1 and
mby method 2. Rank the n+mgoods according to qual-
ity, and let
Xj=⎧
⎨
⎩1i f t h e jth best was produced from
method 1
2 otherwise
For the vector X1,X2,...,Xn+m, which consists of n1’s
andm2’s, let Rdenote the number of runs of 1. For
instance, if n=5,m=2, and X=1, 2, 1, 1, 1, 1, 2, then
R=2. IfF1=F2(that is, if the two methods produce iden-
tically distributed goods), what are the mean and variance
ofR?
7.45. IfX1,X2,X3,a n dX 4are (pairwise) uncorrelated ran-
dom variables, each having mean 0 and variance 1, com-pute the correlations of
(a)X
1+X2andX2+X3;
(b)X1+X2andX3+X4.
7.46. Consider the following dice game, as played at a cer-
tain gambling casino: Players 1 and 2 roll a pair of dice in
turn. The bank then rolls the dice to determine the out-come according to the following rule: Player i,i=1, 2,
wins if his roll is strictly greater than the bank’s. For i=
1, 2, let
I
i=/braceleftbigg
1i f iwins
0 otherwise
and show that I1andI2are positively correlated. Explain
why this result was to be expected.
7.47. Consider a graph having nvertices labeled 1, 2, ...,n,
and suppose that, between each of the/parenleftbigg
n
2/parenrightbigg
pairs of distinct
vertices, an edge is independently present with probability
p. The degree of vertex i,d e s i g n a t e da sD i, is the number
of edges that have vertex ias one of their vertices.
(a)What is the distribution of Di?
(b)Findρ(Di,Dj), the correlation between DiandDj.
7.48. A fair die is successively rolled. Let XandYdenote,
respectively, the number of rolls necessary to obtain a 6
and a 5. Find
(a)E[X];
(b)E[X|Y=1];
(c)E[X|Y=5].
7.49. There are two misshapen coins in a box; their prob-
abilities for landing on heads when they are ﬂipped are,
respectively, .4a n d .7 .O n eo ft h ec o i n si st ob er a n -
domly chosen and ﬂipped 10 times. Given that two of the
ﬁrst three ﬂips landed on heads, what is the conditional
expected number of heads in the 10 ﬂips?

<<<PAGE 369>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 356
356 Chapter 7 Properties of Expectation
7.50. The joint density of XandYis given by
f(x,y)=e−x/ye−y
y,0<x<q,0 <y<q
Compute E[X2|Y=y].
7.51. The joint density of XandYis given by
f(x,y)=e−y
y,0<x<y,0 <y<q
Compute E[X3|Y=y].
7.52. A population is made up of rdisjoint subgroups. Let
pidenote the proportion of the population that is in sub-
group i,i=1,...,r. If the average weight of the members
of subgroup iiswi,i=1,...,r, what is the average weight
of the members of the population?
7.53. A prisoner is trapped in a cell containing 3 doors.
The ﬁrst door leads to a tunnel that returns him to his
cell after 2 days’ travel. The second leads to a tunnel
that returns him to his cell after 4 days’ travel. The
third door leads to freedom after 1 day of travel. If it is
assumed that the prisoner will always select doors 1, 2,and 3 with respective probabilities .5, .3, and .2, what is
the expected number of days until the prisoner reaches
freedom?
7.54. Consider the following dice game: A pair of dice is
rolled. If the sum is 7, then the game ends and you win 0.
If the sum is not 7, then you have the option of either stop-
ping the game and receiving an amount equal to that sumor starting over again. For each value of i,i=2,..., 12, ﬁnd
your expected return if you employ the strategy of stop-ping the ﬁrst time that a value at least as large as iappears.
What value of ileads to the largest expected return?
Hint :L e tX
idenote the return when you use the critical
value i. To compute E[Xi], condition on the initial sum.
7.55. Ten hunters are waiting for ducks to ﬂy by. When a
ﬂock of ducks ﬂies overhead, the hunters ﬁre at the sametime, but each chooses his target at random, independentlyof the others. If each hunter independently hits his target
with probability .6, compute the expected number of ducks
that are hit. Assume that the number of ducks in a ﬂock isa Poisson random variable with mean 6.
7.56. The number of people who enter an elevator on the
ground ﬂoor is a Poisson random variable with mean 10.
If there are Nﬂoors above the ground ﬂoor, and if each
person is equally likely to get off at any one of the N
ﬂoors, independently of where the others get off, computethe expected number of stops that the elevator will makebefore discharging all of its passengers.
7.57. Suppose that the expected number of accidents per
week at an industrial plant is 5. Suppose also that the num-
bers of workers injured in each accident are independent
random variables with a common mean of 2.5. If the num-
ber of workers injured in each accident is independent ofthe number of accidents that occur, compute the expectednumber of workers injured in a week.
7.58. A coin having probability pof coming up heads
is continually ﬂipped until both heads and tails have
appeared. Find
(a)the expected number of ﬂips;
(b)the probability that the last ﬂip lands on heads.
7.59. There are n+1 participants in a game. Each person
independently is a winner with probability p. The winners
share a total prize of 1 unit. (For instance, if 4 people win,
then each of them receives
1
4, whereas if there are no win-
ners, then none of the participants receives anything.) Let
Adenote a speciﬁed one of the players, and let Xdenote
the amount that is received by A.
(a)Compute the expected total prize shared by the
players.
(b)Argue that E[X]=1−(1−p)n+1
n+1.
(c)Compute E[X] by conditioning on whether Ais a win-
ner, and conclude that
E[(1+B)−1]=1−(1−p)n+1
(n+1)p
when Bis a binomial random variable with parameters n
andp.
7.60. Each of m+2 players pays 1 unit to a kitty in order
to play the following game: A fair coin is to be ﬂipped suc-
cessively ntimes, where nis an odd number, and the suc-
cessive outcomes are noted. Before the nﬂips, each player
writes down a prediction of the outcomes. For instance, ifn=3, then a player might write down (H,H,T),w h i c h
means that he or she predicts that the ﬁrst ﬂip will land onheads, the second on heads, and the third on tails. Afterthe coins are ﬂipped, the players count their total number
of correct predictions. Thus, if the actual outcomes are all
heads, then the player who wrote ( H,H,T) would have 2
correct predictions. The total kitty of m+2 is then evenly
split up among those players having the largest number ofcorrect predictions.
Since each of the coin ﬂips is equally likely to land on
either heads or tails, mof the players have decided to
make their predictions in a totally random fashion. Specif-
ically, they will each ﬂip one of their own fair coins n
times and then use the result as their prediction. How-
ever, the ﬁnal 2 of the players have formed a syndicate
and will use the following strategy: One of them will make
predictions in the same random fashion as the other m
players, but the other one will then predict exactly the
opposite of the ﬁrst. That is, when the randomizing mem-
ber of the syndicate predicts an H, the other member pre-
dicts a T. For instance, if the randomizing member of the
syndicate predicts (H ,H,T), then the other one predicts
(T,T,H).

<<<PAGE 370>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 357
A First Course in Probability 357
(a)Argue that exactly one of the syndicate members will
have more than n/2 correct predictions. (Remember, nis
odd.)
(b)LetXdenote the number of the mnonsyndicate play-
ers who have more than n/2 correct predictions. What is
the distribution of X?
(c)With Xas deﬁned in part (b), argue that
E[payoff to the syndicate] =(m+2)
*E/bracketleftbigg1
X+1/bracketrightbigg
(d)Use part (c) of Problem 7.59 to conclude that
E[payoff to the syndicate] =2(m+2)
m+1
*/bracketleftBigg
1−/parenleftbigg1
2/parenrightbiggm+1/bracketrightBigg
and explicitly compute this number when m=1, 2, and 3.
Because it can be shown that
2(m+2)
m+1/bracketleftBigg
1−/parenleftbigg1
2/parenrightbiggm+1/bracketrightBigg
>2
it follows that the syndicate’s strategy always gives it a pos-
itive expected proﬁt.
7.61. LetX1,...be independent random variables with
the common distribution function F, and suppose they
are independent of N, a geometric random variable with
parameter p.L e tM =max(X1,...,XN).
(a)Find P{M…x}by conditioning on N.
(b)Find P{M…x|N=1}.
(c)Find P{M…x|N>1}.
(d)Use (b) and (c) to rederive the probability you found
in (a).7.62. LetU
1,U2,...be a sequence of independent uniform
(0, 1) random variables. In Example 5i, we showed that for
0…x…1,E[N(x)]=ex,w h e r e
N(x)=min⎧
⎨
⎩n:n/summationdisplay
i=1Ui>x⎫
⎬
⎭
This problem gives another approach to establishing that
result.
(a)Show by induction on nthat for 0 <x…1a n da l l nÚ0,
P{N(x)Ún+1}=xn
n!
Hint : First condition on U1and then use the induction
hypothesis.Use part (a) to conclude that
E[N(x)]=ex
7.63. An urn contains 30 balls, of which 10 are red and 8
are blue. From this urn, 12 balls are randomly withdrawn.
LetXdenote the number of red and Ythe number of blue
balls that are withdrawn. Find Cov (X,Y)
(a)by deﬁning appropriate indicator (that is, Bernoulli)
random variables
Xi,Yjsuch that X=10/summationdisplay
i=1Xi,Y=8/summationdisplay
j=1Yj
(b)by conditioning (on either XorY) to determine
E[XY ].
7.64. Type ilight bulbs function for a random amount of
time having mean μiand standard deviation σi,i=1, 2. A
light bulb randomly chosen from a bin of bulbs is a type 1
bulb with probability pand a type 2 bulb with probability
1−p.L e t Xdenote the lifetime of this bulb. Find
(a)E[X];
(b)Var(X).
7.65. The number of winter storms in a good year is a Pois-
son random variable with mean 3, whereas the number in a
bad year is a Poisson random variable with mean 5. If nextyear will be a good year with probability .4 or a bad year
with probability .6, ﬁnd the expected value and variance of
the number of storms that will occur.
7.66. In Example 5c, compute the variance of the length of
time until the miner reaches safety.7.67. Consider a gambler who, at each gamble, either wins
or loses her bet with respective probabilities pand 1 −p.
A popular gambling system known as the Kelley strategy
is to always bet the fraction 2p −1 of your current fortune
when p>
1
2. Compute the expected fortune after ngam-
bles of a gambler who starts with xunits and employs the
Kelley strategy.
7.68. The number of accidents that a person has in a given
year is a Poisson random variable with mean λ. However,
suppose that the value of λchanges from person to person,
being equal to 2 for 60 percent of the population and 3 for
the other 40 percent. If a person is chosen at random, whatis the probability that he will have (a) 0 accidents and (b)
exactly 3 accidents in a certain year? What is the condi-
tional probability that he will have 3 accidents in a givenyear, given that he had no accidents the preceding year?
7.69. Repeat Problem 7.68 when the proportion of the
population having a value of λless than xis equal to
1−e
−x.

<<<PAGE 371>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 358
358 Chapter 7 Properties of Expectation
7.70. Consider an urn containing a large number of coins,
and suppose that each of the coins has some probability p
of turning up heads when it is ﬂipped. However, this value
ofpvaries from coin to coin. Suppose that the composi-
tion of the urn is such that if a coin is selected at random
from it, then the p-value of the coin can be regarded as
being the value of a random variable that is uniformlydistributed over [0, 1]. If a coin is selected at random fromthe urn and ﬂipped twice, compute the probability that
(a)the ﬁrst ﬂip results in a head;
(b)both ﬂips result in heads.
7.71. In Problem 7.70, suppose that the coin is tossed n
times. Let Xdenote the number of heads that occur.
Show that
P{X=i}=1
n+1i=0, 1,...,n
Hint : Make use of the fact that
/integraldisplay1
0xa−1(1−x)b−1dx=(a−1)!(b −1)!
(a+b−1)!
when aandbare positive integers.
7.72. Suppose that in Problem 7.70, we continue to ﬂip the
coin until a head appears. Let Ndenote the number of ﬂips
needed. Find(a)P{NÚi},iÚ1;
(b)P{N=i};
(c)E[N].
7.73. In Example 6b, let Sdenote the signal sent and Rthe
signal received.(a)Compute E[R].
(b)Compute Var(R).
(c)IsRnormally distributed?
(d)Compute Cov(R, S).
7.74. In Example 6c, suppose that Xis uniformly dis-
tributed over (0, 1). If the discretized regions are deter-
mined by a
0=0,a1=1
2,a n d a2=1, calculate the optimal
quantizer Yand compute E[(X−Y)2].
7.75. The moment generating function of Xis given by
MX(t)=exp{2et−2}and that of YbyMY(t)=/parenleftBig
3
4et+1
4/parenrightBig10
.I fX andYare independent, what are
(a)P{X+Y=2}?
(b)P{XY=0}?
(c)E[XY ]?
7.76. LetXbe the value of the ﬁrst die and Ythe sum
of the values when two dice are rolled. Compute the joint
moment generating function of XandY.7.77. The joint density of XandYis given by
f(x,y)=1√
2πe−ye−(x−y)2/20<y<q,
−q<x<q
(a)Compute the joint moment generating function of X
andY.
(b)Compute the individual moment generating functions.
7.78. Two envelopes, each containing a check, are placed
in front of you. You are to choose one of the envelopes,open it, and see the amount of the check. At this point,either you can accept that amount or you can exchange it
for the check in the unopened envelope. What should you
do? Is it possible to devise a strategy that does better thanjust accepting the ﬁrst envelope?
LetAandB,A<B, denote the (unknown) amounts of
the checks, and note that the strategy that randomly selectsan envelope and always accepts its check has an expected
return of (A+B)/2. Consider the following strategy: Let
F(·)be any strictly increasing (that is, continuous) distribu-
tion function. Choose an envelope randomly and open it.If the discovered check has the value x, then accept it with
probability F(x)and exchange it with probability 1 −F(x).
(a)Show that if you employ the latter strategy, then your
expected return is greater than (A+B)/2.
Hint : Condition on whether the ﬁrst envelope has the
value AorB.
Now consider the strategy that ﬁxes a value xand then
accepts the ﬁrst check if its value is greater than xand
exchanges it otherwise.
(b)Show that for any x, the expected return under
the x-strategy is always at least (A+B)/2a n d
that it is strictly larger than (A+B)/2i fx lies between
AandB.
(c)LetXbe a continuous random variable on the whole
line, and consider the following strategy: Generate the
value of X,a n di fX =x, then employ the x-strategy of
part (b). Show that the expected return under this strategy
is greater than (A+B)/2.
7.79. Successive weekly sales, in units of $1, 000, have a
bivariate normal distribution with common mean 40, com-mon standard deviation 6, and correlation .6.
(a)Find the probability that the total of the next 2 weeks’
sales exceeds 90.
(b)If the correlation were .2 rather than .6, do you think
that this would increase or decrease the answer to (a)?
Explain your reasoning.
(c)Repeat (a) when the correlation is .2.

<<<PAGE 372>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 359
A First Course in Probability 359
Theoretical Exercises
7.1.Show that E[(X−a)2] is minimized at a=E[X].
7.2.Suppose that Xis a continuous random variable with
density function f. Show that E[|X−a|] is minimized
when ais equal to the median of F.
Hint :W r i t e
E[|X−a|]=/integraldisplay
|x−a|f(x)dx
Now break up the integral into the regions where x<a
and where x>a, and differentiate.
7.3.Prove Proposition 2.1 when
(a)XandYhave a joint probability mass function;
(b)XandYhave a joint probability density function and
g(x,y)Ú0f o ra l lx, y.
7.4.LetXbe a random variable having ﬁnite expectation
μand variance σ2,a n dl e tg (·)be a twice differentiable
function. Show that
E[g(X)]Lg(μ)+g/prime/prime(μ)
2σ2
Hint : Expand g(·)in a Taylor series about μ. Use the ﬁrst
three terms and ignore the remainder.
7.5.LetA1,A2,...,Anbe arbitrary events, and deﬁne
Ck={at least kof the Aioccur}. Show that
n/summationdisplay
k=1P(Ck)=n/summationdisplay
k=1P(Ak)
Hint :L e tX denote the number of the Aithat occur. Show
that both sides of the preceding equation are equal to
E[X].
7.6.In the text, we noted that
E⎡
⎣q/summationdisplay
i=1Xi⎤⎦=q/summationdisplay
i=1E[Xi]
when the Xiare all nonnegative random variables. Since
an integral is a limit of sums, one might expect that
E/bracketleftbigg/integraldisplayq
0X(t)dt/bracketrightbigg
=/integraldisplayq
0E[X(t)]dt
whenever X(t),0…t<q, are all nonnegative random
variables; this result is indeed true. Use it to give another
proof of the result that for a nonnegative random vari-
ableX,
E[X)=/integraldisplayq
0P{X>t}dtHint : Deﬁne, for each nonnegative t, the random variable
X(t)by
X(t)=/braceleftbigg
1i f t<X
0i f tÚX
Now relate/integraltextq
0X(t)dttoX.
7.7.We say that Xisstochastically larger than Y, written
XÚstY,i f ,f o ra l lt ,
P{X>t}ÚP{Y>t}
Show that if XÚstY,t h e n E[X]ÚE[Y]w h e n
(a)XandYare nonnegative random variables;
(b)XandYare arbitrary random variables.
Hint :W r i t eX as
X=X+−X−
where
X+=/braceleftbigg
XifXÚ0
0i f X<0,X−=/braceleftbigg
0i f XÚ0
−X ifX<0
Similarly, represent YasY+−Y−. Then make use of part
(a).
7.8.Show that Xis stochastically larger than Yif and
only if
E[f(X)]ÚE[f(Y)]
for all increasing functions f.
Hint : Show that XÚstY,t h e n E[f(X)]ÚE[f(Y)] by show-
ing that f(X)Ústf(Y)and then using Theoretical Exercise
7.7. To show that if E[f(X)]ÚE[f(Y)] for all increasing
functions f,t h e n P{X>t}ÚP{Y>t}, deﬁne an appro-
priate increasing function f.
7.9.A coin having probability pof landing on heads is
ﬂipped ntimes. Compute the expected number of runs of
heads of size 1, of size 2, and of size k,1…k…n.
7.10. LetX1,X2,...,Xnbe independent and identically
distributed positive random variables. For k…n, ﬁnd
E⎡
⎢⎢⎢⎢⎢⎢⎣k/summationdisplay
i=1Xi
n/summationdisplay
i=1Xi⎤
⎥⎥⎥⎥⎥⎥⎦
7.11. Consider nindependent trials, each resulting in
any one of rpossible outcomes with probabilities
P
1,P2,...,Pr.L e t Xdenote the number of outcomes that
never occur in any of the trials. Find E[X] and show
that among all probability vectors P1,...,Pr,E[X]i sm i n -
imized when Pi=1/r,i=1,...,r.

<<<PAGE 373>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 360
360 Chapter 7 Properties of Expectation
7.12. LetX1,X2,...be a sequence of independent random
variables having the probability mass function
P{Xn=0}=P {Xn=2}=1/2, nÚ1
The random variable X=/summationtextq
n=1Xn/3nis said to have the
Cantor distribution.F i n dE [X]a n dV a r (X).
7.13. LetX1,...,Xnbe independent and identically dis-
tributed continuous random variables. We say that a
record value occurs at time j,j…n,i f XjÚXifor all
1…i…j. Show that
(a)E[number of record values] =n/summationdisplay
j=11/j;
(b)Var(number of record values )=n/summationdisplay
j=1(j−1)/j2.
7.14. For Example 2i, show that the variance of the num-
ber of coupons needed to amass a full set is equal to
N−1/summationdisplay
i=1iN
(N−i)2
When Nis large, this can be shown to be approximately
equal (in the sense that their ratio approaches 1 as N→q)
toN2π2/6.
7.15. Consider nindependent trials, the ith of which results
in a success with probability Pi.
(a)Compute the expected number of successes in the n
trials—call it μ.
(b)For a ﬁxed value of μ, what choice of P1,...,Pnmaxi-
mizes the variance of the number of successes?
(c)What choice minimizes the variance?
*7.16. Suppose that each of the elements of S={1, 2,...,n}
is to be colored either red or blue. Show that if A1,...,Ar
are subsets of S, there is a way of doing the coloring so
that at mostr/summationtext
i=1(1/2)|Ai|−1of these subsets have all their
elements the same color (where |A|denotes the number
of elements in the set A).
7.17. Suppose that X1andX2are independent random
variables having a common mean μ. Suppose also that
Var(X1)=σ2
1and Var(X 2)=σ2
2. The value of μis
unknown, and it is proposed that μbe estimated by a
weighted average of X1andX2.T h a ti s , λX1+(1−λ)X 2
will be used as an estimate of μfor some appropriate value
ofλ. Which value of λyields the estimate having the low-
est possible variance? Explain why it is desirable to use
this value of λ.
7.18. In Example 4f, we showed that the covariance of
the multinomial random variables NiandNjis equal to
−mP iPjby expressing NiandNjas the sum of indicatorvariables. We could also have obtained that result by usingthe formula
Var(N
i+Nj)=Var(Ni)+Var(Nj)+2C o v (Ni,Nj)
(a)What is the distribution of Ni+Nj?
(b)Use the preceding identity to show that Cov (Ni,Nj)=
−mP iPj.
7.19. Show that XandYare identically distributed and not
necessarily independent, then
Cov(X +Y,X−Y)=0
7.20. The Conditional Covariance Formula . The condi-
tional covariance of XandY, given Z, is deﬁned by
Cov(X ,Y|Z)KE[(X−E[X|Z])(Y−E[Y|Z])|Z]
(a)Show that
Cov(X ,Y|Z)=E[XY|Z]−E[X|Z]E[Y|Z]
(b)Prove the conditional covariance formula
Cov(X,Y)=E[Cov(X ,Y|Z)]
+Cov(E[X|Z],E[Y|Z])
(c)SetX=Yin part (b) and obtain the conditional vari-
ance formula.
7.21. LetX(i),i=1,...,n, denote the order statistics from
as e to fn uniform (0, 1) random variables, and note that
the density function of X(i)is given by
f(x)=n!
(i−1)!(n −i)!xi−1(1−x)n−i0<x<1
(a)Compute Var(X (i)),i=1,...,n.
(b)Which value of iminimizes, and which value maxi-
mizes, Var(X (i))?
7.22. Show that Y=a+bX,t h e n
ρ(X,Y)=/braceleftbigg
+1i f b>0
−1i f b<0
7.23. Show that Zis a standard normal random variable
and if Yis deﬁned by Y=a+bZ+cZ2,t h e n
ρ(Y,Z)=b/radicalbig
b2+2c2
7.24. Prove the Cauchy–Schwarz inequality, namely,
(E[XY ])2…E[X2]E[Y2]

<<<PAGE 374>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 361
A First Course in Probability 361
Hint :U n l e s sY =−tXfor some constant, in which case
the inequality holds with equality, it follows that for all t,
0<E[(tX+Y)2]=E[X2]t2+2E[XY ]t+E[Y2]
Hence, the roots of the quadratic equation
E[X2]t2+2E[XY ]t+E[Y2]=0
must be imaginary, which implies that the discriminant of
this quadratic equation must be negative.
7.25. Show that if XandYare independent, then
E[X|Y=y]=E[X]f o r a l l y
(a)in the discrete case;
(b)in the continuous case.
7.26. Prove that E[g(X)Y|X]=g(X)E[Y|X].
7.27. Prove that if E[Y|X=x]=E[Y]f o ra l l x,t h e n X
andYare uncorrelated; give a counterexample to show
that the converse is not true.
Hint : Prove and use the fact that E[XY ]=E[XE[Y|X]].
7.28. Show that Cov (X,E[Y|X])=Cov(X ,Y).
7.29. LetX1,...,Xnbe independent and identically dis-
tributed random variables. Find
E[X1|X1+ ··· + Xn=x]
7.30. Consider Example 4f, which is concerned with the
multinomial distribution. Use conditional expectation tocompute E[N
iNj], and then use this to verify the formula
for Cov(N i,Nj)given in Example 4f.
7.31. An urn initially contains bblack and wwhite balls.
At each stage, we add rblack balls and then withdraw,
at random, rballs from the b+w+rballs in the urn.
Show that
E[number of white balls after stage t]
=/parenleftbiggb+w
b+w+r/parenrightbiggt
w
7.32. For an event A,l e tI Aequal 1 if Aoccurs and
let it equal 0 if Adoes not occur. For a random
variable X, show that
E[X|A]=E[XIA]
P(A)
7.33. A coin that lands on heads with probability pis con-
tinually ﬂipped. Compute the expected number of ﬂips
that are made until a string of rheads in a row is obtained.
Hint : Condition on the time of the ﬁrst occurrence of tails
to obtain the equationE[X]=(1−p)r/summationdisplay
i=1pi−1(i+E[X])
+(1−p)q/summationdisplay
i=r+1pi−1r
Simplify and solve for E[X].
7.34. For another approach to Theoretical Exercise 7.33,
letTrdenote the number of ﬂips required to obtain a run
ofrconsecutive heads.
(a)Determine E[Tr|Tr−1].
(b)Determine E[Tr]i nt e r m so f E[Tr−1].
(c)What is E[T1]?
(d)What is E[Tr]?
7.35. The probability generating function of the discrete
nonnegative integer valued random variable Xhaving
probability mass function pj,jÚ0, is deﬁned by
φ(s)=E[sX]=q/summationdisplay
j=0pjsj
LetYbe a geometric random variable with parameter
p=1−s,w h e r e0 <s<1. Suppose that Yis independent
ofX, and show that
φ(s)=P{X<Y}
7.36. One ball at a time is randomly selected from an
urn containing awhite and bblack balls until all of the
remaining balls are of the same color. Let Ma,bdenote the
expected number of balls left in the urn when the exper-
iment ends. Compute a recursive formula for Ma,band
solve when a=3a n d b=5.
7.37. An urn contains awhite and bblack balls. After a ball
is drawn, it is returned to the urn if it is white; but if it is
black, it is replaced by a white ball from another urn. Let
Mndenote the expected number of white balls in the urn
after the foregoing operation has been repeated ntimes.
(a)Derive the recursive equation
Mn+1=/parenleftbigg
1−1
a+b/parenrightbigg
Mn+1
(b)Use part (a) to prove that
Mn=a+b−b/parenleftbigg
1−1
a+b/parenrightbiggn
(c)What is the probability that the (n+1)ball drawn is
white?
7.38. The best linear predictor of Ywith respect to X1and
X2is equal to a+bX1+cX2,w h e r e a,b,a n d care chosen
to minimize

<<<PAGE 375>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 362
362 Chapter 7 Properties of Expectation
E[(Y−(a+bX1+cX2))2]
Determine a,b,a n dc.
7.39. The best quadratic predictor of Ywith respect to X
isa+bX+cX2,w h e r e a,b,a n d care chosen to minimize
E[(Y−(a+bX+cX2))2]. Determine a,b,a n d c.
7.40. Use the conditional variance formula to determine
the variance of a geometric random variable Xhaving
parameter p.
7.41. LetXbe a normal random variable with parameters
μ=0a n d σ2=1, and let I, independent of X,b es u c h
thatP{I=1}=1
2=P{I=0}. Now deﬁne Yby
Y=/braceleftbigg
XifI=1
−X ifI=0
In words, Yis equally likely to equal either Xor−X.
(a)AreXandYindependent?
(b)AreIandYindependent?
(c)Show that Yis normal with mean 0 and variance 1.
(d)Show that Cov (X,Y)=0.
7.42. It follows from Proposition 6.1 and the fact that the
best linear predictor of Ywith respect to Xisμy+
ρσy
σx(X−μx)that if
E[Y|X]=a+bX
then
a=μy−ρσy
σxμx b=ρσy
σx
(Why?) Verify this directly.
7.43. Show that for random variables XandZ,
E[(X−Y)2]=E[X2]−E[Y2]
where
Y=E[X|Z]
7.44. Consider a population consisting of individuals able
to produce offspring of the same kind. Suppose that by
the end of its lifetime, each individual will have produced
jnew offspring with probability Pj,jÚ0, independently
of the number produced by any other individual. The
number of individuals initially present, denoted by X0,
is called the size of the zeroth generation. All offspringof the zeroth generation constitute the ﬁrst generation,
and their number is denoted by X
1. In general, let Xn
denote the size of the nth generation. Let μ=q/summationtext
j=0jPjand
σ2=q/summationtext
j=0(j−μ)2Pjdenote, respectively, the mean and the
variance of the number of offspring produced by a singleindividual. Suppose that X0=1 — that is, initially there is
a single individual in the population.
(a)Show that
E[Xn]=μE[Xn−1]
(b)Use part (a) to conclude that
E[Xn]=μn
(c)Show that
Var(Xn)=σ2μn−1+μ2Var(Xn−1)
(d)Use part (c) to conclude that
Var(Xn)=⎧
⎪⎪⎨
⎪⎪⎩σ2μn−1/parenleftbiggμn−1
μ−1/parenrightbigg
ifμZ1
nσ2ifμ=1
The model just described is known as a branching process ,
and an important question for a population that evolves
along such lines is the probability that the population willeventually die out. Let πdenote this probability when the
population starts with a single individual. That is,
π=P{population eventually dies out |X
0=1)
(e)Argue that πsatisﬁes
π=q/summationdisplay
j=0Pjπj
Hint : Condition on the number of offspring of the initial
member of the population.
7.45. Verify the formula for the moment generating func-
tion of a uniform random variable that is given in
Table 7.2. Also, differentiate to verify the formulas for the
mean and variance.
7.46. For a standard normal random variable Z,l e tμ n=
E[Zn]. Show that
μn=⎧
⎪⎨
⎪⎩0w h e n nis odd
(2j)!
2jj!when n=2j
Hint : Start by expanding the moment generating function
ofZinto a Taylor series about 0 to obtain
E[etZ]=et2/2
=q/summationdisplay
j=0(t2/2)j
j!
7.47. LetXbe a normal random variable with mean μand
variance σ2. Use the results of Theoretical Exercise 7.46
to show that

<<<PAGE 376>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 363
A First Course in Probability 363
E[Xn]=[n/2]/summationdisplay
j=0/parenleftbigg
n
2j/parenrightbigg
μn−2jσ2j(2j)!
2jj!
In the preceding equation, [ n/2] is the largest integer less
than or equal to n/2. Check your answer by letting n=1
andn=2.
7.48. IfY=aX+b,w h e r ea andbare constants, express
the moment generating function of Yin terms of the
moment generating function of X.
7.49. The positive random variable Xis said to be a lognor-
mal random variable with parameters μandσ2if log(X )
is a normal random variable with mean μand variance σ2.
Use the normal moment generating function to ﬁnd the
mean and variance of a lognormal random variable.
7.50. LetXhave moment generating function M(t),a n d
deﬁne /Psi1(t)=logM(t). Show that
/Psi1/prime/prime(t)|t=0=Var(X)
7.51. Use Table 7.2 to determine the distribution ofn/summationtext
i=1Xi
when X1,...,Xnare independent and identically
distributed exponential random variables, each having
mean 1/λ.7.52. Show how to compute Cov( X,Y) from the joint
moment generating function of XandY.
7.53. Suppose that X1,...,Xnhave a multivariate normal
distribution. Show that X1,...,Xnare independent ran-
dom variables if and only if
Cov(Xi,Xj)=0w h e n iZj
7.54. IfZis a standard normal random variable, what is
Cov(Z ,Z2)?
7.55. Suppose that Yis a normal random variable with
mean μand variance σ2, and suppose also that the con-
ditional distribution of X, given that Y=y, is normal with
mean yand variance 1.
(a)Argue that the joint distribution of X,Yis the same
as that of Y+Z,Ywhen Zis a standard normal random
variable that is independent of Y.
(b)Use the result of part (a) to argue that X,Yhas a
bivariate normal distribution.
(c)Find E[X], Var(X ),a n dC o r r (X,Y).
(d)Find E[Y|X=x].
(e)What is the conditional distribution of Ygiven that
X=x?
Self-Test Problems and Exercises
7.1.Consider a list of mnames, where the same name may
appear more than once on the list. Let n(i),i=1,...,m,
denote the number of times that the name in position i
appears on the list, and let ddenote the number of distinct
names on the list.
(a)Express din terms of the variables m,n(i),i=1,...,m.
LetUbe a uniform (0, 1) random variable, and let X=
[mU ]+1.
(b)What is the probability mass function of X?
(c)Argue that E[m/n(X )]=d.
7.2.An urn has nwhite and mblack balls that are removed
one at a time in a randomly chosen order. Find the
expected number of instances in which a white ball is
immediately followed by a black one.
7.3.Twenty individuals consisting of 10 married couples
are to be seated at 5 different tables, with 4 people at each
table.
(a)If the seating is done “at random,” what is the expected
number of married couples that are seated at the same
table?
(b)If 2 men and 2 women are randomly chosen to be
seated at each table, what is the expected number of mar-
ried couples that are seated at the same table?7.4.If a die is to be rolled until all sides have appeared at
least once, ﬁnd the expected number of times that outcome1 appears.
7.5.Ad e c ko f2 ncards consists of nred and nblack cards.
The cards are shufﬂed and then turned over one at a time.
Suppose that each time a red card is turned over, we win
1 unit if more red cards than black cards have been turned
over by that time. (For instance, if n=2 and the result
isrbrb , then we would win a total of 2 units.) Find the
expected amount that we win.
7.6.LetA
1,A2,...,Anbe events, and let Ndenote the
number of them that occur. Also, let I=1i fa l lo ft h e s e
events occur, and let it be 0 otherwise. Prove Bonferroni’s
inequality, namely,
P(A1···An)Ún/summationdisplay
i=1P(Ai)−(n−1)
Hint : Argue ﬁrst that N…n−1+I.
7.7.LetXbe the smallest value obtained when knum-
bers are randomly chosen from the set 1, ...,n.F i n d E[X]
by interpreting Xas a negative hypergeometric random
variable.

<<<PAGE 377>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 364
364 Chapter 7 Properties of Expectation
7.8.An arriving plane carries rfamilies. A total of njof
these families have checked in a total of jpieces of lug-
gage,/summationtext
jnj=r. Suppose that when the plane lands, the
N=/summationtext
jjnjpieces of luggage come out of the plane in a ran-
dom order. As soon as a family collects all of its luggage,
it immediately departs the airport. If the Sanchez family
checked in jpieces of luggage, ﬁnd the expected number
of families that depart after they do.
*7.9.Nineteen items on the rim of a circle of radius 1 are
to be chosen. Show that for any choice of these points,there will be an arc of (arc) length 1 that contains at least4o ft h e m .
7.10. LetXbe a Poisson random variable with mean λ.
Show that if λis not too small, then
Var(√
X)L.25
Hint : Use the result of Theoretical Exercise 7.4 to approx-
imate E[√
X].
7.11. Suppose in Self-Test Problem 7.3 that the 20 people
are to be seated at seven tables, three of which have 4 seats
and four of which have 2 seats. If the people are randomlyseated, ﬁnd the expected value of the number of married
couples that are seated at the same table.
7.12. Individuals 1 through n,n>1, are to be recruited
into a ﬁrm in the following manner: Individual 1 starts
the ﬁrm and recruits individual 2. Individuals 1 and 2 will
then compete to recruit individual 3. Once individual 3 is
recruited, individuals 1, 2, and 3 will compete to recruitindividual 4, and so on. Suppose that when individuals
1, 2,...,icompete to recruit individual i+1, each of them
is equally likely to be the successful recruiter.
(a)Find the expected number of the individuals 1, ...,n
who did not recruit anyone else.
(b)Derive an expression for the variance of the number of
individuals who did not recruit anyone else, and evaluate
it for n=5.
7.13. The nine players on a basketball team consist of 2
centers, 3 forwards, and 4 backcourt players. If the play-
ers are paired up at random into three groups of size 3
each, ﬁnd (a) the expected value and (b) the variance ofthe number of triplets consisting of one of each type of
player.
7.14. A deck of 52 cards is shufﬂed and a bridge hand of
13 cards is dealt out. Let XandYdenote, respectively, the
number of aces and the number of spades in the hand.
(a)Show that XandYare uncorrelated.
(b)Are they independent?
7.15. Each coin in a bin has a value attached to it. Each
time that a coin with value pis ﬂipped, it lands on heads
with probability p. When a coin is randomly chosen fromthe bin, its value is uniformly distributed on (0, 1). Sup-
pose that after the coin is chosen but before it is ﬂipped,
you must predict whether it will land on heads or on tails.You will win 1 if you are correct and will lose 1 otherwise.
(a)What is your expected gain if you are not told the value
of the coin?
(b)Suppose now that you are allowed to inspect the coin
before it is ﬂipped, with the result of your inspection being
that you learn the value of the coin. As a function of p,t h e
value of the coin, what prediction should you make?
(c)Under the conditions of part (b), what is your expected
gain?
7.16. In Self-Test Problem 7.1, we showed how to use the
value of a uniform (0, 1) random variable (commonly
called a random number ) to obtain the value of a random
variable whose mean is equal to the expected number ofdistinct names on a list. However, its use required that onechoose a random position and then determine the num-
ber of times that the name in that position appears on the
list. Another approach, which can be more efﬁcient whenthere is a large amount of replication of names, is as fol-
lows: As before, start by choosing the random variable
Xas in Problem 7.1. Now identify the name in position
X, and then go through the list, starting at the beginning,
until that name appears. Let Iequal 0 if you encounter that
name before getting to position X,a n dl e tI equal 1 if your
ﬁrst encounter with the name is at position X. Show that
E[mI]=d.
Hint : Compute E[I] by using conditional expectation.
7.17. A total of mitems are to be sequentially distributed
among ncells, with each item independently being put in
celljwith probability p
j,j=1,...,n. Find the expected
number of collisions that occur, where a collision occurs
whenever an item is put into a nonempty cell.
7.18. LetXbe the length of the initial run in a random
ordering of nones and mzeros. That is, if the ﬁrst kvalues
are the same (either all ones or all zeros), then XÚk.F i n d
E[X].
7.19. There are nitems in a box labeled Handmin a box
labeled T. A coin that comes up heads with probability p
and tails with probability 1 −pis ﬂipped. Each time it
comes up heads, an item is removed from the Hbox, and
each time it comes up tails, an item is removed from the
Tbox. (If a box is empty and its outcome occurs, then no
items are removed.) Find the expected number of coin ﬂips
needed for both boxes to become empty.
Hint : Condition on the number of heads in the ﬁrst n+m
ﬂips.
7.20. LetXbe a nonnegative random variable having dis-
tribution function F. Show that if F(x)=1−F(x),t h e n
E[Xn]=/integraldisplayq
0xn−1F(x)dx

<<<PAGE 378>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 365
A First Course in Probability 365
Hint : Start with the identity
Xn=n/integraldisplayX
0xn−1dx
=n/integraldisplayq
0xn−1IX(x)dx
where
Ix(x)=/braceleftbigg
1, if x<X
0, otherwise
*7.21. Let a1,...,an, not all equal to 0, be such that/summationtextn
i=1ai=0. Show that there is a permutation i1,...,in
such that/summationtextnj=1aijaij+1<0.
Hint : Use the probabilistic method. (It is interesting that
there need not be a permutation whose sum of products
of successive pairs is positive. For instance, if n=3,
a1=a2=−1, and a3=2, there is no such permutation.)
7.22. Suppose that Xi,i=1, 2, 3, are independent Poisson
random variables with respective means λi,i=1, 2, 3. Let
X=X1+X2andY=X2+X3. The random vector X,Y
is said to have a bivariate Poisson distribution.
(a)Find E[X]a n d E[Y].
(b)Find Cov (X,Y).
(c)Find the joint probability mass function P{X=i,
Y=j}.
7.23. Let(Xi,Yi),i=1,..., be a sequence of independent
and identically distributed random vectors. That is, X1,Y1
is independent of, and has the same distribution as, X2,Y2,
and so on. Although XiandYican be dependent, Xiand
Yjare independent when iZj.L e t
μx=E[Xi],μy=E[Yi],σ2
x=Var(Xi),
σ2
y=Var(Yi),ρ=Corr(X i,Yi)
Find Corr (/summationtextn
i=1Xi,/summationtextnj=1Yj).
7.24. Three cards are randomly chosen without replace-
ment from an ordinary deck of 52 cards. Let Xdenote the
number of aces chosen.
(a)Find E[X|the ace of spades is chosen].
(b)Find E[X|at least one ace is chosen].
7.25. Let/Phi1be the standard normal distribution function,
and let Xbe a normal random variable with mean μand
variance 1. We want to ﬁnd E[/Phi1(X)]. To do so, let Zbe
a standard normal random variable that is independent of
X,a n dl e t
I=/braceleftbigg
1, if Z<X
0, if ZÚX
(a)Show that E[I|X=x]=/Phi1(x).
(b)Show that E[/Phi1(X)]=P{Z<X}.
(c)Show that E[/Phi1(X)]=/Phi1/parenleftBig
μ√
2/parenrightBig
.
Hint : What is the distribution of X−Z?The preceding comes up in statistics. Suppose you are
about to observe the value of a random variable Xthat
is normally distributed with an unknown mean μand vari-
ance 1, and suppose that you want to test the hypothesisthat the mean μis greater than or equal to 0. Clearly
you would want to reject this hypothesis if Xis suf-
ﬁciently small. If it results that X=x, then the p-
value of the hypothesis that the mean is greater than
or equal to 0 is deﬁned to be the probability that X
would be as small as xifμwere equal to 0 (its small-
est possible value if the hypothesis were true). (A smallp-value is taken as an indication that the hypothesis is
probably false.) Because Xhas a standard normal dis-
tribution when μ=0, the p-value that results when
X=xis/Phi1(x).Therefore, the preceding shows that the
expected p-value that results when the true mean is μ
is/Phi1/parenleftBig
μ√
2/parenrightBig
.
7.26. A coin that comes up heads with probability pis
ﬂipped until either a total of nheads or of mtails is
amassed. Find the expected number of ﬂips.Hint : Imagine that one continues to ﬂip even after the
goal is attained. Let Xdenote the number of ﬂips needed
to obtain nheads, and let Ydenote the number of
ﬂips needed to obtain mtails. Note that max(X ,Y)+
min(X ,Y)=X+Y.Compute E[max(X,Y)] by con-
ditioning on the number of heads in the ﬁrst n+m−1
ﬂips.
7.27. Ad e c ko f ncards numbered 1through n, initially in
any arbitrary order, is shufﬂed in the following manner: At
each stage, we randomly choose one of the cards and move
it to the front of the deck, leaving the relative positionsof the other cards unchanged. This procedure is contin-
ued until all but one of the cards has been chosen. At this
point, it follows by symmetry that all n! possible orderings
are equally likely. Find the expected number of stages that
are required.
7.28. Suppose that a sequence of independent trials in
which each trial is a success with probability pis per-
formed until either a success occurs or a total of ntrials
has been reached. Find the mean number of trials that are
performed.
Hint : The computations are simpliﬁed if you use the
identity that for a nonnegative integer valued random
variable X,
E[X]=
q/summationdisplay
i=1P{XÚi}
7.29. Suppose that XandYare both Bernoulli random
variables. Show that XandYare independent if and only
if Cov (X,Y)=0.
7.30. In the generalized match problem, there are nindi-
viduals of whom niwear hat size i,/summationtextr
i=1ni=n.There are

<<<PAGE 379>>>

July 14, 2014 M07_ROSSS4772_09_SE_C07 page 366
366 Chapter 7 Properties of Expectation
alsonhats, of which hiare of size i,/summationtextr
i=1hi=n.If each
individual randomly chooses a hat (without replacement),
ﬁnd the expected number who choose a hat that is their
size.
7.31. For random variables XandY, show that
/radicalbig
Var(X+Y)…/radicalbig
Var(X)+/radicalbig
Var(Y)
That is, show that the standard deviation of a sum is
always less than or equal to the sum of the standard
deviations.7.32. Starting with
etX=1+tX+t2X2
2!+t3X3
3!+...+tnXn
n!+...
show that
(a)
M(t)=E[etX]=1+tE[X]+t2E[X2]
2!+...+tnE[Xn]
n!+...
(b) Use (a) to show thatdn
dtnM(t)|t=0=E[Xn]

<<<PAGE 380>>>

July 14, 2014 M08_ROSSS4772_09_SE_C08 page 367
Chapter
Limit Theorems8
Contents
8.1 Introduction
8.2 Chebyshev’s Inequality and the Weak
Law of Large Numbers
8.3 The Central Limit Theorem
8.4 The Strong Law of Large Numbers8.5 Other Inequalities
8.6 Bounding the Error Probability WhenApproximating a Sum of Independent
Bernoulli Random Variables by a PoissonRandom Variable
8.1 Introduction
The most important theoretical results in probability theory are limit theorems. Ofthese, the most important are those classiﬁed either under the heading laws of large
numbers or under the heading central limit theorems . Usually, theorems are consid-
ered to be laws of large numbers if they are concerned with stating conditions underwhich the average of a sequence of random variables converges (in some sense) to
the expected average. By contrast, central limit theorems are concerned with deter-
mining conditions under which the sum of a large number of random variables has aprobability distribution that is approximately normal.
8.2 Chebyshev’s Inequality and the Weak Law of Large Numbers
We start this section by proving a result known as Markov’s inequality.
Proposition
2.1Markov’s inequality
IfXis a random variable that takes only nonnegative values, then for any value
a>0,
P{XÚa}…E[X]
a
Proof Fora>0, let
I=/braceleftBigg
1i f XÚa
0 otherwise
and note that, since XÚ0,
I…X
a
367

<<<PAGE 381>>>

July 14, 2014 M08_ROSSS4772_09_SE_C08 page 368
368 Chapter 8 Limit Theorems
Taking expectations of the preceding inequality yields
E[I]…E[X]
a
which, because E[I]=P{XÚa}, proves the result.
As a corollary, we obtain Proposition 2.2.
Proposition
2.2Chebyshev’s inequality
IfXis a random variable with ﬁnite mean μand variance σ2, then for any value
k>0,
P{|X−μ|Úk}…σ2
k2
Proof Since (X−μ)2is a nonnegative random variable, we can apply Markov’s
inequality (with a=k2) to obtain
P{(X−μ)2Úk2}…E[(X−μ)2]
k2(2.1)
But since (X−μ)2Úk2if and only if |X−μ|Úk, Equation (2.1) is equivalent to
P{|X−μ|Úk}…E[(X−μ)2]
k2=σ2
k2
and the proof is complete.
The importance of Markov’s and Chebyshev’s inequalities is that they enable
us to derive bounds on probabilities when only the mean or both the mean and the
variance of the probability distribution are known. Of course, if the actual distribu-
tion were known, then the desired probabilities could be computed exactly and we
would not need to resort to bounds.
Example
2aSuppose that it is known that the number of items produced in a factory during aweek is a random variable with mean 50.
(a)What can be said about the probability that this week’s production willexceed 75?
(b)If the variance of a week’s production is known to equal 25, then what canbe said about the probability that this week’s production will be between 40and 60?
Solution LetXbe the number of items that will be produced in a week.
(a) By Markov’s inequality,
P{X>75}…E[X]
75=50
75=2
3
(b) By Chebyshev’s inequality,
P{|X−50|Ú10}…σ2
102=1
4
Hence,
P{|X−50|<10}Ú1−1
4=3
4
so the probability that this week’s production will be between 40 and 60 is atleast .75. .

<<<PAGE 382>>>

July 14, 2014 M08_ROSSS4772_09_SE_C08 page 369
A First Course in Probability 369
As Chebyshev’s inequality is valid for all distributions of the random variable
X, we cannot expect the bound on the probability to be very close to the actual
probability in most cases. For instance, consider Example 2b.
Example
2bIfXis uniformly distributed over the interval (0, 10), then, since E[X]=5 and
Var(X)=25
3, it follows from Chebyshev’s inequality that
P{|X−5|>4}…25
3(16)L.52
whereas the exact result is
P{|X−5|>4}=.20
Thus, although Chebyshev’s inequality is correct, the upper bound that it provides is
not particularly close to the actual probability.
Similarly, if Xis a normal random variable with mean μand variance σ2,
Chebyshev’s inequality states that
P{|X−μ|>2σ}…1
4
whereas the actual probability is given by
P{|X−μ|>2σ}=P/braceleftBigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleX−μ
σ/vextendsingle/vextendsingle/vextendsingle/vextendsingle>2/bracerightBigg
=2[1−/Phi1(2)]L.0456 .
Chebyshev’s inequality is often used as a theoretical tool in proving results. This
use is illustrated ﬁrst by Proposition 2.3 and then, most importantly, by the weak law
of large numbers.
Proposition
2.3If Var(X )=0, then
P{X=E[X]}=1
In other words, the only random variables having variances equal to 0 are those thatare constant with probability 1.
Proof By Chebyshev’s inequality, we have, for any nÚ1,
P/braceleftbigg
|X−μ|>1
n/bracerightbigg
=0
Letting n→qand using the continuity property of probability yields
0=limn→qP/braceleftbigg
|X−μ|>1
n/bracerightbigg
=P/braceleftBigg
limn→q/braceleftbigg
|X−μ|>1
n/bracerightbigg/bracerightBigg
=P{XZμ}
and the result is established.
Theorem
2.1The weak law of large numbers
LetX1,X2,...be a sequence of independent and identically distributed random vari-
ables, each having ﬁnite mean E[Xi]=μ. Then, for any ε> 0,
P/braceleftBigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleX
1+ ··· + Xn
n−μ/vextendsingle/vextendsingle/vextendsingle/vextendsingleÚε/bracerightBigg
→0a s n→q

<<<PAGE 383>>>

July 14, 2014 M08_ROSSS4772_09_SE_C08 page 370
370 Chapter 8 Limit Theorems
Proof We shall prove the theorem only under the additional assumption that the
random variables have a ﬁnite variance σ2. Now, since
E/bracketleftbiggX1+ ··· + Xn
n/bracketrightbigg
=μand Var/parenleftbiggX1+ ··· + Xn
n/parenrightbigg
=σ2
n
it follows from Chebyshev’s inequality that
P/braceleftBigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleX
1+ ··· + Xn
n−μ/vextendsingle/vextendsingle/vextendsingle/vextendsingleÚε/bracerightBigg
…σ2
nε2
and the result is proven.
The weak law of large numbers was originally proven by James Bernoulli for
the special case where the Xiare 0, 1 (that is, Bernoulli) random variables. His
statement and proof of this theorem were presented in his book Ars Conjectandi,
which was published in 1713, eight years after his death, by his nephew Nicholas
Bernoulli. Note that because Chebyshev’s inequality was not known in Bernoulli’s
time, Bernoulli had to resort to a quite ingenious proof to establish the result. The
general form of the weak law of large numbers presented in Theorem 2.1 was provedby the Russian mathematician Khintchine.
8.3 The Central Limit Theorem
The central limit theorem is one of the most remarkable results in probability theory.Loosely put, it states that the sum of a large number of independent random vari-ables has a distribution that is approximately normal. Hence, it not only provides
a simple method for computing approximate probabilities for sums of independent
random variables, but also helps explain the remarkable fact that the empirical fre-quencies of so many natural populations exhibit bell-shaped (that is, normal) curves.
In its simplest form, the central limit theorem is as follows.
Theorem
3.1The central limit theorem
LetX
1,X2,...be a sequence of independent and identically distributed random vari-
ables, each having mean μand variance σ2. Then the distribution of
X1+ ··· + Xn−nμ
σ√n
tends to the standard normal as n→q. That is, for −q<a<q,
P/braceleftBigg
X1+ ··· + Xn−nμ
σ√n…a/bracerightBigg
→1√
2π/integraldisplaya
−qe−x2/2dx asn→q
The key to the proof of the central limit theorem is the following lemma, which
we state without proof.
Lemma
3.1LetZ1,Z2,...be a sequence of random variables having distribution functions FZn
and moment generating functions MZn,nÚ1, and let Zbe a random variable having
distribution function FZand moment generating function MZ.I f MZn(t)→
MZ(t)for all t, then FZn(t)→FZ(t)for all tat which FZ(t)is continuous.
If we let Zbe a standard normal random variable, then, since MZ(t)=et2/2,
it follows from Lemma 3.1 that if MZn(t)→et2/2asn→q, then FZn(t)→/Phi1(t)as
n→q.
We are now ready to prove the central limit theorem.

<<<PAGE 384>>>

July 14, 2014 M08_ROSSS4772_09_SE_C08 page 371
A First Course in Probability 371
Proof of the Central Limit Theorem: Let us assume at ﬁrst that μ=0 and σ2=1.
We shall prove the theorem under the assumption that the moment generating func-
tion of the Xi,M(t), exists and is ﬁnite. Now, the moment generating function of
Xi/√nis given by
E⎡
⎣exp/braceleftBigg
tXi√n/bracerightBigg⎤⎦=M/parenleftBigg
t
√n/parenrightBigg
Thus, the moment generating function ofn/summationtext
i=1Xi/√nis given by/bracketleftbigg
M/parenleftBig
t√n/parenrightBig/bracketrightbiggn
.L e t
L(t)=logM(t)
and note that
L(0)=0
L/prime(0)=M/prime(0)
M(0)
=μ
=0
L/prime/prime(0)=M(0)M/prime/prime(0)−[M/prime(0)]2
[M(0)]2
=E[X2]
=1
Now, to prove the theorem, we must show that [ M(t/√n)]n→et2/2asn→q,o r ,
equivalently, that nL(t/√n)→t2/2a s n→q. To show this, note that
limn→qL(t/√n)
n−1=limn→q−L/prime(t/√n)n−3/2t
−2n−2by L’H ˆopital’s rule
=limn→q/bracketleftBigg
L/prime(t/√n)t
2n−1/2/bracketrightBigg
=limn→q/bracketleftBigg
−L/prime/prime(t/√n)n−3/2t2
−2n−3/2/bracketrightBigg
again by L’H ˆopital’s rule
=limn→q⎡⎣L
/prime/prime/parenleftBigg
t√n/parenrightBigg
t2
2⎤⎦
=t
2
2
Thus, the central limit theorem is proven when μ=0 and σ2=1. The result
now follows in the general case by considering the standardized random variables
X∗
i=(Xi−μ)/σ and applying the preceding result, since E[X∗
i]=0, Var(X∗
i)=1.
Remark Although Theorem 3.1 states only that, for each a,
P/braceleftBigg
X1+ ··· + Xn−nμ
σ√n…a/bracerightBigg
→/Phi1( a)
it can, in fact, be shown that the convergence is uniform in a. [We say that fn(a)→f(a)
uniformly in aif, for each ε> 0, there exists an Nsuch that |fn(a)−f(a)|<εfor
allawhenever nÚN.] .

<<<PAGE 385>>>

July 14, 2014 M08_ROSSS4772_09_SE_C08 page 372
372 Chapter 8 Limit Theorems
The ﬁrst version of the central limit theorem was proven by DeMoivre around
1733 for the special case where the Xiare Bernoulli random variables with p=1
2.
The theorem was subsequently extended by Laplace to the case of arbitrary p. (Since
a binomial random variable may be regarded as the sum of nindependent and identi-
cally distributed Bernoulli random variables, this justiﬁes the normal approximation
to the binomial that was presented in Section 5.4.1.) Laplace also discovered themore general form of the central limit theorem given in Theorem 3.1. His proof,
however, was not completely rigorous and, in fact, cannot easily be made rigorous.
A truly rigorous proof of the central limit theorem was ﬁrst presented by the Russianmathematician Liapounoff in the period 1901–1902.
Figure 8.1 illustrates the central limit theorem by plotting the probability mass
functions of nindependent random variables having a speciﬁed mass function when
(a)n=5, (b) n=10, (c) n=25, and (d) n=100.
Example
3aAn astronomer is interested in measuring the distance, in light-years, from his obser-vatory to a distant star. Although the astronomer has a measuring technique, he
knows that because of changing atmospheric conditions and normal error, each time
a measurement is made, it will not yield the exact distance, but merely an estimate.
As a result, the astronomer plans to make a series of measurements and then use the
average value of these measurements as his estimated value of the actual distance.If the astronomer believes that the values of the measurements are independent
and identically distributed random variables having a common mean d(the actual
Central Limit Theorem
Enter the probabilities and the number of random
variables to be summed. The output gives the massfunction of the sum along with its mean andvariance.
P0 .25
P1 .15
P2 .1
P3 .2
P4 .3
n = 5Start
Quit
Mean = 10.75
Variance = 12.6375
20 15 10 5 0
i0.000.050.100.15
p(i)
Figure 8.1(a)

<<<PAGE 386>>>

July 14, 2014 M08_ROSSS4772_09_SE_C08 page 373
A First Course in Probability 373
Central Limit Theorem
Enter the probabilities and the number of random
variables to be summed. The output gives the massfunction of the sum along with its mean andvariance.
P0 .25
P1 .15
P2 .1
P3 .2
P4 .3
n = 10Start
Quit
Mean = 21.5
Variance = 25.275
40 30 20 10 0
i0.000.08
p(i)0.06
0.04
0.02
Figure 8.1(b)
distance) and a common variance of 4 (light-years), how many measurements need
he make to be reasonably sure that his estimated distance is accurate to within ;.5
light-year?
Solution Suppose that the astronomer decides to make nobservations. If X1,
X2,...,Xnare the nmeasurements, then, from the central limit theorem, it
follows that
Zn=n/summationdisplay
i=1Xi−nd
2√n
has approximately a standard normal distribution. Hence,
P⎧
⎪⎪⎨
⎪⎪⎩−.5…n/summationdisplay
i=1Xi
n−d….5⎫
⎪⎪⎬
⎪⎪⎭=P/braceleftBigg
−.5√
n
2…Zn….5√n
2/bracerightBigg
L/Phi1/parenleftBigg√n
4/parenrightBigg
−/H9278/parenleftBigg
−√n
4/parenrightBigg
=2/Phi1/parenleftBigg√n
4/parenrightBigg
−1
Therefore, if the astronomer wants, for instance, to be 95 percent certain that his
estimated value is accurate to within .5 light-year, he should make n∗measurements,
where n∗is such that

<<<PAGE 387>>>

July 14, 2014 M08_ROSSS4772_09_SE_C08 page 374
374 Chapter 8 Limit Theorems
Central Limit Theorem
Enter the probabilities and the number of random
variables to be summed. The output gives the massfunction of the sum along with its mean andvariance.
P0 .25
P1 .15
P2 .1
P3 .2
P4 .3
n = 25Start
Quit
Mean = 53.75
Variance = 63.1875
100 0
i0.000.05
p(i)0.04
0.03
0.02
0.01
80 60 40 20
Figure 8.1(c)
2/Phi1/parenleftBigg√
n∗
4/parenrightBigg
−1=.95 or /Phi1/parenleftBigg√
n∗
4/parenrightBigg
=.975
Thus, from Table 5.1 of Chapter 5,
√
n∗
4=1.96 or n∗=(7.84)2L61.47
Asn∗is not integral valued, he should make 62 observations.
Note, however, that the preceding analysis has been done under the assumption
that the normal approximation will be a good approximation when n=62. Although
this will usually be the case, in general the question of how large nneed be before
the approximation is “good” depends on the distribution of the Xi. If the astronomer
is concerned about this point and wants to take no chances, he can still solve his
problem by using Chebyshev’s inequality. Since
E⎡
⎣n/summationdisplay
i=1Xi
n⎤⎦=d Var⎛⎝
n/summationdisplay
i=1Xi
n⎞⎠=4
n

<<<PAGE 388>>>

July 14, 2014 M08_ROSSS4772_09_SE_C08 page 375
A First Course in Probability 375
Central Limit Theorem
Enter the probabilities and the number of random
variables to be summed. The output gives the massfunction of the sum along with its mean andvariance.
P0 .25
P1 .15
P2 .1
P3 .2
P4 .3
n = 100Start
Quit
Mean = 215.
Variance = 252.75
400 0
i0.0000.030
p(i)
300 200 1000.025
0.020
0.015
0.010
0.005
Figure 8.1(d)
Chebyshev’s inequality yields
P⎧
⎪⎨
⎪⎩/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
i=1Xi
n−d/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle>.5⎫
⎪⎬
⎪⎭…4
n(.5)2=16
n
Hence, if he makes n=16/.05 =320 observations, he can be 95 percent certain that
his estimate will be accurate to within .5 light-year. .
Example
3bThe number of students who enroll in a psychology course is a Poisson random vari-
able with mean 100. The professor in charge of the course has decided that if the
number enrolling is 120 or more, he will teach the course in two separate sections,
whereas if fewer than 120 students enroll, he will teach all of the students togetherin a single section. What is the probability that the professor will have to teach two
sections?
Solution The exact solution
e−100q/summationdisplay
i=120(100)i
i!
does not readily yield a numerical answer. However, by recalling that a Poisson ran-
dom variable with mean 100 is the sum of 100 independent Poisson random variables,

<<<PAGE 389>>>

July 14, 2014 M08_ROSSS4772_09_SE_C08 page 376
376 Chapter 8 Limit Theorems
each with mean 1, we can make use of the central limit theorem to obtain an approx-
imate solution. If Xdenotes the number of students who enroll in the course, we
have
P{XÚ120}= P{XÚ119.5} (the continuity correction )
=P/braceleftBigg
X−100√
100Ú119.5 −100√
100/bracerightBigg
L1−/Phi1(1.95)
L.0256
where we have used the fact that the variance of a Poisson random variable is equalto its mean. .
Example
3cIf 10 fair dice are rolled, ﬁnd the approximate probability that the sum obtained isbetween 30 and 40, inclusive.
Solution LetXidenote the value of the ith die, i=1, 2,..., 10. Since
E(Xi)=7
2,V a r (Xi)=E[X2
i]−(E[Xi])2=35
12,
the central limit theorem yields
P{29.5 …X…40.5}= P⎧
⎪⎪⎪⎨
⎪⎪⎪⎩29.5−35
/radicalbigg
350
12…X−35/radicalbigg
350
12…40.5−35/radicalbigg
350
12⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
L2/Phi1(1.0184) −1
L.692 .
Example
3dLetXi,i=1,..., 10, be independent random variables, each uniformly distributed
over (0, 1). Calculate an approximation to P/braceleftBigg
10/summationtext
i=1Xi>6/bracerightBigg
.
Solution Since E[Xi]=1
2and Var (Xi)=1
12, we have, by the central limit theorem,
P⎧
⎨
⎩10/summationdisplay
1Xi>6⎫
⎬
⎭=P⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩10/summationdisplay
1Xi−5
/radicalBigg
10/parenleftbigg1
12/parenrightbigg>6−5/radicalBigg
10/parenleftbigg1
12/parenrightbigg⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
L1−/Phi1(√
1.2)
L.1367
Hence,10/summationtext
i=1Xiwill be greater than 6 only 14 percent of the time. .

<<<PAGE 390>>>

July 14, 2014 M08_ROSSS4772_09_SE_C08 page 377
A First Course in Probability 377
Example
3eAn instructor has 50 exams that will be graded in sequence. The times required to
grade the 50 exams are independent, with a common distribution that has mean
20 minutes and standard deviation 4 minutes. Approximate the probability that theinstructor will grade at least 25 of the exams in the ﬁrst 450 minutes of work.
Solution If we let Xibe the time that it takes to grade exam i, then
X=25/summationdisplay
i=1Xi
is the time it takes to grade the ﬁrst 25 exams. Because the instructor will grade atleast 25 exams in the ﬁrst 450 minutes of work if the time it takes to grade the ﬁrst 25exams is less than or equal to 450, we see that the desired probability is P{X…450}.
To approximate this probability, we use the central limit theorem. Now,
E[X]=
25/summationdisplay
i=1E[Xi]=25(20) =500
and
Var(X)=25/summationdisplay
i=1Var(Xi)=25(16) =400
Consequently, with Zbeing a standard normal random variable, we have
P{X…450}= P/braceleftBigg
X−500√
400…450−500√
400/bracerightBigg
LP{Z…−2.5}
=P{ZÚ2.5}
=1−/Phi1(2.5)L.006 .
Central limit theorems also exist when the Xiare independent, but not neces-
sarily identically distributed random variables. One version, by no means the most
general, is as follows.
Theorem
3.2Central limit theorem for independent random variables
LetX1,X2,...be a sequence of independent random variables having respective
means and variances μi=E[Xi],σ2
i=Var(Xi).I f( a )t h e Xiare uniformly
bounded—that is, if for some M,P{|Xi|<M}=1 for all i, and (b)q/summationtext
i=1σ2
i=q—then
P⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩n/summationdisplay
i=1(Xi−μi)
/radicaltp/radicalvertex/radicalvertex/radicalbtn/summationdisplay
i=1σ2
i…a⎫
⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎭→/Phi1(a)asn→q

<<<PAGE 391>>>

July 14, 2014 M08_ROSSS4772_09_SE_C08 page 378
378 Chapter 8 Limit Theorems
Historical note
Pierre-Simon, Marquis de Laplace (1749–1827)
The central limit theorem was originally stated and proven by the French math-
ematician Pierre-Simon, Marquis de Laplace, who came to the theorem fromhis observations that errors of measurement (which can usually be regarded
as being the sum of a large number of tiny forces) tend to be normally dis-
tributed. Laplace, who was also a famous astronomer (and indeed was called“the Newton of France”), was one of the great early contributors to both prob-
ability and statistics. Laplace was also a popularizer of the uses of probability
in everyday life. He strongly believed in its importance, as is indicated by thefollowing quotations taken from his published book Analytical Theory of Prob-
ability: “We see that the theory of probability is at bottom only common sensereduced to calculation; it makes us appreciate with exactitude what reasonableminds feel by a sort of instinct, often without being able to account for it. ...It is
remarkable that this science, which originated in the consideration of games ofchance, should become the most important object of human knowledge. ...The
most important questions of life are, for the most part, really only problems of
probability.”
The application of the central limit theorem to show that measurement
errors are approximately normally distributed is regarded as an important con-
tribution to science. Indeed, in the seventeenth and eighteenth centuries, thecentral limit theorem was often called the law of frequency of errors.L i s t e nt o
the words of Francis Galton (taken from his book Natural Inheritance, published
in 1889): “I know of scarcely anything so apt to impress the imagination as thewonderful form of cosmic order expressed by the ‘Law of Frequency of Error.’
The Law would have been personiﬁed by the Greeks and deiﬁed, if they had
known of it. It reigns with serenity and in complete self-effacement amidst thewildest confusion. The huger the mob and the greater the apparent anarchy, the
more perfect is its sway. It is the supreme law of unreason.”
8.4 The Strong Law of Large Numbers
Thestrong law of large numbers is probably the best-known result in probability
theory. It states that the average of a sequence of independent random variableshaving a common distribution will, with probability 1, converge to the mean of that
distribution.
Theorem
4.1The strong law of large numbers
LetX
1,X2,...be a sequence of independent and identically distributed random vari-
ables, each having a ﬁnite mean μ=E[Xi]. Then, with probability 1,
X1+X2+ ··· + Xn
n→μ as n→q†
As an application of the strong law of large numbers, suppose that a sequence
of independent trials of some experiment is performed. Let Ebe a ﬁxed event of the
†That is, the strong law of large numbers states that
P{lim
n→q(X1+ ··· + Xn)/n=μ}=1

<<<PAGE 392>>>

July 14, 2014 M08_ROSSS4772_09_SE_C08 page 379
A First Course in Probability 379
experiment, and denote by P(E)the probability that Eoccurs on any particular trial.
Letting
Xi=/braceleftBigg
1i f Eoccurs on the ith trial
0i f Edoes not occur on the ith trial
we have, by the strong law of large numbers, that with probability 1,
X1+ ··· + Xn
n→E[X]=P(E) (4.1)
Since X1+ ··· + Xnrepresents the number of times that the event Eoccurs in the
ﬁrstntrials, we may interpret Equation (4.1) as stating that with probability 1, the
limiting proportion of time that the event Eoccurs is just P(E).
Although the theorem can be proven without this assumption, our proof of the
strong law of large numbers will assume that the random variables Xihave a ﬁnite
fourth moment. That is, we will suppose that E[X4
i]=K<q.
Proof of the Strong Law of Large Numbers: To begin, assume that μ, the mean
of the Xi, is equal to 0. Let Sn=n/summationtext
i=1Xiand consider
E[S4
n]=E[(X1+ ··· + Xn)(X 1+ ··· + Xn)
*(X1+ ··· + Xn)(X 1+ ··· + Xn)]
Expanding the right side of the preceding equation results in terms of the form
X4
i,X3
iXj,X2
iX2
j,X2
iXjXk, and XiXjXkXl
where i,j,k, and lare all different. Because all the Xihave mean 0, it follows by
independence that
E[X3
iXj]=E[X3
i]E[Xj]=0
E[X2
iXjXk]=E[X2
i]E[Xj]E[Xk]=0
E[XiXjXkXl]=0
Now, for a given pair iandj, there will be/parenleftBigg
4
2/parenrightBigg
=6 terms in the expansion that will
equal X2
iX2
j. Hence, upon expanding the preceding product and taking expectations
term by term, it follows that
E[S4
n]=nE[X4
i]+6/parenleftBigg
n
2/parenrightBigg
E[X2
iX2
j]
=nK+3n(n−1)E[X2
i]E[X2
j]
where we have once again made use of the independence assumption. Now, since
0…Var(X2
i)=E[X4
i]−(E[X2
i])2
we have
(E[X2
i])2…E[X4
i]=K
Therefore, from the preceding, we obtain
E[S4n]…nK+3n(n−1)K

<<<PAGE 393>>>

July 14, 2014 M08_ROSSS4772_09_SE_C08 page 380
380 Chapter 8 Limit Theorems
which implies that
E/bracketleftBigg
S4
n
n4/bracketrightBigg
…K
n3+3K
n2
Therefore,
E⎡
⎣q/summationdisplay
n=1S4
n
n4⎤
⎦=q/summationdisplay
n=1E/bracketleftBigg
S4
n
n4/bracketrightBigg
<q
But the preceding implies that with probability 1,q/summationtext
n=1S4n/n4<q.( F o ri ft h e r ei sa
positive probability that the sum is inﬁnite, then its expected value is inﬁnite.) But
the convergence of a series implies that its nth term goes to 0; so we can conclude
that with probability 1,
limn→qS4
n
n4=0
But if S4n/n4=(Sn/n)4goes to 0, then so must Sn/n; hence, we have proven that with
probability 1,Sn
n→0a s n→q
When μ, the mean of the Xi, is not equal to 0, we can apply the preceding argu-
ment to the random variables Xi−μto obtain that with probability 1,
limn→qn/summationdisplay
i=1(Xi−μ)
n=0
Figure 8.2(a)

<<<PAGE 394>>>

July 14, 2014 M08_ROSSS4772_09_SE_C08 page 381
A First Course in Probability 381
Figure 8.2(b)
or, equivalently,
limn→qn/summationdisplay
i=1Xi
n=μ
which proves the result.
Figure 8.2 illustrates the strong law by giving the results of a simulation of ninde-
pendent random variables having a speciﬁed probability mass function. The averages
of the nvariables are given when (a) n=100, (b) n=1000, and (c) n=10, 000.
Many students are initially confused about the difference between the weak and
the strong laws of large numbers. The weak law of large numbers states that for anyspeciﬁed large value n
∗,(X1+···+ Xn∗)/n∗is likely to be near μ. However, it does
not say that (X1+ ··· + Xn)/nis bound to stay near μfor all values of nlarger than
n∗. Thus, it leaves open the possibility that large values of |(X1+ ··· + Xn)/n−μ|
can occur inﬁnitely often (though at infrequent intervals). The strong law shows thatthis cannot occur. In particular, it implies that, with probability 1, for any positive
value ε, /vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
1Xi
n−μ/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
will be greater than εonly a ﬁnite number of times.
The strong law of large numbers was originally proven, in the special case of
Bernoulli random variables, by the French mathematician Borel. The general form
of the strong law presented in Theorem 4.1 was proven by the Russian mathemati-
cian A. N. Kolmogorov.

<<<PAGE 395>>>

July 14, 2014 M08_ROSSS4772_09_SE_C08 page 382
382 Chapter 8 Limit Theorems
Figure 8.2(c)
8.5 Other Inequalities
We are sometimes confronted with situations in which we are interested in obtain-
ing an upper bound for a probability of the form P{X−μÚa}, where ais some
positive value and when only the mean μ=E[X] and variance σ2=Var(X)of the
distribution of Xare known. Of course, since X−μÚa>0 implies that |X−μ|Úa,
it follows from Chebyshev’s inequality that
P{X−μÚa}…P{|X−μ|Úa}…σ2
a2when a>0
However, as the following proposition shows, it turns out that we can do better.
Proposition
5.1One-sided Chebyshev inequality
IfXis a random variable with mean 0 and ﬁnite variance σ2, then, for any a>0,
P{XÚa}…σ2
σ2+a2
Proof Letb>0 and note that
XÚais equivalent to X+bÚa+b
Hence,
P{XÚa}=P {X+bÚa+b}
…P{(X+b)2Ú(a+b)2}

<<<PAGE 396>>>

July 14, 2014 M08_ROSSS4772_09_SE_C08 page 383
A First Course in Probability 383
where the inequality is obtained by noting that since a+b>0,X+bÚa+b
implies that (X+b)2Ú(a+b)2. Upon applying Markov’s inequality, the
preceding yields that
P{XÚa}…E[(X+b)2]
(a+b)2=σ2+b2
(a+b)2
Letting b=σ2/a[which is easily seen to be the value of bthat minimizes
(σ2+b2)/(a+b)2] gives the desired result.
Example
5aIf the number of items produced in a factory during a week is a random variable
with mean 100 and variance 400, compute an upper bound on the probability that
this week’s production will be at least 120.
Solution It follows from the one-sided Chebyshev inequality that
P{XÚ120}= P{X−100Ú20}…400
400+(20)2=1
2
Hence, the probability that this week’s production will be 120 or more is at most1
2.
If we attempted to obtain a bound by applying Markov’s inequality, then we
would have obtained
P{XÚ120}…E(X)
120=5
6
which is a far weaker bound than the preceding one. .
Suppose now that Xhas mean μand variance σ2. Since both X−μandμ−X
have mean 0 and variance σ2, it follows from the one-sided Chebyshev inequality
that, for a>0,
P{X−μÚa}…σ2
σ2+a2
and
P{μ−XÚa}…σ2
σ2+a2
Thus, we have the following corollary.
Corollary
5.1IfE[X]=μand Var (X)=σ2, then, for a>0,
P{XÚμ+a}…σ2
σ2+a2
P{X…μ−a}…σ2
σ2+a2
Example
5bA set of 200 people consisting of 100 men and 100 women is randomly divided into
100 pairs of 2 each. Give an upper bound to the probability that at most 30 of these
pairs will consist of a man and a woman.
Solution Number the men arbitrarily from 1 to 100, and for i=1, 2,...100, let
Xi=/braceleftBigg
1i f m a n iis paired with a woman
0 otherwise

<<<PAGE 397>>>

July 14, 2014 M08_ROSSS4772_09_SE_C08 page 384
384 Chapter 8 Limit Theorems
Then X, the number of man–woman pairs, can be expressed as
X=100/summationdisplay
i=1Xi
Because man iis equally likely to be paired with any of the other 199 people, of
which 100 are women, we have
E[Xi]=P{Xi=1}=100
199
Similarly, for iZj,
E[XiXj]=P{Xi=1,Xj=1}
=P{Xi=1}P{Xj=1|Xi=1}=100
19999
197
where P{Xj=1|Xi=1}= 99/197, since, given that man iis paired with a woman,
manjis equally likely to be paired with any of the remaining 197 people, of which
99 are women. Hence, we obtain
E[X]=100/summationdisplay
i=1E[Xi]
=(100)100
199
L50.25
Var(X)=100/summationdisplay
i=1Var(Xi)+2/summationdisplay
i<j/summationdisplay
Cov(Xi,Xj)
=100100
19999
199+2/parenleftBigg
100
2/parenrightBigg/bracketleftBigg
100
19999
197−/parenleftbigg100
199/parenrightbigg2/bracketrightBigg
L25.126
The Chebyshev inequality then yields
P{X…30}…P{|X−50.25| Ú20.25} …25.126
(20.25)2L.061
Thus, there are fewer than 6 chances in 100 that fewer than 30 men will be paired with
women. However, we can improve on this bound by using the one-sided Chebyshevinequality, which yields
P{X…30}= P{X…50.25 −20.25}
…25.126
25.126 +(20.25)2
L.058 .
When the moment generating function of the random variable Xis known, we
can obtain even more effective bounds on P{XÚa}.L e t
M(t)=E[etX]

<<<PAGE 398>>>

July 14, 2014 M08_ROSSS4772_09_SE_C08 page 385
A First Course in Probability 385
be the moment generating function of the random variable X. Then, for t>0,
P{XÚa}= P{etXÚeta}
…E[etX]e−taby Markov’s inequality
Similarly, for t<0,
P{X…a}=P {etXÚeta}
…E[etX]e−ta
Thus, we have the following inequalities, known as Chernoff bounds .
Proposition
5.2Chernoff bounds
P{XÚa}…e−taM(t)for all t>0
P{X…a}…e−taM(t)for all t<0
Since the Chernoff bounds hold for all tin either the positive or negative quadrant,
we obtain the best bound on P{XÚa}by using the tthat minimizes e−taM(t).
Example
5cChernoff bounds for the standard normal random variable
IfZis a standard normal random variable, then its moment generating function is
M(t)=et2/2, so the Chernoff bound on P{ZÚa}is given by
P{ZÚa}…e−taet2/2for all t>0
Now the value of t,t>0, that minimizes et2/2−tais the value that minimizes t2/2−ta,
which is t=a. Thus, for a>0, we have
P{ZÚa}…e−a2/2
Similarly, we can show that, for a<0,
P{Z…a}…e−a2/2.
Example
5dChernoff bounds for the Poisson random variable
IfXis a Poisson random variable with parameter λ, then its moment generating
function is M(t)=eλ(et−1). Hence, the Chernoff bound on P{XÚi}is
P{XÚi}…eλ(et−1)e−itt>0
Minimizing the right side of the preceding inequality is equivalent to minimizing
λ(et−1)−it, and calculus shows that the minimal value occurs when et=i/λ.
Provided that i/λ > 1, this minimizing value of twill be positive. Therefore, assuming
thati>λ and letting et=i/λin the Chernoff bound yields
P{XÚi}…eλ(i/λ−1)/parenleftbiggλ
i/parenrightbiggi
or, equivalently,
P{XÚi}…e−λ(eλ)i
ii.

<<<PAGE 399>>>

July 14, 2014 M08_ROSSS4772_09_SE_C08 page 386
386 Chapter 8 Limit Theorems
Example
5eConsider a gambler who is equally likely to either win or lose 1 unit on every play,
independently of his past results. That is, if Xiis the gambler’s winnings on the ith
play, then the Xiare independent and
P{Xi=1}= P{Xi=−1}=1
2
LetSn=n/summationtext
i=1Xidenote the gambler’s winnings after nplays. We will use the Chernoff
bound on P{SnÚa}. To start, note that the moment generating function of Xiis
E[etX]=et+e−t
2
Now, using the McLaurin expansions of etande−t, we see that
et+e−t=1+t+t2
2!+t3
3!+ ··· +/parenleftBigg
1−t+t2
2!−t3
3!+ ···/parenrightBigg
=2/braceleftBigg
1+t2
2!+t4
4!+ ···/bracerightBigg
=2q/summationdisplay
n=0t2n
(2n)!
…2q/summationdisplay
n=0(t2/2)n
n!since(2n)! Ún!2n
=2et2/2
Therefore,
E[etX]Úet2/2
Since the moment generating function of the sum of independent random variables
is the product of their moment generating functions, we have
E[etSn]=(E[etX])n
…ent2/2
Using the preceding result along with the Chernoff bound gives
P{SnÚa}…e−taent2/2t>0
The value of tthat minimizes the right side of the preceding is the value that min-
imizes nt2/2−ta, and this value is t=a/n. Supposing that a>0 (so that the
minimizing tis positive) and letting t=a/nin the preceding inequality yields
P{SnÚa}…e−a2/2na>0
This latter inequality yields, for example,
P{S10Ú6}…e−36/ 20L.1653

<<<PAGE 400>>>

July 14, 2014 M08_ROSSS4772_09_SE_C08 page 387
A First Course in Probability 387
whereas the exact probability is
P{S10Ú6}=P {gambler wins at least 8 of the ﬁrst 10 games }
=/parenleftBigg
10
8/parenrightBigg
+/parenleftBigg
10
9/parenrightBigg
+/parenleftBigg
10
10/parenrightBigg
210=56
1024L.0547 .
The next inequality is one having to do with expectations rather than probabili-
ties. Before stating it, we need the following deﬁnition.
Deﬁnition
A twice-differentiable real-valued function f(x)is said to be convex iff/prime/prime(x)Ú0
for all x; similarly, it is said to be concave iff/prime/prime(x)…0.
Some examples of convex functions are f(x)=x2,f(x)=eax, and f(x)=−x1/n
forxÚ0. Iff(x)is convex, then g(x)=−f(x)is concave, and vice versa.
Proposition
5.3Jensen’s inequality
Iff(x)is a convex function, then
E[f(X)]Úf(E[X])
provided that the expectations exist and are ﬁnite.
Proof Expanding f(x)in a Taylor’s series expansion about μ=E[X] yields
f(x)=f(μ)+f/prime(μ)(x −μ)+f/prime/prime(ξ)(x −μ)2
2
where ξis some value between xandμ. Since f/prime/prime(ξ)Ú0, we obtain
f(x)Úf(μ)+f/prime(μ)(x −μ)
Hence,
f(X)Úf(μ)+f/prime(μ)(X −μ)
Taking expectations yields
E[f(X)]Úf(μ)+f/prime(μ)E [X−μ]=f(μ)
and the inequality is established.
Example
5fAn investor is faced with the following choices: Either she can invest all of her money
in a risky proposition that would lead to a random return Xthat has mean m,o r
she can put the money into a risk-free venture that will lead to a return of mwith
probability 1. Suppose that her decision will be made on the basis of maximizing
the expected value of u(R), where Ris her return and uis her utility function. By
Jensen’s inequality, it follows that if uis a concave function, then E[u(X)]…u(m),
so the risk-free alternative is preferable, whereas if uis convex, then E[u(X)]Úu(m),
so the risky investment alternative would be preferred. .

<<<PAGE 401>>>

July 14, 2014 M08_ROSSS4772_09_SE_C08 page 388
388 Chapter 8 Limit Theorems
8.6 Bounding the Error Probability When Approximating a Sum of
Independent Bernoulli Random Variables by a Poisson Random
Variable
In this section, we establish bounds on how closely a sum of independent Bernoulli
random variables is approximated by a Poisson random variable with the same mean.
Suppose that we want to approximate the sum of independent Bernoulli randomvariables with respective means p
1,p2,...,pn. Starting with a sequence Y1,...,Yn
of independent Poisson random variables, with Yihaving mean pi, we will construct
a sequence of independent Bernoulli random variables X1,...,Xnwith parameters
p1,...,pnsuch that
P{XiZYi}…p2
i for each i
Letting X=n/summationtext
i=1XiandY=n/summationtext
i=1Yi, we will use the preceding inequality to con-
clude that
P{XZY}…n/summationdisplay
i=1p2
i
Finally, we will show that the preceding inequality implies that for any set of real
numbers A,
|P{X∈A}− P{Y∈A}|…n/summationdisplay
i=1p2
i
Since Xis the sum of independent Bernoulli random variables and Yis a Poisson
random variable, the latter inequality will yield the desired bound.
To show how the task is accomplished, let Yi,i=1,...,nbe independent Pois-
son random variables with respective means pi.N o wl e t U1,...,Unbe independent
random variables that are also independent of the Yi’s and are such that
Ui=/braceleftBigg
0 with probability (1−pi)epi
1 with probability 1 −(1−pi)epi
This deﬁnition implicitly makes use of the inequality
e−pÚ1−p
in assuming that (1−pi)epi…1.
Next, deﬁne the random variables Xi,i=1,...,n,b y
Xi=/braceleftBigg
0i f Yi=Ui=0
1 otherwise
Note that
P{Xi=0}= P{Yi=0}P{Ui=0}= e−pi(1−pi)epi=1−pi
P{Xi=1}= 1−P{Xi=0}=p i

<<<PAGE 402>>>

July 14, 2014 M08_ROSSS4772_09_SE_C08 page 389
A First Course in Probability 389
Now, if Xiis equal to 0, then so must Yiequal 0 (by the deﬁnition of Xi). Therefore,
P{XiZYi}=P{Xi=1,YiZ1}
=P{Yi=0,Xi=1}+ P{Yi>1}
=P{Yi=0,Ui=1}+ P{Yi>1}
=e−pi[1−(1−pi)epi]+1−e−pi−pie−pi
=pi−pie−pi
…p2
i(since 1 −e−p…p)
Now let X=n/summationtext
i=1XiandY=n/summationtext
i=1Yi, and note that Xis the sum of independent
Bernoulli random variables and Yis Poisson with the expected value E[Y]=E[X]=
n/summationtext
i=1pi. Note also that the inequality XZYimplies that XiZYifor some i,s o
P{XZY}…P{XiZYifor some i}
…n/summationdisplay
i=1P{XiZYi}(Boole’s inequality )
…n/summationdisplay
i=1p2
i
For any event B,l e t IB, the indicator variable for the event B, be deﬁned by
IB=/braceleftBigg
1i f Boccurs
0 otherwise
Note that for any set of real numbers A,
I{X∈A}−I{Y∈A}…I{XZY}
The preceding inequality follows from the fact that since an indicator variable is
either 0 or 1, the left-hand side equals 1 only when I{X∈A}=1 and I{Y∈A}=0. But
this would imply that X∈AandY/negationslash∈A, which means that XZY, so the right side
would also equal 1. Upon taking expectations of the preceding inequality, we obtain
P{X∈A}− P{Y∈A}…P{XZY}
By reversing XandY, we obtain, in the same manner,
P{Y∈A}− P{X∈A}…P{XZY}
Thus, we can conclude that
|P{X∈A}− P{Y∈A}|…P{XZY}
Therefore, we have proven that with λ=n/summationtext
i=1pi,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleP⎧
⎨
⎩n/summationdisplay
i=1Xi∈A⎫
⎬
⎭−/summationdisplay
i∈Ae−λλi
i!/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle…
n/summationdisplay
i=1p2
i

<<<PAGE 403>>>

July 14, 2014 M08_ROSSS4772_09_SE_C08 page 390
390 Chapter 8 Limit Theorems
Remark When all the piare equal to p,Xis a binomial random variable. Hence,
the preceding inequality shows that, for any set of nonnegative integers A,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
i∈A/parenleftBigg
n
i/parenrightBigg
pi(1−p)n−i−/summationdisplay
i∈Ae−np(np)i
i!/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle…np
2.
Summary
Two useful probability bounds are provided by the
Markov andChebyshev inequalities. The Markov inequal-
ity is concerned with nonnegative random variables and
says that for Xof that type,
P{XÚa}…E[X]
a
for every positive value a. The Chebyshev inequality,
which is a simple consequence of the Markov inequality,
states that if Xhas mean μand variance σ2, then, for every
positive k,
P{|X−μ|Úkσ}…1
k2
The two most important theoretical results in probability
are the central limit theorem and the strong law of large
numbers. Both are concerned with a sequence of inde-pendent and identically distributed random variables. Thecentral limit theorem says that if the random variables
have a ﬁnite mean μand a ﬁnite variance σ
2, then thedistribution of the sum of the ﬁrst nof them is, for large
n, approximately that of a normal random variable withmean nμand variance nσ
2.T h a ti s ,i fX i,iÚ1, is the
sequence, then the central limit theorem states that for
every real number a,
limn→qP/braceleftBigg
X1+ ··· +X n−nμ
σ√n…a/bracerightBigg
=1√
2π/integraldisplaya
−qe−x2/2dx
Thestrong law of large numbers requires only that the ran-
dom variables in the sequence have a ﬁnite mean μ.I t
states that with probability 1, the average of the ﬁrst nof
them will converge to μasngoes to inﬁnity. This implies
that if Ais any speciﬁed event of an experiment for which
independent replications are performed, then the limitingproportion of experiments whose outcomes are in Awill,
with probability 1, equal P(A). Therefore, if we accept the
interpretation that “with probability 1” means “with cer-tainty,” we obtain the theoretical justiﬁcation for the long-
run relative frequency interpretation of probabilities.
Problems
8.1.Suppose that Xis a random variable with mean and
variance both equal to 20. What can be said about P{0<
X<40}?
8.2.From past experience, a professor knows that the test
score of a student taking her ﬁnal examination is a randomvariable with mean 75.
(a)Give an upper bound for the probability that a stu-
dent’s test score will exceed 85.
(b)Suppose, in addition, that the professor knows that
the variance of a student’s test score is equal to 25. What
can be said about the probability that a student will scorebetween 65 and 85?
(c)How many students would have to take the examina-
tion to ensure with probability at least .9 that the class
average would be within 5 of 75? Do not use the centrallimit theorem.
8.3.Use the central limit theorem to solve part (c) of Prob-
lem 8.2.8.4.LetX
1,...,X20be independent Poisson random vari-
ables with mean 1.(a)Use the Markov inequality to obtain a bound on
P⎧
⎨
⎩20/summationdisplay
1Xi>15⎫
⎬
⎭
(b)Use the central limit theorem to approximate
P⎧
⎨
⎩20/summationdisplay
1Xi>15⎫
⎬
⎭.
8.5.Fifty numbers are rounded off to the nearest inte-
ger and then summed. If the individual round-off errors
are uniformly distributed over (−.5,.5), approximate the
probability that the resultant sum differs from the exact
s u mb ym o r et h a n3 .
8.6.A die is continually rolled until the total sum of all
rolls exceeds 300. Approximate the probability that at
least 80 rolls are necessary.

<<<PAGE 404>>>

July 14, 2014 M08_ROSSS4772_09_SE_C08 page 391
A First Course in Probability 391
8.7.A person has 100 light bulbs whose lifetimes are inde-
pendent exponentials with mean 5 hours. If the bulbs are
used one at a time, with a failed bulb being replaced imme-
diately by a new one, approximate the probability that
there is still a working bulb after 525 hours.
8.8.In Problem 8.7, suppose that it takes a random time,
uniformly distributed over (0, .5), to replace a failed bulb.
Approximate the probability that all bulbs have failed by
time 550.
8.9.IfXis a gamma random variable with parameters
(n, 1), approximately how large must nbe so that
P/braceleftBigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleX
n−1/vextendsingle/vextendsingle/vextendsingle/vextendsingle>.01/bracerightBigg
<.01?
8.10. Civil engineers believe that W, the amount of weight
(in units of 1000 pounds) that a certain span of a bridge can
withstand without structural damage resulting, is normally
distributed with mean 400 and standard deviation 40. Sup-
pose that the weight (again, in units of 1000 pounds) of acar is a random variable with mean 3 and standard devi-
ation .3. Approximately how many cars would have to be
on the bridge span for the probability of structural damageto exceed .1?
8.11. Many people believe that the daily change of price of
a company’s stock on the stock market is a random vari-
able with mean 0 and variance σ
2.T h a ti s ,i fY nrepresents
the price of the stock on the nth day, then
Yn=Yn−1+XnnÚ1
where X1,X2,...are independent and identically dis-
tributed random variables with mean 0 and variance σ2.
Suppose that the stock’s price today is 100. If σ2=1, what
can you say about the probability that the stock’s price will
exceed 105 after 10 days?
8.12. We have 100 components that we will put in use in a
sequential fashion. That is, component 1 is initially put in
use, and upon failure, it is replaced by component 2, which
is itself replaced upon failure by component 3, and so on.
If the lifetime of component iis exponentially distributed
with mean 10 +i/10,i=1,..., 100, estimate the proba-
bility that the total life of all components will exceed 1200.Now repeat when the life distribution of component iis
uniformly distributed over (0, 20 +i/5),i=1,..., 100.
8.13. Student scores on exams given by a certain instructor
have mean 74 and standard deviation 14. This instructor isabout to give two exams, one to a class of size 25 and theother to a class of size 64.
(a)Approximate the probability that the average test
score in the class of size 25 exceeds 80.
(b)Repeat part (a) for the class of size 64.(c)Approximate the probability that the average test
score in the larger class exceeds that of the other class by
more than 2.2 points.
(d)Approximate the probability that the average test
score in the smaller class exceeds that of the other class
by more than 2.2 points.
8.14. A certain component is critical to the operation of an
electrical system and must be replaced immediately upon
failure. If the mean lifetime of this type of component is100 hours and its standard deviation is 30 hours, how many
of these components must be in stock so that the probabil-
ity that the system is in continual operation for the next2000 hours is at least .95?
8.15. An insurance company has 10,000 automobile poli-
cyholders. The expected yearly claim per policyholder is
$240, with a standard deviation of $800. Approximate theprobability that the total yearly claim exceeds $2.7 million.
8.16. A.J. has 20 jobs that she must do in sequence, with
the times required to do each of these jobs being indepen-
dent random variables with mean 50 minutes and standarddeviation 10 minutes. M.J. has 20 jobs that he must do in
sequence, with the times required to do each of these jobs
being independent random variables with mean 52 min-utes and standard deviation 15 minutes.
(a)Find the probability that A.J. ﬁnishes in less than 900
minutes.
(b)Find the probability that M.J. ﬁnishes in less than 900
minutes.(c)Find the probability that A.J. ﬁnishes before M.J.
8.17. Redo Example 5b under the assumption that the
number of man–woman pairs is (approximately) normally
distributed. Does this seem like a reasonable supposition?
8.18. Repeat part (a) of Problem 8.2 when it is known
that the variance of a student’s test score is equal
to 25.
8.19. A lake contains 4 distinct types of ﬁsh. Suppose that
each ﬁsh caught is equally likely to be any one of these
types. Let Ydenote the number of ﬁsh that need be caught
to obtain at least one of each type.
(a)Give an interval ( a,b) such that P{a…Y…b}
Ú.90.
(b)Using the one-sided Chebyshev inequality, how many
ﬁsh need we plan on catching so as to be at least 90 percent
certain of obtaining at least one of each type?
8.20. IfXis a nonnegative random variable with mean 25,
what can be said about
(a)E[X
3]?
(b)E[√
X]?
(c)E[logX]?
(d)E[e−X]?

<<<PAGE 405>>>

July 14, 2014 M08_ROSSS4772_09_SE_C08 page 392
392 Chapter 8 Limit Theorems
8.21. LetXbe a nonnegative random variable. Prove that
E[X]…(E[X2])1/2…(E[X3])1/3…···
8.22. Would the results of Example 5f change if the
investor were allowed to divide her money and invest the
fraction α,0<α<1, in the risky proposition and invest
the remainder in the risk-free venture? Her return for such
a split investment would be R=αX+(1−α)m.
8.23. LetXbe a Poisson random variable with mean 20.
(a)Use the Markov inequality to obtain an upper
bound on
p=P{XÚ26}(b)Use the one-sided Chebyshev inequality to obtain an
upper bound on p.
(c)Use the Chernoff bound to obtain an upper bound
onp.
(d)Approximate pby making use of the central limit
theorem.
(e)Determine pby running an appropriate program.
8.24. IfXis a Poisson random variable with mean 100,
thenP{X>120} is approximately
(a).02,
(b).5 or
(c).3?
Theoretical Exercises
8.1.IfXhas variance σ2,t h e n σ, the positive square root
of the variance, is called the standard deviation .I fXhas
mean μand standard deviation σ, show that
P{|X−μ|Úkσ}…1
k2
8.2.IfXhas mean μand standard deviation σ,t h er a t i o
rK|μ|/σ is called the measurement signal-to-noise ratio
ofX. The idea is that Xcan be expressed as X=μ+
(X−μ), with μrepresenting the signal and X−μthe
noise. If we deﬁne |(X−μ)/μ| KDas the relative devia-
tion of Xfrom its signal (or mean) μ, show that for α> 0,
P{D…α}Ú1−1
r2α2
8.3.Compute the measurement signal-to-noise ratio—
that is, |μ|/σ ,w h e r e μ=E[X]a n d σ2=Var(X)—of the
following random variables:
(a)Poisson with mean λ;
(b)binomial with parameters nandp;
(c)geometric with mean 1/ p;
(d)uniform over (a,b);
(e)exponential with mean 1 /λ;
(f)normal with parameters μ,σ2.
8.4.LetZn,nÚ1, be a sequence of random variables and c
a constant such that for each ε>0,P{|Zn−c|>ε}→0a s
n→q. Show that for any bounded continuous function g,
E[g(Zn)]→g(c) asn→q
8.5.Letf(x)be a continuous function deﬁned for 0 …x…1.
Consider the functions
Bn(x)=n/summationdisplay
k=0f/parenleftbiggk
n/parenrightbigg/parenleftbigg
n
k/parenrightbigg
xk(1−x)n−k(called Bernstein polynomials) and prove that
limn→qBn(x)=f(x)
Hint :L e t X1,X2,...be independent Bernoulli random
variables with mean x. Show that
Bn(x)=E/bracketleftBigg
f/parenleftbiggX1+ ··· + Xn
n/parenrightbigg/bracketrightBigg
and then use Theoretical Exercise 8.4.
Since it can be shown that the convergence of Bn(x)to
f(x)is uniform in x, the preceding reasoning provides a
probabilistic proof of the famous Weierstrass theorem of
analysis, which states that any continuous function on aclosed interval can be approximated arbitrarily closely by
a polynomial.
8.6.(a)LetXbe a discrete random variable whose pos-
sible values are 1, 2, ....I fP{X=k}is nonincreasing in
k=1, 2,..., prove that
P{X=k}…2E[X]
k2
(b)LetXbe a nonnegative continuous random variable
having a nonincreasing density function. Show that
f(x)…2E[X]
x2for all x>0
8.7.Suppose that a fair die is rolled 100 times. Let Xibe
the value obtained on the ith roll. Compute an approxi-
mation for
P⎧
⎨
⎩100/productdisplay
1Xi…a100⎫
⎬
⎭1<a<6

<<<PAGE 406>>>

July 14, 2014 M08_ROSSS4772_09_SE_C08 page 393
A First Course in Probability 393
8.8.Explain why a gamma random variable with parame-
ters(t,λ)has an approximately normal distribution when t
is large.
8.9.Suppose a fair coin is tossed 1000 times. If the ﬁrst 100
tosses all result in heads, what proportion of heads would
you expect on the ﬁnal 900 tosses? Comment on the state-
ment “The strong law of large numbers swamps but does
not compensate.”
8.10. IfXis a Poisson random variable with mean λ, show
that for i<λ,
P{X…i}…e−λ(eλ)i
ii
8.11. LetXbe a binomial random variable with parame-
tersnandp. Show that, for i>np,
(a)minimum
t>0e−tiE[etX] occurs when tis such that et=
iq
(n−i)p,w h e r eq =1−p.
(b)P{XÚi}…nn
ii(n−i)n−ipi(1−p)n−i.
8.12. The Chernoff bound on a standard normal ran-
dom variable Zgives P{Z>a}…e−a2/2,a>0. Show,by considering the density of Z, that the right side of
the inequality can be reduced by the factor 2. That is,
show that
P{Z>a}…1
2e−a2/2a>0
8.13. Show that if E[X]<0a n d θZ0 is such that E[eθX]=
1, then θ> 0.
8.14. LetX1,X2,...be a sequence of independent and
identically distributed random variables with distribution
F, having a ﬁnite mean and variance. Whereas the cen-
tral limit theorem states that the distribution of/summationtextn
i=1Xi
approaches a normal distribution as ngoes to inﬁnity, it
gives us no information about how large nneed be before
the normal becomes a good approximation. Whereas in
most applications, the approximation yields good results
whenever nÚ20, and oftentimes for much smaller values
ofn, how large a value of nis needed depends on the dis-
tribution of Xi. Give an example of a distribution Fsuch
that the distribution of/summationtext100
i=1Xiis not close to a normal
distribution.
Hint: Think Poisson.
Self-Test Problems And Exercises
8.1.The number of automobiles sold weekly at a certain
dealership is a random variable with expected value 16.
Give an upper bound to the probability that
(a)next week’s sales exceed 18;
(b)next week’s sales exceed 25.
8.2.Suppose in Problem 8.14 that the variance of the num-
ber of automobiles sold weekly is 9.(a)Give a lower bound to the probability that next week’s
sales are between 10 and 22, inclusively.
(b)Give an upper bound to the probability that next
week’s sales exceed 18.
8.3.If
E[X]=75E[Y]=75 Var(X )=10
Var(Y)=12 Cov(X ,Y)=−3
give an upper bound to
(a)P{|X−Y|>15};
(b)P{X>Y+15};
(c)P{Y>X+15}.
8.4.Suppose that the number of units produced daily at
factory Ais a random variable with mean 20 and stan-
dard deviation 3 and the number produced at factory B
is a random variable with mean 18 and standard deviation
6. Assuming independence, derive an upper bound for theprobability that more units are produced today at factory
Bthan at factory A.
8.5.The amount of time that a certain type of component
functions before failing is a random variable with proba-
bility density function
f(x)=2x0<x<1
Once the component fails, it is immediately replaced byanother one of the same type. If we let X
idenote the life-
time of the ith component to be put in use, then Sn=n/summationtext
i=1Xi
represents the time of the nth failure. The long-term rate
at which failures occur, call it r, is deﬁned by
r=limn→qn
Sn
Assuming that the random variables Xi,iÚ1, are inde-
pendent, determine r.
8.6.In Self-Test Problem 8.5, how many components
would one need to have on hand to be approximately 90percent certain that the stock would last at least 35 days?
8.7.The servicing of a machine requires two separate
steps, with the time needed for the ﬁrst step being an expo-
nential random variable with mean .2 hour and the time
for the second step being an independent exponential ran-
dom variable with mean .3 hour. If a repair person has 20

<<<PAGE 407>>>

July 14, 2014 M08_ROSSS4772_09_SE_C08 page 394
394 Chapter 8 Limit Theorems
machines to service, approximate the probability that all
the work can be completed in 8 hours.
8.8.On each bet, a gambler loses 1 with probability .7,
loses 2 with probability .2, or wins 10 with probability .1.
Approximate the probability that the gambler will be los-ing after his ﬁrst 100 bets.
8.9.Determine tso that the probability that the repair per-
son in Self-Test Problem 8.7 ﬁnishes the 20 jobs within time
tis approximately equal to .95.
8.10. A tobacco company claims that the amount of nico-
tine in one of its cigarettes is a random variable with mean2.2 mg and standard deviation .3 mg. However, the aver-
age nicotine content of 100 randomly chosen cigaretteswas 3.1 mg. Approximate the probability that the aver-
age would have been as high as or higher than 3.1 if the
company’s claims were true.
8.11. Each of the batteries in a collection of 40 batteries
is equally likely to be either a type A or a type B battery.
Type A batteries last for an amount of time that has mean
50 and standard deviation 15; type B batteries last for an
amount of time that has mean 30 and standard deviation 6.
(a)Approximate the probability that the total life of all 40
batteries exceeds 1700.(b)Suppose it is known that 20 of the batteries are type A
and 20 are type B. Now approximate the probability that
the total life of all 40 batteries exceeds 1700.
8.12. A clinic is equally likely to have 2, 3, or 4 doctors
volunteer for service on a given day. No matter how manyvolunteer doctors there are on a given day, the numbers
of patients seen by these doctors are independent Poissonrandom variables with mean 30. Let Xdenote the number
of patients seen in the clinic on a given day.
(a)Find E[X].
(b)Find Var (X).
(c)Use a table of the standard normal probability distri-
bution to approximate P{X>65}.
8.13. The strong law of large numbers states that with
probability 1, the successive arithmetic averages of a
sequence of independent and identically distributed ran-
dom variables converge to their common mean μ.W h a t
do the successive geometric averages converge to? That is,
what is
lim
n→q⎛
⎝n/productdisplay
i=1Xi⎞⎠1/n
8.14. Each new book donated to a library must be pro-
cessed. Suppose that the time it takes to process a book
has mean 10 minutes and standard deviation 3 minutes. Ifa librarian has 40 books to process,
(a)approximate the probability that it will take more than
420 minutes to process all these books;
(b)approximate the probability that at least 25 books will
be processed in the ﬁrst 240 minutes.
What assumptions have you made?

<<<PAGE 408>>>

July 14, 2014 M09_ROSSS4772_09_SE_C09 page 395
Chapter
Additional Topics
in Probability 9
Contents
9.1 The Poisson Process
9.2 Markov Chains9.3 Surprise, Uncertainty, and Entropy
9.4 Coding Theory and Entropy
9.1 The Poisson Process
Before we deﬁne a Poisson process, let us recall that a function fis said to be o(h) if
lim
h→0f(h)
h=0.
That is, fiso(h) if, for small values of h,f(h)is small even in relation to h. Suppose
now that “events” are occurring at random points at time, and let N(t)denote the
number of events that occur in the time interval [0, t]. The collection of random
variables {N(t),tÚ0}is said to be a Poisson process having rate λ,λ> 0, if
(i)N(0)=0.
(ii) The numbers of events that occur in disjoint time intervals are independent.
(iii) The distribution of the number of events that occur in a given interval depends
only on the length of that interval and not on its location.
(iv)P{N(h)=1}=λh+o(h).
(v)P{N(h)Ú2}=o(h).
Thus, condition (i) states that the process begins at time 0. Condition (ii), the
independent increment assumption, states, for instance, that the number of events
that occur by time t[that is, N(t)] is independent of the number of events that occur
between tandt+s[that is, N(t+s)−N(t)]. Condition (iii), the stationary increment
assumption, states that the probability distribution of N(t+s)−N(t)is the same
for all values of t.
In Chapter 4, we presented an argument, based on the Poisson distribution being
a limiting version of the binomial distribution, that the foregoing conditions imply
thatN(t)has a Poisson distribution with mean λt. We will now obtain this result by
a different method.
395

<<<PAGE 409>>>

July 14, 2014 M09_ROSSS4772_09_SE_C09 page 396
396 Chapter 9 Additional Topics in Probability
Lemma
1.1For a Poisson process with rate λ,
P{N(t)=0}= e−λt
Proof LetP0(t)=P{N(t)=0}. We derive a differential equation for P0(t)in the
following manner:
P0(t+h)=P{N(t+h)=0}
=P{N(t)=0,N(t+h)−N(t)=0}
=P{N(t)=0}P{N(t+h)−N(t)=0}
=P0(t)[1−λh+o(h)]
where the ﬁnal two equations follow from condition (ii) plus the fact that conditions
(iv) and (v) imply that P{N(h)=0}=1 −λh+o(h). Hence,
P0(t+h)−P0(t)
h=−λP0(t)+o(h)
h
Now, letting h→0, we obtain
P/prime
0(t)=−λP0(t)
or, equivalently,P/prime
0(t)
P0(t)=−λ
which implies, by integration, that
logP0(t)=−λt+c
or
P0(t)=Ke−λt
Since P0(0)=P{N(0)=0}= 1, we arrive at
P0(t)=e−λt
For a Poisson process, let T1denote the time the ﬁrst event occurs. Further, for
n>1, let Tndenote the time elapsed between the (n−1)and the nth event. The
sequence {Tn,n=1, 2,...}is called the sequence of interarrival times . For instance, if
T1=5 and T2=10, then the ﬁrst event of the Poisson process would have occurred
at time 5 and the second at time 15.
We shall now determine the distribution of the Tn. To do so, we ﬁrst note that
the event {T1>t}takes place if and only if no events of the Poisson process occur
in the interval [0, t]; thus,
P{T1>t}=P{N(t)=0}= e−λt
Hence, T1has an exponential distribution with mean 1 /λ.N o w ,
P{T2>t}=E [P{T2>t|T1}]
However,
P{T2>t|T1=s}=P{0 events in (s,s+t]|T1=s}
=P{0 events in (s,s+t]}
=e−λt
where the last two equations followed from the assumptions about independent andstationary increments. From the preceding, we conclude that T
2is also an exponen-
tial random variable with mean 1/λ and, furthermore, that T2is independent of T1.
Repeating the same argument yields Proposition 1.1.

<<<PAGE 410>>>

July 14, 2014 M09_ROSSS4772_09_SE_C09 page 397
A First Course in Probability 397
Proposition
1.1T1,T2,...are independent exponential random variables, each with mean 1 /λ.
Another quantity of interest is Sn, the arrival time of the nth event, also called
thewaiting time until the nth event. It is easily seen that
Sn=n/summationdisplay
i=1TinÚ1
hence, from Proposition 1.1 and the results of Section 5.6.1, it follows that Snhas a
gamma distribution with parameters nandλ. That is, the probability density of Snis
given by
fSn(x)=λe−λx(λx)n−1
(n−1)!xÚ0
We are now ready to prove that N(t)is a Poisson random variable with mean λt.
Theorem
1.1For a Poisson process with rate λ,
P{N(t)=n}=e−λt(λt)n
n!
Proof Note that the nth event of the Poisson process will occur before or at time tif
and only if the number of events that occur by tis at least n. That is,
N(t)Ún3Sn…t
so
P{N(t)=n}=P {N(t)Ún}− P{N(t)Ún+1}
=P{Sn…t}−P{Sn+1…t}
=/integraldisplayt
0λe−λx(λx)n−1
(n−1)!dx−/integraldisplayt
0λe−λx(λx)n
n!dx
But the integration-by-parts formula/integraltext
ud v=uv−/integraltext
vd u with u=e−λxand
dv=λ[(λx)n−1/(n−1)!]dxyields
/integraldisplayt
0λe−λx(λx)n−1
(n−1)!dx=e−λt(λt)n
n!+/integraldisplayt
0λe−λx(λx)n
n!dx
which completes the proof.
9.2 Markov Chains
Consider a sequence of random variables X0,X1,..., and suppose that the set of
possible values of these random variables is {0, 1,...,M}. It will be helpful to inter-
pretXnas being the state of some system at time n, and, in accordance with this
interpretation, we say that the system is in state iat time nifXn=i. The sequence
of random variables is said to form a Markov chain if, each time the system is in state
i, there is some ﬁxed probability—call it Pij—that the system will next be in state j.
That is, for all i0,...,in−1,i,j,
P{Xn+1=j|Xn=i,Xn−1=in−1,...,X1=i1,X0=i0}=P ij

<<<PAGE 411>>>

July 14, 2014 M09_ROSSS4772_09_SE_C09 page 398
398 Chapter 9 Additional Topics in Probability
The values Pij,0…i…M,0…j…N, are called the transition probabilities of the
Markov chain, and they satisfy
PijÚ0M/summationdisplay
j=0Pij=1 i=0, 1,...,M
(Why?) It is convenient to arrange the transition probabilities Pijin a square array
as follows: /vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleP
00P01···P0M
P10P11···P1M
#
#
#
PM0PM1···PMM/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Such an array is called a matrix .
Knowledge of the transition probability matrix and of the distribution of X
0
enables us, in theory, to compute all probabilities of interest. For instance, the joint
probability mass function of X0,...,Xnis given by
P{Xn=in,Xn−1=in−1,...,X1=i1,X0=i0}
=P{Xn=in|Xn−1=in−1,...,X0=i0}P{Xn−1=in−1,...,X0=i0}
=Pin−1,inP{Xn−1=in−1,...,X0=i0}
and continual repetition of this argument demonstrates that the preceding is equal to
Pin−1,inPin−2,in−1···P i1,i2Pi0,i1P{X0=i0}
Example
2aSuppose that whether it rains tomorrow depends on previous weather conditionsonly through whether it is raining today. Suppose further that if it is raining today,
then it will rain tomorrow with probability α, and if it is not raining today, then it will
rain tomorrow with probability β.
If we say that the system is in state 0 when it rains and state 1 when it does not,
then the preceding system is a two-state Markov chain having transition probability
matrix /vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleα1−α
β1−β/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
That is, P
00=α=1−P01,P10=β=1−P11. .
Example
2bConsider a gambler who either wins 1 unit with probability por loses 1 unit with
probability 1 −pat each play of the game. If we suppose that the gambler will quit
playing when his fortune hits either 0 or M, then the gambler’s sequence of fortunes
is a Markov chain having transition probabilities
Pi,i+1=p=1−Pi,i−1 i=1,...,M−1
P00=PMM=1.
Example
2cThe husband-and-wife physicists Paul and Tatyana Ehrenfest considered a concep-
tual model for the movement of molecules in which Mmolecules are distributed
among 2 urns. At each time point, one of the molecules is chosen at random and isremoved from its urn and placed in the other one. If we let X
ndenote the number

<<<PAGE 412>>>

July 14, 2014 M09_ROSSS4772_09_SE_C09 page 399
A First Course in Probability 399
of molecules in the ﬁrst urn immediately after the nth exchange, then {X0,X1,...}is
a Markov chain with transition probabilities
Pi,i+1=M−i
M0…i…M
Pi,i−1=i
M0…i…M
Pij=0i f j=ior|j−i|>1.
Thus, for a Markov chain, Pijrepresents the probability that a system in state i
will enter state jat the next transition. We can also deﬁne the two-stage transition
probability P(2)
ijthat a system presently in state iwill be in state jafter two additional
transitions. That is,
P(2)
ij=P{Xm+2=j|Xm=i}
TheP(2)
ijcan be computed from the Pijas follows:
P(2)
ij=P{X2=j|X0=i}
=M/summationdisplay
k=0P{X2=j,X1=k|X 0=i}
=M/summationdisplay
k=0P{X2=j|X1=k,X0=i}P{X1=k|X 0=i}
=M/summationdisplay
k=0PkjPik
In general, we deﬁne the n-stage transition probabilities, denoted as P(n)
ij,b y
P(n)
ij=P{Xn+m=j|Xm=i}
Proposition 2.1, known as the Chapman–Kolmogorov equations, shows how the P(n)
ij
can be computed.
Proposition
2.1The Chapman–Kolmogorov equations
P(n)
ij=M/summationdisplay
k=0P(r)
ikP(n−r)
kjfor all 0 <r<n
Proof
P(n)
ij=P{Xn=j|X0=i}
=/summationdisplay
kP{Xn=j,Xr=k|X 0=i}
=/summationdisplay
kP{Xn=j|Xr=k,X0=i}P{Xr=k|X 0=i}
=/summationdisplay
kP(n−r)
kjP(r)
ik
Example
2dA random walk
An example of a Markov chain having a countably inﬁnite state space is the random
walk, which tracks a particle as it moves along a one-dimensional axis. Suppose that

<<<PAGE 413>>>

July 14, 2014 M09_ROSSS4772_09_SE_C09 page 400
400 Chapter 9 Additional Topics in Probability
at each point in time, the particle will move either one step to the right or one step
to the left with respective probabilities pand 1 −p. That is, suppose the particle’s
path follows a Markov chain with transition probabilities
Pi,i+1=p=1−Pi,i−1 i=0,;1,...
If the particle is at state i, then the probability that it will be at state jafter ntran-
sitions is the probability that (n−i+j)/2 of these steps are to the right and
n−[(n−i+j)/2]=(n+i−j)/2 are to the left. Since each step will be to
the right, independently of the other steps, with probability p, it follows that the
preceding is just the binomial probability
Pn
ij=/parenleftBigg
n
(n−i+j)/2/parenrightBigg
p(n−i+j )/2(1−p)(n+i−j )/2
where/parenleftBigg
n
x/parenrightBigg
is taken to equal 0 when xis not a nonnegative integer less than or
equal to n. The preceding formula can be rewritten as
P2n
i,i+2k=/parenleftBigg
2n
n+k/parenrightBigg
pn+k(1−p)n−kk=0,;1,...,;n
P2n+1
i,i+2k+1=/parenleftBigg
2n+1
n+k+1/parenrightBigg
pn+k+1(1−p)n−k
k=0,;1,...,;n,−(n+1) .
Although the P(n)
ijdenote conditional probabilities, we can use them to derive
expressions for unconditional probabilities by conditioning on the initial state. Forinstance,
P{X
n=j}=/summationdisplay
iP{Xn=j|X0=i}P{X0=i}
=/summationdisplay
iP(n)
ijP{X0=i}
For a large number of Markov chains, it turns out that P(n)
ijconverges, as n→q,t oa
value πjthat depends only on j. That is, for large values of n, the probability of being
in state jafter ntransitions is approximately equal to πj, no matter what the initial
state was. It can be shown that a sufﬁcient condition for a Markov chain to possessthis property is that for some n>0,
P(n)
ij>0 for all i,j=0, 1,...,M (2.1)
Markov chains that satisfy Equation (2.1) are said to be ergodic. Since Proposi-
tion 2.1 yields
P(n+1)
ij=M/summationdisplay
k=0P(n)
ikPkj
it follows, by letting n→q, that for ergodic chains,
πj=M/summationdisplay
k=0πkPkj (2.2)

<<<PAGE 414>>>

July 14, 2014 M09_ROSSS4772_09_SE_C09 page 401
A First Course in Probability 401
Furthermore, since 1 =M/summationtext
j=0P(n)
ij, we also obtain, by letting n→q,
M/summationdisplay
j=0πj=1 (2.3)
In fact, it can be shown that the πj,0…j…M, are the unique nonnegative solutions
of Equations (2.2) and (2.3). All this is summed up in Theorem 2.1, which we state
without proof.
Theorem
2.1For an ergodic Markov chain,
πj=limn→qP(n)
ij
exists, and the πj,0…j…M, are the unique nonnegative solutions of
πj=M/summationdisplay
k=0πkPkj
M/summationdisplay
j=0πj=1
Example
2eConsider Example 2a, in which we assume that if it rains today, then it will rain
tomorrow with probability α, and if it does not rain today, then it will rain tomorrow
with probability β. From Theorem 2.1, it follows that the limiting probabilities π0
andπ1of rain and of no rain, respectively, are given by
π0=απ0+βπ1
π1=(1−α)π 0+(1−β)π 1
π0+π1=1
which yields
π0=β
1+β−απ1=1−α
1+β−α
For instance, if α=.6 and β=.3, then the limiting probability of rain on the nth day
isπ0=3
7. .
The quantity πjis also equal to the long-run proportion of time that the Markov
chain is in state j,j=0,...,M. To see intuitively why this might be so, let Pjdenote
the long-run proportion of time the chain is in state j. (It can be proven using the
strong law of large numbers that for an ergodic chain, such long-run proportions
exist and are constants.) Now, since the proportion of time the chain is in state kis
Pk, and since, when in state k, the chain goes to state jwith probability Pkj,i tf o l l o w s
that the proportion of time the Markov chain is entering state jfrom state kis equal
toPkPkj. Summing over all kshows that Pj, the proportion of time the Markov chain
is entering state j, satisﬁes
Pj=/summationdisplay
kPkPkj
Since clearly it is also true that/summationdisplay
jPj=1

<<<PAGE 415>>>

July 14, 2014 M09_ROSSS4772_09_SE_C09 page 402
402 Chapter 9 Additional Topics in Probability
it thus follows, since by Theorem 2.1 the πj,j=0,...,Mare the unique solution of
the preceding, that Pj=πj,j=0,...,M. The long-run proportion interpretation of
πjis generally valid even when the chain is not ergodic.
Example
2fSuppose in Example 2c that we are interested in the proportion of time that there
arejmolecules in urn 1, j=0,...,M. By Theorem 2.1, these quantities will be the
unique solution of
π0=π1*1
M
πj=πj−1*M−j+1
M+πj+1*j+1
Mj=1,...,M
πM=πM−1*1
M
M/summationdisplay
j=0πj=1
However, as it is easily checked that
πj=/parenleftBigg
M
j/parenrightBigg/parenleftbigg1
2/parenrightbiggM
j=0,...,M
satisfy the preceding equations, it follows that these are the long-run proportions
of time that the Markov chain is in each of the states. (See Problem 9.11 for an
explanation of how one might have guessed at the foregoing solution.) .
9.3 Surprise, Uncertainty, and Entropy
Consider an event Ethat can occur when an experiment is performed. How sur-
prised would we be to hear that Edoes, in fact, occur? It seems reasonable to sup-
pose that the amount of surprise engendered by the information that Ehas occurred
should depend on the probability of E. For instance, if the experiment consists of
rolling a pair of dice, then we would not be too surprised to hear that Ehas occurred
when Erepresents the event that the sum of the dice is even (and thus has proba-
bility1
2), whereas we would certainly be more surprised to hear that Ehas occurred
when Eis the event that the sum of the dice is 12 (and thus has probability1
36).
In this section, we attempt to quantify the concept of surprise. To begin, let
us agree to suppose that the surprise one feels upon learning that an event Ehas
occurred depends only on the probability of E, and let us denote by S(p) the sur-
prise evoked by the occurrence of an event having probability p. We determine the
functional form of S(p) by ﬁrst agreeing on a set of reasonable conditions that S(p)
should satisfy and then proving that these axioms require that S(p) have a speciﬁed
form. We assume throughout that S(p) is deﬁned for all 0 <p…1 but is not deﬁned
for events having p=0.
Our ﬁrst condition is just a statement of the intuitive fact that there is no surprise
in hearing that an event that is sure to occur has indeed occurred.
Axiom 1
S(1)=0
Our second condition states that the more unlikely an event is to occur, the
greater is the surprise evoked by its occurrence.

<<<PAGE 416>>>

July 14, 2014 M09_ROSSS4772_09_SE_C09 page 403
A First Course in Probability 403
Axiom 2
S(p) is a strictly decreasing function of p; that is, if p<q, then S(p)> S(q).
The third condition is a mathematical statement of the fact that we would intui-
tively expect a small change in pto correspond to a small change in S(p).
Axiom 3
S(p) is a continuous function of p.
To motivate the ﬁnal condition, consider two independent events EandFhav-
ing respective probabilities P(E)=pandP(F)=q. Since P(EF)=pq, the surprise
evoked by the information that both EandFhave occurred is S(pq). Now, sup-
pose that we are told ﬁrst that Ehas occurred and then, afterward, that Fhas also
occurred. Since S(p) is the surprise evoked by the occurrence of E, it follows that
S(pq) −S(p) represents the additional surprise evoked when we are informed that
Fhas also occurred. However, because Fis independent of E, the knowledge that E
occurred does not change the probability of F; hence, the additional surprise should
just be S(q). This reasoning suggests the ﬁnal condition.
Axiom 4
S(pq) =S(p)+S(q) 0<p…1, 0 <q…1
We are now ready for Theorem 3.1, which yields the structure of S(p).
Theorem
3.1IfS(·)satisﬁes Axioms 1 through 4, then
S(p)=−Clog2p
where Cis an arbitrary positive integer.
Proof It follows from Axiom 4 that
S(p2)=S(p)+S(p)=2S(p)
and by induction that
S(pm)=mS(p) (3.1)
Also, since, for any integral n,S(p)=S(p1/n···p1/n)=nS(p1/n), it follows that
S(p1/n)=1
nS(p) (3.2)
Thus, from Equations (3.1) and (3.2), we obtain
S(pm/n)=mS(p1/n)
=m
nS(p)
which is equivalent to
S(px)=xS(p) (3.3)
whenever xis a positive rational number. But by the continuity of S(Axiom 3), it
follows that Equation (3.3) is valid for all nonnegative x. (Reason this out.)
Now, for any p,0<p…1, let x=− log2p. Then p=/parenleftBig
1
2/parenrightBigx
, and from Equa-
tion (3.3),
S(p)=S/parenleftBigg/parenleftbigg1
2/parenrightbiggx/parenrightBigg
=xS/parenleftbigg1
2/parenrightbigg
=−Clog2p
where C=S/parenleftBig
1
2/parenrightBig
>S(1)=0 by Axioms 2 and 1.

<<<PAGE 417>>>

July 14, 2014 M09_ROSSS4772_09_SE_C09 page 404
404 Chapter 9 Additional Topics in Probability
It is usual to let Cequal 1, in which case the surprise is said to be expressed in
units of bits(short for binary digits).
Next, consider a random variable Xthat must take on one of the values x1,...,xn
with respective probabilities p1,...,pn. Since −logpirepresents the surprise evoked
ifXtakes on the value xi,†it follows that the expected amount of surprise we shall
receive upon learning the value of Xis given by
H(X)=−n/summationdisplay
i=1pilogpi
The quantity H(X)is known in information theory as the entropy of the random
variable X. (In case one of the pi=0, we take 0 log 0 to equal 0.) It can be shown
(and we leave it as an exercise) that H(X)is maximized when all of the piare equal.
(Is this intuitive?)
Since H(X)represents the average amount of surprise one receives upon learn-
ing the value of X, it can also be interpreted as representing the amount of uncer-
tainty that exists as to the value of X. In fact, in information theory, H(X)is inter-
preted as the average amount of information received when the value of Xis
observed. Thus, the average surprise evoked by X, the uncertainty of X, or the aver-
age amount of information yielded by Xall represent the same concept viewed from
three slightly different points of view.
Now consider two random variables XandYthat take on the respective values
x1,...,xnandy1,...,ymwith joint mass function
p(xi,yj)=P{X=xi,Y=yj}
It follows that the uncertainty as to the value of the random vector (X,Y), denoted
byH(X,Y), is given by
H(X,Y)=−/summationdisplay
i/summationdisplay
jp(xi,yj)logp(xi,yj)
Suppose now that Yis observed to equal yj. In this situation, the amount of uncer-
tainty remaining in Xis given by
HY=yj(X)=−/summationdisplay
ip(xi|yj)logp(xi|yj)
where
p(xi|yj)=P{X=xi|Y=yj}
Hence, the average amount of uncertainty that will remain in Xafter Yis observed
is given by
HY(X)=/summationdisplay
jHY=yj(X)pY(yj)
where
pY(yj)=P{Y=yj}
Proposition 3.1 relates H(X,Y)toH(Y)andHY(X). It states that the uncertainty as
to the value of XandYis equal to the uncertainty of Yplus the average uncertainty
remaining in Xwhen Yis to be observed.
†For the remainder of this chapter, we write log xfor log2x.A l s o ,w eu s el nx for logex.

<<<PAGE 418>>>

July 14, 2014 M09_ROSSS4772_09_SE_C09 page 405
A First Course in Probability 405
Proposition
3.1H(X,Y)=H(Y)+HY(X)
Proof Using the identity p(xi,yj)=pY(yj)p(x i|yj)yields
H(X,Y)=−/summationdisplay
i/summationdisplay
jp(xi,yj)logp(xi,yj)
=−/summationdisplay
i/summationdisplay
jpY(yj)p(x i|yj)[log pY(yj)+logp(xi|yj)]
=−/summationdisplay
jpY(yj)logpY(yj)/summationdisplay
ip(xi|yj)
−/summationdisplay
jpY(yj)/summationdisplay
ip(xi|yj)logp(xi|yj)
=H(Y)+HY(X)
It is a fundamental result in information theory that the amount of uncertainty in
a random variable Xwill, on the average, decrease when a second random variable
Yis observed. Before proving this statement, we need the following lemma, whose
proof is left as an exercise.
Lemma
3.1lnx…x−1x>0
with equality only at x=1.
Theorem
3.2 HY(X)…H(X)
with equality if and only if XandYare independent.
Proof
HY(X)−H(X)=−/summationdisplay
i/summationdisplay
jp(xi|yj)log[p(xi|yj)]p(y j)
+/summationdisplay
i/summationdisplay
jp(xi,yj)logp(xi)
=/summationdisplay
i/summationdisplay
jp(xi,yj)log/bracketleftBigg
p(xi)
p(xi|yj)/bracketrightBigg
…loge/summationdisplay
i/summationdisplay
jp(xi,yj)/bracketleftBigg
p(xi)
p(xi|yj)−1/bracketrightBigg
by Lemma 3.1
=loge⎡
⎣/summationdisplay
i/summationdisplay
jp(xi)p(y j)−/summationdisplay
i/summationdisplay
jp(xi,yj)⎤⎦
=loge[1−1]
=0
9.4 Coding Theory and Entropy
Suppose that the value of a discrete random vector Xis to be observed at location
Aand then transmitted to location Bvia a communication network that consists of
two signals, 0 and 1. In order to do this, it is ﬁrst necessary to encode each possible

<<<PAGE 419>>>

July 14, 2014 M09_ROSSS4772_09_SE_C09 page 406
406 Chapter 9 Additional Topics in Probability
value of Xin terms of a sequence of 0’s and 1’s. To avoid any ambiguity, it is usu-
ally required that no encoded sequence can be obtained from a shorter encoded
sequence by adding more terms to the shorter.
For instance, if Xcan take on four possible values x1,x2,x3, and x4, then one
possible coding would be
x1%00
x2%01
x3%10
x4%11(4.1)
That is, if X=x1, then the message 00 is sent to location B, whereas if X=x2, then
01 is sent to B, and so on. A second possible coding is
x1%0
x2%10
x3%110
x4%111(4.2)
However, a coding such as
x1%0
x2%1
x3%00
x4%01
is not allowed because the coded sequences for x3andx4are both extensions of the
one for x1.
One of the objectives in devising a code is to minimize the expected number of
bits (that is, binary digits) that need to be sent from location Ato location B.F o r
example, if
P{X=x1}=1
2
P{X=x2}=1
4
P{X=x3}=1
8
P{X=x4}=1
8
then the code given by Equation (4.2) would expect to send1
2(1)+1
4(2)+1
8(3)+
1
8(3)=1.75 bits, whereas the code given by Equation (4.1) would expect to send 2
bits. Hence, for the preceding set of probabilities, the encoding in Equation (4.2) is
more efﬁcient than that in Equation (4.1).
The preceding discussion raises the following question: For a given random
vector X, what is the maximum efﬁciency achievable by an encoding scheme? The
answer is that for any coding, the average number of bits that will be sent is at least
as large as the entropy of X. To prove this result, known in information theory as
thenoiseless coding theorem , we shall need Lemma 4.1.

<<<PAGE 420>>>

July 14, 2014 M09_ROSSS4772_09_SE_C09 page 407
A First Course in Probability 407
Lemma
4.1LetXtake on the possible values x1,...,xN. Then, in order to be able to encode
the values of Xin binary sequences (none of which is an extension of another) of
respective lengths n1,...,nN, it is necessary and sufﬁcient that
N/summationdisplay
i=1/parenleftbigg1
2/parenrightbiggni
…1
Proof For a ﬁxed set of Npositive integers n1,...,nN,l e twjdenote the number of
thenithat are equal to j,j=1,.... For there to be a coding that assigns nibits to the
value xi,i=1,...,N, it is clearly necessary that w1…2. Furthermore, because no
binary sequence is allowed to be an extension of any other, we must have w2…22−
2w1. (This follows because 22is the number of binary sequences of length 2, whereas
2w1is the number of sequences that are extensions of the w1binary sequence of
length 1.) In general, the same reasoning shows that we must have
wn…2n−w12n−1−w22n−2− ··· − wn−12 (4.3)
forn=1,.... In fact, a little thought should convince the reader that these condi-
tions are not only necessary, but also sufﬁcient for a code to exist that assigns nibits
toxi,i=1,...,N.
Rewriting inequality (4.3) as
wn+wn−12+wn−222+ ··· + w12n−1…2nn=1,...
and dividing by 2nyields the necessary and sufﬁcient conditions, namely,
n/summationdisplay
j=1wj/parenleftbigg1
2/parenrightbiggj
…1 for all n (4.4)
However, becausen/summationtext
j=1wj/parenleftBig
1
2/parenrightBigj
is increasing in n, it follows that Equation (4.4) will be
true if and only if
q/summationdisplay
j=1wj/parenleftbigg1
2/parenrightbiggj
…1
The result is now established, since, by the deﬁnition of wjas the number of nithat
equal j, it follows that
q/summationdisplay
j=1wj/parenleftbigg1
2/parenrightbiggj
=N/summationdisplay
i=1/parenleftbigg1
2/parenrightbiggni
We are now ready to prove Theorem 4.1.
Theorem
4.1The noiseless coding theorem
LetXtake on the values x1,...,xNwith respective probabilities p(x1),...,p(xN).
Then, for any coding of Xthat assigns nibits to xi,
N/summationdisplay
i=1nip(xi)ÚH(X)=−N/summationdisplay
i=1p(xi)logp(xi)

<<<PAGE 421>>>

July 14, 2014 M09_ROSSS4772_09_SE_C09 page 408
408 Chapter 9 Additional Topics in Probability
Proof LetPi=p(xi),qi=2−ni/slashBigN/summationtext
j=12−nj,i=1,...,N. Then
−N/summationdisplay
i=1Pilog/parenleftbiggPi
qi/parenrightbigg
=− logeN/summationdisplay
i=1Piln/parenleftbiggPi
qi/parenrightbigg
=logeN/summationdisplay
i=1Piln/parenleftbiggqi
Pi/parenrightbigg
…logeN/summationdisplay
i=1Pi/parenleftbiggqi
Pi−1/parenrightbigg
by Lemma 3.1
=0 sinceN/summationdisplay
i=1Pi=N/summationdisplay
i=1qi=1
Hence,
−N/summationdisplay
i=1PilogPi…−N/summationdisplay
i=1Pilogqi
=N/summationdisplay
i=1niPi+log⎛
⎜⎝N/summationdisplay
j=12−nj⎞
⎟⎠
…N/summationdisplay
i=1niPiby Lemma 4.1
Example
4aConsider a random variable Xwith probability mass function
p(x1)=1
2p(x2)=1
4p(x3)=p(x4)=1
8
Since
H(X)=−/bracketleftbigg1
2log1
2+1
4log1
4+1
4log1
8/bracketrightbigg
=1
2+2
4+3
4
=1.75
it follows from Theorem 4.1 that there is no more efﬁcient coding scheme than
x1%0
x2%10
x3%110
x4%111.
For most random vectors, there does not exist a coding for which the average
number of bits sent attains the lower bound H(X). However, it is always possible to
devise a code such that the average number of bits is within 1 of H(X). To prove this,
deﬁne nito be the integer satisfying
−logp(xi)…ni<−logp(xi)+1

<<<PAGE 422>>>

July 14, 2014 M09_ROSSS4772_09_SE_C09 page 409
A First Course in Probability 409
Now,
N/summationdisplay
i=12−ni…N/summationdisplay
i=12logp(xi)=N/summationdisplay
i=1p(xi)=1
so, by Lemma 4.1, we can associate sequences of bits having lengths niwith the
xi,i=1,...,N. The average length of such a sequence,
L=N/summationdisplay
i=1nip(xi)
satisﬁes
−N/summationdisplay
i=1p(xi)logp(xi)…L<−N/summationdisplay
i=1p(xi)logp(xi)+1
or
H(X)…L<H(X)+1
Example
4bSuppose that 10 independent tosses of a coin having probability pof coming up
heads are made at location Aand the result is to be transmitted to location B.T h e
outcome of this experiment is a random vector X=(X1,...,X10), where Xiis 1 or
0 according to whether or not the outcome of the ith toss is heads. By the results of
this section, it follows that L, the average number of bits transmitted by any code,
satisﬁes
H(X)…L
with
L<H(X)+1
for at least one code. Now, since the Xiare independent, it follows from Proposi-
tion 3.1 and Theorem 3.2 that
H(X)=H(X1,...,X10)=10/summationdisplay
i=1H(Xi)
=−10[plogp+(1−p)log(1−p)]
Ifp=1
2, then H(X)=10, and it follows that we can do no better than just encoding
Xby its actual value. For example, if the ﬁrst 5 tosses come up heads and the last 5
tails, then the message 1111100000 is transmitted to location B.
However, if pZ1
2, we can often do better by using a different coding scheme.
For instance, if p=1
4, then
H(X)=−10/parenleftbigg1
4log1
4+3
4log3
4/parenrightbigg
=8.11
Thus, there is an encoding for which the average length of the encoded message is
no greater than 9.11.

<<<PAGE 423>>>

July 14, 2014 M09_ROSSS4772_09_SE_C09 page 410
410 Chapter 9 Additional Topics in Probability
One simple coding that is more efﬁcient in this case than the identity code is
to break up (X1,...,X10)into 5 pairs of 2 random variables each and then, for i=
1, 3, 5, 7, 9, code each of the pairs as follows:
Xi=0,Xi+1=0%0
Xi=0,Xi+1=1%10
Xi=1,Xi+1=0%110
Xi=1,Xi+1=1%111
The total message transmitted is the successive encodings of the preceding pairs.
For instance, if the outcome TTTHHTTTTH is observed, then the message
010110010 is sent. The average number of bits needed to transmit the message with
this code is
5/bracketleftBigg
1/parenleftbigg3
4/parenrightbigg2
+2/parenleftbigg1
4/parenrightbigg/parenleftbigg3
4/parenrightbigg
+3/parenleftbigg1
4/parenrightbigg/parenleftbigg3
4/parenrightbigg
+3/parenleftbigg1
4/parenrightbigg2/bracketrightBigg
=135
16
L8.44.
Up to this point, we have assumed that the message sent at location Ais received
without error at location B. However, there are always certain errors that can occur
because of random disturbances along the communications channel. Such random
disturbances might lead, for example, to the message 00101101, sent at A, being
received at Bin the form 01101101.
Let us suppose that a bit transmitted at location Awill be correctly received at
location Bwith probability p, independently from bit to bit. Such a communications
system is called a binary symmetric channel . Suppose further that p=.8 and we
want to transmit a message consisting of a large number of bits from AtoB. Thus,
direct transmission of the message will result in an error probability of .20 for each
bit, which is quite high. One way to reduce this probability of bit error would be to
transmit each bit 3 times and then decode by majority rule. That is, we could use the
following scheme:
Encode Decode Encode Decode
0→000000
001
010100⎫
⎪⎪⎪⎬
⎪⎪⎪⎭→01 →111111
110
101011⎫
⎪⎪⎪⎬
⎪⎪⎪⎭→1
Note that if no more than one error occurs in transmission, then the bit will be
correctly decoded. Hence, the probability of bit error is reduced to
(.2)3+3(.2)2(.8)=.104
a considerable improvement. In fact, it is clear that we can make the probability of
bit error as small as we want by repeating the bit many times and then decoding by
majority rule. For instance, the scheme
Encode Decode
0→string of 17 0’s By majority rule1→string of 17 1’s
will reduce the probability of bit error to below .01.

<<<PAGE 424>>>

July 14, 2014 M09_ROSSS4772_09_SE_C09 page 411
A First Course in Probability 411
Table 9.1 Repetition of Bits Encoding Scheme.
Probability of error Rate
(per bit) (bits transmitted per signal)
.20 1
.10 .33/parenleftBig
=1
3/parenrightBig
.01 .06/parenleftBig
=1
17/parenrightBig
The problem with this type of encoding scheme is that although it decreases the
probability of bit error, it does so at the cost of also decreasing the effective rate of
bits sent per signal. (See Table 9.1.)
In fact, at this point it may appear inevitable to the reader that decreasing the
probability of bit error to 0 always results in also decreasing the effective rate at
which bits are transmitted per signal to 0. However, a remarkable result of informa-tion theory known as the noisy coding theorem and due to Claude Shannon demon-
strates that this is not the case. We now state this result as Theorem 4.2.
Theorem
4.2The noisy coding theorem
There is a number Csuch that for any value Rthat is less than C, and for any ε>0,
there exists a coding–decoding scheme that transmits at an average rate of Rbits
sent per signal and with an error (per bit) probability of less than ε. The largest such
value of C—call it C
∗†—is called the channel capacity, and for the binary symmetric
channel,
C∗=1+plogp+(1−p)log(1−p)
Summary
ThePoisson process having rate λis a collection of ran-
dom variables {N(t),tÚ0}that relate to an underlying
process of randomly occurring events. For instance, N(t)
represents the number of events that occur between times
0a n d t. The deﬁning features of the Poisson process are as
follows:
(i) The number of events that occur in disjoint time
intervals are independent.
(ii) The distribution of the number of events that occur
in an interval depends only on the length of theinterval.
(iii) Events occur one at a time.
(iv) Events occur at rate λ.
It can be shown that N(t)is a Poisson random variable with
mean λt. In addition, if T
i,iÚ1, are the times between the
successive events, then they are independent exponential
random variables with rate λ.
A sequence of random variables Xn,nÚ0, each of
which takes on one of the values 0, ...,M,i ss a i dt ob e
aMarkov chain with transition probabilities Pi,jif, for all
n,i0,...,in,i,j,P{Xn+1=j|Xn=i,Xn−1=in−1,...,X0=i0}=Pi,j
If we interpret Xnas the state of some process at time
n, then a Markov chain is a sequence of successive statesof a process that has the property that whenever it enters
state i, then, independently of all past states, the next state
isjwith probability P
i,j, for all states iandj. For many
Markov chains, the probability of being in state jat time n
converges to a limiting value that does not depend on theinitial state. If we let π
j,j=0,...,M, denote these limit-
ing probabilities, then they are the unique solution of theequations
π
j=M/summationdisplay
i=0πiPi,jj=0,...,M
M/summationdisplay
j=1πj=1
Moreover, πjis equal to the long-run proportion of time
that the chain is in state j.
†For an entropy interpretation of C∗, see Theoretical Exercise 9.18.

<<<PAGE 425>>>

July 14, 2014 M09_ROSSS4772_09_SE_C09 page 412
412 Chapter 9 Additional Topics in Probability
LetXbe a random variable that takes on one of
npossible values according to the set of probabilities
{p1,...,pn}. The quantity
H(X)=−n/summationdisplay
i=1pilog2(pi)is called the entropy ofX. It can be interpreted as rep-
resenting either the average amount of uncertainty that
exists regarding the value of Xor the average informa-
tion received when Xis observed. Entropy has important
implications for binary codings of X.
Problems and Theoretical Exercises
9.1.Customers arrive at a bank at a Poisson rate λ. Sup-
pose that two customers arrived during the ﬁrst hour.What is the probability that
(a)both arrived during the ﬁrst 20 minutes?
(b)at least one arrived during the ﬁrst 20 minutes?
9.2. Cars cross a certain point in the highway in accor-
dance with a Poisson process with rate λ=3 per minute. If
Al runs blindly across the highway, what is the probability
that he will be uninjured if the amount of time that it takeshim to cross the road is sseconds? (Assume that if he is on
the highway when a car passes by, then he will be injured.)Do this exercise for s=2, 5, 10, 20.
9.3. Suppose that in Problem 9.2, Al is agile enough to
escape from a single car, but if he encounters two or morecars while attempting to cross the road, then he is injured.
What is the probability that he will be unhurt if it takes
himsseconds to cross? Do this exercise for s=5, 10,
20, 30.
9.4. Suppose that 3 white and 3 black balls are distributed
in two urns in such a way that each urn contains 3 balls.
We say that the system is in state iif the ﬁrst urn contains
iwhite balls, i=0, 1, 2, 3. At each stage, 1 ball is drawn
from each urn and the ball drawn from the ﬁrst urn isplaced in the second, and conversely with the ball from the
second urn. Let X
ndenote the state of the system after
thenth stage, and compute the transition probabilities of
the Markov chain {Xn,nÚ0}.
9.5. Consider Example 2a. If there is a 50–50 chance of
rain today, compute the probability that it will rain 3 days
from now if α=.7a n d β=.3.
9.6. Compute the limiting probabilities for the model of
Problem 9.4.
9.7.A transition probability matrix is said to be doubly
stochastic if
M/summationdisplay
i=0Pij=1
for all states j=0, 1,...,M. Show that such a Markov
chain is ergodic, then/producttext
j=1/(M +1),j=0, 1,...,M.
9.8. On any given day, Buffy is either cheerful (c), so-so
(s), or gloomy (g). If she is cheerful today, then she will bec, s, or g tomorrow with respective probabilities .7, .2, and
.1. If she is so-so today, then she will be c, s, or g tomorrowwith respective probabilities .4, .3, and .3. If she is gloomy
today, then Buffy will be c, s, or g tomorrow with prob-
abilities .2, .4, and .4. What proportion of time is Buffycheerful?
9.9. Suppose that whether it rains tomorrow depends on
past weather conditions only through the past 2 days.
Speciﬁcally, suppose that if it has rained yesterday and
today, then it will rain tomorrow with probability .8; if itrained yesterday but not today, then it will rain tomorrow
with probability .3; if it rained today but not yesterday,
then it will rain tomorrow with probability .4; and if it
has not rained either yesterday or today, then it will rain
tomorrow with probability .2. What proportion of daysdoes it rain?
9.10. A certain person goes for a run each morning. When
he leaves his house for his run, he is equally likely to go
out either the front or the back door, and similarly, when
he returns, he is equally likely to go to either the front orthe back door. The runner owns 5 pairs of running shoes,
which he takes off after the run at whichever door he hap-
pens to be. If there are no shoes at the door from whichhe leaves to go running, he runs barefooted. We are inter-
ested in determining the proportion of time that he runs
barefooted.
(a)Set this problem up as a Markov chain. Give the states
and the transition probabilities.
(b)Determine the proportion of days that he runs bare-
footed.
9.11. This problem refers to Example 2f.
(a)Verify that the proposed value of π
jsatisﬁes the neces-
sary equations.
(b)For any given molecule, what do you think is the (lim-
iting) probability that it is in urn 1?(c)Do you think that the events that molecule j,jÚ1, is
in urn 1 at a very large time would be (in the limit) inde-
pendent?
(d)Explain why the limiting probabilities are as given.
9.12. Determine the entropy of the sum that is obtained
when a pair of fair dice is rolled.

<<<PAGE 426>>>

July 14, 2014 M09_ROSSS4772_09_SE_C09 page 413
A First Course in Probability 413
9.13. Prove that if Xcan take on any of npossible values
with respective probabilities P1,...,Pn,t h e n H(X)is max-
imized when Pi=1/n,i=1,...,n.W h a ti sH (X)equal to
in this case?
9.14. A pair of fair dice is rolled. Let
X=/braceleftbigg
1 if the sum of the dice is 6
0 otherwise
and let Yequal the value of the ﬁrst die. Compute (a)
H(Y),( b ) HY(X),a n d( c )H (X,Y).
9.15. A coin having probability p=2
3of coming up heads
is ﬂipped 6 times. Compute the entropy of the outcome ofthis experiment.
9.16. A random variable can take on any of npossible
values x
1,...,xnwith respective probabilities p(xi),i=
1,...,n. We shall attempt to determine the value of Xby
asking a series of questions, each of which can be answered
“yes” or “no.” For instance, we may ask “Is X=x1?” or“IsXequal to either x1orx2orx3?” and so on. What can
you say about the average number of such questions thatyou will need to ask to determine the value of X?
9.17. Show that for any discrete random variable Xand
function f,
H(f(X))…H(X)
9.18. In transmitting a bit from location Ato location B,
if we let Xdenote the value of the bit sent at location
AandYdenote the value received at location B,t h e n
H(X)−H
Y(X)is called the rate of transmission of infor-
mation from A to B. The maximal rate of transmission, as
a function of P{X=1}=1 −P{X=0}, is called the
channel capacity. Show that for a binary symmetric chan-
nel with P{Y=1|X=1}= P{Y=0|X=0}= p,
the channel capacity is attained by the rate of transmis-
sion of information when P{X=1}=1
2and its value is
1+plogp+(1−p)log(1 −p).
Self-Test Problems and Exercises
9.1.Events occur according to a Poisson process with rate
λ=3 per hour.
(a)What is the probability that no events occur between
times 8 and 10 in the morning?
(b)What is the expected value of the number of events
that occur between times 8 and 10 in the morning?(c)What is the expected time of occurrence of the ﬁfth
event after 2
P. M .?
9.2. Customers arrive at a certain retail establishment
according to a Poisson process with rate λper hour. Sup-
pose that two customers arrive during the ﬁrst hour. Find
the probability that
(a)both arrived in the ﬁrst 20 minutes;
(b)at least one arrived in the ﬁrst 30 minutes.
9.3. Four out of every ﬁve trucks on the road are fol-
lowed by a car, while one out of every six cars is followedby a truck. What proportion of vehicles on the road are
trucks?
9.4. A certain town’s weather is classiﬁed each day as
being rainy, sunny, or overcast, but dry. If it is rainy one
day, then it is equally likely to be either sunny or overcastthe following day. If it is not rainy, then there is one chance
in three that the weather will persist in whatever state it is
in for another day, and if it does change, then it is equallylikely to become either of the other two states. In the long
run, what proportion of days are sunny? What proportion
are rainy?
9.5. LetXbe a random variable that takes on 5 possible
values with respective probabilities .35, .2, .2, .2, and .05.
Also, let Ybe a random variable that takes on 5 possible
values with respective probabilities .05, .35, .1, .15, and .35.
(a)Show that H(X)>H (Y).
(b)Using the result of Problem 9.13, give an intuitive
explanation for the preceding inequality.
References
Sections 9.1 and 9.2
[1]Kemeny,J . , L .S nell,a n dA . K napp .Denumerable Markov Chains.N e wY o r k :
D. Van Nostrand Company, 1966.
[2]Parzen,E . Stochastic Processes. San Francisco: Holden-Day, Inc., 1962.
[3]Ross,S .M . Introduction to Probability Models, 10th ed. San Diego: Academic
Press, Inc., 2010.
[4]Ross,S .M .Stochastic Processes, 2d ed. New York: John Wiley & Sons, Inc., 1996.

<<<PAGE 427>>>

July 14, 2014 M09_ROSSS4772_09_SE_C09 page 414
414 Chapter 9 Additional Topics in Probability
Sections 9.3 and 9.4
[5]Abramson,N . Information Theory and Coding . New York: McGraw-Hill Book
Company, 1963.
[6]McEliece,R . Theory of Information and Coding . Reading, MA: Addison-Wesley
Publishing Co., Inc., 1977.
[7]Peterson,W . , a n d E .W eldon. Error Correcting Codes, 2d ed. Cambridge, MA:
The MIT Press, 1972.

<<<PAGE 428>>>

July 14, 2014 M10_ROSSS4772_09_SE_C10 page 415
Chapter
Simulation10
Contents
10.1 Introduction
10.2 General Techniques for Simulating
Continuous Random Variables10.3 Simulating from Discrete Distributions
10.4 Variance Reduction Techniques
10.1 Introduction
How can we determine the probability of our winning a game of solitaire?(By solitaire, we mean any one of the standard solitaire games played with an ordi-
nary deck of 52 playing cards and with some ﬁxed playing strategy.) One possible
approach is to start with the reasonable hypothesis that all (52)! possible arrange-ments of the deck of cards are equally likely to occur and then attempt to determine
how many of these lead to a win. Unfortunately, there does not appear to be any sys-
tematic method for determining the number of arrangements that lead to a win, andas (52)! is a rather large number and the only way to determine whether a particular
arrangement leads to a win seems to be by playing the game out, it can be seen that
this approach will not work.
In fact, it might appear that the determination of the probability of winning
at solitaire is mathematically intractable. However, all is not lost, for probabilityfalls not only within the realm of mathematics, but also within the realm of appliedscience; and, as in all applied sciences, experimentation is a valuable technique. For
our solitaire example, experimentation takes the form of playing a large number of
such games or, better yet, programming a computer to do so. After playing, say, n
games, if we let
X
i=/braceleftBigg
1i f t h e ith game results in a win
0 otherwise
then Xi,i=1,...,nwill be independent Bernoulli random variables for which
E[Xi]=P{win at solitaire }
Hence, by the strong law of large numbers, we know that
n/summationdisplay
i=1Xi
n=number of games won
number of games played
415

<<<PAGE 429>>>

July 14, 2014 M10_ROSSS4772_09_SE_C10 page 416
416 Chapter 10 Simulation
will, with probability 1, converge to P{win at solitaire }. That is, by playing a large
number of games, we can use the proportion of games won as an estimate of the
probability of winning. This method of empirically determining probabilities by
means of experimentation is known as simulation.
In order to use a computer to initiate a simulation study, we must be able to
generate the value of a uniform (0, 1) random variable; such variates are called ran-
dom numbers . To generate them, most computers have a built-in subroutine, called a
random-number generator, whose output is a sequence of pseudorandom numbers —
a sequence of numbers that is, for all practical purposes, indistinguishable from asample from the uniform (0, 1) distribution. Most random-number generators start
with an initial value X
0, called the seed, and then recursively compute values by
specifying positive integers a,c, and m, and then letting
Xn+1=(aXn+c)modulo mn Ú0 (1.1)
where the foregoing means that aXn+cis divided by mand the remainder is taken
as the value of Xn+1. Thus, each Xnis either 0, 1, ...,m−1, and the quantity Xn/mis
taken as an approximation to a uniform (0, 1) random variable. It can be shown thatsubject to suitable choices for a,c, and m, Equation (1.1) gives rise to a sequence
of numbers that look as if they were generated from independent uniform (0, 1)random variables.
As our starting point in simulation, we shall suppose that we can simulate from
the uniform (0, 1) distribution, and we shall use the term random numbers to mean
independent random variables from this distribution.
In the solitaire example, we would need to program a computer to play out the
game starting with a given ordering of the cards. However, since the initial orderingis supposed to be equally likely to be any of the (52)! possible permutations, it is also
necessary to be able to generate a random permutation. Using only random num-bers, the following algorithm shows how this can be accomplished. The algorithm
begins by randomly choosing one of the elements and then putting it in position n;i t
then randomly chooses among the remaining elements and puts the choice in posi-
tionn−1, and so on. The algorithm efﬁciently makes a random choice among the
remaining elements by keeping these elements in an ordered list and then randomlychoosing a position on that list.
Example
1aGenerating a random permutation
Suppose we are interested in generating a permutation of the integers 1, 2, ...,n
such that all n! possible orderings are equally likely. Then, starting with any initial
permutation, we will accomplish this after n−1 steps, where we interchange the
positions of two of the numbers of the permutation at each step. Throughout, we
will keep track of the permutation by letting X(i),i=1,...,ndenote the number
currently in position i. The algorithm operates as follows:
1. Consider any arbitrary permutation, and let X(i)denote the element in posi-
tioni,i=1...,n. [For instance, we could take X(i)=i,i=1,...,n.]
2. Generate a random variable Nnthat is equally likely to equal any of the values
1, 2,...,n.
3. Interchange the values of X(Nn)andX(n). The value of X(n)will now remain
ﬁxed. [For instance, suppose that n=4 and initially X(i)=i,i=1, 2, 3, 4. If
N4=3, then the new permutation is X(1)=1,X(2)=2,X(3)=4,X(4)=3,
and element 3 will remain in position 4 throughout.]
4. Generate a random variable Nn−1that is equally likely to be either 1, 2, ...,
n−1.

<<<PAGE 430>>>

July 14, 2014 M10_ROSSS4772_09_SE_C10 page 417
A First Course in Probability 417
5. Interchange the values of X(Nn−1)andX(n−1).[ I f N3=1, then the new
permutation is X(1)=4,X(2)=2,X(3)=1,X(4)=3.]
6. Generate Nn−2, which is equally likely to be either 1, 2, ...,n−2.
7. Interchange the values of X(Nn−2)andX(n−2).[ I f N2=1, then the new
permutation is X(1)=2,X(2)=4,X(3)=1,X(4)=3, and this is the ﬁnal
permutation.]
8. Generate Nn−3, and so on. The algorithm continues until N2is generated, and
after the next interchange the resulting permutation is the ﬁnal one.
To implement this algorithm, it is necessary to be able to generate a random
variable that is equally likely to be any of the values 1, 2, ...,k. To accomplish this,
letUdenote a random number—that is, Uis uniformly distributed on (0, 1)—and
note that kUis uniform on (0,k). Hence,
P{i−1<kU<i}=1
ki=1,...,k
so if we take Nk=[kU]+1, where [x] is the integer part of x(that is, the largest
integer less than or equal to x), then Nkwill have the desired distribution.
The algorithm can now be succinctly written as follows:
Step 1. LetX(1),...,X(n)be any permutation of 1, 2, ...,n. [For instance, we
can set X(i)=i,i=1,...,n.]
Step 2. LetI=n.
Step 3. Generate a random number Uand set N=[IU]+1.
Step 4. Interchange the values of X(N)andX(I).
Step 5. Reduce the value of Iby 1, and if I>1, go to step 3.
Step 6. X(1),...,X(n)is the desired random generated permutation.
The foregoing algorithm for generating a random permutation is extremely use-
ful. For instance, suppose that a statistician is developing an experiment to compare
the effects of mdifferent treatments on a set of nsubjects. He decides to split the
subjects into mdifferent groups of respective sizes n1,n2,...,nm, where/summationtextm
i=1ni=n,
with the members of the ith group to receive treatment i. To eliminate any bias in
the assignment of subjects to treatments (for instance, it would cloud the meaning
of the experimental results if it turned out that all the “best” subjects had been putin the same group), it is imperative that the assignment of a subject to a given group
be done “at random.” How is this to be accomplished?
†
A simple and efﬁcient procedure is to arbitrarily number the subjects 1 through
nand then generate a random permutation X(1),...,X(n)of 1, 2, ...,n. Now assign
subjects X(1),X(2),...,X(n1)to be in group 1; X(n1+1),...,X(n1+n2)to be in
group 2; and, in general, group jis to consist of subjects numbered X(n1+n2+
··· + nj−1+k),k=1,...,nj. .
10.2 General Techniques for Simulating Continuous
Random Variables
In this section, we present two general methods for using random numbers to simu-late continuous random variables.
†Another technique for randomly dividing the subjects when m=2 was presented in Example 2g of Chapter 6.
The preceding procedure is faster, but requires more space than the one of Example 2g.

<<<PAGE 431>>>

July 14, 2014 M10_ROSSS4772_09_SE_C10 page 418
418 Chapter 10 Simulation
10.2.1 The Inverse Transformation Method
A general method for simulating a random variable having a continuous
distribution—called the inverse transformation method —is based on the following
proposition.
Proposition
2.1LetUbe a uniform (0, 1) random variable. For any continuous distribution function
F, if we deﬁne the random variable Yby
Y=F−1(U)
then the random variable Yhas distribution function F.[F−1(x)is deﬁned to equal
that value yfor which F(y)=x.]
Proof
FY(a)=P{Y…a}
=P{F−1(U)…a} (2.1)
Now, since F(x)is a monotone function, it follows that F−1(U)…aif and only if
U…F(a). Hence, from Equation (2.1), we have
FY(a)=P{U…F(a)}
=F(a)
It follows from Proposition 2.1 that we can simulate a random variable Xhaving
a continuous distribution function Fby generating a random number Uand then
setting X=F−1(U).
Example
2aSimulating an exponential random variable
IfF(x)=1−e−x, then F−1(u)is that value of xsuch that
1−e−x=u
or
x=− log(1−u)
Hence, if Uis a uniform (0, 1) variable, then
F−1(U)=− log(1−U)
is exponentially distributed with mean 1. Since 1 −Uis also uniformly distributed on
(0, 1), it follows that −logUis exponential with mean 1. Since cXis exponential with
mean cwhen Xis exponential with mean 1, it follows that −clogUis exponential
with mean c. .
The results of Example 2a can also be utilized to stimulate a gamma random
variable.
Example
2bSimulating a gamma (n, λ) random variable
To simulate from a gamma distribution with parameters (n,λ)when nis an integer,
we use the fact that the sum of nindependent exponential random variables, each
having rate λ, has this distribution. Hence, if U1,...,Unare independent uniform
(0, 1) random variables, then

<<<PAGE 432>>>

July 14, 2014 M10_ROSSS4772_09_SE_C10 page 419
A First Course in Probability 419
X=−n/summationdisplay
i=11
λlogUi=−1
λlog⎛
⎝n/productdisplay
i=1Ui⎞⎠
has the desired distribution. .
10.2.2 The Rejection Method
Suppose that we have a method for simulating a random variable having density
function g(x). We can use this method as the basis for simulating from the continu-
ous distribution having density f(x)by simulating Yfrom gand then accepting the
simulated value with a probability proportional to f(Y)/g(Y).
Speciﬁcally, let cbe a constant such that
f(y)
g(y)…cfor all y
We then have the following technique for simulating a random variable havingdensity f.
Rejection Method
Step 1. Simulate Yhaving density gand simulate a random number U.
Step 2. IfU…f(Y)/cg(Y),s e t X=Y. Otherwise return to step 1.
The rejection method is expressed pictorially in Figure 10.1. We now prove that
it works.
Generate
Y /H11011 gStart
Generate a
random number
U Is
f(Y)——––cg(Y )  U /H11088YesSet X = Y
No
Figure 10.1 Rejection method for simulating a random variable Xhaving density func-
tionf.
Proposition
2.2The random variable Xgenerated by the rejection method has density function f.
Proof LetXbe the value obtained and let Ndenote the number of necessary itera-
tions. Then
P{X…x}= P{YN…x}
=P/braceleftbigg
Y…x|U…f(Y)
cg(Y)/bracerightbigg
=P/braceleftbigg
Y…x,U…f(Y)
cg(Y)/bracerightbigg
K
where K=P{U…f(Y)/cg(Y)}. Now, by independence, the joint density function
ofYandUis
f(y,u)=g(y) 0<u<1

<<<PAGE 433>>>

July 14, 2014 M10_ROSSS4772_09_SE_C10 page 420
420 Chapter 10 Simulation
so, using the foregoing, we have
P{X…x}=1
K/integraldisplay/integraldisplay
y…x
0…u…f(y)/cg(y)g(y)du dy
=1
K/integraldisplayx
−q/integraldisplayf(y)/cg(y)
0du g(y)dy
=1
cK/integraldisplayx
−qf(y)dy(2.2)
Letting Xapproach qand using the fact that fis a density gives
1=1
cK/integraldisplayq
−qf(y)dy=1
cK
Hence, from Equation (2.2), we obtain
P{X…x}=/integraldisplayx
−qf(y)dy
which completes the proof.
Remarks (a) Note that the way in which we “accept the value Ywith probability
f(Y)/cg(Y)” is by generating a random number Uand then accepting YifU…
f(Y)/cg(Y).
(b) Since each iteration will independently result in an accepted value with prob-
ability P{U…f(Y)/cg(Y)}=K=1/c, it follows that the number of iterations has a
geometric distribution with mean c. .
Example
2cSimulating a normal random variable
To simulate a unit normal random variable Z(that is, one with mean 0 and vari-
ance 1), note ﬁrst that the absolute value of Zhas probability density function
f(x)=2√
2πe−x2/20<x<q (2.3)
We will start by simulating from the preceding density function by using the rejection
method, with gbeing the exponential density function with mean 1—that is,
g(x)=e−x0<x<q
Now, note that
f(x)
g(x)=/radicalbigg
2
πexp/braceleftBigg
−(x2−2x)
2/bracerightBigg
=/radicalbigg
2
πexp/braceleftBigg
−(x2−2x+1)
2+1
2/bracerightBigg
=/radicalbigg
2e
πexp/braceleftBigg
−(x−1)2
2/bracerightBigg
(2.4)
…/radicalbigg
2e
π

<<<PAGE 434>>>

July 14, 2014 M10_ROSSS4772_09_SE_C10 page 421
A First Course in Probability 421
Hence, we can take c=/radicalbig
2e/π ; so, from Equation (2.4),
f(x)
cg(x)=exp/braceleftBigg
−(x−1)2
2/bracerightBigg
Therefore, using the rejection method, we can simulate the absolute value of a unit
normal random variable as follows:
(a) Generate independent random variables YandU,Ybeing exponential with
rate 1 and Ubeing uniform on (0, 1).
(b) If U…exp{−(Y−1)2/2},s e tX =Y. Otherwise, return to (a).
Once we have simulated a random variable Xhaving Equation (2.3) as its density
function, we can then generate a unit normal random variable Zby letting Zbe
equally likely to be either Xor−X.
In step (b), the value Yis accepted if U…exp{−(Y−1)2/2}, which is equivalent
to−logUÚ(Y−1)2/2. However, in Example 2a, it was shown that −logUis
exponential with rate 1, so steps (a) and (b) are equivalent to
(a/prime) Generate independent exponentials Y1andY2, each with rate 1.
(b/prime)I fY2Ú(Y1−1)2/2, set X=Y1. Otherwise, return to (a/prime).
Suppose now that the foregoing results in Y1being accepted—so we know that Y2
is larger than (Y1−1)2/2. By how much does the one exceed the other? To answer
this question, recall that Y2is exponential with rate 1; hence, given that it exceeds
some value, the amount by which Y2exceeds (Y1−1)2/2 [that is, its “additional life”
beyond the time (Y1−1)2/2] is (by the memoryless property) also exponentially
distributed with rate 1. That is, when we accept step (b/prime), not only do we obtain X
(the absolute value of a unit normal), but, by computing Y2−(Y1−1)2/2, we
also can generate an exponential random variable (that is independent of X) having
rate 1.
Summing up, then, we have the following algorithm that generates an exponen-
tial with rate 1 and an independent unit normal random variable:
Step 1. Generate Y1, an exponential random variable with rate 1.
Step 2. Generate Y2, an exponential random variable with rate 1.
Step 3. IfY2−(Y1−1)2/2>0, set Y=Y2−(Y1−1)2/2 and go to step
4. Otherwise, go to step 1.
Step 4. Generate a random number U, and set
Z=/braceleftBigg
Y1ifU…1
2
−Y 1ifU>1
2
The random variables ZandYgenerated by the foregoing algorithm are inde-
pendent, with Zbeing normal with mean 0 and variance 1 and Ybeing exponential
with rate 1. (If we want the normal random variable to have mean μand variance
σ2, we just take μ+σZ.)
Remarks (a) Since c=/radicalbig
2e/π L1.32, the algorithm requires a geometrically dis-
tributed number of iterations of step 2 with mean 1.32.
(b) If we want to generate a sequence of unit normal random variables, then we
can use the exponential random variable Yobtained in step 3 as the initial exponen-
tial needed in step 1 for the next normal to be generated. Hence, on the average, wecan simulate a unit normal by generating 1 .64(= 2*1.32−1)exponentials and
computing 1.32 squares..

<<<PAGE 435>>>

July 14, 2014 M10_ROSSS4772_09_SE_C10 page 422
422 Chapter 10 Simulation
Example
2dSimulating normal random variables: the polar method
It was shown in Example 7b of Chapter 6 that if XandYare independent unit nor-
mal random variables, then their polar coordinates R=/radicalbig
X2+Y2,/H9008=tan−1(Y/X)
are independent, with R2being exponentially distributed with mean 2 and /H9008being
uniformly distributed on (0, 2π) . Hence, if U1andU2are random numbers, then,
using the result of Example 2a, we can set
R=(−2l o g U1)1/2
Θ=2πU2
from which it follows that
X=RcosΘ=(−2l o g U1)1/2cos(2πU2)
Y=RsinΘ=(−2l o g U1)1/2sin(2πU2) (2.5)
are independent unit normals. .
The preceding approach to generating unit normal random variables is called
theBox–Muller approach . Its efﬁciency suffers somewhat from its need to compute
the sine and cosine values. There is, however, a way to get around this potentially
time-consuming difﬁculty. To begin, note that if Uis uniform on (0, 1), then 2 Uis
uniform on (0, 2), so 2 U−1 is uniform on (−1, 1). Thus, if we generate random
numbers U1andU2and set
V1=2U1−1
V2=2U2−1
then(V1,V2)is uniformly distributed in the square of area 4 centered at (0, 0). (See
Figure 10.2.)
Suppose now that we continually generate such pairs (V1,V2)until we obtain
one that is contained in the disk of radius 1 centered at (0, 0)—that is, until V2
1+
V2
2…1. It then follows that such a pair (V1,V2)is uniformly distributed in the disk.
(1, 1)
(1, –1)(–1, 1)
(–1, –1)RV2
V1/H9258
V12+=  1V22
= (0, 0)
= (V1, V2)
Figure 10.2

<<<PAGE 436>>>

July 14, 2014 M10_ROSSS4772_09_SE_C10 page 423
A First Course in Probability 423
Now, let R,/H9008denote the polar coordinates of this pair. Then it is easy to verify that
Rand/H9008are independent, with R2being uniformly distributed on (0, 1) and /H9008being
uniformly distributed on (0, 2π) . (See Problem 10.13.)
Since
sin/H9008=V2
R=V2/radicalBig
V2
1+V2
2
cos/H9008=V1
R=V1/radicalBig
V2
1+V2
2
it follows from Equation (2.5) that we can generate independent unit normals Xand
Yby generating another random number Uand setting
X=(−2l o g U)1/2V1/R
Y=(−2l o g U)1/2V2/R
In fact, because (conditional on V2
1+V2
2…1)R2is uniform on (0, 1) and is inde-
pendent of θ, we can use it instead of generating a new random number U, thus
showing that
X=(−2l o g R2)1/2V1
R=/radicalbigg
−2l o g S
SV1
Y=(−2l o g R2)1/2V2
R=/radicalbigg
−2l o g S
SV2
are independent unit normals, where
S=R2=V2
1+V2
2
Summing up, we have the following approach to generating a pair of independent
unit normals:
Step 1. Generate random numbers U1andU2.
Step 2. SetV1=2U1−1,V2=2U2−1,S=V2
1+V2
2.
Step 3. IfS>1, return to step 1.
Step 4. Return the independent unit normals
X=/radicalbigg
−2l o g S
SV1,Y=/radicalbigg
−2l o g S
SV2
The preceding algorithm is called the polar method . Since the probability that a
random point in the square will fall within the circle is equal to π/4 (the area of the
circle divided by the area of the square), it follows that, on average, the polar methodwill require 4 /πL1.273 iterations of step 1. Hence, it will, on average, require 2.546
random numbers, 1 logarithm, 1 square root, 1 division, and 4.546 multiplications togenerate 2 independent unit normals.
Example
2eSimulating a chi-squared random variable
The chi-squared distribution with ndegrees of freedom is the distribution of χ2
n=
Z2
1+ ··· + Z2
n, where Zi,i=1,...,nare independent unit normals. Now, it was
shown in Section 6.3 of Chapter 6 that Z2
1+Z2
2has an exponential distribution
with rate1
2. Hence, when nis even (say, n=2k),χ2
2khas a gamma distribution

<<<PAGE 437>>>

July 14, 2014 M10_ROSSS4772_09_SE_C10 page 424
424 Chapter 10 Simulation
with parameters/parenleftBig
k,1
2/parenrightBig
. Thus, −2l o g (/producttextk
i=1Ui)has a chi-squared distribution with
2kdegrees of freedom. Accordingly, we can simulate a chi-squared random variable
with 2 k+1 degrees of freedom by ﬁrst simulating a unit normal random variable Z
and then adding Z2to the foregoing. That is,
χ2
2k+1=Z2−2l o g⎛
⎝k/productdisplay
i=1Ui⎞⎠
where Z,U
1,...,Unare independent, Zis a unit normal, and U1,...,Unare uniform
(0, 1) random variables.
10.3 Simulating from Discrete Distributions
All of the general methods for simulating random variables from continuous dis-
tributions have analogs in the discrete case. For instance, if we want to simulate arandom variable Zhaving probability mass function
P{X=x
j}=Pj,j=0, 1,...,/summationdisplay
jPj=1
we can use the following discrete time analog of the inverse transform technique:
To simulate Xfor which P{X=xj}=P j,l e tU be uniformly distributed over
(0, 1) and set
X=⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩x
1ifU…P1
x2ifP1<U…P1+P2
#
#
#
xjifj−1/summationdisplay
1Pi<U…j/summationdisplay
iPi
#
#
#
Since
P{X=xj}=P⎧
⎨
⎩j−1/summationdisplay
1Pi<U…j/summationdisplay
1Pi⎫
⎬
⎭=Pj
it follows that Xhas the desired distribution.
Example
3aThe geometric distribution
Suppose that independent trials, each of which results in a “success” with probability
p,0<p<1, are continually performed until a success occurs. Letting Xdenote the
necessary number of trials; then
P{X=i}=(1−p)i−1pi Ú1
which is seen by noting that X=iif the ﬁrst i−1 trials are all failures and the ith
trial is a success. The random variable Xis said to be a geometric random variable
with parameter p. Since

<<<PAGE 438>>>

July 14, 2014 M10_ROSSS4772_09_SE_C10 page 425
A First Course in Probability 425
j−1/summationdisplay
i=1P{X=i}=1 −P{X>j−1}
=1−P{ﬁrst j−1 are all failures }
=1−(1−p)j−1jÚ1
we can simulate such a random variable by generating a random number Uand then
setting Xequal to that value jfor which
1−(1−p)j−1<U…1−(1−p)j
or, equivalently, for which
(1−p)j…1−U<(1−p)j−1
Since 1 −Uhas the same distribution as U, we can deﬁne Xby
X=min{j:(1−p)j…U}
=min{j:jlog(1 −p)…logU}
=min/braceleftBigg
j:jÚlogU
log(1 −p)/bracerightBigg
where the inequality has changed sign because log(1−p) is negative [since log (1−p)
>log 1=0]. Using the notation [ x] for the integer part of x(that is, [x] is the largest
integer less than or equal to x), we can write
X=1+/bracketleftBigg
logU
log(1 −p)/bracketrightBigg
.
As in the continuous case, special simulating techniques have been developed
for the more common discrete distributions. We now present two of these.
Example
3bSimulating a binomial random variable
A binomial ( n,p) random variable can easily be simulated by recalling that it can
be expressed as the sum of nindependent Bernoulli random variables. That is, if
U1,...,Unare independent uniform (0, 1) variables, then letting
Xi=/braceleftBigg
1i f Ui<p
0 otherwise
it follows that XKn/summationtext
i=1Xiis a binomial random variable with parameters nandp.
Example
3cSimulating a Poisson random variable
To simulate a Poisson random variable with mean λ, generate independent uniform
(0, 1) random variables U1,U2,...stopping at
N=min⎧
⎨
⎩n:n/productdisplay
i=1Ui<e−λ⎫
⎬
⎭

<<<PAGE 439>>>

July 14, 2014 M10_ROSSS4772_09_SE_C10 page 426
426 Chapter 10 Simulation
The random variable XKN−1 has the desired distribution. That is, if we continue
generating random numbers until their product falls below e−λ, then the number
required, minus 1, is Poisson with mean λ.
That XKN−1 is indeed a Poisson random variable having mean λcan perhaps
be most easily seen by noting that
X+1=min⎧
⎨
⎩n:n/productdisplay
i=1Ui<e−λ⎫
⎬
⎭
is equivalent to
X=max⎧
⎨
⎩n:n/productdisplay
i=1UiÚe−λ⎫
⎬
⎭where0/productdisplay
i=1UiK1
or, taking logarithms, to
X=max⎧
⎨
⎩n:n/summationdisplay
i=1logUiÚ−λ⎫
⎬
⎭
or
X=max⎧
⎨
⎩n:n/summationdisplay
i=1−logUi…λ⎫
⎬
⎭
However, −logUiis exponential with rate 1, so Xcan be thought of as being the
maximum number of exponentials having rate 1 that can be summed and still be
less than λ. But by recalling that the times between successive events of a Poisson
process having rate 1 are independent exponentials with rate 1, it follows that Xis
equal to the number of events by time λof a Poisson process having rate 1; thus, X
has a Poisson distribution with mean λ. .
10.4 Variance Reduction Techniques
LetX1,...,Xnhave a given joint distribution, and suppose that we are interested in
computing
θKE[g(X1,...,Xn)]
where gis some speciﬁed function. It sometimes turns out that it is extremely dif-
ﬁcult to analytically compute θ, and when such is the case, we can attempt to use
simulation to estimate θ. This is done as follows: Generate X(1)
1,...,X(1)
nhaving the
same joint distribution as X1,...,Xnand set
Y1=g(X(1)
1,...,X(1)
n)
Now let X(2)
1,...,X(2)
nsimulate a second set of random variables (independent of
the ﬁrst set) having the distribution of X1,...,Xnand set
Y2=g(X(2)
1,...,X(2)
n)

<<<PAGE 440>>>

July 14, 2014 M10_ROSSS4772_09_SE_C10 page 427
A First Course in Probability 427
Continue this until you have generated k(some predetermined number) sets and so
have also computed Y1,Y2,...,Yk.N o w ,Y 1,...,Ykare independent and identically
distributed random variables, each having the same distribution as g(X1,...,Xn).
Thus, if we let Ydenote the average of these krandom variables—that is, if
Y=k/summationdisplay
i=1Yi
k
then
E[Y]=θ
E[(Y−θ)2]=Var(Y)
Hence, we can use Yas an estimate of θ. Since the expected square of the difference
between Yandθis equal to the variance of Y, we would like this quantity to be as
small as possible. [In the preceding situation, Var (Y)=Var(Yi)/k, which is usually
not known in advance, but must be estimated from the generated values Y1,...,Yn.]
We now present three general techniques for reducing the variance of our estimator.
10.4.1 Use of Antithetic Variables
In the foregoing situation, suppose that we have generated Y1andY2, which are
identically distributed random variables having mean θ.N o w ,
Var/parenleftbiggY1+Y2
2/parenrightbigg
=1
4[Var(Y 1)+Var(Y2)+2Cov(Y1,Y2)]
=Var(Y1)
2+Cov(Y1,Y2)
2
Hence, it would be advantageous (in the sense that the variance would be reduced)
ifY1andY2were negatively correlated rather than being independent. To see how
we could arrange this, let us suppose that the random variables X1,...,Xnare inde-
pendent and, in addition, that each is simulated via the inverse transform technique.
That is, Xiis simulated from F−1
i(Ui), where Uiis a random number and Fiis the
distribution of Xi. Thus, Y1can be expressed as
Y1=g(F−1
1(U1),...,F−1
n(Un))
Now, since 1 −Uis also uniform over (0, 1) whenever Uis a random number (and
is negatively correlated with U), it follows that Y2deﬁned by
Y2=g(F−1
1(1−U1),...,F−1
n(1−Un))
will have the same distribution as Y1. Hence, if Y1andY2were negatively correlated,
then generating Y2by this means would lead to a smaller variance than if it were
generated by a new set of random numbers. (In addition, there is a computational
savings because, rather than having to generate nadditional random numbers, we
need only subtract each of the previous nnumbers from 1.) Although we cannot,
in general, be certain that Y1andY2will be negatively correlated, this often turns
out to be the case, and indeed it can be proven that it will be so whenever gis a
monotonic function.

<<<PAGE 441>>>

July 14, 2014 M10_ROSSS4772_09_SE_C10 page 428
428 Chapter 10 Simulation
10.4.2 Variance Reduction by Conditioning
Let us start by recalling the conditional variance formula (see Section 7.5.4)
Var(Y)=E[Var(Y |Z)]+Var(E[Y|Z])
Now, suppose that we are interested in estimating E[g(X1,...,Xn)] by simulating
X=(X1,...,Xn)and then computing Y=g(X). If, for some random variable Z
we can compute E[Y|Z], then, since Var (Y|Z)Ú0, it follows from the preceding
conditional variance formula that
Var(E[Y|Z])…Var(Y)
Thus, since E[E[Y|Z]]=E[Y], it follows that E[Y|Z] is a better estimator of E[Y]
than is Y.
Example
4aEstimation of π
LetU1andU2be random numbers and set Vi=2Ui−1,i=1, 2. As noted in
Example 2d, (V1,V2)will be uniformly distributed in the square of area 4 centered
at (0, 0). The probability that this point will fall within the inscribed circle of radius 1
centered at (0, 0) (see Figure 10.2) is equal to π/4 (the ratio of the area of the circle
to that of the square). Hence, upon simulating a large number nof such pairs and
setting
Ij=/braceleftBigg
1i f t h e jth pair falls within the circle
0 otherwise
it follows that Ij,j=1,...,n, will be independent and identically distributed random
variables having E[Ij]=π/4. Thus, by the strong law of large numbers,
I1+ ··· + In
n→π
4asn→q
Therefore, by simulating a large number of pairs (V1,V2)and multiplying the pro-
portion of them that fall within the circle by 4, we can accurately approximate π.
The preceding estimator can, however, be improved upon by using conditional
expectation. If we let Ibe the indicator variable for the pair (V1,V2), then, rather
than using the observed value of I, it is better to condition on V1and so utilize
E[I|V1]=P{V2
1+V2
2…1|V 1}
=P{V2
2…1−V2
1|V1}
Now,
P{V2
2…1−V2
1|V1=ν}=P{V2
2…1−ν2}
=P{−/radicalbig
1−ν2…V2…/radicalbig
1−ν2}
=/radicalbig
1−ν2
so
E[I|V1]=/radicalBig
1−V2
1

<<<PAGE 442>>>

July 14, 2014 M10_ROSSS4772_09_SE_C10 page 429
A First Course in Probability 429
Thus, an improvement on using the average value of Ito estimate π/4i st ou s et h e
average value of/radicalBig
1−V2
1. Indeed, since
E/bracketleftbigg/radicalBig
1−V2
1/bracketrightbigg
=/integraldisplay1
−11
2/radicalbig
1−ν2dν=/integraldisplay1
0/radicalbig
1−u2du=E/bracketleftBig/radicalbig
1−U2/bracketrightBig
where Uis uniform over (0, 1), we can generate nrandom numbers Uand use the
average value of/radicalbig
1−U2as our estimate of π/4. (Problem 10.14 shows that this
estimator has the same variance as the average of the nvalues,/radicalbig
1−V2.)
The preceding estimator of πcan be improved even further by noting that the
function g(u)=/radicalbig
1−u2,0…u…1, is a monotonically decreasing function of u,
and so the method of antithetic variables will reduce the variance of the estimator
ofE[/radicalbig
1−U2]. That is, rather than generating nrandom numbers and using the
average value of/radicalbig
1−U2as an estimator of π/4, we would obtain an improved
estimator by generating only n/2 random numbers Uand then using one-half the
average of/radicalbig
1−U2+/radicalbig
1−(1−U)2as the estimator of π/4.
The following table gives the estimates of πresulting from simulations, using
n=10, 000, based on the three estimators.
Method Estimate of π
Proportion of the random points that fall in the circle 3.1612
Average value of/radicalbig
1−U2 3.128448
Average value of/radicalbig
1−U2+/radicalbig
1−(1−U)2 3.139578
A further simulation using the ﬁnal approach and n=64, 000 yielded the estimate
3.143288. .
10.4.3 Control Variates
Again, suppose that we want to use simulation to estimate E[g(X)], where X=
(X1,...,Xn). But suppose now that for some function f, the expected value of f(X)
is known—say, it is E[f(X)]=μ. Then, for any constant a, we can also use
W=g(X)+a[f(X)−μ]
as an estimator of E[g(X)]. Now,
Var(W)=Var[g(X)]+a2Var[f(X)]+2aCov[g (X),f(X)] (4.1)
Simple calculus shows that the foregoing is minimized when
a=−Cov[ f(X),g(X)]
Var[f(X)](4.2)
and for this value of a,
Var(W)=Var[g(X)]−Cov[f (X),g(X)]2
Var[f(X)](4.3)
Unfortunately, neither Var[ f(X)] nor Cov[f (X)], g(X)] is usually known, so we can-
not in general obtain the foregoing reduction in variance. One approach in practice
is to use the simulated data to estimate these quantities. This approach usually yields
almost all of the theoretically possible reduction in variance.

<<<PAGE 443>>>

July 14, 2014 M10_ROSSS4772_09_SE_C10 page 430
430 Chapter 10 Simulation
Summary
LetFbe a continuous distribution function and Ua uni-
form (0, 1) random variable. Then the random variable
F−1(U)has distribution function F,w h e r eF−1(u)is that
value xsuch that F(x)=u. Applying this result, we can
use the values of uniform (0, 1) random variables, called
random numbers , to generate the values of other random
variables. This technique is called the inverse transform
method.
Another technique for generating random variables
i sb a s e do nt h erejection method. Suppose that we have
an efﬁcient procedure for generating a random variablefrom the density function gand that we desire to generate
a random variable having density function f. The rejection
method for accomplishing this starts by determining a con-stant csuch that
maxf(x)
g(x)…c
It then proceeds as follows:
1. Generate Yhaving density g.
2. Generate a random number U.3. If U…f(Y)/cg(Y),s e t X=Yand stop.
4. Return to step 1.
The number of passes through step 1 is a geometric ran-dom variable with mean c.
Standard normal random variables can be efﬁciently
simulated by the rejection method (with gbeing exponen-
tial with mean 1) or by the technique known as the polar
algorithm.
To estimate a quantity θ, one often generates the
values of a partial sequence of random variables whose
expected value is θ. The efﬁciency of this approach is
increased when these random variables have a small vari-ance. Three techniques that can often be used to specifyrandom variables with mean θand relatively small vari-
ances are
1. the use of antithetic variables,
2. the use of conditional expectations, and
3. the use of control variates.
Problems
10.1. The following algorithm will generate a random per-
mutation of the elements 1, 2, ...,n. It is somewhat faster
than the one presented in Example 1a but is such that no
position is ﬁxed until the algorithm ends. In this algorithm,P(i)can be interpreted as the element in position i.
Step 1. Set k=1.
Step 2. Set P(1)=1.
Step 3. If k=n, stop. Otherwise, let k=k+1.
Step 4. Generate a random number Uand let
P(k)=P([kU ]+1)
P([kU ]+1)=k
Go to step 3.
(a)Explain in words what the algorithm is doing.
(b)Show that at iteration k—that is, when the value of
P(k)is initially set— P(1),P(2),...,P(k)is a random per-
mutation of 1, 2, ...,k.
Hint : Use induction and argue that
P
k{i1,i2,...,ij−1,k,ij,...,ik−2,i}
=Pk−1{i1,i2,...,ij−1,i,ij,...,ik−2}1
k
=1
k!by the induction hypothesis10.2. Develop a technique for simulating a random vari-
able having density function
f(x)=/braceleftBigg
e2x−q<x<0
e−2x0<x<q
10.3. Give a technique for simulating a random variable
having the probability density function
f(x)=⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩1
2(x−2) 2…x…3
1
2/parenleftbigg
2−x
3/parenrightbigg
3<x…6
0 otherwise
10.4. Present a method for simulating a random variable
having distribution function
F(x)=⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩0 x…−3
1
2+x
6−3<x<0
1
2+x2
320<x…4
1 x>4

<<<PAGE 444>>>

July 14, 2014 M10_ROSSS4772_09_SE_C10 page 431
A First Course in Probability 431
10.5. Use the inverse transformation method to present
an approach for generating a random variable from the
Weibull distribution
F(t)=1−e−atβtÚ0
10.6. Give a method for simulating a random variable hav-
ing failure rate function
(a)λ(t)=c;
(b)λ(t)=ct;
(c)λ(t)=ct2;
(d)λ(t)=ct3.
10.7. LetFbe the distribution function
F(x)=xn0<x<1
(a)Give a method for simulating a random variable hav-
ing distribution Fthat uses only a single random number.
(b)Let U1,...,Unbe independent random numbers.
Show that
P{max(U1,...,Un)…x}=xn
(c)Use part (b) to give a second method of simulating a
random variable having distribution F.
10.8. Suppose it is relatively easy to simulate from Fifor
each i=1,...,n. How can we simulate from
(a)F(x)=n/producttext
i=1Fi(x)?
(b)F(x)=1−n/producttext
i=1[1−Fi(x)]?
10.9. Suppose we have a method for simulating random
variables from the distributions F1andF2. Explain how to
simulate from the distribution
F(x)=pF1(x)+(1−p)F 2(x) 0<p<1
Give a method for simulating from
F(x)=⎧
⎨
⎩1
3(1−e−3x)+2
3x0<x…1
1
3(1−e−3x)+2
3x>1
10.10. In Example 2c we simulated the absolute value of a
unit normal by using the rejection procedure on exponen-
tial random variables with rate 1. This raises the question
of whether we could obtain a more efﬁcient algorithm byusing a different exponential density—that is, we could usethe density g(x)=λe
−λx. Show that the mean number
of iterations needed in the rejection scheme is minimized
when λ=1.
10.11. Use the rejection method with g(x)=1, 0<x<1,
to determine an algorithm for simulating a random vari-
able having density function
f(x)=/braceleftBigg
60x3(1−x)20<x<1
0 otherwise
10.12. Explain how you could use random numbers to
approximate/integraltext1
0k(x)dx,w h e r ek(x) is an arbitrary func-
tion.
Hint :I fU is uniform on (0, 1), what is E[k(U)]?
10.13. Let(X,Y)be uniformly distributed in the circle of
radius 1 centered at the origin. Its joint density is thus
f(x,y)=1
π0…x2+y2…1
LetR=(X2+Y2)1/2and/H9258=tan−1(Y/X)denote
the polar coordinates of ( X,Y). Show that Rand/H9258are
independent, with R2being uniform on (0, 1) and /H9258being
uniform on (0, 2π) .
10.14. In Example 4a, we showed that
E[(1−V2)1/2]=E[(1−U2)1/2]=π
4
when Vis uniform (−1, 1) andUis uniform (0, 1). Now
show that
Var[(1−V2)1/2]=Var[(1−U2)1/2]
and ﬁnd their common value.
10.15. (a)Verify that the minimum of (4.1) occurs when a
is as given by (4.2).
(b)Verify that the minimum of (4.1) is given by (4.3).
10.16. LetXbe a random variable on (0, 1) whose density
isf(x). Show that we can estimate/integraltext1
0g(x)dxby simulat-
ingXand then taking g(X)/f(X)as our estimate. This
method, called importance sampling, tries to choose fsim-
ilar in shape to g,s ot h a tg (X)/f(X)has a small variance.
Self-Test Problems and Exercises
10.1. The random variable Xhas probability density
function
f(x)=Cex0<x<1
(a)Find the value of the constant C.
(b)Give a method for simulating such a random variable.10.2. Give an approach for simulating a random variable
having probability density function
f(x)=30(x2−2x3+x4)0<x<1
10.3. Give an efﬁcient algorithm to simulate the value of a
random variable with probability mass function
p1=.15 p2=.2p3=.35 p4=.30

<<<PAGE 445>>>

July 14, 2014 M10_ROSSS4772_09_SE_C10 page 432
432 Chapter 10 Simulation
10.4. IfXis a normal random variable with mean μand
variance σ2, deﬁne a random variable Ythat has the same
distribution as Xand is negatively correlated with it.
10.5. LetXandYbe independent exponential random
variables with mean 1.(a)Explain how we could use simulation to estimate
E[eXY].
(b)Show how to improve the estimation approach in part
(a) by using a control variate.
Reference
[1]Ross,S .M .Simulation. 5th ed. San Diego: Academic Press, Inc., 2012.

<<<PAGE 446>>>

Answers to Selected Problems
Chapter 1
1.67,600,000; 19,656,000 2.1296 4.24; 4 5.144;
18 6.2401 7.720; 72; 144; 72 8.120; 1260; 34,650
9.27,720 10.40,320; 10,080; 1152; 2880; 384 11.720; 72;
144 12.24,300,000; 17,100,720 13.190 14.2,598,960
16.42; 94 17.604,800 18.600 19.896; 1000; 910
20.36; 26 21.35 22.18 23.48 25.52!/( 13!)4
27.27,720 28.65,536; 2520 29.12,600; 945 30.564,480
31.165; 35 32.1287; 14,112 33.220; 572
Chapter 2
9.74 10..4; .1 11.70; 2 12..5; .32; 149/198
13.20,000; 12,000; 11,000; 68,000; 10,000 14.1.057
15..0020; .4226; .0475; .0211; .00024 17.9.10947 *10−6
18..048 19.5/18 20..9052 22.(n+1)/2n23.5/12
25..4 26..492929 28..0888; .2477; .1243; .2099
30.1/18; 1/6; 1/2 31.2/9; 1/9 33.70/323 34.1001;
120; 495 36..0045; .0588 37..0833; .5 38.4 39..48
40.1/64; 21/64; 36/64; 6/64 41..5177 44..3; .2; .1 46.5
48..01697 49..4329 50.2.6084 *10−652..09145;
.4268 53.12/35 54..0511 55..2198; .0342
Chapter 3
1.1/3 2.1/6; 1/5; 1/4; 1/3; 1/2; 1 3..339 5.6/91
6.1/2 7.2/3 8.1/2 9.7/11 10..22 11.1/17;
1/33 12..504; .3629 14.35/768; 210/768 15..4848
16..9835 17..0792; .264 18..331; .383; .286; .4862
19.44.29; 41.18 20..4; 1/26 21..496; 3/14; 9/62 22.5/9;
1/6; 5/54 23.4/9; 1/2 24.1/3; 1/2 26.20/21; 40/41
28.3/128; 29/1536 29..0893 30.7/12; 3/5 33..76,
49/76 34.27/31 35..62, 10/19 36.1/2 37.1/3; 1/5;
1 38.12/37 39.46/185 40.3/13; 5/13; 5/52; 15/52
41.43/459 42.1.03 percent; .3046 43.4/9 45.1/11
48.2/3 50..175; 38/165; 17/33 51..65; 56/65; 8/65; 1/65;
14/35; 12/35; 9/35 52..11; 16/89; 12/27; 3/5; 9/25 55.9
57.(c) 2/3 60.2/3; 1/3; 3/4 61.1/6; 3/20 65..4375
69.9; 9; 18; 110; 4; 4; 8; 120 all over 128 70.1/9; 1/18
71.38/64; 13/64; 13/64 73.1/16; 1/32; 5/16; 1/4; 31/32
74.9/19 75.3/4, 7/12 78.2p3(1−p)+2p(1 −p)3;
p2/(1−2p+2p2)79..5550 81..9530 83..5; .6; .8
84.9/19; 6/19; 4/19; 7/15; 53/165; 7/33 87.9/16 90.97/142;
15/26; 33/102
Chapter 4
1.p(4)=6/91; p(2)=8/91; p(1)=32/91; p(0)=1/91;
p(−1) =16/91; p(−2) =28/91 4.(a) 1/2; 5/18; 5/36; 5/84;
5/252; 1/252; 0; 0; 0; 0 5.n−2i;i=0,...,n 6.p(3)=
p(−3) =1/8; p(1)=p(−1) =3/8 12.p(4)=1/16;
p(3) =1/8; p(2) =1/16; p(0) =1/2; p(−i) =p(i);
p(0)=1 13.p(0)=.28;p(500) =.27,p(1000) =.315;p(1500) =.09;p(2000) =.045 14.p(0)=1/2;p(1)=1/6;
p(2) =1/12; p(3) =1/20; p(4) =1/5 17.1/4; 1/6;
1/12; 1/2 19.1/2; 1/10; 1/5; 1/10; 1/10 20..5918; no;
−.108 21.39.28; 37 24.p=11/18; maximum =23/72
25..46, 1.3 26.11/2; 17/5 27.A(p+1/10) 28.3/5
31.p∗32.11−10(.9)1033.3 35.−.067; 1.089
37.82.2; 84.5 39.3/8 40.11/243 42.2.8; 1.476 45.3
50.1/10; 1/10 51.e−.2;1−1.2e−.253.1−e−.6;
1−e−219.1856.253 57..5768; .6070 59..3935; .3033;
.0902 60..8886 61..4082 63..0821; .2424 65..3935;
.2293; .3935 66.2/(2n−1);2/(2n−2);e−167.2/n;
(2n−3)/(n −1)2;e−268.e−10e−570.p+(1−p)e−λt
71..1500; .1012 73.5.8125 74.32/243; 4864/6561;
160/729; 160/729 78.18(17)n−1/(35)n81.3/10; 5/6;
75/138 82..3439 83.1.5
Chapter 5
2.3.5e−5/23.no; no 4.1/2, .8999 5.1−(.01)1/5
6.4, 0,q 7.3/5; 6/5 8.2 10.2/3; 2/3 11.2/5
13.2/3; 1/3 15..7977; .6827; .3695; .9522; .1587
16.(.9938)1017..315; .136 18.22.66 19.14.56
20..9994; .75; .977 22..974 23..9253; .1767 26..0606;
.0525 28..8363 29..9993 32.e−1;e−1/234.e−1;1/3
38.3/5 40.1/y
Chapter 6
2.(a) 14/39; 10/39; 10/39; 5/39 (b) 84; 70; 70; 70; 40;
40; 40; 15 all divided by 429 3.15/26; 5/26; 5/26; 1/26
4.(a) 64/169; 40/169; 40/169; 25/169; 64/169 7.p(i,j)=
p2(1−p)i+j8.c=1/8; E[X]= 0 9.(12x2+
6x)/ 7;15/56; .8625; 5/7; 8/7 10.1/2; 1−e−a11..1458
12.39.3e−513.1/6; 1/2 15.π/4 16.n(1/2)n−1
17.1/3 18.7/9 19.–log(y),0 <y<1;1, 0<x<1;
1/2; 1/4 21.2/5; 2/5 22.no; 1/3 23.1/2; 2/3;
1/20; 1/18 25.e−1/i! 28.1
2e−t;1−3e−229..0326
30..3772; .2061 31..0829; .3766 32.5/16; .0228
33.e−2;1−3e−235.5/13; 8/13 36.1/6; 5/6; 1/4; 3/4
41.(y+1)2xe−x(y+1);xe−xy;e−x42.1/2+3y/( 4x)−
y3/(4x3)46.(1−2d/L)347..79297 48.1−e−5λa;
(1−e−λa)552.r/π 53.r56.(a)u/(ν+1)2
Chapter 7
1.52.5/12 2.324; 198.8 3.1/2; 1/4; 0 4.1/6; 1/4; 1/2
5.3/2 6.35 7..9; 4.9; 4.2 8.(1−(1−p)N)/p
10..6; 0 11.2(n−1)p(1 −p) 12.(3n2−n)/(4n −2),
3n2/(4n− 2) 14.m/(1−p) 15.1/2 18.4
21..9301; 87.5755 22.14.7 23.147/110 26.n/(n+1);
1/(n+1) 29.437
35; 12; 4;123
3531.175/6 33.14, 45
34.20/19; 360/361 35.21.2; 18.929; 49.214 36.−n/36
433

<<<PAGE 447>>>

434 Answers to Selected Problems
37.0 38.1/8 41.6; 112/33 42.100/19; 16,200/6137;
10/19; 3240/6137 45.1/2; 0 47.1/(n−1) 48.6; 7;
5.8192 49.6.07 50.2y251.y3/4 53.12 54.8
56.N(1−e−10/ N) 57.12.5 63.−96/145 65.4.2;
5.16 66.218 67.x[1+(2p−1)2]n69.1/2; 1/16; 2/81
70.1/2, 1/3 72.1/i; [i(i+1)]−1;q 73.μ;1+σ2; yes;σ2
79..176; .141
Chapter 8
1.Ú19/20 2.15/17; Ú3/4; Ú10 3.Ú3 4.…4/3; .8428
5..1416 6..9431 7..3085 8..6932 9.(327)210.117 11.Ú.057 13..0162; .0003; .2514; .2514
14.nÚ23 16..013; .018; .691 18.….2 23..769; .357;
.4267; .1093; .112184 24.answer is (a)
Chapter 9
1.1/9; 5/9 3..9735; .9098; .7358; .5578 10.(b)1/6
14.2.585; .5417; 3.1267 15.5.5098

<<<PAGE 448>>>

July 14, 2014 Z02_ROSSS4772_09_SE_BM page 435
Solutions to Self-Test Problems and Exercises
Chapter 1
1.1. (a)There are 4! different orderings of the letters C, D,
E, F. For each of these orderings, we can obtain an order-
ing with A and B next to each other by inserting A and B,either in the order A, B or in the order B, A, in any of 5
places, namely, either before the ﬁrst letter of the permuta-
tion of C, D, E, F, or between the ﬁrst and second, and so on.Hence, there are 2 ·5·4!=240 arrangements. Another way
of solving this problem is to imagine that B is glued to theback of A. Then there are 5! orderings in which A is immedi-ately before B. Since there are also 5! orderings in which B is
immediately before A, we again obtain a total of 2 ·5!=240
different arrangements.
(b)There are 6! =720 possible arrangements, and since
there are as many with A before B as with B before A, there
are 360 arrangements.
(c)Of the 720 possible arrangements, there are as many that
have A before B before C as have any of the 3! possible
orderings of A, B, and C. Hence, there are 720 /6=120 pos-
sible orderings.
(d)Of the 360 arrangements that have A before B, half will
have C before D and half D before C. Hence, there are 180
arrangements having A before B and C before D.
(e)Gluing B to the back of A and D to the back of C yields
4!=24 different orderings in which B immediately follows
A and D immediately follows C. Since the order of A and
B and of C and D can be reversed, there are 4 ·24=96
different arrangements.
(f)There are 5! orderings in which E is last. Hence, there are
6!−5!=600 orderings in which E is not last.
1.2. 3! 4! 3! 3!, since there are 3! possible orderings of coun-
tries and then the countrymen must be ordered.
1.3. (a)10·9·8=720
(b)8·7·6+2·3·8·7=672. The result of part (b) follows
because there are 8 ·7·6 choices not including A or B and
there are 3 ·8·7 choices in which a speciﬁed one of A and
B, but not the other, serves. The latter follows because the
serving member of the pair can be assigned to any of the 3
ofﬁces, the next position can then be ﬁlled by any of the other8 people, and the ﬁnal position by any of the remaining 7.
(c)8·7·6+3·2·8=384.
(d)3·9·8=216.
(e)9·8·7+9·8=576.
1.4. (a)⎫parenleftBigg
10
7⎫parenrightBigg
(b)⎫parenleftBigg
5
3⎫parenrightBigg⎫parenleftBigg
54⎫parenrightBigg
+⎫parenleftBigg
54⎫parenrightBigg⎫parenleftBigg
53⎫parenrightBigg
+⎫parenleftBigg
55⎫parenrightBigg⎫parenleftBigg
52⎫parenrightBigg
1.5.⎫parenleftBigg
7
3, 2, 2⎫parenrightBigg
=2101.6. There are⎫parenleftBigg
73⎫parenrightBigg
=35 choices of the three places for the
letters. For each choice, there are (26)
3(10)4different license
plates. Hence, altogether there are 35 ·(26)3·(10)4different
plates.
1.7. Any choice of rof the nitems is equivalent to a choice
ofn−r, namely, those items not selected.
1.8. (a)10·9·9···9=10·9n−1
(b)⎫parenleftBigg
n
i⎫parenrightBigg
9n−i,s i n c et h e r ea r e⎫parenleftBigg
n
i⎫parenrightBigg
choices of the iplaces to
put the zeroes and then each of the other n−ipositions can
be any of the digits 1, ...,9 .
1.9. (a)⎫parenleftBigg
3n
3⎫parenrightBigg
(b)3⎫parenleftBigg
n
3⎫parenrightBigg
(c)⎫parenleftBigg
31⎫parenrightBigg⎫parenleftBigg
21⎫parenrightBigg⎫parenleftBigg
n
2⎫parenrightBigg⎫parenleftBigg
n
1⎫parenrightBigg
=3n
2(n−1)
(d)n3
(e)⎫parenleftBigg
3n
3⎫parenrightBigg
=3⎫parenleftBigg
n
3⎫parenrightBigg
+3n2(n−1)+n3
1.10. There are 9 ·8·7·6·5 numbers in which no digit is
repeated. There are⎫parenleftBigg
52⎫parenrightBigg
·8·7·6 numbers in which only
one speciﬁed digit appears twice, so there are 9⎫parenleftBigg
52⎫parenrightBigg
·8·7·6
numbers in which only a single digit appears twice. There are
7·
5!
2!2!numbers in which two speciﬁed digits appear twice,
so there are⎫parenleftBigg
9
2⎫parenrightBigg
7·5!
2!2!numbers in which two digits appear
twice. Thus, the answer is
9·8·7·6·5+9⎫parenleftBigg
52⎫parenrightBigg
·8·7·6+⎫parenleftBigg
92⎫parenrightBigg
7·5!
2!2!
1.11. (a)We can regard this as a seven-stage experiment.
First choose the 6 married couples that have a representativein the group, and then select one of the members of each of
these couples. By the generalized basic principle of counting,
there are⎫parenleftbig
10
6⎫parenrightbig
26different choices.
(b)First select the 6 married couples that have a representa-
tive in the group, and then select the 3 of those couples that
are to contribute a man. Hence, there are⎫parenleftbig10
6⎫parenrightbig⎫parenleftbig6
3⎫parenrightbig
=10!
4!3!3!
different choices. Another way to solve this is to ﬁrst select
3 men and then select 3 women not related to the selected
men. This shows that there are⎫parenleftbig10
3⎫parenrightbig⎫parenleftbig7
3⎫parenrightbig
=10!
3!3!4!different
choices.
435

<<<PAGE 449>>>

July 14, 2014 Z02_ROSSS4772_09_SE_BM page 436
436 Solutions to Self-Test Problems and Exercises
1.12.⎫parenleftBigg
8
3⎫parenrightBigg⎫parenleftBigg
73⎫parenrightBigg
+⎫parenleftBigg
84⎫parenrightBigg⎫parenleftBigg
72⎫parenrightBigg
=3430. The ﬁrst term gives the
number of committees that have 3 women and 3 men; thesecond gives the number that have 4 women and 2 men.
1.13. (number of solutions of x1+ ··· + x5=4)(number
of solutions of x1+ ··· + x5=5)(number of solutions of
x1+ ··· + x5=6)=⎫parenleftBigg
8
4⎫parenrightBigg⎫parenleftBigg
94⎫parenrightBigg⎫parenleftBigg
10
4⎫parenrightBigg
.
1.14. Since there are⎫parenleftBigg
j−1
n−1⎫parenrightBigg
positive vectors whose sum
isj, there must bek⎫summationtext
j=n⎫parenleftBigg
j−1
n−1⎫parenrightBigg
such vectors. But⎫parenleftBigg
j−1
n−1⎫parenrightBigg
is the number of subsets of size nfrom the set of numbers
{1,...,k}in which jis the largest element in the subset. Con-
sequently,k⎫summationtext
j=n⎫parenleftBigg
j−1
n−1⎫parenrightBigg
is just the total number of subsets of
sizenfrom a set of size k, showing that the preceding answer
is equal to⎫parenleftBigg
kn⎫parenrightBigg
.
1.15. Let us ﬁrst determine the number of different results in
which kpeople pass. Because there are⎫parenleftBigg
nk⎫parenrightBigg
different groups
of size kandk! possible orderings of their scores, it follows
that there are⎫parenleftBigg
n
k⎫parenrightBigg
k! possible results in which kpeople pass.
Consequently, there are n⎫summationtext
k=0⎫parenleftBigg
nk⎫parenrightBigg
k! possible results.
1.16. The number of subsets of size 4 is⎫parenleftbig20
4⎫parenrightbig
=4845. Because
the number of these that contain none of the ﬁrst ﬁve ele-
ments is⎫parenleftbig15
4⎫parenrightbig
=1365, the number that contain at least one is
3480. Another way to solve this problem is to note that there
are⎫parenleftbig5
i⎫parenrightbig⎫parenleftbig15
4−i⎫parenrightbig
that contain exactly iof the ﬁrst ﬁve elements
and sum this for i=1, 2, 3, 4.
1.17. Multiplying both sides by 2, we must show that
n(n−1)=k(k−1)+2k(n−k)+(n−k)(n−k−1)
This follows because the right side is equal to
k2(1−2+1)+k(−1 +2n−n−n+1)+n(n−1)
For a combinatorial argument, consider a group of nitems
and a subgroup of kof the nitems. Then⎫parenleftbigk
2⎫parenrightbig
is the number
of subsets of size 2 that contain 2 items from the subgroup ofsizek,k(n−k)is the number that contain 1 item from the
subgroup, and⎫parenleftbig
n−k
2⎫parenrightbig
is the number that contain 0 items from
the subgroup. Adding these terms gives the total number ofsubgroups of size 2, namely,⎫parenleftbig
n
2⎫parenrightbig
.
1.18. There are 3 choices that can be made from families
consisting of a single parent and 1 child; there are 3 ·1·2=6
choices that can be made from families consisting of a singleparent and 2 children; there are 5 ·2·1=10 choices that
can be made from families consisting of 2 parents and a sin-
gle child; there are 7 ·2·2=28 choices that can be made
from families consisting of 2 parents and 2 children; there are
6·2·3=36 choices that can be made from families consist-
ing of 2 parents and 3 children. Hence, there are 83 possible
choices.
1.19. First choose the 3 positions for the digits, and then put
in the letters and digits. Thus, there are⎫parenleftbig8
3⎫parenrightbig
·26·25·24·
23·22·10·9·8 different plates. If the digits must be
consecutive, then there are 6 possible positions for the digits,
showing that there are now 6 ·26·25·24·23·22·10·9·8
different plates.
1.20. There are rndifferent nletter sequences that can be
formed using the ﬁrst rletters of the alphabet. For given
nonnegative integers x1,...,xrsuch that⎫summationtextn
i=1xi=r,t h e
number of the different sequences that use letter iexactly xi
times for each i=1,...,n, is the number of permutations of
nvalues, of which xiare equal to ifor each i=1,...,r;which
is equal ton!
x1!···xr!. As each nletter sequence is of exactly one
of the preceding types, the result follows.
Chapter 2
2.1. (a)2·3·4=24
(b)2·3=6
(c)3·4=12
(d)AB={(c,p a s t a , i),(c,r i c e ,i), (c, potatoes, i)}
(e)8
(f)ABC ={(c,r i c e , i)}
2.2. LetAbe the event that a suit is purchased, Bbe the
event that a shirt is purchased, and Cbe the event that a tie
is purchased. Then
P(A∪B∪C)=.22+.30+.28−.11−.14−.10+.06=.51
(a)1−.51=.49
(b)The probability that two or more items are purchased is
P(AB∪AC∪BC)=.11+.14+.10−.06−.06
−.06+.06=.23
Hence, the probability that exactly 1 item is purchased is
.51−.23=.28.
2.3. By symmetry, the 14th card is equally likely to be any
of the 52 cards; thus, the probability is 4/52. A more formal
argument is to count the number of the 52! outcomes for
w h i c ht h e1 4 t hc a r di sa na c e .T h i sy i e l d s
p=4·51·50···2·1
(52)!=4
52

<<<PAGE 450>>>

July 14, 2014 Z02_ROSSS4772_09_SE_BM page 437
Solutions to Self-Test Problems and Exercises 437
Letting Abe the event that the ﬁrst ace occurs on the 14th
card, we have
P(A)=48·47···36 ·4
52·51···40 ·39=.0312
2.4. LetDdenote the event that the minimum temperature
is 70 degrees. Then
P(A∪B)=P(A)+P(B)−P(AB) =.7−P(AB)
P(C∪D)=P(C)+P(D)−P(CD) =.2+P(D)−P(DC)
Since A∪B=C∪DandAB=CD, subtracting one of
the preceding equations from the other yields
0=.5−P(D)
orP(D)=.5.
2.5. (a)52·48·44·40
52·51·50·49=.6761
(b)52·39·26·13
52·51·50·49=.1055
2.6. LetRbe the event that both balls are red, and let Bbe
the event that both are black. Then
P(R∪B)=P(R)+P(B)=3·4
6·10+3·6
6·10=1/2
2.7. (a)1⎫parenleftBigg
40
8⎫parenrightBigg=1.3*10−8
(b)⎫parenleftBigg
8
7⎫parenrightBigg⎫parenleftBigg
32
1⎫parenrightBigg
⎫parenleftBigg
40
8⎫parenrightBigg=3.3*10−6
(c)⎫parenleftBigg
86⎫parenrightBigg⎫parenleftBigg
32
2⎫parenrightBigg
⎫parenleftBigg
40
8⎫parenrightBigg+1.3*10−8+3.3*10−6=1.8*10−4
2.8. (a)3·4·4·3⎫parenleftBigg
14
4⎫parenrightBigg =.1439
(b)⎫parenleftBigg
42⎫parenrightBigg⎫parenleftBigg
42⎫parenrightBigg
⎫parenleftBigg
14
4⎫parenrightBigg=.0360
(c)⎫parenleftBigg
84⎫parenrightBigg
⎫parenleftBigg
14
4⎫parenrightBigg=.0699
2.9. LetS=n⎫uniontext
i=1Ai, and consider the experiment of ran-
domly choosing an element of S.T h e nP (A)=N(A)/N(S),
and the results follow from Propositions 4.3 and 4.4.
2.10. Since there are 5! =120 outcomes in which the
position of horse number 1 is speciﬁed, it follows thatN(A)=360. Similarly, N(B)=120, and N(AB) =2·
4!=48. Hence, from Self-Test Problem 2.9, we obtain
N(A∪B)=432.
2.11. One way to solve this problem is to start with the com-
plementary probability that at least one suit does not appear.LetA
i,i=1, 2, 3, 4, be the event that no cards from suit i
appear. Then
P⎛
⎝4⎫uniondisplay
i=1Ai⎞⎠=⎫summationdisplay
iP(Ai)−⎫summationdisplay
j⎫summationdisplay
i:i<jP(AiAj)
+ ··· − P(A1A2A3A4)
=4⎫parenleftBigg
39
5⎫parenrightBigg
⎫parenleftBigg
52
5⎫parenrightBigg−⎫parenleftBigg
4
2⎫parenrightBigg⎫parenleftBigg
26
5⎫parenrightBigg
⎫parenleftBigg
52
5⎫parenrightBigg+⎫parenleftBigg
43⎫parenrightBigg⎫parenleftBigg
13
5⎫parenrightBigg
⎫parenleftBigg
52
5⎫parenrightBigg
=4⎫parenleftBigg
39
5⎫parenrightBigg
⎫parenleftBigg
52
5⎫parenrightBigg−6⎫parenleftBigg
26
5⎫parenrightBigg
⎫parenleftBigg
52
5⎫parenrightBigg+4⎫parenleftBigg
13
5⎫parenrightBigg
⎫parenleftBigg
52
5⎫parenrightBigg
The desired probability is then 1 minus the preceding.
Another way to solve is to let A be the event that all 4 suits
are represented, and then use
P(A)=P(n,n,n,n,o)+P(n,n,n,o,n)+P(n,n,o,n,n)
+P(n,o,n,n,n)
where P(n,n,n,o,n), for instance, is the probability that the
ﬁrst card is from a new suit, the second is from a new suit, thethird is from a new suit, the fourth is from an old suit (that is,
one which has already appeared) and the ﬁfth is from a new
suit. This gives
P(A)=52·39·26·13·48+52·39·26·36·13
52·51·50·49·48
+52·39·24·26·13+52·12·39·26·13
52·51·50·49·48
=52·39·26·13(48 +36+24+12)
52·51·50·49·48
=.2637
2.12. There are (10)!/25different divisions of the 10 players
into a ﬁrst roommate pair, a second roommate pair, and so
on. Hence, there are (10)!/( 5!25)divisions into 5 roommate
pairs. There are⎫parenleftBigg
6
2⎫parenrightBigg⎫parenleftBigg
42⎫parenrightBigg
ways of choosing the front-
court and backcourt players to be in the mixed roommate
pairs and then 2 ways of pairing them up. As there is then1 way to pair up the remaining two backcourt players and
4!/(2!2
2)=3 ways of making two roommate pairs from
the remaining four frontcourt players, the desired probabil-
ity is

<<<PAGE 451>>>

July 14, 2014 Z02_ROSSS4772_09_SE_BM page 438
438 Solutions to Self-Test Problems and Exercises
P{2 mixed pairs}=⎫parenleftBigg
6
2⎫parenrightBigg⎫parenleftBigg
42⎫parenrightBigg
(2)(3)
(10)!/( 5!25)=.5714
2.13. LetRdenote the event that letter Ris repeated; simi-
larly, deﬁne the events EandV.T h e n
P{same letter }=P(R)+P(E)+P(V)
=2
71
8+3
71
8+1
71
8=3
28
2.14. LetB1=A1,Bi=Ai⎛
⎝i−1⎫uniontext
j=1Aj⎞⎠c
,i>1. Then
P⎛⎝q⎫uniondisplay
i=1Ai⎞⎠=P⎛⎝
q⎫uniondisplay
i=1Bi⎞⎠
=q⎫summationdisplay
i=1P(Bi)
…q⎫summationdisplay
i=1P(Ai)
where the ﬁnal equality uses the fact that the Biare mutually
exclusive. The inequality then follows, since Bi(Ai.
2.15.
P⎛⎝q⎫intersectiondisplay
i=1Ai⎞⎠=1−P⎛
⎜⎝⎛
⎝q⎫intersectiondisplay
i=1Ai⎞⎠c⎞
⎟⎠
=1−P⎛
⎝q⎫uniondisplay
i=1Ac
i⎞
⎠
Ú1−q⎫summationdisplay
i=1P(Ac
i)
=1
2.16. The number of partitions for which {1}is a subset is
equal to the number of partitions of the remaining n−1 ele-
ments into k−1 nonempty subsets, namely, Tk−1(n−1).
Because there are Tk(n−1)partitions of {2,...,n−1}into
knonempty subsets and then a choice of kof them in which
to place element 1, it follows that there are kTk(n−1)par-
titions for which {1}is not a subset. Hence, the result follows.
2.17. LetR,W,Bdenote, respectively, the events that there
are no red, no white, and no blue balls chosen. Then
P(R∪W∪B)=P(R)+P(W)+P(B)−P(RW)
−P(RB) −P(WB) +P(RWB )=⎫parenleftBigg
13
5⎫parenrightBigg
⎫parenleftBigg
18
5⎫parenrightBigg+⎫parenleftBigg
12
5⎫parenrightBigg
⎫parenleftBigg
18
5⎫parenrightBigg+⎫parenleftBigg
11
5⎫parenrightBigg
⎫parenleftBigg
18
5⎫parenrightBigg−⎫parenleftBigg
7
5⎫parenrightBigg
⎫parenleftBigg
18
5⎫parenrightBigg
−⎫parenleftBigg
65⎫parenrightBigg
⎫parenleftBigg
18
5⎫parenrightBigg−⎫parenleftBigg
55⎫parenrightBigg
⎫parenleftBigg
18
5⎫parenrightBigg
L0.2933
Thus, the probability that all colors appear in the chosen sub-
set is approximately 1 −0.2933 =0.7067.
2.18. (a)8·7·6·5·4
17·16·15·14·13=2
221
(b)Because there are 9 nonblue balls, the probability is
9·8·7·6·5
17·16·15·14·13=9
442.
(c)Because there are 3! possible orderings of the different
colors and all possibilities for the ﬁnal 3 balls are equally
likely, the probability is3!·4·8·5
17·16·15=4
17.
(d)The probability that the red balls are in a speciﬁed 4 spots
is4·3·2·1
17·16·15·14.Because there are 14 possible locations of
the red balls where they are all together, the probability is
14·4·3·2·1
17·16·15·14=1
170.
2.19. (a)The probability that the 10 cards consist of 4 spades,
3 hearts, 2 diamonds, and 1 club is⎫parenleftBig
13
4⎫parenrightBig⎫parenleftBig
13
3⎫parenrightBig⎫parenleftBig
13
2⎫parenrightBig⎫parenleftBig
13
1⎫parenrightBig
⎫parenleftBig
52
10⎫parenrightBig .
Because there are 4! possible choices of the suits to have
4, 3, 2, and 1 cards, respectively, it follows that the probability
is24⎫parenleftBig
13
4⎫parenrightBig⎫parenleftBig
13
3⎫parenrightBig⎫parenleftBig
13
2⎫parenrightBig⎫parenleftBig
13
1⎫parenrightBig
⎫parenleftBig
52
10⎫parenrightBig .
(b)Because there are⎫parenleftBig4
2⎫parenrightBig
=6 choices of the two suits that
are to have 3 cards and then 2 choices for the suit to have 4
cards, the probability is12⎫parenleftBig
13
3⎫parenrightBig⎫parenleftBig
13
3⎫parenrightBig⎫parenleftBig
13
4⎫parenrightBig
⎫parenleftBig
52
10⎫parenrightBig .
2.20. All the red balls are removed before all the blue ones
if and only if the very last ball removed is blue. Because all
30 balls are equally likely to be the last ball removed, theprobability is 10 /30.
Chapter 3
3.1. (a)P(no aces) =⎫parenleftBigg
35
13⎫parenrightBigg⎫slashBig⎫parenleftBigg
3913⎫parenrightBigg
(b)1−P(no aces) −4⎫parenleftBigg
35
12⎫parenrightBigg
⎫parenleftBigg
39
13⎫parenrightBigg

<<<PAGE 452>>>

July 14, 2014 Z02_ROSSS4772_09_SE_BM page 439
Solutions to Self-Test Problems and Exercises 439
(c)P(iaces)=⎫parenleftBigg
3
i⎫parenrightBigg⎫parenleftBigg
3613−i⎫parenrightBigg
⎫parenleftBigg
39
13⎫parenrightBigg
3.2. LetLidenote the event that the life of the battery is
greater than 10, 000 *imiles.
(a)P(L2|L1)=P(L1L2)/P(L1)=P(L2)/P(L1)=1/2
(b)P(L3|L1)=P(L1L3)/P(L1)=P(L3)/P(L1)=1/8
3.3. Put 1 white and 0 black balls in urn one, and the remain-
ing 9 white and 10 black balls in urn two.
3.4. LetTbe the event that the transferred ball is white,
and let Wbe the event that a white ball is drawn from urn
B.T h e n
P(T|W)=P(W|T)P(T)
P(W|T)P(T)+P(W|Tc)P(Tc)
=(2/7)(2/3)
(2/7)(2/3) +(1/7)(1/3)=4/5
3.5. (a)P(E|E∪F)=P(E(E∪F))
P(E∪F)=P(E)
P(E)+P(F)
since E(E∪F)=EandP(E∪F)=P(E)+P(F)because
EandFare mutually exclusive.
(b)P(Ej|∪q
i=1Ei)=P(Ej(∪q
i=1Ei))
P(∪qi=1Ei))=P(Ej)⎫summationtextqi=1P(Ei)
3.6. LetBidenote the event that ball iis black, and let
Ri=Bc
i.T h e n
P(B1|R2)=P(R2|B1)P(B1)
P(R2|B1)P(B1)+P(R2|R1)P(R1)
=[r/[(b+r+c)][b/(b +r)]
[r/(b+r+c)][b/(b +r)]+[(r+c)/(b +r+c)][r/(b+r)]
=b
b+r+c
3.7. LetBdenote the event that both cards are aces.
(a)P{B|yes to ace of spades }=P{B, yes to ace of spades }
P{yes to ace of spades }
=⎫parenleftbigg
1
1⎫parenrightbigg⎫parenleftbigg
31⎫parenrightbigg
⎫parenleftbigg
522⎫parenrightbigg⎫slashBig⎫parenleftbigg
11⎫parenrightbigg⎫parenleftbigg
511⎫parenrightbigg
⎫parenleftbigg
522⎫parenrightbigg
=3/51
(b)Since the second card is equally likely to be any of the
remaining 51, of which 3 are aces, we see that the answer in
this situation is also 3/51.
(c)Because we can always interchange which card is consid-
ered ﬁrst and which is considered second, the result should
be the same as in part (b). A more formal argument is as
follows:
P{B|second is ace}=P{B, second is ace}
P{second is ace }=P(B)
P(B)+P{ﬁrst is not ace, second is ace }
=(4/52)(3/51)
(4/52)(3/51) +(48/52)(4/51)
=3/51
(d) P{B|at least one}=P(B)
P{at least one}
=(4/52)(3/51)
1−(48/52)(47/51)
=1/33
3.8.P(H|E)
P(G|E)=P(HE)
P(GE)=P(H)P(E|H)
P(G)P(E|G)
Hypothesis His 1.5 times as likely.
3.9. LetAdenote the event that the plant is alive and let W
be the event that it was watered.
(a) P(A)=P(A|W)P(W)+P(A|Wc)P(Wc)
=(.85)(.9) +(.2)(.1) =.785
(b) P(Wc|Ac)=P(Ac|Wc)P(Wc)
P(Ac)
=(.8)(.1)
.215=16
43
3.10. (a)LetRbe the event that at least one red ball is cho-
sen. Then
P(R)=1−P(Rc)=1−⎫parenleftBig22
6⎫parenrightBig
⎫parenleftBig30
6⎫parenrightBig
(b)LetG2be the event there are exactly 2 green balls cho-
sen. Working with the reduced sample space yields
P(G2|Rc)=⎫parenleftBig10
2⎫parenrightBig⎫parenleftBig12
4⎫parenrightBig
⎫parenleftBig22
6⎫parenrightBig
3.11. LetWbe the event that the battery works, and let C
andDdenote the events that the battery is a type Cand that
it is a type Dbattery, respectively.
(a)P(W)=P(W|C)P(C)+P(W|D)P(D)=.7(8/14) +
.4(6/14) =4/7
(b)P(C|Wc)=P(CWc)
P(Wc)=P(Wc|C)P(C)
3/7=.3(8/ 14)
3/7=.4
3.12. LetLibe the event that Maria likes book i,i=1, 2.
Then
P(L2|Lc
1)=P(Lc
1L2)
P(Lc
1)=P(Lc
1L2)
.4
Using that L2is the union of the mutually exclusive events
L1L2andLc
1L2, we see that
.5=P(L2)=P(L1L2)+P(Lc
1L2)=.4+P(Lc1L2)
Thus,
P⎫parenleftBig
L2|Lc1⎫parenrightBig
=.1
.4=.25

<<<PAGE 453>>>

July 14, 2014 Z02_ROSSS4772_09_SE_BM page 440
440 Solutions to Self-Test Problems and Exercises
3.13. (a)This is the probability that the last ball removed is
blue. Because each of the 30 balls is equally likely to be the
last one removed, the probability is 1 /3.
(b)This is the probability that the last red or blue ball to be
removed is a blue ball. Because it is equally likely to be any
of the 30 red or blue balls, the probability that it is blue is 1 /3.
(c)LetB1,R2,G3denote, respectively, the events that the
ﬁrst color removed is blue, the second is red, and the third is
green. Then
P(B1R2G3)=P(G3)P(R2|G3)P(B1|R2G3)=8
3820
30=8
57
where P(G3)is just the probability that the very last ball is
green and P(R2|G3)is computed by noting that given that
the last ball is green, each of the 20 red and 10 blue ballsis equally likely to be the last of that group to be removed,
so the probability that it is one of the red balls is 20 /30. (Of
course, P(B
1|R2G3)=1.)
(d)P(B1)=P(B1G2R3)+P(B1R2G3)=20
388
18+8
57
=64
1713.14. LetHbe the event that the coin lands heads, let Thbe
the event that Bis told that the coin landed heads, let Fbe
the event that Aforgets the result of the toss, and let Cbe the
event that Bis told the correct result. Then
(a) P(Th)=P(Th|F)P(F)+P(Th|Fc)P(Fc)
=(.5)(.4) +P(H)(.6)
=.68
(b) P(C)=P(C|F)P(F)+P(C|Fc)P(Fc)
=(.5)(.4) +1(.6)=.80
(c)P(H|Th)=P(HT h)
P(Th)
Now,
P(HT h)=P(HT h|F)P(F)+P(HT h|Fc)P(Fc)
=P(H|F)P(Th|HF)P(F)+P(H)P(Fc)
=(.8)(.5)(.4) +(.8)(.6) =.64
giving the result P(H|Th)=.64/.68 =16/17.
3.15. Since the black rat has a brown sibling, we can con-
clude that both of its parents have one black and onebrown gene.
(a)P(2b l a c k |at least one) =P(2)
P(at least one )=1/4
3/4=1
3
(b)LetFbe the event that all 5 offspring are black, let B2be
the event that the black rat has 2 black genes, and let B1be
the event that it has 1 black and 1 brown gene. Then
P(B2|F)=P(F|B2)P(B2)
P(F|B2)P(B2)+P(F|B1)P(B1)
=(1)(1/3)
(1)(1/3) +(1/2)5(2/3)=16
17
3.16. LetFbe the event that a current ﬂows from AtoB,
and let Cibe the event that relay icloses. Then
P(F)=P(F|C1)p1+P(F|Cc
1)(1−p1)Now,
P(F|C1)=P(C4∪C2C5∪C3C5)
=p4+p2p5+p3p5−p4p2p5
−p4p3p5−p2p3p5+p4p2p5p3
Also,
P(F|Cc
1)=P(C2C5∪C2C3C4)
=p2p5+p2p3p4−p2p3p4p5
Hence, for part (a), we obtain
P(F)=p1(p4+p2p5+p3p5−p4p2p5
−p4p3p5−p2p3p5+p4p2p5p3)
+(1−p1)p2(p5+p3p4−p3p4p5)
For part (b), let qi=1−pi.T h e n
P(C3|F)=P(F|C3)P(C3)/P(F)
=p3[1−P(Cc
1Cc
2∪Cc
4Cc
5)]/P(F)
=p3(1−q1q2−q4q5+q1q2q4q5)/P(F)
3.17. LetAbe the event that component 1 is working, and
letFbe the event that the system functions.
(a)P(A|F)=P(AF)
P(F)=P(A)
P(F)=1/2
1−(1/ 2)2=2
3
where P(F)was computed by noting that it is equal to 1
minus the probability that components 1 and 2 are both
failed.
(b)P(A|F)=P(AF)
P(F)=P(F|A)P(A)
P(F)=(3/4)(1/ 2)
(1/2)3+3(1/ 2)3=3
4
where P(F)was computed by noting that it is equal to the
probability that all 3 components work plus the three proba-
bilities relating to exactly 2 of the components working.
3.18. If we assume that the outcomes of the successive spins
are independent, then the conditional probability of the nextoutcome is unchanged by the result that the previous 10 spins
landed on black.
3.19. Condition on the outcome of the initial tosses:
P(Aodd)=P1(1−P2)(1−P3)+(1−P1)P2P3
+P1P2P3P(Aodd)
+(1−P1)(1−P2)(1−P3)P(Aodd)
so,
P(Aodd)=P1(1−P2)(1−P3)+(1−P1)P2P3
P1+P2+P3−P1P2−P1P3−P2P3
3.20. LetAandBbe the events that the ﬁrst trial is larger
and that the second is larger, respectively. Also, let Ebe the
event that the results of the trials are equal. Then
1=P(A)+P(B)+P(E)
But, by symmetry, P(A)=P(B): thus,
P(B)=1−P(E)
2=1−n⎫summationdisplay
i=1p2
i
2

<<<PAGE 454>>>

July 14, 2014 Z02_ROSSS4772_09_SE_BM page 441
Solutions to Self-Test Problems and Exercises 441
Another way of solving the problem is to note that
P(B)=⎫summationdisplay
i⎫summationdisplay
j>iP{ﬁrst trial results in i, second trial results in j}
=⎫summationdisplay
i⎫summationdisplay
j>ipipj
To see that the two expressions derived for P(B)are equal,
observe that
1=n⎫summationdisplay
i=1pin⎫summationdisplay
j=1pj
=⎫summationdisplay
i⎫summationdisplay
jpipj
=⎫summationdisplay
ip2
i+⎫summationdisplay
i⎫summationdisplay
jZipipj
=⎫summationdisplay
ip2
i+2⎫summationdisplay
i⎫summationdisplay
j>ipipj
3.21. LetE={Agets more heads than B};t h e n
P(E)=P(E|Aleads after both ﬂip n)P(Aleads after both ﬂip n)
+P(E|even after both ﬂip n)P(even after both ﬂip n)
+P(E|Bleads after both ﬂip n)P(Bleads after both ﬂip n)
=P(Aleads) +1
2P(even)
Now, by symmetry,
P(Aleads) =P(Bleads)
=1−P(even)
2
Hence,
P(E)=1
2
3.22. (a)Not true: In rolling 2 dice, let E={ sum is 7 },
F={1st die does not land on 4 },a n d G={2nd die does not
land on 3 }.T h e n
P(E|F∪G)=P{7, not (4, 3)}
P{not(4, 3)}=5/36
35/36=5/35ZP(E)
(b) P(E(F∪G))=P(EF∪EG)
=P(EF)+P(EG) since EFG=∅
=P(E)[P(F)+P(G)]
=P(E)P(F∪G) since FG=∅
(c)P(G|EF )=P(EFG )
P(EF)
=P(E)P(FG)
P(EF)since Eis independent of FG
=P(E)P(F)P(G)
P(E)P(F)by independence
=P(G).3.23. (a)necessarily false; if they were mutually exclusive,
then we would have
0=P(AB)ZP(A)P(B)
(b)necessarily false; if they were independent, then we
would have
P(AB) =P(A)P(B)> 0
(c)necessarily false; if they were mutually exclusive, then we
would have
P(A∪B)=P(A)+P(B)=1.2
(d)possibly true
3.24. The probabilities in parts (a), (b), and (c) are .5,(.8)3=
.512, and (.9)7L.4783, respectively.
3.25. LetDi,i=1, 2, denote the event that radio iis defec-
tive. Also, let AandBbe the events that the radios were
produced at factory Aand at factory B, respectively. Then
P(D2|D1)=P(D1D2)
P(D1)
=P(D1D2|A)P(A)+P(D1D2|B)P(B)
P(D1|A)P(A)+P(D1|B)P(B)
=(.05)2(1/2) +(.01)2(1/2)
(.05)(1/2) +(.01)(1/2)
=13/300
3.26. We are given that P(AB) =P(B)and must show that
this implies that P(BcAc)=P(Ac). One way is as follows:
P(BcAc)=P((A∪B)c)
=1−P(A∪B)
=1−P(A)−P(B)+P(AB)
=1−P(A)
=P(Ac)
3.27. The result is true for n=0. With Aidenoting the event
that there are ired balls in the urn after stage n, assume that
P(Ai)=1
n+1, i=1,...,n+1
Now let Bj,j=1,...,n+2, denote the event that there are
jred balls in the urn after stage n+1. Then
P(Bj)=n+1⎫summationdisplay
i=1P(Bj|Ai)P(Ai)
=1
n+1n+1⎫summationdisplay
i=1P(Bj|Ai)
=1
n+1[P(Bj|Aj−1)+P(Bj|Aj)]
Because there are n+2 balls in the urn after stage n,i t
follows that P(Bj|Aj−1)is the probability that a red ball is

<<<PAGE 455>>>

July 14, 2014 Z02_ROSSS4772_09_SE_BM page 442
442 Solutions to Self-Test Problems and Exercises
chosen when j−1o ft h en +2 balls in the urn are red and
P(Bj|Aj)is the probability that a red ball is not chosen when
jof the n+2 balls in the urn are red. Consequently,
P(Bj|Aj−1)=j−1
n+2, P(Bj|Aj)=n+2−j
n+2
Substituting these results into the equation for P(Bj)gives
P(Bj)=1
n+1⎫bracketleftbiggj−1
n+2+n+2−j
n+2⎫bracketrightbigg
=1
n+2
This completes the induction proof.
3.28. IfAiis the event that player ireceives an ace, then
P(Ai)=1−⎫parenleftBigg
2n−2
n⎫parenrightBigg
⎫parenleftBigg
2n
n⎫parenrightBigg=1−1
2n−1
2n−1=3n−1
4n−2
By arbitrarily numbering the aces and noting that the player
who does not receive ace number one will receive nof the
remaining 2 n−1 cards, we see that
P(A1A2)=n
2n−1
Therefore,
P(Ac
2|A1)=1−P(A2|A1)=1−P(A1A2)
P(A1)=n−1
3n−1
We may regard the card division outcome as the result of two
trials, where trial i,i=1, 2, is said to be a success if ace num-
berigoes to the ﬁrst player. Because the locations of the two
aces become independent as ngoes to inﬁnity, with each one
being equally likely to be given to either player, it follows
that the trials become independent, each being a success with
probability 1/2. Hence, in the limiting case where n→q,t h e
problem becomes one of determining the conditional proba-
bility that two heads result, given that at least one does, when
two fair coins are ﬂipped. Becausen−1
3n−1converges to 1/3, the
answer agrees with that of Example 2b.
3.29. (a)For any permutation i1,...,inof 1, 2, ...,n,t h e
probability that the successive types collected is i1,...,inis
pi1···p in=⎫producttextn
i=1pi. Consequently, the desired probability
isn!⎫producttextn
i=1pi.
(b)Fori1,...,ikall distinct,
P(Ei1···E ik)=⎫parenleftbiggn−k
n⎫parenrightbiggn
which follows because there are no coupons of types
i1,...,ikwhen each of the nindependent selections is one
of the other n−ktypes. It now follows by the inclusion–
exclusion identity that
P(∪n
i=1Ei)=n⎫summationdisplay
k=1(−1)k+1⎫parenleftbiggn
k⎫parenrightbigg⎫parenleftbiggn−k
n⎫parenrightbiggnBecause 1 −P(∪n
i=1Ei)is the probability that one of each
type is obtained, by part (a) it is equal ton!
nn. Substituting this
into the preceding equation gives
1−n!
nn=n⎫summationdisplay
k=1(−1)k+1⎫parenleftbiggn
k⎫parenrightbigg⎫parenleftbiggn−k
n⎫parenrightbiggn
or
n!=nn−n⎫summationdisplay
k=1(−1)k+1⎫parenleftbiggn
k⎫parenrightbigg
(n−k)n
or
n!=n⎫summationdisplay
k=0(−1)k⎫parenleftbiggn
k⎫parenrightbigg
(n−k)n
3.30. P(E|E∪F)=P(E|F(E∪F))P(F|E∪F)
+P(E|Fc(E∪F))P(Fc|E∪F)
Using
F(E∪F)=Fand Fc(E∪F)=FcE
gives
P(E|E∪F)=P(E|F)P(F|E∪F)+P(E|EFc)P(Fc|E∪F)
=P(E|F)P(F|E∪F)+P(Fc|E∪F)
ÚP(E|F)P(F|E∪F)+P(E|F)P(Fc|E∪F)
=P(E|F)
3.31. P(A∪B)=P(A∪B|A)P (A)+P(A∪B|Ac)P(Ac)
=1(.6)+.1(.4) =.64
Chapter 4
4.1. Since the probabilities sum to 1, we must have
4P{X=3}+.5=1, implying that P{X=0}=.375,P{X=3}
=.125. Hence, E[X]=1(.3)+2(.2)+3(.125) =1.075.
4.2. The relationship implies that pi=cip0,i=1, 2, where
pi=P{X=i}. Because these probabilities sum to 1, it fol-
lows that
p0(1+c+c2)=1*p0=1
1+c+c2
Hence,
E[X]=p1+2p2=c+2c2
1+c+c2
4.3. LetXbe the number of ﬂips. Then the probability mass
function of Xis
p2=p2+(1−p)2,p3=1−p2=2p(1−p)
Hence,
E[X]=2p2+3p3=2p2+3(1−p2)=3−p2−(1−p)2

<<<PAGE 456>>>

July 14, 2014 Z02_ROSSS4772_09_SE_BM page 443
Solutions to Self-Test Problems and Exercises 443
4.4. The probability that a randomly chosen family will have
ichildren is ni/m. Thus,
E[X]=r⎫summationdisplay
i=1ini/m
Also, since there are inichildren in families having ichildren,
it follows that the probability that a randomly chosen child is
from a family with ichildren is ini/r⎫summationtext
i=1ini. Therefore,
E[Y]=r⎫summationdisplay
i=1i2ni
r⎫summationdisplay
i=1ini
Thus, we must show that
r⎫summationdisplay
i=1i2ni
r⎫summationdisplay
i=1iniÚr⎫summationdisplay
i=1ini
r⎫summationdisplay
i=1ni
or, equivalently, that
r⎫summationdisplay
j=1njr⎫summationdisplay
i=1i2niÚr⎫summationdisplay
i=1inir⎫summationdisplay
j=1jnj
or, equivalently, that
r⎫summationdisplay
i=1r⎫summationdisplay
j=1i2ninjÚr⎫summationdisplay
i=1r⎫summationdisplay
j=1ijninj
But, for a ﬁxed pair i,j, the coefﬁcient of ninjin the left-side
summation of the preceding inequality is i2+j2, whereas
its coefﬁcient in the right-hand summation is 2 ij.H e n c e ,i t
sufﬁces to show that
i2+j2Ú2ij
which follows because (i−j)2Ú0.
4.5. Letp=P{X=1}.T h e n E[X]=pand Var (X)=
p(1−p),s o
p=3p(1−p)
implying that p=2/3. Hence, P{X=0}= 1/3.
4.6. If you wager xon a bet that wins the amount wagered
with probability pand loses that amount with probability
1−p, then your expected winnings are
xp−x(1−p)=(2p−1)x
which is positive (and increasing in x) if and only if p>1/2.
Thus, if p…1/2, one maximizes one’s expected return by
wagering 0, and if p>1/2, one maximizes one’s expectedreturn by wagering the maximal possible bet. Therefore, if
the information is that the .6 coin was chosen, then you
should bet 10; if the information is that the .3 coin was cho-sen, then you should bet 0. Hence, your expected payoff is
1
2(1.2−1)10+1
20−C=1−C
Since your expected payoff is 0 without the information
(because in this case the probability of winning is1
2(.6)+
1
2(.3)<1/2), it follows that if the information costs less than
1, then it pays to purchase it.
4.7. (a)If you turn over the red paper and observe the value
x, then your expected return if you switch to the blue paper is
2x(1/2) +x/2(1/2) =5x/4>x
Thus, it would always be better to switch.
(b)Suppose the philanthropist writes the amount xon the
red paper. Then the amount on the blue paper is either 2 xor
x/ 2 .N o t et h a ti f x/2Úy, then the amount on the blue paper
will be at least yand will thus be accepted. Hence, in this
case, the reward is equally likely to be either 2 xorx/2, so
E[Ry(x)]=5x/4, if x/2Úy
Ifx/2<y…2x, then the blue paper will be accepted if its
value is 2x and rejected if it is x/2. Therefore,
E[Ry(x)]=2x(1/2) +x(1/2) =3x/2, if x/2<y…2x
Finally, if 2x <y, then the blue paper will be rejected. Hence,
in this case, the reward is x,s o
Ry(x)=x,i f 2 x<y
That is, we have shown that when the amount xis written on
the red paper, the expected return under the y-policy is
E[Ry(x)]=⎧
⎪⎨
⎪⎩x ifx<y/2
3x/2i f y/2…x<2y
5x/4i f xÚ2y
4.8. Suppose that nindependent trials, each of which results
in a success with probability p, are performed. Then the num-
ber of successes will be less than or equal to iif and only if
the number of failures is greater than or equal to n−i.B u t
since each trial is a failure with probability 1 −p, it follows
that the number of failures is a binomial random variable
with parameters nand 1 −p.H e n c e ,
P{Bin(n,p)…i}=P{Bin(n,1−p)Ún−i}
=1−P{Bin(n,1−p)…n−i−1}
The ﬁnal equality follows from the fact that the probabilitythat the number of failures is greater than or equal to n−i
is 1 minus the probability that it is less than n−i.

<<<PAGE 457>>>

July 14, 2014 Z02_ROSSS4772_09_SE_BM page 444
444 Solutions to Self-Test Problems and Exercises
4.9. Since E[X]=np,V a r (X)=np(1−p), we are given that
np=6,np(1−p)=2.4. Thus, 1 −p=.4, or p=.6,n=10.
Hence,
P{X=5}=⎫parenleftBigg
10
5⎫parenrightBigg
(.6)5(.4)5
4.10. LetXi,i=1,...,m, denote the number on the ith ball
drawn. Then
P{X…k}= P{X1…k,X2…k,...,Xm…k}
=P{X1…k}P{X2…k}···P {Xm…k}
=⎫parenleftbiggk
n⎫parenrightbiggm
Therefore,
P{X=k}=P{X…k}−P{X…k−1}=⎫parenleftbiggk
n⎫parenrightbiggm
−⎫parenleftbiggk−1
n⎫parenrightbiggm
4.11. (a)Given that Awins the ﬁrst game, it will win the
series if, from then on, it wins 2 games before team Bwins 3
games. Thus,
P{Awins|Awins ﬁrst }=4⎫summationdisplay
i=2⎫parenleftBigg
4
i⎫parenrightBigg
pi(1−p)4−i
(b)
P{Awins ﬁrst| Awins}=P{Awins|Awins ﬁrst}P {Awins ﬁrst }
P{Awins}
=4⎫summationdisplay
i=2⎫parenleftbigg
4
i⎫parenrightbigg
pi+1(1−p)4−i
5⎫summationdisplay
i=3⎫parenleftbigg
5
i⎫parenrightbigg
pi(1−p)5−i
4.12. To obtain the solution, condition on whether the team
wins this weekend:
.54⎫summationdisplay
i=3⎫parenleftBigg
4
i⎫parenrightBigg
(.4)i(.6)4−i+.54⎫summationdisplay
i=3⎫parenleftBigg
4
i⎫parenrightBigg
(.7)i(.3)4−i
4.13. LetCbe the event that the jury makes the correct deci-
sion, and let Fbe the event that four of the judges agreed.
Then
P(C)=7⎫summationdisplay
i=4⎫parenleftbigg7
i⎫parenrightbigg
(.7)i(.3)7−i
Also,
P(C|F)=P(CF)
P(F)
=⎫parenleftBig7
4⎫parenrightBig
(.7)4(.3)3
⎫parenleftBig74⎫parenrightBig
(.7)4(.3)3+⎫parenleftBig73⎫parenrightBig
(.7)3(.3)4
=.74.14. Assuming that the number of hurricanes can be
approximated by a Poisson random variable, we obtain the
solution
3⎫summationdisplay
i=0e−5.2(5.2)i/i!
4.15. E[Y]=q⎫summationdisplay
i=1iP{X=i}/P{X>0}
=E[X]/P{X>0}
=λ
1−e−λ
4.16. (a)1/n
(b)LetDbe the event that girl iand girl jchoose different
boys. Then
P(GiGj)=P(GiGj|D)P(D)+P(GiGj|Dc)P(Dc)
=(1/n)2(1−1/n)
=n−1
n3
Therefore,
P(Gi|Gj)=n−1
n2
(c) , (d) Because, when nis large, P(Gi|Gj)is small and
nearly equal to P(Gi), it follows from the Poisson paradigm
that the number of couples is approximately Poisson dis-
tributed with mean⎫summationtextn
i=1P(Gi)=1. Hence, P0Le−1and
PkLe−1/k!
(e)To determine the probability that a given set of kgirls all
are coupled, condition on whether or not Doccurs, where D
is the event that they all choose different boys. This gives
P(Gi1···G ik)=P(Gi1···G ik|D)P(D)
+P(Gi1···G ik|Dc)P(Dc)
=P(Gi1···G ik|D)P(D)
=(1/n)kn(n−1)···(n −k+1)
nk
=n!
(n−k)!n2k
Therefore,
⎫summationdisplay
i1<...< ikP(Gi1···G ik)=⎫parenleftbiggn
k⎫parenrightbigg
P(Gi1···G ik)
=n!n!
(n−k)!(n −k)!k!n2k
and the inclusion–exclusion identity yields
1−P0=P(∪n
i=1Gi)=n⎫summationdisplay
k=1(−1)k+1 n!n!
(n−k)!(n −k)!k!n2k

<<<PAGE 458>>>

July 14, 2014 Z02_ROSSS4772_09_SE_BM page 445
Solutions to Self-Test Problems and Exercises 445
4.17. (a)Because woman iis equally likely to be paired with
any of the remaining 2 n−1 people, P(Wi)=1
2n−1
(b)Because, conditional on Wj, woman iis equally likely to
be paired with any of 2n −3 people, P(Wi|Wj)=1
2n−3
(c)When nis large, the number of wives paired with
their husbands will approximately be Poisson with mean⎫summationtextn
i=1P(Wi)=n
2n−1L1/2. Therefore, the probability that
there is no such pairing is approximately e−1/2.
(d)It reduces to the match problem.
4.18. (a)⎫parenleftBigg
8
3⎫parenrightBigg
(9/19)3(10/19)5(9/19) =⎫parenleftBigg
83⎫parenrightBigg
(9/19)
4(10/19)5
(b)IfWis her ﬁnal winnings and Xis the number of bets she
makes, then, since she would have won 4 bets and lost X−4
bets, it follows that
W=20−5(X−4)=40−5X
Hence,
E[W]=40−5E[X]=40−5[4/( 9/19)] =−20/9
4.19. The probability that a round does not result in an “odd
person” is equal to 1/4, the probability that all three coinsland on the same side.
(a)(1/4)
2(3/4)=3/64
(b)(1/4)4=1/256
4.20. Letq=1−p.T h e n
E[1/X ]=q⎫summationdisplay
i=11
iqi−1p
=p
qq⎫summationdisplay
i=1qi/i
=p
qq⎫summationdisplay
i=1⎫integraldisplayq
0xi−1dx
=p
q⎫integraldisplayq
0q⎫summationdisplay
i=1xi−1dx
=p
q⎫integraldisplayq
01
1−xdx
=p
q⎫integraldisplay1
p1
ydy
=−p
qlog(p)
4.21. SinceX−b
a−bwill equal 1 with probability por 0 with
probability 1 −p, it follows that it is a Bernoulli random
variable with parameter p. Because the variance of such a
Bernoulli random variable is p(1−p), we have
p(1−p)=Var⎫parenleftbiggX−b
a−b⎫parenrightbigg
=1
(a−b)2Var(X−b)
=1
(a−b)2Var(X)Hence,
Var(X)=(a−b)2p(1−p)
4.22. LetXdenote the number of games that you play and
Ythe number of games that you lose.
(a)After your fourth game, you will continue to play until
you lose. Therefore, X−4 is a geometric random variable
with parameter 1 −p,s o
E[X]=E[4+(X−4)]=4+E[X−4]=4+1
1−p
(b)If we let Zdenote the number of losses you have in the
ﬁrst 4 games, then Zis a binomial random variable with
parameters 4 and 1 −p. Because Y=Z+1, we have
E[Y]=E[Z+1]=E[Z]+1=4(1−p)+1
4.23. A total of nwhite balls will be withdrawn before a total
ofmblack balls if and only if there are at least nwhite balls
in the ﬁrst n+m−1 withdrawals. (Compare with the prob-
lem of the points, Example 4j of Chapter 3.) With Xequal to
the number of white balls among the ﬁrst n+m−1 balls
withdrawn, Xis a hypergeometric random variable, and it
follows that
P{XÚn}=n+m−1⎫summationdisplay
i=nP{X=i}
=n+m−1⎫summationdisplay
i=n⎫parenleftBigg
Ni⎫parenrightBigg⎫parenleftBigg
M
n+m−1−i⎫parenrightBigg
⎫parenleftBigg
N+M
n+m−1⎫parenrightBigg
4.24. Because each ball independently goes into urn iwith
the same probability pi, it follows that Xiis a binomial ran-
dom variable with parameters n=10,p=pi.
First note that Xi+Xjis the number of balls that go
into either urn ior urn j. Then, because each of the 10 balls
independently goes into one of these urns with probabilityp
i+pj, it follows that Xi+Xjis a binomial random variable
with parameters 10 and pi+pj.
By the same logic, X1+X2+X3is a binomial random
variable with parameters 10 and p1+p2+p3. Therefore,
P{X1+X2+X3=7}=⎫parenleftbigg10
7⎫parenrightbigg
(p1+p2+p3)7(p4+p5)3
4.25. LetXiequal 1 if person ihas a match, and let it equal
0o t h e r w i s e .T h e n
X=n⎫summationdisplay
i=1Xi
is the number of matches. Taking expectations gives
E[X]=E⎡
⎣n⎫summationdisplay
i=1Xi⎤⎦=n⎫summationdisplay
i=1E[Xi]=n⎫summationdisplay
i=1P{Xi=1}=n⎫summationdisplay
i=11/n=1

<<<PAGE 459>>>

July 14, 2014 Z02_ROSSS4772_09_SE_BM page 446
446 Solutions to Self-Test Problems and Exercises
where the ﬁnal equality follows because person iis equally
likely to end up with any of the nhats.
To compute Var(X ), we use Equation (9.1), which states
that
E[X2]=n⎫summationdisplay
i=1E[Xi]+n⎫summationdisplay
i=1⎫summationdisplay
jZiE[XiXj]
Now, for iZj,
E[XiXj]=P{Xi=1,Xj=1}=P{Xi=1}P{Xj=1|Xi=1}
=1
n1
n−1
Hence,
E[X2]=1+n⎫summationdisplay
i=1⎫summationdisplay
jZi1
n(n−1)
=1+n(n−1)1
n(n−1)=2
which yields
Var(X)=2−12=1
4.26. With q=1−p, we have, on the one hand,
P(E)=q⎫summationdisplay
i=1P{X=2i}
=q⎫summationdisplay
i=1pq2i−1
=pqq⎫summationdisplay
i=1(q2)i−1
=pq1
1−q2
=pq
(1−q)(1+q)=q
1+q
On the other hand,
P(E)=P(E|X=1)p+P(E|X>1)q=qP(E|X>1)
However, given that the ﬁrst trial is not a success, the num-
ber of trials needed for a success is 1 plus the geometrically
distributed number of additional trials required. Therefore,
P(E|X>1)=P(X+1 is even) =P(Ec)=1−P(E)
which yields P(E)=q/(1+q).
4.27. The probability that either team wins 3 of the ﬁrst 4
games is 2⎫parenleftBigg
43⎫parenrightBigg
(1/2)
4=1/2. Because the team with only
1 win would then have to win the following 3 games, thedesired probability is 1/16.
4.28. (a)The negative binomial represents the number of
balls withdrawn in a similar experiment but with the excep-tion that the withdrawn ball would be replaced before the
next drawing.(b)Using the hint, we note that X=rif the ﬁrst r−1 balls
withdrawn contain exactly k−1 white balls and the next
withdrawn ball is white. Hence,
P(X=r)=⎫parenleftBigg
n
k−1⎫parenrightBigg⎫parenleftBigg
m
r−k⎫parenrightBigg
⎫parenleftBigg
n+m
r−1⎫parenrightBiggn−k+1
n+m−r+1,
k…r…m+k
Chapter 5
5.1. LetXbe the number of minutes played.
(a)P{X>15}=1 −P{X…15}=1 −5(.025) =.875
(b)P{20<X<35}=10(.05) +5(.025) =.625
(c)P{X<30}=10(.025) +10(.05) =.75
(d)P{X>36}=4(.025) =.1
5.2. (a)1=⎫integraltext1
0cxndx=c/(n+1)*c=n+1
(b)P{X>x}=(n +1)⎫integraltext1
xxndx=xn+1⎫vextendsingle⎫vextendsingle⎫vextendsingle1
x=1−xn+1
5.3. First, let us ﬁnd cby using
1=⎫integraldisplay2
0cx4dx=32c/5*c=5/32
(a)E[X]=5
32⎫integraltext2
0x5dx=5
3264
6=5/3
(b)E[X2]=5
32⎫integraltext2
0x6dx=5
32128
7=20/7*Var(X)=
20/7−(5/3)2=5/63
5.4. Since
1=⎫integraldisplay1
0(ax+bx2)dx=a/2+b/3
.6=⎫integraldisplay1
0(ax2+bx3)dx=a/3+b/4
we obtain a=3.6,b=−2.4. Hence,
(a)P{X<1/2}=⎫integraltext1/2
0(3.6x −2.4x2)dx=(1.8x2−
.8x3)⎫vextendsingle⎫vextendsingle⎫vextendsingle1/2
0=.35
(b)E[X2]=⎫integraltext1
0(3.6x3−2.4x4)dx=.42*Var(X)=.06
5.5. Fori=1,...,n,
P{X=i}=P{Int(nU)=i−1}
=P{i−1…nU<i}
=P⎫braceleftbiggi−1
n…U<i
n⎫bracerightbigg
=1/n
5.6. If you bid x,7 0…x…140, then you will either win
the bid and make a proﬁt of x−100 with probability

<<<PAGE 460>>>

July 14, 2014 Z02_ROSSS4772_09_SE_BM page 447
Solutions to Self-Test Problems and Exercises 447
(140−x)/70 or lose the bid and make a proﬁt of 0 other-
wise. Therefore, your expected proﬁt if you bid xis
1
70(x−100)(140 −x)=1
70(240x −x2−14000)
Differentiating and setting the preceding equal to 0 gives
240−2x=0
Therefore, you should bid $120, 000. Your expected proﬁt
will be 40/7 thousand dollars.
5.7. (a)P{U>.1}= 9/10
(b)P{U>.2|U>.1}=P {U>.2}/P{U>.1}= 8/9
(c)P{U>.3|U>.2,U>.1}=P {U>.3}/P{U>.2}=7/8
(d)P{U>.3}= 7/10
The answer to part (d) could also have been obtained bymultiplying the probabilities in parts (a), (b), and (c).
5.8. LetXbe the test score, and let Z=(X−100)/ 15. Note
thatZis a standard normal random variable.
(a)P{X>125}=P {Z>25/15} L.0478
(b)P{90<X<110}= P{−10/15 <Z<10/15}
=P{Z<2/3}−P {Z<−2/3}
=P{Z<2/3}−[1 −P{Z<2/3}]
L.4950
5.9. LetXbe the travel time. We want to ﬁnd xsuch that
P{X>x}=.05
which is equivalent to
P⎫braceleftbiggX−40
7>x−40
7⎫bracerightbigg
=.05
That is, we need to ﬁnd xsuch that
P⎫braceleftbigg
Z>x−40
7⎫bracerightbigg
=.05
where Zis a standard normal random variable. But
P{Z>1.645}=.05
Thus,
x−40
7=1.645 or x=51.515
Therefore, you should leave no later than 8.485 minutes after12
P. M .
5.10. LetXbe the tire life in units of one thousand, and let
Z=(X−34)/4. Note that Zis a standard normal random
variable.(a)P{X>40}=P {Z>1.5}L.0668
(b)P{30<X<35}=P {−1<Z<.25}=P {Z<.25}−
P{Z>1}L.44
(c)P{X>40|X >30}=P {X>40}/P {X>30}
=P{Z>1.5}/P {Z>−1}L.0795.11. LetXbe next year’s rainfall and let Z=(X−40.2)/ 8.4.
(a)P{X>44}=P {Z>3.8/8.4} LP{Z>.4524} L.3255
(b)⎫parenleftBigg
7
3⎫parenrightBigg
(.3255)3(.6745)4
5.12. LetMiandWidenote, respectively, the numbers of
men and women in the samples that earn, in units of $1, 000,
at least iper year. Also, let Zbe a standard normal random
variable.(a)
P{W
25Ú70}
=P{W25Ú69.5}
=P⎫braceleftBigg
W25−200(.34)√200(.34)(.66)Ú69.5−200(.34)√200(.34)(.66)⎫bracerightBigg
LP{ZÚ.2239}
L.4114
(b)
P{M25…120}
=P{M25…120.5}
=P⎫braceleftBigg
M25−(200)(.587)√(200)(.587)(.413)…120.5 −(200)(.587)√(200)(.587)(.413)⎫bracerightBigg
LP{Z….4452}
L.6719
(c)
P{M20Ú150}
=P{M20Ú149.5}
=P⎫braceleftBigg
M20−(200)(.745)√(200)(.745)(.255)Ú149.5 −(200)(.745)√(200)(.745)(.255)⎫bracerightBigg
LP{ZÚ.0811}
L.4677
P{W20Ú100}
=P{W20Ú99.5}
=P⎫braceleftBigg
W20−(200)(.534)√(200)(.534)(.466)Ú99.5−(200)(.534)√(200)(.534)(.466)⎫bracerightBigg
LP{ZÚ−1.0348}
L.8496
Hence,
P{M20Ú150}P {W20Ú100}L.3974
5.13. The lack of memory property of the exponential gives
the result e−4/5.
5.14. (a)e−22=e−4
(b)F(3)−F(1)=e−1−e−9
(c)λ(t)=2te−t2/e−t2=2t

<<<PAGE 461>>>

July 14, 2014 Z02_ROSSS4772_09_SE_BM page 448
448 Solutions to Self-Test Problems and Exercises
(d)LetZbe a standard normal random variable. Use the
identity E[X]=⎫integraltextq
0P{X>x}dxto obtain
E[X]=⎫integraldisplayq
0e−x2dx
=2−1/2⎫integraldisplayq
0e−y2/2dy
=2−1/2√
2πP{Z>0}
=√π/2
(e)Use the result of Theoretical Exercise 5.5 to obtain
E[X2]=⎫integraldisplayq
02xe−x2dx=−e−x2⎫vextendsingle⎫vextendsingle⎫vextendsingle⎫vextendsingleq
0=1
Hence, Var (X)=1−π/4.
5.15. (a)P{X>6}=exp{−⎫integraltext6
0λ(t)dt}=e−3.45
(b)P{X<8|X>6}= 1−P{X>8|X>6}
=1−P{X>8}/P{X>6}
=1−e−5.65/e−3.45
L.8892
5.16. ForxÚ0,
F1/X(x)=P{1/X…x}
=P{X…0}+ P{XÚ1/x}
=1/2+1−FX(1/x)
Differentiation yields
f1/X(x)=x−2fX(1/x)
=1
x2π(1+(1/x)2)
=fX(x)
The proof when x<0 is similar.
5.17. IfXdenotes the number of the ﬁrst nbets that you win,
then the amount that you will be winning after nbets is
35X−(n−X)=36X−n
Thus, we want to determine
a=P{36X −n>0}=P {X>n/36}
when Xis a binomial random variable with parameters nand
p=1/38.
(a)When n=34,
a=P{XÚ1}
=P{X>.5} (the continuity correction )
=P⎫braceleftBigg
X−34/38⎫radicalbig
34(1/38)(37/38)>.5−34/38⎫radicalbig
34(1/38)(37/38)⎫bracerightBigg=P⎫braceleftBigg
X−34/38⎫radicalbig
34(1/38)(37/38)>−.4229⎫bracerightBigg
L/Phi1(.4229)
L.6638
(Because you will be ahead after 34 bets if you win at least
1 bet, the exact probability in this case is 1 −(37/38)34=
.5961.)
(b)When n=1000,
a=P{X>27.5}
=P⎫braceleftBigg
X−1000/38⎫radicalbig
1000(1/38)(37/38)>27.5−1000/38⎫radicalbig
1000(1/38)(37/38)⎫bracerightBigg
L1−/Phi1(.2339)
L.4075
The exact probability—namely, the probability that a bino-
mial n=1000, p=1/38 random variable is greater than
27—is .3961.
(c)When n=100, 000,
a=P{X>2777.5}
=P⎫braceleftBigg
X−100000/38⎫radicalbig
100000(1/38)(37/38)>2777.5 −100000/38⎫radicalbig
100000(1/38)(37/38)⎫bracerightBigg
L1−/Phi1(2.883)
L.0020
The exact probability in this case is .0021.
5.18. IfXdenotes the lifetime of the battery, then the
desired probability, P{X>s+t|X>t}, can be determined
as follows:
P{X>s+t|X>t}=P{X>s+t,X>t}
P{X>t}
=P{X>s+t}
P{X>t}
=P{X>s+t|battery is type 1 }p1
+P{X>s+t|battery is type 2 }p2
P{X>t|battery is type 1 }p1
+P{X>t|battery is type 2 }p2
=e−λ1(s+t)p1+e−λ2(s+t)p2
e−λ1tp1+e−λ2tp2
Another approach is to directly condition on the type of
battery and then use the lack-of-memory property of expo-
nential random variables. That is, we could do the following:
P{X>s+t|X>t}
=P{X>s+t|X>t,t y p e1 }P{type 1|X >t}
+P{X>s+t|X>t,t y p e2 }P{type 2|X >t}
=e−λ1sP{type 1|X >t}+e−λ2sP{type 2|X >t}
Now for i=1, 2, use

<<<PAGE 462>>>

July 14, 2014 Z02_ROSSS4772_09_SE_BM page 449
Solutions to Self-Test Problems and Exercises 449
P{type i|X>t}=P{type i,X>t}
P{X>t}
=P{X>t|type i}pi
P{X>t|type 1 }p1+P{X>t|type 2}p 2
=e−λitpi
e−λ1tp1+e−λ2tp2
5.19. LetXibe an exponential random variable with mean i,
i=1, 2.
(a)The value cshould be such that P{X1>c}=.05. There-
fore,
e−c=.05=1/20
orc=log(20) =2.996.
(b)P{X2>c}=e−c/2=1√
20=.2236
5.20. (a)
E[(Z−c)+]=1√
2π⎫integraldisplayq
−q(x−c)+e−x2/2dx
=1√
2π⎫integraldisplayq
c(x−c)e−x2/2dx
=1√
2π⎫integraldisplayq
cxe−x2/2dx−1√
2π⎫integraldisplayq
cce−x2/2dx
=−1√
2πe−x2/2|q
c−c(1−/Phi1(c))
=1√
2πe−c2/2−c(1−/Phi1(c))
(b)Using the fact that Xhas the same distribution as μ+
σZ,w h e r eZ is a standard normal random variable, yields
E[(X−c)+]=E[(μ+σZ−c)+]
=E⎡
⎣⎫parenleftBigg
σ⎫parenleftbigg
Z−c−μ
σ⎫parenrightbigg⎫parenrightBigg+⎤⎦
=E⎫bracketleftBigg
σ⎫parenleftbigg
Z−c−μ
σ⎫parenrightbigg+⎫bracketrightBigg
=σE⎫bracketleftBigg⎫parenleftbigg
Z−c−μ
σ⎫parenrightbigg+⎫bracketrightBigg
=σ⎫bracketleftBigg
1√
2πe−a2/2−a(1−/Phi1(a))⎫bracketrightBigg
where a=c−μ
σ.
5.21. Only (b) is true.
5.22. (a)Ifb>0, then for 0 <x<b,
P⎫parenleftbig
bU<x⎫parenrightbig
=P⎫braceleftbig
U<x/b⎫bracerightbig
=x/b.
Hence,
fbU(x)=1/b,0 <x<b
The argument when b<0 is similar.(b)Fora<x<1+a,
P⎫braceleftbig
a+U<x⎫bracerightbig
=P⎫braceleftbig
U<x−a⎫bracerightbig
=x−a
Differentiation yields
fa+U(x)=1,a<x<1+a
(c)a+(b−a)U
(d)For 0 <x<1/2,
P⎫braceleftBig
min⎫parenleftbig
U,1−U⎫parenrightbig
<x⎫bracerightBig
=P⎫parenleftBig⎫braceleftbig
U<x⎫bracerightbig
∪⎫braceleftbig
U>1−x⎫bracerightbig⎫parenrightBig
=P⎫braceleftbig
U<x⎫bracerightbig
+P⎫braceleftbig
U>1−x⎫bracerightbig
=2x
Differentiating gives
fmin(U ,1−U)(x)=2, 0 <x<1/2
(e)Using that max (U,1−U)=1−min(U ,1−U),t h e
result follows from (a), (b), and (d). A direct argument is
that, for 1/2 <x<1,
P⎫braceleftBig
max⎫parenleftbig
U,1−U⎫parenrightbig
<x⎫bracerightBig
=1−P⎫braceleftBig
max⎫parenleftbig
U,1−U⎫parenrightbig
>x⎫bracerightBig
=1−P⎫parenleftBig⎫braceleftbig
U>x⎫bracerightbig
∪⎫braceleftbig
U<1−x⎫bracerightbig⎫parenrightBig
=1−⎫parenleftbig
1−x⎫parenrightbig
−⎫parenleftbig
1−x⎫parenrightbig
=2x−1
Hence,
fmax(U,1−U)(x)=2, 1/2 <x<1
Chapter 6
6.1. (a)3C+6C=1*C=1/9
(b)Letp(i,j)=P{X=i,Y=j}.T h e n
p(1, 1) =4/9,p(1, 0) =2/9,P(0, 1)=1/9,p(0, 0) =2/9
(c)(12)!
26(1/9)6(2/9)6
(d)(12)!
(4!)3(1/3)12
(e)12⎫summationtext
i=8⎫parenleftBigg
12
i⎫parenrightBigg
(2/3)i(1/3)12−i
6.2. (a)With pj=P{XYZ =j}, we have
p6=p2=p4=p12=1/4
Hence,
E[XYZ ]=(6+2+4+12)/4=6
(b)With qj=P{XY+XZ+YZ=j}, we have
q11=q5=q8=q16=1/4
Hence,
E[XY+XZ+YZ]=(11+5+8+16)/4=10

<<<PAGE 463>>>

July 14, 2014 Z02_ROSSS4772_09_SE_BM page 450
450 Solutions to Self-Test Problems and Exercises
6.3. In this solution, we will make use of the identity
⎫integraldisplayq
0e−xxndx=n!
which follows because e−xxn/n!,x>0, is the density func-
tion of a gamma random variable with parameters n+1a n d
λand must thus integrate to 1.
(a)1=C⎫integraldisplayq
0e−y⎫integraldisplayy
−y(y−x)dx dy
=C⎫integraldisplayq
0e−y2y2dy=4C
Hence, C=1/4.
(b)Since the joint density is nonzero only when y>xand
y>−x, we have, for x>0,
fX(x)=1
4⎫integraldisplayq
x(y−x)e−ydy
=1
4⎫integraldisplayq
0ue−(x+u)du
=1
4e−x
Forx<0,
fX(x)=1
4⎫integraldisplayq
−x(y−x)e−ydy
=1
4[−ye−y−e−y+xe−y]q
−x
=(−2xex+ex)/4
(c)fY(y)=1
4e−y⎫integraltexty
−y(y−x)dx=1
2y2e−y
(d)E[X]=1
4⎫bracketleftBigg⎫integraldisplayq
0xe−xdx+⎫integraldisplay0
−q(−2x2ex+xex)dx⎫bracketrightBigg
=1
4⎫bracketleftbigg
1−⎫integraldisplayq
0(2y2e−y+ye−y)dy⎫bracketrightbigg
=1
4[1−4−1]=−1
(e)E[Y]=1
2⎫integraltextq
0y3e−ydy=3
6.4. The multinomial random variables Xi,i=1,...,r,r e p -
resent the numbers of each of the types of outcomes 1, ...,r
that occur in nindependent trials when each trial results
in one of the outcomes 1, ...,rwith respective probabili-
tiesp1,...,pr. Now, say that a trial results in a category 1
outcome if that trial resulted in any of the outcome types
1,...,r1; say that a trial results in a category 2 outcome if that
trial resulted in any of the outcome types r1+1,...,r1+r2;
and so on. With these deﬁnitions, Y1,...,Ykrepresent the
numbers of category 1 outcomes, category 2 outcomes, up
to category koutcomes when nindependent trials that each
result in one of the categories 1, ...,kwith respective prob-
abilities⎫summationtextri−1+ri
j=ri−1+1pj,i=1,...,k, are performed. But by
deﬁnition, such a vector has a multinomial distribution.6.5. (a)Letting pj=P{XYZ =j}, we have
p1=1/8, p2=3/8, p4=3/8, p8=1/8
(b)Letting pj=P{XY+XZ+YZ=j}, we have
p3=1/8, p5=3/8, p8=3/8, p12=1/8
(c)Letting pj=P{X2+YZ=j}, we have
p2=1/8, p3=1/4, p5=1/4, p6=1/4, p8=1/8
6.6. (a)1=⎫integraldisplay1
0⎫integraldisplay5
1(x/5+cy)dy dx
=⎫integraldisplay1
0(4x/5 +12c)dx
=12c+2/5
Hence, c=1/20.
(b)No, the density does not factor.
(c)P{X+Y>3}=⎫integraldisplay1
0⎫integraldisplay5
3−x(x/5+y/20) dy dx
=⎫integraldisplay1
0[(2+x)x/5 +25/40− (3−x)2/40]dx
=1/5+1/15+5/8−19/120 =11/15
6.7. (a)Yes, the joint density function factors.
(b)fX(x)=x⎫integraltext2
0ydy=2x,0 <x<1
(c)fY(y)=y⎫integraltext1
0xdx=y/2, 0 <y<2
(d)
P{X<x,Y<y}=P {X<x}P{Y<y}
=min(1,x2)min(1, y2/4), x>0,y>0
(e)E[Y]=⎫integraltext2
0y2/2dy=4/3
(f)P{X+Y<1}=⎫integraldisplay1
0x⎫integraldisplay1−x
0yd yd x
=1
2⎫integraldisplay1
0x(1−x)2dx=1/24
6.8. LetTidenote the time at which a shock type i,o fi=
1, 2, 3, occurs. For s>0,t>0,
P{X1>s,X2>t}=P{T1>s,T2>t,T3>max(s, t)}
=P{T1>s}P{T2>t}P{T3>max(s,t)}
=exp{−λ1s}exp{−λ2t}exp{− λ3max(s, t)}
=exp{−(λ1s+λ2t+λ3max(s, t))}
6.9. (a)No, advertisements on pages having many ads are
less likely to be chosen than are ones on pages with few ads.
(b)1
mn(i)
n

<<<PAGE 464>>>

July 14, 2014 Z02_ROSSS4772_09_SE_BM page 451
Solutions to Self-Test Problems and Exercises 451
(c)m⎫summationdisplay
i=1n(i)
nm=n/n,w h e r e n=m⎫summationdisplay
i=1n(i)/m
(d)(1−n/n)k−11
mn(i)
n1
n(i)=(1−n/n)k−1/(nm)
(e)q⎫summationdisplay
k=11
nm(1−n/n)k−1=1
nm.
(f)The number of iterations is geometric with mean n√n
6.10. (a)P{X=i}=1/m, i=1,...,m.
(b) Step 2. Generate a uniform (0, 1) random variable U.
IfU<n(X)/n, go to step 3. Otherwise return to step 1.
Step 3. Generate a uniform (0, 1) random variable U,a n d
select the element on page Xin position [n(X )U]+1.
6.11. Yes, they are independent. This can be easily seen by
considering the equivalent question of whether XNis inde-
pendent of N. But this is indeed so, since knowing when the
ﬁrst random variable greater than coccurs does not affect
the probability distribution of its value, which is the uniform
distribution on ( c,1 ) .
6.12. Letpidenote the probability of obtaining ipoints on a
single throw of the dart. Then
p30=π/36
p20=4π/36−p30=π/12
p10=9π/36−p20−p30=5π/36
p0=1−p10−p20−p30=1−π/4
(a)π/12
(b)π/9
(c)1−π/4
(d)π(30/36 +20/12 +50/36) =35π/ 9
(e)(π/4)2
(f)2(π/ 36)(1 −π/4)+2(π/ 12)(5π/ 36)
6.13. LetZbe a standard normal random variable.
(a)
P⎧
⎨
⎩4⎫summationdisplay
i=1Xi>0⎫
⎬
⎭=P⎧
⎪⎪⎪⎨
⎪⎪⎪⎩4⎫summationdisplay
i=1Xi−6
√
24>−6√
24⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
LP{Z>−1.2247} L.8897
(b)
P⎧
⎨
⎩4⎫summationdisplay
i=1Xi>0⎫vextendsingle⎫vextendsingle⎫vextendsingle2⎫summationdisplay
i=1Xi=− 5⎫
⎬
⎭=P{X3+X4>5}
=P⎫braceleftBigg
X3+X4−3√
12>2/√
12⎫bracerightBigg
LP{Z>.5774} L.2818(c)
p⎧
⎨
⎩4⎫summationdisplay
i=1Xi>0|X 1=5⎫
⎬
⎭=P{X2+X3+X4>−5}
=P⎫braceleftBigg
X2+X3+X4−4.5√
18>−9.5/√
18⎫bracerightBigg
LP{Z>−2.239} L.9874
6.14. In the following, Cdoes not depend on n.
P{N=n|X=x}=fX|N(x|n)P {N=n}/fX(x)
=C1
(n−1)!(λx)n−1(1−p)n−1
=C(λ(1−p)x)n−1/(n−1)!
which shows that, conditional on X=x,N−1 is a Poisson
random variable with mean λ(1−p)x.T h a ti s ,
P{N=n|X=x}=P {N−1=n−1|X =x}
=e−λ(1−p)x(λ(1− p)x)n−1/(n−1)!, nÚ1.
6.15. (a)The Jacobian of the transformation is
J=⎫vextendsingle⎫vextendsingle⎫vextendsingle⎫vextendsingle⎫vextendsingle10
11⎫vextendsingle⎫vextendsingle⎫vextendsingle⎫vextendsingle⎫vextendsingle=1
As the equations u=x,v=x+yimply that x=u,y=
v−u, we obtain
f
U,V(u,v)=fX,Y(u,v−u)=1, 0 <u<1, 0 <v−u<1
or, equivalently,
fU,V(u,v)=1, max(v −1, 0)< u<min(v,1)
(b)For 0 <v<1,
fV(v)=⎫integraldisplayv
0du=v
For 1…v…2,
fV(v)=⎫integraldisplay1
v−1du=2−v
6.16. LetUbe a uniform random variable on (7, 11). If you
bidx,7…x…10, you will be the high bidder with probability
(P{U<x})3=⎫parenleftBigg
P⎫braceleftbiggU−7
4<x−7
4⎫bracerightbigg⎫parenrightBigg3
=⎫parenleftbiggx−7
4⎫parenrightbigg3
Hence, your expected gain—call it E[G(x)]—if you bid xis
E[G(x)] =1
64(x−7)3(10−x)
Calculus shows this is maximized when x=37/4.

<<<PAGE 465>>>

July 14, 2014 Z02_ROSSS4772_09_SE_BM page 452
452 Solutions to Self-Test Problems and Exercises
6.17. Leti1,i2,...,in, be a permutation of 1, 2, ...,n.T h e n
P{X1=i1,X2=i2,...,Xn=in}
=P{X1=i1}P{X2=i2}···P {Xn=in}
=pi1pi2···p in
=p1p2···pn
Therefore, the desired probability is n!p1p2···p n,w h i c h
reduces ton!
nnwhen all pi=1/n.
6.18. (a)Becausen⎫summationtext
i=1Xi=n⎫summationtext
i=1Yi, it follows that N=2M.
(b)Consider the n−kcoordinates whose Y-values are equal
to 0, and call them the red coordinates. Because the kcoordi-
nates whose X-values are equal to 1 are equally likely to be
any of the⎫parenleftBigg
n
k⎫parenrightBigg
sets of kcoordinates, it follows that the num-
ber of red coordinates among these kcoordinates has the
same distribution as the number of red balls chosen whenone randomly chooses kof a set of nballs of which n−k
are red. Therefore, Mis a hypergeometric random variable.
(c)E[N]=E[2M]=2E[M]=2k(n−k)
n
(d)Using the formula for the variance of a hypergeometric
given in Example 8j of Chapter 4, we obtain
Var(N)=4V a r(M)=4n−k
n−1k(1−k/n)(k/n)
6.19. (a)First note that Sn−Sk=n⎫summationtext
i=k+1Ziis a normal
random variable with mean 0 and variance n−kthat is
independent of Sk. Consequently, given that Sk=y,Snis
a normal random variable with mean yand variance n−k.
(b)Because the conditional density function of Skgiven that
Sn=xis a density function whose argument is y, anything
that does not depend on ycan be regarded as a constant. (For
instance, xis regarded as a ﬁxed constant.) In the following,
the quantities Ci,i=1, 2, 3, 4 are all constants that do not
depend on y:
fSk|Sn(y|x)=fSk,Sn(y,x)
fSn(x)
=C1fSn|Sk(x|y)f Sk(y)⎫parenleftBigg
where C1=1
fSn(x)⎫parenrightBigg
=C11√
2π√
n−ke−(x−y)2/2(n−k) 1√
2π√
ke−y2/2k
=C2exp⎫braceleftBigg
−(x−y)2
2(n−k)−y2
2k⎫bracerightBigg
=C3exp⎫braceleftBigg
2xy
2(n−k)−y2
2(n−k)−y2
2k⎫bracerightBigg
=C3exp⎫braceleftBigg
−n
2k(n−k)⎫parenleftbigg
y2−2k
nxy⎫parenrightbigg⎫bracerightBigg=C3exp⎧
⎨
⎩−n
2k(n−k)⎫bracketleftBigg⎫parenleftbigg
y−k
nx⎫parenrightbigg2
−⎫parenleftbiggk
nx⎫parenrightbigg2⎫bracketrightBigg⎫
⎬
⎭
=C4exp⎫braceleftBigg
−n
2k(n−k)⎫parenleftbigg
y−k
nx⎫parenrightbigg2⎫bracerightBigg
But we recognize the preceding as the density function
of a normal random variable with meank
nxand variance
k(n−k)
n.
6.20. (a)
P{X6>X1|X1=max(X 1,...,X5)}
=P{X6>X1,X1=max(X 1,...,X5)}
P{X1=max(X 1,...,X5)}
=P{X6=max(X 1,...,X6),X1=max(X 1,...,X5)}
1/5
=51
61
5=1
6
Thus, the probability that X6is the largest value is inde-
pendent of which is the largest of the other ﬁve values. (Of
course, this would not be true if the Xihad different distri-
butions.)
(b)One way to solve this problem is to condition on whether
X6>X1.N o w ,
P{X6>X2|X1=max(X 1,...,X5),X6>X1}=1
Also, by symmetry,
P{X6>X2|X1=max(X 1,...,X5),X6<X1}=1
2
From part (a),
P{X6>X1|X1=max(X 1,...,X5)}=1
6
Thus, conditioning on whether X6>X1yields the result
P{X6>X2|X1=max(X 1,...,X5)}=1
6+1
25
6=7
12
6.21. P⎫braceleftbig
X>s,Y>t⎫bracerightbig
=1−P⎫parenleftBig⎫braceleftbig
X…s⎫bracerightbig
∪⎫braceleftbig
Y…t⎫bracerightbig⎫parenrightBig
=1−P⎫braceleftbig
X…s⎫bracerightbig
−P⎫braceleftbig
Y…t⎫bracerightbig
+P⎫braceleftbig
X…s,Y…t⎫bracerightbig
Chapter 7
7.1. (a)d=m⎫summationtext
i=11/n(i)
(b)P{X=i}=P{[mU ]=i−1}=P {i−1…mU<i}=
1/m, i=1,...,m
(c)E⎫bracketleftbiggm
n(X)⎫bracketrightbigg
=m⎫summationtext
i=1m
n(i)P{X=i}=m⎫summationdisplay
i=1m
n(i)1
m=d

<<<PAGE 466>>>

July 14, 2014 Z02_ROSSS4772_09_SE_BM page 453
Solutions to Self-Test Problems and Exercises 453
7.2. LetIjequal 1 if the jth ball withdrawn is white and the
(j+1)is black, and let Ijequal 0 otherwise. If Xis the num-
ber of instances in which a white ball is immediately followed
by a black one, then we may express Xas
X=n+m−1⎫summationdisplay
j=1Ij
Thus,
E[X]=n+m−1⎫summationdisplay
j=1E[Ij]
=n+m−1⎫summationdisplay
j=1P{jthselection is white, (j+1)is black }
=n+m−1⎫summationdisplay
j=1P{jthselection is white }P{(j+1)is black |jthis white }
=n+m−1⎫summationdisplay
j=1n
n+mm
n+m−1
=nm
n+m
The preceding used the fact that each of the n+mballs is
equally likely to be the jth one selected and, given that that
selection is a white ball, each of the other n+m−1 balls
is equally likely to be the next ball chosen.
7.3. Arbitrarily number the couples, and then let Ijequal 1 if
married couple number j,j=1,..., 10, is seated at the same
table. Then, if Xrepresents the number of married couples
that are seated at the same table, we have
X=10⎫summationdisplay
j=1Ij
so
E[X]=10⎫summationdisplay
j=1E[Ij]
(a)To compute E[Ij] in this case, consider wife number j.
Since each of the⎫parenleftBigg
19
3⎫parenrightBigg
groups of size 3 not including her
is equally likely to be the remaining members of her table, it
follows that the probability that her husband is at her table is
⎫parenleftBigg
1
1⎫parenrightBigg⎫parenleftBigg
18
2⎫parenrightBigg
⎫parenleftBigg
19
3⎫parenrightBigg=3
19
Hence, E[Ij]=3/19 and so
E[X]=30/19(b)In this case, since the 2 men at the table of wife jare
equally likely to be any of the 10 men, it follows that theprobability that one of them is her husband is 2/10, so
E[I
j]=2/10 and E[X]=2
7.4. From Example 2i, we know that the expected number of
times that the die need be rolled until all sides have appearedat least once is 6(1 +1/2+1/3+1/4+1/5+1/6)=14.7.
Now, if we let X
idenote the total number of times that side
iappears, then, since6⎫summationtext
i=1Xiis equal to the total number of
rolls, we have
14.7=E⎡
⎣6⎫summationdisplay
i=1Xi⎤⎦=6⎫summationdisplay
i=1E[Xi]
But, by symmetry, E[Xi] will be the same for all i, and thus it
follows from the preceding that E[X1]=14.7/6 =2.45.
7.5. LetIjequal 1 if we win 1 when the jt hr e dc a r dt os h o w
is turned over, and let Ijequal 0 otherwise. (For instance, I1
will equal 1 if the ﬁrst card turned over is red.) Hence, if Xis
our total winnings, then
E[X]=E⎡
⎢⎣n⎫summationdisplay
j=1Ij⎤
⎥⎦=n⎫summationdisplay
j=1E[Ij]
Now, Ijwill equal 1 if jred cards appear before jblack cards.
By symmetry, the probability of this event is equal to 1/2;
therefore, E[Ij]=1/2a n d E[X]=n/2.
7.6. To see that N…n−1+I, note that if all events occur,
then both sides of the preceding inequality are equal to n,
whereas if they do not all occur, then the inequality reduces
toN…n−1, which is clearly true in this case. Taking expec-
tations yields
E[N]…n−1+E[I]
However, if we let Iiequal 1 if Aioccurs and 0 other-
wise, then
E[N]=E⎡
⎣n⎫summationdisplay
i=1Ii⎤⎦=n⎫summationdisplay
i=1E[Ii]=n⎫summationdisplay
i=1P(Ai)
Since E[I]=P(A1···A n), the result follows.
7.7.Imagine that the values 1, 2, ...,nare lined up in their
numerical order and that the kvalues selected are con-
sidered special. From Example 3e, the position of the ﬁrst
special value, equal to the smallest value chosen, has mean
1+n−k
k+1=n+1
k+1.
For a more formal argument, note that XÚjif none of
thej−1 smallest values are chosen. Hence,

<<<PAGE 467>>>

July 14, 2014 Z02_ROSSS4772_09_SE_BM page 454
454 Solutions to Self-Test Problems and Exercises
P{XÚj}=⎫parenleftBigg
n−j+1
k⎫parenrightBigg
⎫parenleftBigg
n
k⎫parenrightBigg =⎫parenleftBigg
n−k
j−1⎫parenrightBigg
⎫parenleftBigg
n
j−1⎫parenrightBigg
which shows that Xhas the same distribution as the random
variable of Example 3e (with the notational change that the
total number of balls is now nand the number of special balls
isk).
7.8. LetXdenote the number of families that depart after
the Sanchez family leaves. Arbitrarily number all the N−1
non-Sanchez families, and let Ir,1…r…N−1, equal 1 if
family rdeparts after the Sanchez family does. Then
X=N−1⎫summationdisplay
r=1Ir
Taking expectations gives
E[X]=N−1⎫summationdisplay
r=1P{family rdeparts after the Sanchez family}
Now consider any non-Sanchez family that checked in k
pieces of luggage. Because each of the k+jpieces of luggage
checked in either by this family or by the Sanchez family is
equally likely to be the last of these k+jto appear, the
probability that this family departs after the Sanchez fam-
ily isk
k+j. Because the number of non-Sanchez families who
checked in kpieces of luggage is nkwhen kZj,o rn j−1
when k=j, we obtain
E[X]=⎫summationdisplay
kknk
k+j−1
2
7.9. Let the neighborhood of any point on the rim be the arc
starting at that point and extending for a length 1. Consider
a uniformly chosen point on the rim of the circle—that is, the
probability that this point lies on a speciﬁed arc of length x
isx
2π—and let Xdenote the number of points that lie in its
neighborhood. With Ijdeﬁned to equal 1 if item number j
is in the neighborhood of the random point and to equal 0otherwise, we have
X=19⎫summationdisplay
j=1Ij
Taking expectations gives
E[X]=19⎫summationdisplay
j=1P{item jlies in the neighborhood of the
random point }
But because item jwill lie in its neighborhood if the random
point is located on the arc of length 1 going from item jin
the counterclockwise direction, it follows thatP{item jlies in the neighborhood of the random point }=1
2π
Hence,
E[X]=19
2π>3
Because E[X]>3, at least one of the possible values of X
must exceed 3, proving the result.
7.10. Ifg(x)=x1/2,t h e n
g/prime(x)=1
2x−1/2,g/prime/prime(x)=−1
4x−3/2
so the Taylor series expansion of√xabout λgives
√
XL√
λ+1
2λ−1/2(X−λ)−1
8λ−3/2(X−λ)2
Taking expectations yields
E[√
X]L√
λ+1
2λ−1/2E[X−λ]−1
8λ−3/2E[(X−λ)2]
=√
λ−1
8λ−3/2λ
=√
λ−1
8λ−1/2
Hence,
Var(√
X)=E[X]−(E[√
X])2
Lλ−⎫parenleftbigg√
λ−1
8λ−1/2⎫parenrightbigg2
=1/4−1
64λ
L1/4
7.11. Number the tables so that tables 1, 2, and 3 are the ones
with four seats and tables 4, 5, 6, and 7 are the ones with two
seats. Also, number the women, and let Xi,jequal 1 if woman
iis seated with her husband at table j. Note that
E[Xi,j]=⎫parenleftBigg
2
2⎫parenrightBigg⎫parenleftBigg
18
2⎫parenrightBigg
⎫parenleftBigg
20
4⎫parenrightBigg=3
95,j=1, 2, 3
and
E[Xi,j]=1⎫parenleftBigg
20
2⎫parenrightBigg=1
190,j=4, 5, 6, 7
Now, Xdenotes the number of married couples that are
seated at the same table, we have

<<<PAGE 468>>>

July 14, 2014 Z02_ROSSS4772_09_SE_BM page 455
Solutions to Self-Test Problems and Exercises 455
E[X]=E⎡
⎢⎣10⎫summationdisplay
i=17⎫summationdisplay
j=1Xi,j⎤
⎥⎦
=10⎫summationdisplay
i=13⎫summationdisplay
j=1E[Xi,j]+10⎫summationdisplay
i=17⎫summationdisplay
j=4E[Xi,j]
7.12. LetXiequal 1 if individual idoes not recruit anyone,
and let Xiequal 0 otherwise. Then
E[Xi]=P{idoes not recruit any of i+1,i+2,...,n}
=i−1
ii
i+1···n−2
n−1
=i−1
n−1
Hence,
E⎡
⎣n⎫summationdisplay
i=1Xi⎤⎦=n⎫summationdisplay
i=1i−1
n−1=n
2
From the preceding, we also obtain
Var(Xi)=i−1
n−1⎫parenleftbigg
1−i−1
n−1⎫parenrightbigg
=(i−1)(n−i)
(n−1)2
Now, for i<j,
E[XiXj]=i−1
i···j−2
j−1j−2
jj−1
j+1···n−3
n−1
=(i−1)(j−2)
(n−2)(n−1)
Thus,
Cov(X i,Xj)=(i−1)(j−2)
(n−2)(n−1)−i−1
n−1j−1
n−1
=(i−1)(j−n)
(n−2)(n−1)2
Therefore,
Var⎛⎝n⎫summationdisplay
i=1Xi⎞⎠=n⎫summationdisplay
i=1Var(Xi)+2n−1⎫summationdisplay
i=1n⎫summationdisplay
j=i+1Cov(Xi,Xj)
=n⎫summationdisplay
i=1(i−1)(n−i)
(n−1)2+2n−1⎫summationdisplay
i=1n⎫summationdisplay
j=i+1(i−1)(j−n)
(n−2)(n−1)2
=1
(n−1)2n⎫summationdisplay
i=1(i−1)(n−i)
−1
(n−2)(n−1)2n−1⎫summationdisplay
i=1(i−1)(n−i)(n−i−1)7.13. LetXiequal 1 if the ith triple consists of one of each
type of player. Then
E[Xi]=⎫parenleftBigg
2
1⎫parenrightBigg⎫parenleftBigg
31⎫parenrightBigg⎫parenleftBigg
41⎫parenrightBigg
⎫parenleftBigg
93⎫parenrightBigg =2
7
Hence, for part (a), we obtain
E⎡
⎣3⎫summationdisplay
i=1Xi⎤⎦=6/7
It follows from the preceding that
Var(X
i)=(2/7)(1 −2/7)=10/49
Also, for iZj,
E[XiXj]=P{Xi=1,Xj=1}
=P{Xi=1}P{Xj=1|Xi=1}
=⎫parenleftBigg
2
1⎫parenrightBigg⎫parenleftBigg
31⎫parenrightBigg⎫parenleftBigg
41⎫parenrightBigg
⎫parenleftBigg
9
3⎫parenrightBigg⎫parenleftBigg
1
1⎫parenrightBigg⎫parenleftBigg
21⎫parenrightBigg⎫parenleftBigg
31⎫parenrightBigg
⎫parenleftBigg
6
3⎫parenrightBigg
=6/70
Hence, for part (b), we obtain
Var⎛
⎝3⎫summationdisplay
i=1Xi⎞⎠=3⎫summationdisplay
i=1Var(Xi)+2⎫summationdisplay⎫summationdisplay
j>1Cov(Xi,Xj)
=30/49 +2⎫parenleftBigg
3
2⎫parenrightBigg⎫parenleftbigg6
70−4
49⎫parenrightbigg
=312
490
7.14. LetXi,i=1,..., 13, equal 1 if the ith card is an ace and
letXib e0o t h e r w i s e .L e tY jequal 1 if the jth card is a spade
and let Yj=0o t h e r w i s e .N o w ,
Cov(X ,Y)=Cov⎛
⎜⎝13⎫summationdisplay
i=1Xi,13⎫summationdisplay
j=1Yj⎞
⎟⎠
=13⎫summationdisplay
i=113⎫summationdisplay
j=1Cov(X i,Yj)
However, Xiis clearly independent of Yjbecause know-
ing the suit of a particular card gives no information about
whether it is an ace and thus cannot affect the probabil-ity that another speciﬁed card is an ace. More formally, let
A
i,s,Ai,h,Ai,d,Ai,cbe the events, respectively, that card iis
a spade, a heart, a diamond, and a club. Then

<<<PAGE 469>>>

July 14, 2014 Z02_ROSSS4772_09_SE_BM page 456
456 Solutions to Self-Test Problems and Exercises
P{Yj=1}=1
4(P{Yj=1|Ai,s}+P{Yj=1|Ai,h}
+P{Yj=1|Ai,d}+P{Yj=1|Ai,c})
But, by symmetry, we have
P{Yj=1|Ai,s}=P{Yj=1|Ai,h}=P{Yj=1|Ai,d}
=P{Yj=1|Ai,c}
Therefore,
P{Yj=1}=P {Yj=1|Ai,s}
As the preceding implies that
P{Yj=1}=P {Yj=1|Ac
i,s}
we see that YjandXiare independent. Hence, Cov(X i,Yj)=
0, and thus Cov (X,Y)=0.
The random variables XandY, although uncorrelated,
are not independent. This follows, for instance, from the
fact that
P{Y=13|X=4}= 0ZP{Y=13}
7.15. (a)Your expected gain without any information is 0.
(b)You should predict heads if p>1/2 and tails otherwise.
(c)Conditioning on V, the value of the coin, gives
E[Gain] =⎫integraldisplay1
0E[Gain|V =p]dp
=⎫integraldisplay1/2
0[1(1− p)−1(p)] dp+⎫integraldisplay1
1/2[1(p)− 1(1−p)] dp
=1/2
7.16. Given that the name chosen appears in n(X)different
positions on the list, since each of these positions is equallylikely to be the one chosen, it follows that
E[I|n(X)]=P{I=1|n(X )}= 1/n(X )
Hence,
E[I]=E[1/n(X )]
Thus, E[mI]=E[m/n(X )]=d.
7.17. Letting Xiequal 1 if a collision occurs when the ith item
is placed, and letting it equal 0 otherwise, we can express the
total number of collisions Xas
X=m⎫summationdisplay
i=1Xi
Therefore,
E[X]=m⎫summationdisplay
i=1E[Xi]To determine E[Xi], condition on the cell in which it is
placed.
E[Xi]=⎫summationdisplay
jE[Xi|placed in cell j]pj
=⎫summationdisplay
jP{icauses collision |placed in cell j]pj
=⎫summationdisplay
j[1−(1−pj)i−1]pj
=1−⎫summationdisplay
j(1−pj)i−1pj
The next to last equality used the fact that, conditional onitemibeing placed in cell j,i t e mi will cause a collision if any
of the preceding i−1 items were put in cell j. Thus,
E[X]=m−m⎫summationdisplay
i=1n⎫summationdisplay
j=1(1−pj)i−1pj
Interchanging the order of the summations gives
E[X]=m−n+n⎫summationdisplay
j=1(1−pj)m
Looking at the result shows that we could have derivedit more easily by taking expectations of both sides of theidentity
number of nonempty cells =m−X
The expected number of nonempty cells is then found bydeﬁning an indicator variable for each cell, equal to 1 if thatcell is nonempty and to 0 otherwise, and then taking the
expectation of the sum of these indicator variables.
7.18. LetLdenote the length of the initial run. Conditioning
on the ﬁrst value gives
E[L]=E[L|ﬁrst value is one]n
n+m
+E[L|ﬁrst value is zero]m
n+m
Now, if the ﬁrst value is one, then the length of the run will bethe position of the ﬁrst zero when considering the remaining
n+m−1 values, of which n−1 are ones and mare zeroes.
(For instance, if the initial value of the remaining n+m−1
is zero, then L=1.) As a similar result is true given that
the ﬁrst value is a zero, we obtain from the preceding, upon
using the result from Example 3e, that
E[L]=n+m
m+1n
n+m+n+m
n+1m
n+m
=n
m+1+m
n+1
7.19. LetXbe the number of ﬂips needed for both boxes to
become empty, and let Ydenote the number of heads in the
ﬁrstn+mﬂips. Then

<<<PAGE 470>>>

July 14, 2014 Z02_ROSSS4772_09_SE_BM page 457
Solutions to Self-Test Problems and Exercises 457
E[X]=n+m⎫summationdisplay
i=0E[X|Y=i]P{Y=i}
=n+m⎫summationdisplay
i=0E[X|Y=i]⎫parenleftBigg
n+m
i⎫parenrightBigg
pi(1−p)n+m−i
Now, if the number of heads in the ﬁrst n+mﬂips is i,
i…n, then the number of additional ﬂips is the number of
ﬂips needed to obtain an additional n−iheads. Similarly, if
the number of heads in the ﬁrst n+mﬂips is i,i>n, then,
because there would have been a total of n+m−i<m
tails, the number of additional ﬂips is the number needed
to obtain an additional i−nheads. Since the number of
ﬂips needed for joutcomes of a particular type is a negative
binomial random variable whose mean is jdivided by the
probability of that outcome, we obtain
E[X]=n⎫summationdisplay
i=0n−i
p⎫parenleftBigg
n+m
i⎫parenrightBigg
pi(1−p)n+m−i
+n+m⎫summationdisplay
i=n+1i−n
1−p⎫parenleftBigg
n+m
i⎫parenrightBigg
pi(1−p)n+m−i
7.20. Taking expectations of both sides of the identity given
in the hint yields
E[Xn]=E⎫bracketleftbigg
n⎫integraldisplayq
0xn−1IX(x)dx⎫bracketrightbigg
=n⎫integraldisplayq
0E[xn−1IX(x)]dx
=n⎫integraldisplayq
0xn−1E[IX(x)]dx
=n⎫integraldisplayq
0xn−1F(x)dx
Taking the expectation inside the integral sign is justiﬁed
because all the random variables IX(x),0 <x<q, are non-
negative.
7.21. Consider a random permutation I1,...,Inthat is
equally likely to be any of the n! permutations. Then
E[aIjaIj+1]=⎫summationdisplay
kE[aIjaIj+1|Ij=k]P{Ij=k}
=1
n⎫summationdisplay
kakE[aIj+1|Ij=k]
=1
n⎫summationdisplay
kak⎫summationdisplay
iaiP{Ij+1=i|Ij=k}
=1
n(n−1)⎫summationdisplay
kak⎫summationdisplay
iZkai
=1
n(n−1)⎫summationdisplay
kak(−ak)
<0where the ﬁnal equality followed from the assumption that⎫summationtextn
i=1ai=0.Since the preceding shows that
E⎡
⎢⎣n−1⎫summationdisplay
j=1aIjaIj+1⎤
⎥⎦<0
it follows that there must be some permutation i1,...,infor
which
n−1⎫summationdisplay
j=1aijaij+1<0
7.22. (a)E[X]=λ1+λ2,E[Y]=λ2+λ3
(b)Cov(X,Y)=Cov(X1+X2,X2+X3)
=Cov(X1,X2+X3)+Cov(X 2,X2+X3)
=Cov(X2,X2)
=Var(X2)
=λ2
(c)Conditioning on X2gives
P{X=i,Y=j}
=⎫summationdisplay
kP{X=i,Y=j|X2=k}P{X2=k}
=⎫summationdisplay
kP{X1=i−k,X3=j−k|X 2=k}e−λ2λk
2/k!
=⎫summationdisplay
kP{X1=i−k,X3=j−k}e−λ2λk
2/k!
=⎫summationdisplay
kP{X1=i−k}P{X3=j−k}e−λ2λk2/k!
=min(i,j )⎫summationdisplay
k=0e−λ1λi−k
1
(i−k)!e−λ3λj−k
3
(j−k)!e−λ2λk2
k!
7.23. Corr⎛
⎜⎝⎫summationdisplay
iXi,⎫summationdisplay
jYj⎞
⎟⎠=Cov(⎫summationtext
iXi,⎫summationtext
jYj)
⎫radicalBig
Var(⎫summationtext
iXi)Var(⎫summationtext
jYj)
=⎫summationtext
i⎫summationtext
jCov(X i,Yj)
⎫radicalBig
nσ2xnσ2y
=⎫summationtext
iCov(X i,Yi)+⎫summationtext
i⎫summationtext
jZiCov(X i,Yj)
nσxσy
=nρσxσy
nσxσy
=ρ
where the next to last equality used the fact that
Cov(X i,Yi)=ρσxσy
7.24. LetXiequal 1 if the ith card chosen is an ace, and let it
equal 0 otherwise. Because
X=3⎫summationdisplay
i=1Xi

<<<PAGE 471>>>

July 14, 2014 Z02_ROSSS4772_09_SE_BM page 458
458 Solutions to Self-Test Problems and Exercises
andE[Xi]=P{Xi=1}= 1/13, it follows that E[X]=3/13.
But, with Abeing the event that the ace of spades is chosen,
we have
E[X]=E[X|A]P(A)+E[X|Ac]P(Ac)
=E[X|A]3
52+E[X|Ac]49
52
=E[X|A]3
52+49
52E⎡
⎣3⎫summationdisplay
i=1Xi|Ac⎤⎦
=E[X|A]3
52+49
523⎫summationdisplay
i=1E[Xi|Ac]
=E[X|A]3
52+49
5233
51
Using that E[X]=3/13 gives the result
E[X|A]=52
3⎫parenleftbigg3
13−49
523
17⎫parenrightbigg
=19
17=1.1176
Similarly, letting Lbe the event that at least one ace is cho-
sen, we have
E[X]=E[X|L]P(L)+E[X|Lc]P(Lc)
=E[X|L]P(L)
=E[X|L]⎫parenleftbigg
1−48·47·46
52·51·50⎫parenrightbigg
Thus,
E[X|L]=3/13
1−48·47·46
52·51·50L1.0616
Another way to solve this problem is to number the four
aces, with the ace of spades having number 1, and then let
Yiequal 1 if ace number iis chosen and 0 otherwise. Then
E[X|A]=E⎡
⎣4⎫summationdisplay
i=1Yi|Y1=1⎤⎦
=1+4⎫summationdisplay
i=2E[Yi|Y1=1]
=1+3·2
51=19/17
where we used that the fact given that the ace of spades is
chosen the other two cards are equally likely to be any pair
of the remaining 51 cards; so the conditional probability thatany speciﬁed card (not equal to the ace of spades) is chosen
is 2/51. Also,
E[X|L]=E⎡
⎣4⎫summationdisplay
i=1Yi|L⎤⎦=4⎫summationdisplay
i=1E[Yi|L]=4P{Y1=1|L}Because
P{Y1=1|L}=P (A|L) =P(AL)
P(L)=P(A)
P(L)=3/52
1−48·47·46
52·51·50
we obtain the same answer as before.
7.25. (a)E[I|X=x]=P{Z<X|X=x}=P {Z<x|X=x}
=P{Z<x}=/Phi1( x)
(b)It follows from part (a) that E[I|X]=/Phi1(X).T h e r e f o r e ,
E[I]=E[E[I|X]]=E[/Phi1(X)]
The result now follows because E[I]=P{I=1}=P{Z<X}.
(c)Since X−Zis normal with mean μand variance 2, we
have
P{X>Z}=P{X−Z>0}
=P⎫braceleftBigg
X−Z−μ√
2>−μ√
2⎫bracerightBigg
=1−/Phi1⎫parenleftBigg
−μ√
2⎫parenrightBigg
=/Phi1⎫parenleftBigg
μ√
2⎫parenrightBigg
7.26. LetNbe the number of heads in the ﬁrst n+m−1
ﬂips. Let M=max(X ,Y)be the number of ﬂips needed to
amass at least nheads and at least mtails. Conditioning on
Ngives
E[M]=⎫summationdisplay
iE[M|N=i]P{N=i}
=n−1⎫summationdisplay
i=0E[M|N=i]P{N=i}+n+m−1⎫summationdisplay
i=nE[M|N=i]P{N=i}
Now, suppose we are given that there are a total of iheads
in the ﬁrst n+m−1 trials. If i<n, then we have already
obtained at least mtails, so the additional number of ﬂips
needed is equal to the number needed for an additional n−i
heads; similarly, if iÚn, then we have already obtained at
least nheads, so the additional number of ﬂips needed is
equal to the number needed for an additional m−(n+
m−1−i)tails. Consequently, we have
E[M]=n−1⎫summationdisplay
i=0⎫parenleftbigg
n+m−1+n−i
p⎫parenrightbigg
P{N=i}
+n+m−1⎫summationdisplay
i=n⎫parenleftbigg
n+m−1+i+1−n
1−p⎫parenrightbigg
P{N=i}
=n+m−1+n−1⎫summationdisplay
i=0n−i
p⎫parenleftbiggn+m−1
i⎫parenrightbigg
pi(1−p)n+m−1−i
+n+m−1⎫summationdisplay
i=ni+1−n
1−p⎫parenleftbiggn+m−1
i⎫parenrightbigg
pi(1−p)n+m−1−i

<<<PAGE 472>>>

July 14, 2014 Z02_ROSSS4772_09_SE_BM page 459
Solutions to Self-Test Problems and Exercises 459
The expected number of ﬂips to obtain either nheads or m
tails, E[min(X ,Y)], is now given by
E[min(X ,Y)]=E[X+Y−M]=n
p+m
1−p−E[M]
7.27. This is just the expected time to collect n−1o ft h en
types of coupons in Example 2i. By the results of that exam-
ple the solution is
1+n
n−1+n
n−2+...+n
2
7.28. With q=1−p,
E[X]=q⎫summationdisplay
i=1P{XÚi}=n⎫summationdisplay
i=1P{XÚi}=n⎫summationdisplay
i=1qi−1=1−qn
p
7.29. Cov(X ,Y)=E[XY ]−E[X]E[Y]
=P(X=1,Y=1)−P(X=1)P(Y=1)
Hence,
Cov(X,Y)=03P(X=1,Y=1)=P(X=1)P(Y=1)
Because
Cov(X ,Y)=Cov(1 −X,1−Y)=−Cov(1−X,Y)
=−Cov(X ,1−Y)
the preceding shows that all of the following are equivalent
when XandYare Bernoulli:
1. Cov(X ,Y)=0
2.P(X=1,Y=1)=P(X=1)P(Y=1)
3.P(1−X=1, 1−Y=1)=P(1−X=1)P(1−Y=1)
4.P(1−X=1,Y=1)=P(1−X=1)P(Y=1)
5.P(X=1, 1−Y=1)=P(X=1)P(1−Y=1)
7.30. Number the individuals, and let Xi,jequal 1 if the jth
individual who has hat size ichooses a hat of that size, and
letXi,jequal 0 otherwise. Then the number of individuals
who choose a hat of their size is
X=r⎫summationdisplay
i=1ni⎫summationdisplay
j=1Xi,j
Hence,
E[X]=r⎫summationdisplay
i=1ni⎫summationdisplay
j=1E[Xi,j]=r⎫summationdisplay
i=1ni⎫summationdisplay
j=1hi
n=1
nr⎫summationdisplay
i=1hini
7.31. Letting σ2xandσ2ybe, respectively, the variances of X
and of Y, we obtain, upon squaring both sides, the equivalent
inequality
Var(X+Y)…σ2
x+σ2
y+2σxσyUsing that Var(X +Y)=σ2x+σ2y+2Cov(X,Y), the pre-
ceding inequality becomes
Corr(X ,Y)=Cov(X,Y)
σxσy…1
which has already been established.
7.32. Take expectations, using that the expected value of a
sum is the sum of the expectations, and then differentiate.
Chapter 8
8.1. LetXdenote the number of sales made next week, and
note that Xis integral. From Markov’s inequality, we obtain
the following:
(a)P{X>18}=P {XÚ19}…E[X]
19=16/19
(b)P{X>25}=P {XÚ26}…E[X]
26=16/26
8.2. (a)P{10…X…22}=P {|X−16|…6}
=P{|X−μ|…6}
=1−P{|X−μ|>6}
Ú1−9/36=3/4
(b)P{XÚ19}=P {X−16Ú3}…9
9+9=1/2
In part (a), we used Chebyshev’s inequality; in part (b), we
used its one-sided version. (See Proposition 5.1.)
8.3. First note that E[X−Y]=0a n d
Var(X−Y)=Var(X)+Var(Y)−2Cov(X ,Y)=28
Using Chebyshev’s inequality in part (a) and the one-sidedversion in parts (b) and (c) gives the following results:
(a)P{|X−Y|>15}…28/225
(b)P{X−Y>15}…28
28+225=28/253
(c)P{Y−X>15}…28
28+225=28/253
8.4. IfXis the number produced at factory AandYthe
number produced at factory B,t h e n
E[Y−X]=− 2, Var(Y −X)=36+9=45
P{Y−X>0}= P{Y−XÚ1}
=P{Y−X+2Ú3}…45
45+9=45/54
8.5. Note ﬁrst that
E[Xi]=⎫integraldisplay1
02x2dx=2/3

<<<PAGE 473>>>

July 14, 2014 Z02_ROSSS4772_09_SE_BM page 460
460 Solutions to Self-Test Problems and Exercises
Now use the strong law of large numbers to obtain
r=limn→qn
Sn
=limn→q1
Sn/n
=1
limn→qSn/n
=1/(2/3)=3/2
8.6. Because E[Xi]=2/3a n d
E[X2
i]=⎫integraldisplay1
02x3dx=1/2
we have Var(X i)=1/2−(2/3)2=1/18. Thus, if there are
ncomponents on hand, then
P{SnÚ35}=P {SnÚ34.5} (the continuity correction )
=P⎫braceleftBigg
Sn−2n/3⎫radicalbig
n/18Ú34.5−2n/3⎫radicalbig
n/18⎫bracerightBigg
LP⎫braceleftBigg
ZÚ34.5−2n/3⎫radicalbig
n/18⎫bracerightBigg
where Zis a standard normal random variable. Since
P{Z>−1.284}=P {Z<1.284} L.90
we see that nshould be chosen so that
(34.5 −2n/3) L−1.284⎫radicalbig
n/18
A numerical computation gives the result n=55.
8.7. IfXis the time required to service a machine, then
E[X]=.2+.3=.5
Also, since the variance of an exponential random variable
is equal to the square of its mean, we have
Var(X)=(.2)2+(.3)2=.13
Therefore, with Xibeing the time required to service job
i,i=1,..., 20, and Zbeing a standard normal random vari-
able, it follows that
P{X1+···+ X20<8}=P⎫braceleftBigg
X1+···+ X20−10√
2.6<8−10√
2.6⎫bracerightBigg
LP{Z<−1.24035}
L.1074
8.8. Note ﬁrst that if Xis the gambler’s winnings on a single
bet, then
E[X]=−.7−.4+1=−.1,E[X2]=.7+.8+10=11.5
→Var(X)=11.49Therefore, with Zhaving a standard normal distribution,
P{X1+···+ X100…−.5}=P⎫braceleftBigg
X1+···+ X100+10√
11.49…−.5+ 10√
11.49⎫bracerightBigg
LP{Z….2803}
L.6104
8.9. Using the notation of Problem 8.7, we have
P{X1+···+ X20<t}=P⎫braceleftBigg
X1+···+ X20−10√
2.6<t−10√
2.6⎫bracerightBigg
LP⎫braceleftBigg
Z<t−10√
2.6⎫bracerightBigg
Now, P{Z<1.645} L.95, so tshould be such that
t−10√
2.6L1.645
which yields tL12.65.
8.10. If the claim were true, then, by the central limit theo-
rem, the average nicotine content (call it X) would approx-
imately have a normal distribution with mean 2.2 and stan-
dard deviation .03. Thus, the probability that it would be as
high as 3.1 is
P{X>3.1}=P⎫braceleftbiggX−2.2
.03>3.1−2.2
.03⎫bracerightbigg
LP{Z>30}
L0
where Zis a standard normal random variable.
8.11. (a)If we arbitrarily number the batteries and let Xi
denote the life of battery i,i=1,..., 40, then the Xiare
independent and identically distributed random variables.
To compute the mean and variance of the life of, say, bat-
tery 1, we condition on its type. Letting Iequal 1 if battery 1
is type Aand letting it equal 0 if it is type B, we have
E[X1|I=1]=50 , E[X1|I=0]=30
yielding
E[X1]=50P{I=1}+ 30P{I=0}=50(1/2) +30(1/2) =40
In addition, using the fact that E[W2]=(E[W])2+Var(W),
we have
E[X2
1|I=1]=(50)2+(15)2=2725 ,
E[X2
1|I=0]=(30)2+62=936
yielding
E[X2
1]=(2725)(1/2) +(936)(1/2) =1830.5
Thus, X1,...,X40are independent and identically dis-
tributed random variables having mean 40 and variance
1830.5 −1600=230.5. Hence, with S=⎫summationtext40
i=1Xi, we have
E[S]=40(40) =1600 , Var(S) =40(230.5) =9220

<<<PAGE 474>>>

July 14, 2014 Z02_ROSSS4772_09_SE_BM page 461
Solutions to Self-Test Problems and Exercises 461
and the central limit theorem yields
P{S>1700}= P⎫braceleftBigg
S−1600√
9220>1700−1600√
9220⎫bracerightBigg
LP{Z>1.041}
=1−/Phi1(1.041) =.149
(b)For this part, let SAbe the total life of all the type Abat-
teries and let SBb et h et o t a ll i f eo fa l lt h et y p eB batteries.
Then, by the central limit theorem, SAhas approximately a
normal distribution with mean 20 (50)=1000 and variance
20(225) =4500, and SBhas approximately a normal distri-
bution with mean 20(30) =600 and variance 20 (36)=720.
Because the sum of independent normal random variables
is also a normal random variable, it follows that SA+SB
is approximately normal with mean 1600 and variance 5220.
Consequently, with S=SA+SB,
P{S>1700}= P⎫braceleftBigg
S−1600√
5220>1700−1600√
5220⎫bracerightBigg
LP{Z>1.384}
=1−/Phi1(1.384) =.083
8.12. LetNdenote the number of doctors who volunteer.
Conditional on the event N=i, the number of patients
seen is distributed as the sum of iindependent Poisson ran-
dom variables with common mean 30. Because the sum ofindependent Poisson random variables is also a Poisson ran-
dom variable, it follows that the conditional distribution of
Xgiven that N=iis Poisson with mean 30 i. Therefore,
E[X|N]=30N Var(X|N)=30N
As a result,
E[X]=E[E[X|N]]=30E[N]=90
Also, by the conditional variance formula,
Var(X)=E[Var(X |N)]+Var(E[X|N])
=30E[N]+(30)2Var(N)
Because
Var(N)=1
3(22+32+42)−9=2/3
we obtain Var (X)=690.
To approximate P{X>65}, we would not be justiﬁed in
assuming that the distribution of Xis approximately that of
a normal random variable with mean 90 and variance 690.What we do know, however, is that
P{X>65}= 4⎫summationdisplay
i=2P{X>65|N=i}P{N=i}=1
34⎫summationdisplay
i=2Pi(65)where Pi(65) is the probability that a Poisson random vari-
able with mean 30 iis greater than 65. That is,
Pi(65)=1−65⎫summationdisplay
j=0e−30i(30i)j/j!
Because a Poisson random variable with mean 30 ihas the
same distribution as does the sum of 30i independent Pois-
son random variables with mean 1, it follows from the central
limit theorem that its distribution is approximately normalwith mean and variance equal to 30 i. Consequently, with
X
ibeing a Poisson random variable with mean 30 iandZ
being a standard normal random variable, we can approxi-mate
Pi(65) as follows:
Pi(65)=P{X>65}
=P{XÚ65.5}
=P⎫braceleftBigg
X−30i√
30iÚ65.5−30i√
30i⎫bracerightBigg
LP⎫braceleftBigg
ZÚ65.5−30i√
30i⎫bracerightBigg
Therefore,
P2(65)LP{ZÚ.7100} L.2389
P3(65)LP{ZÚ−2.583} L.9951
P4(65)LP{ZÚ−4.975} L1
leading to the result
P{X>65}L.7447
If we would have mistakenly assumed that Xwas approx-
imately normal, we would have obtained the approximateanswer .8244. (The exact probability is .7440.)
8.13. Take logarithms and then apply the strong law of large
numbers to obtain
log⎡
⎢⎣⎛
⎝n⎫productdisplay
i=1Xi⎞⎠1/n⎤
⎥⎦=1
nn⎫summationdisplay
i=1log(X i)→E[log(X i)]
Therefore,⎛
⎝n⎫productdisplay
i=1Xi⎞⎠1/n
→eE[log(X i)]
8.14. LetXibe the time it takes to process book i, and let
Sn=⎫summationtextn
i=1Xi.
(a)With Zbeing a standard normal
P⎫braceleftbig
S40>420⎫bracerightbig
=P⎫braceleftBigg
S40−400√
40·9>420−400√
40·9⎫bracerightBigg
LP⎫braceleftBigg
Z>20√
360⎫bracerightBigg
L.146

<<<PAGE 475>>>

July 14, 2014 Z02_ROSSS4772_09_SE_BM page 462
462 Solutions to Self-Test Problems and Exercises
(b) P⎫braceleftbig
S25…240⎫bracerightbig
=P⎫braceleftBigg
S25−250√
25·9…240−250√
25·9⎫bracerightBigg
LP⎫braceleftbigg
Z…−10
15⎫bracerightbigg
L.2525
We have assumed that the successive book processing times
are independent.
Chapter 9
9.1. From axiom (iii), it follows that the number of events
that occur between times 8 and 10 has the same distributionas the number of events that occur by time 2 and thus is a
Poisson random variable with mean 6. Hence, we obtain thefollowing solutions for parts (a) and (b):
(a)P{N(10)−N(8)=0}=e
−6
(b)E[N(10)−N(8)]=6
(c)It follows from axioms (ii) and (iii) that from any point
in time onward, the process of events occurring is a Pois-
son process with rate λ. Hence, the expected time of the
ﬁfth event after 2 P. M . is 2+E[S5]=2+5/3. That is, the
expected time of this event is 3:40 P. M .
9.2. (a)
P{N(1/3)=2|N(1)=2}
=P{N(1/3)=2,N(1)=2}
P{N(1)=2}
=P{N(1/3)=2,N(1)−N(1/3)=0}
P{N(1)=2}
=P{N(1/3)=2}P{N(1)−N(1/3)=0}
P{N(1)=2}(by axiom (ii))
=P{N(1/3)=2}P{N(2/3)=0}
P{N(1)=2}(by axiom (iii))
=e−λ/3(λ/3)2/2!e−2λ/ 3
e−λλ2/2!
=1/9
(b)
P{N(1/2)Ú1|N(1)=2}= 1−P{N(1/2)=0|N(1)=2}
=1−P{N(1/2)=0,N(1)=2}
P{N(1)=2}
=1−P{N(1/2)=0,N(1)−N(1/2)=2}
P{N(1)=2}
=1−P{N(1/2)=0}P{N(1)−N(1/2)=2}
P{N(1)=2}
=1−P{N(1/2)=0}P{N(1/2)=2}
P{N(1)=2}
=1−e−λ/2e−λ/2(λ/2)2/2!
e−λλ2/2!
=1−1/4=3/4
9.3. Fix a point on the road and let Xnequal 0 if the nth vehi-
cle to pass is a car and let it equal 1 if it is a truck, nÚ1. Wenow suppose that the sequence Xn,nÚ1, is a Markov chain
with transition probabilities
P0,0=5/6, P0,1=1/6, P1,0=4/5, P1,1=1/5
Then the long-run proportion of times is the solution of
π0=π0(5/6) +π1(4/5)
π1=π0(1/6) +π1(1/5)
π0+π1=1
Solving the preceding equations gives
π0=24/29 π1=5/29
Thus, 2400/29 L83 percent of the vehicles on the road
are cars.
9.4. The successive weather classiﬁcations constitute a
Markov chain. If the states are 0 for rainy, 1 for sunny, and 2
for overcast, then the transition probability matrix is as fol-
lows:
P=01 /21/2
1/31 /31/3
1/31 /31/3
The long-run proportions satisfy
π0=π1(1/3) +π2(1/3)
π1=π0(1/2) +π1(1/3) +π2(1/3)
π2=π0(1/2) +π1(1/3) +π2(1/3)
1=π0+π1+π2
The solution of the preceding system of equations is
π0=1/4, π1=3/8, π2=3/8
Hence, three-eighths of the days are sunny and one-fourthare rainy.
9.5. (a)A direct computation yields
H(X)/H(Y)L1.06
(b)Both random variables take on two of their values with
the same probabilities .35 and .05. The difference is that if
they do not take on either of those values, then X, but not Y,
is equally likely to take on any of its three remaining possi-
ble values. Hence, from Theoretical Exercise 9.13, we would
expect the result of part (a).
Chapter 10
10.1. (a)1=C⎫integraltext1
0exdx*C=1/(e−1)
(b)F(x)=C⎫integraltextx
0eydy=ex−1
e−1,0…x…1
Hence, if we let X=F−1(U),t h e n
U=eX−1
e−1

<<<PAGE 476>>>

July 14, 2014 Z02_ROSSS4772_09_SE_BM page 463
Solutions to Self-Test Problems and Exercises 463
or
X=log(U (e−1)+1)
Thus, we can simulate the random variable Xby generating a
random number Ua n dt h e ns e t t i n g X=log(U (e−1)+1).
10.2. Use the acceptance–rejection method with g(x)=
1, 0<x<1. Calculus shows that the maximum value of
f(x)/g(x)occurs at a value of x,0<x<1, such that
2x−6x2+4x3=0
or, equivalently, when
4x2−6x+2=(4x−2)(x−1)=0
The maximum thus occurs when x=1/2, and it follows that
C=maxf(x)/g(x)=30(1/4 −2/8+1/16) =15/8
Hence, the algorithm is as follows:
Step 1. Generate a random number U1.
Step 2. Generate a random number U2.
Step 3. IfU2…16(U2
1−2U3
1+U4
1),s e tX =U1; else
return to Step 1.
10.3. It is most efﬁcient to check the higher probability val-
ues ﬁrst, as in the following algorithm:Step 1. Generate a random number U.
Step 2. IfU….35, set X=3 and stop.
Step 3. IfU….65, set X=4 and stop.
Step 4. IfU….85, set X=2 and stop.
Step 5. X=1.
10.4. 2μ−X
10.5. (a)Generate 2 nindependent exponential random
variables with mean 1, Xi,Yi,i=1,...,n, and then use the
estimatorn⎫summationtext
i=1eXiYi/n.
(b)We can use XYas a control variate to obtain an estimator
of the type
n⎫summationdisplay
i=1(eXiYi+cXiYi)/n
Another possibility would be to use XY+X2Y2/2a st h e
control variate and so obtain an estimator of the type
n⎫summationdisplay
i=1(eXiYi+c[XiYi+X2
iY2
i/2−1/2])/ n
The motivation behind the preceding formula is based on the
fact that the ﬁrst three terms of the MacLaurin series expan-
sion of exyare 1 +xy+(x2y2)/2.

<<<PAGE 477>>>

Index
A
absolutely continuous random
variables, seecontinuous random
variables
Analytic Theory of Probability
(Laplace), 378
antithetic variables, 427
Archimedes, 197
Ars Conjectandi, 135, 370
associative law for events, 24axioms of probability, 25–27
axioms of surprise, 402
B
ballot problem, 107
Banach match problem, 150
basic principle of counting, 2
generalized, 2
Bayes’s formula, 69–75Bernoulli, Jacques, 135
Bernoulli, James, 86, 127, 135, 370Bernoulli, Nicholas, 135, 370
Bernoulli random variable, 127
Bernoulli trials, 107
Bernstein polynomials, 392
Bertrand’s paradox, 186best prize problem, 326–327
beta distribution, 207, 216, 264
binary symmetric channel, 410binomial coefﬁcients, 7
binomial random variable, 127–128,
132, 173, 249
normal approximation, 193–196
approximation to hypergeometric,
153–154
computing its mass function, 134moments of, 131–132, 299
simulation of, 425sums of independent, 247, 339
with randomly chosen success
probability, 327–328
binomial theorem, 7–8birthday problem, 37, 139–140, 171
bivariate exponential distribution,
278
bivariate normal distribution,
253–254, 319–320
Bonferroni’s inequality, 53, 363
Boole’s inequality, 55, 283
Borel, 381Box-Muller simulation technique, 422
branching process, 362
bridge, 36, 58Buffon’s needle problem, 231–232,
275C
Cantor distribution, 360
Cauchy distribution, 206
Cauchy-Schwarz inequality, 360center of gravity, 121
central limit theorem, 187, 370–371, 377
channel capacity, 411, 413Chapman-Kolmogorov equations, 399
Chebychev’s inequality, 368
one-sided, 382
and weak law of large numbers, 370
Chernoff bound, 385chi-squared distribution, 204, 242–243
density function, 243
relation to gamma distribution,
242–243
simulation of, 423–424
coding theory, 405
and entropy, 407
combinations, 5–9combinatorial analysis, 1
combinatorial identities, 7, 17–19, 116
commutative law for events, 24
complement of an event, 23complete graph, 88
computing probabilities by
conditioning, 62–69, 325
concave function, 287conditional covariance formula, 360
conditional distribution,
continuous case, 250–252
discrete case, 248–250
conditional expectation, 313–314
computing expectations by
conditioning, 315
use in prediction, 330
use in simulation, 428
conditional independence, 94conditional probability, 56–57, 106
as a long run relative frequency,
61–62
as a probability function, 89–90satisfying axioms of probability,
89–90
conditional probability density
function, 250–251
conditional probability distribution
function, 248
conditional probability mass function,
248
conditional variance, 328–329
conditional variance formula, 351
continuity correction, 195continuity property of probability, 42–44
continuous random variable, 176control variate, 429
convex function, 387convolution, 239
correlation, 310–311
correlation coefﬁcient, 305–313coupon collecting problems, 114–116,
286, 297, 301–304
covariance, 305craps, 50, 317–319
cumulative distribution function, 116
properties of, 159–160
D
de Mere, Chevalier, 81
DeMoivre, A., 193, 196–197, 372
DeMoivre-Laplace limit theorem,
193–194
DeMorgan’s laws, 25dependent events, 75
dependent random variables, 229discrete random variables, 116–117
discrete uniform random variable, 116
distribution function, seecumulative
distribution function,
distribution of a function of a random
variable, 208–209
distributive law for events, 24
DNA match, 73
dominant genes, 102double exponential distribution, see
Laplace distribution
doubly stochastic matrix, 412
E
Ehrenfest urn model, 398–399entropy, 404
ergodic Markov chain, 400–401
Erlang distribution, 204evaluating evidence, 70
event, 22
decreasing sequence of, 42
increasing sequence of, 42
independent, 75
mutually exclusive, 23
exchangeable random variables, 267
expectation, 119, 215, 280, 349–350
as a center of gravity, 121
of a beta random variable, 208
of a binomial random variable,
131–132, 284
of a continuous random variable,
179–180
of an exponential random variable,
198
of a function of a random variable,
121–122, 181
465

<<<PAGE 478>>>

466 Index
expectation (Continued)
of a gamma random variable, 205
of a geometric random variable, 148
of a hypergeometric random
variable, 153–154, 285
of a negative binomial random
variable, 150–151, 284
of a nonnegative random variable,
170
of a normal random variable, 189of number of matches, 285–286
of number of runs, 287
of a Poisson random variable, 137of sums of a random number of
random variables, 317, 341–343
of sums of random variables,
155–157, 281
of the number of successes,
157–158, 298
of uniform random variables, 185table of, 339–340
expected value, seeexpectation
exponential random variable, 197,
215–216, 265–266rate of, 202relation to half life, 237simulation of, 418
sums of, 242
F
failure rate function, seehazard rate
function
Fermat, P ., 81, 86
Fermat’s combinatorial identity, 18ﬁrst moment, seemean
frequency interpretation of
probability, 26, 119
G
Galton, F., 378gambler’s ruin problem, 84–87
multiple player, 83–84
game theory, 165gamma distribution, 203–204, 216,
242, 264
relation to chi-squared
distribution, 204, 242–243
relation to exponential
distribution, 242
relation to Poisson process, 204
simulation of, 418–419
gamma function, 203–204, 216
relation to beta function, 208
Gauss, J.F.K., 196–197Gaussian distribution, seenormal
distribution
genetics, 102, 104geometric random variable, 147, 172simulation of, 424–425
geometrical probability, 186
H
Hamilton path, 293–294
hazard rate function, 201–202
Huygens, C., 86hypergeometric random variable, 151
relation to binomial, 153–154moments of, 299–300
I
importance sampling, 431inclusion-exclusion, 30–31, 291
bounds, 31–32
independent events, 75–79, 107
conditional, 94
independent increments, 395independent random variables, 228,
233–234, 238–239
indicator random variables, 120information, 404–405
interarrival times, 396
integer solutions of equations, 12–14
intersection of events, 22–23
inverse transform method, 418
discrete, 424
J
Jensen’s inequality, 387joint cumulative probability
distribution function, 220, 227
joint moment generating function, 343
joint probability density function,
223–224, 227
of functions of random variables,
260, 264–265
joint probability mass function, 221
jointly continuous random variables,
223, 227
K
k-of-n system, 103keno, 169
Khintchine, 387
knockout tournament, 11
Kolmogorov, A., 381
L
Laplace, P ., 193, 372, 378Laplace distribution, 201–202
Laplace’s rule of succession, 95, 109
law of frequency of errors,law of total probability, 69
laws of large numbers, 367
Legendre theorem, 216
Liapounoff, 372
limit of events, 42linear prediction, 333
lognormal distribution, 210, 245
M
marginal distribution, 221
Markov chain, 397
Markov’s inequality, 367
matching problem, 39–40, 53, 60,
93–94, 138, 301
maximum likelihood estimates, 170maximums-minimums identity, 295–296
mean of a random variable, 125
measurable events, 28median of a random variable, 215, 359
memoryless random variable, 199–200
Mendel, G., 129
midrange, 276
minimax theorem, 165mode of a random variable, 215
moment generating function, 334–335
of a binomial random variable, 336
of a chi-squard random variable,
341–342
of an exponential random variable,
337
of a normal random variable,
337–338
of a Poisson random variable, 336of a sum of independent random
variables, 338
of a sum of a random number of
random variables, 342
tables for, 339–340
moments of a random variable, 125
of the number of events that occur,
298
multinomial coefﬁcients, 9–10multinomial distribution, 228,
249–250, 312–313
multinomial theorem, 10multiplication rule of probability, 59–60
multivariate normal distribution,
345–346
mutually exclusive events, 23
N
negative binomial random variables,
149relation to binomial, 172
relation to geometric, 149
negative hypergeometric random
variable, 175, 302–303
Newton, I., 197
noiseless coding theorem, 407
noisy coding theorem, 411normal random variables, 187
approximation to binomial, 193–195characterization of, 232–233

<<<PAGE 479>>>

Index 467
joint distribution of sample mean
and sample variance, 348–349
moments of, 363
simulation, 263
simulation by polar method, 422–423
simulation by rejection method,
419–420
sums of independent, 243–244, 341
null event, 23
null set, 23
O
odds of an event, 68order statistics, 256
P
Parallel system, 78Pareto, 155
partition, 52–53Pascal, B., 81
Pascal random variable, seenegative
binomial random variable
Pearson, K., 197
permutations, 3–5personal view of probability, 46
Poisson, S., 136
Poisson paradigm, 138–141
Poisson process, 144–145, 395
Poisson random variable, 135–136,
170–171, 229–230, 249
as an approximation to binomial,
136, 388
as an approximation to the number
of events that occur, 138–141
bounds on its probabilities, 385, 393computing its probabilities, 146
simulation of, 425–426sums of independent, 246–247, 341
poker, 35–36polar algorithm, 422–423
Polya’s urn model, 268
posterior probability, 96prior probability, 96
probabilistic method, 89, 293
probability of an event, 26
as a continuous set function, 42–44as a limiting proportion, 25–26as a measure of belief, 46–47
probability density function, 176
of a function of a random variable,
209
relation to cumulative distribution
function, 177, 179
probability mass function, 116–117
relation to cumulative distribution
function, 118
problem of the points, 81–82, 150Q
quick sort algorithm, 289–291
R
random number, 416
pseudo, 416
random permutation, 416–417, 430
random sample, 259
random subset, 172, 234–236, 417random variables, 112random walk, 288–289, 399–400range of a random sample, 259
Rayleigh density function, 203, 262
record value, 360reduced sample space, 58rejection method of simulation, 419–420
relative frequency deﬁnition of
probability, 25–26
Riemann zeta function, 155
round robin tournament, 108–109
runs, 41–42, 53, 92–93
longest, 140–144
S
sample mean, 283
sample median, 258
sample space, 21–22
sample variance, 307sampling from a ﬁnite population, 196sampling with replacement, 50
sequential updating of information,
96–97
serve and rally games, 82–83
Shannon, C., 411
signal to noise ratio, 392
simulation, 415St. Petersburg paradox, 165standard deviation, 127, 392
inequality, 366
standard normal distribution function,
189–191, 215bounds, 385
table of, 190
standard normal random variable,
189, 215
moments of, 362
stationary increments, 395Stieltjes integral, 349–350Stirling’s approximation, 134stochastically larger, 359
strong law of large numbers, 378–381
subjective probability, seepersonal
probability
subset, 23
superset, 23
surprise, 402T
t distribution, 252–253
transition probabilities of a Markov
chain, 398
trials, 77triangular distribution, 240twin problem, 67
U
uncertainty, 404uncorrelated random variables, 311–312
uniform random variables, 184
sums of independent, 240–241
union of events, 22–23
probability formula for, 28–30
unit normal random variable, see
standard normal random variable
utility, 124–125
V
value at risk, 193
variance, 125–126, 215
as a moment of inertia, 126of a beta random variable, 208of a binomial random variable, 132,
308
of an exponential random variable,
198
of a gamma random variable, 205of a geometric random variable,
148–149, 321–322
of a hypergeometric random
variable, 153–154
of a negative binomial random
variable, 150–151
of a normal random variable, 189
of a Poisson random variable, 138
of a sum of a random number of
random variables, 317, 330
of sums of random variables,
306–307
of a uniform random variable, 185
of the number of successes, 158
tables for, 339–340
Venn diagrams, 23–24
von Neumann, 165
W
weak law of large numbers, 369–370
Weibull distribution, 205, 216
relation to exponential, 216
Weierstrass theorem, 392
Y
Yule-Simons distribution, 170–171
Z
zeta distribution, 155Zipf distribution, seezeta distribution

<<<PAGE 480>>>

This page intentionally left blank 

<<<PAGE 481>>>

Common Discrete Distributions
•Bernoulli(p) Xindicates whether a trial that results in a success with probability
pis a success or not.
P{X=1}=p
P{X=0}=1 −p
E[X]=p,V a r (X)=p(1−p).
•Binomial (n,p)Xrepresents the number of successes in nindependent trials when
each trial is a success with probability p.
P{X=i}=/parenleftbiggn
i/parenrightbigg
pi(1−p)n−i,i=0, 1,...,n
E[X]=np,V a r (X)=np(1−p).
Note. Binomial (1,p)=Bernoulli(p).
•Geometric(p) Xis the number of trials needed to obtain a success when each trial
is independently a success with probability p.
P(X=i)=p(1−p)i−1,i=1, 2,...,
E[X]=1
p,V a r (X)=1−p
p2.
•Negative Binomial (r,p)Xis the number of trials needed to obtain a total of r
successes when each trial is independently a success with probability p.
P(X=i)=/parenleftbiggi−1
r−1/parenrightbigg
pr(1−p)i−r,i=r,r+1,r+2,...
E[X]=r
p,V a r (X)=r1−p
p2.
Notes .
1.Negative Binomial (1,p)=Geometric(p).
2.Sum of rindependent Geometric (p)random variables is Negative Binomial (r,p)
•Poisson (λ)Xis used to model the number of events that occur when these events
are either independent or weakly dependent and each has a small probability of
occurrence.
P{X=i}=e−λλi/i!, i=0, 1, 2, ...
E[X]=λ,V a r (X)=λ.
Notes .
1.A Poisson random variable Xwith parameter λ=npprovides a good approx-
imation to a Binomial (n,p)random variable when nis large and pis small.
2.If events are occurring one at a time in a random manner for which (a) the
number of events that occur in disjoint time intervals is independent and (b) theprobability of an event occurring in any small time interval is approximately λ
times the length of the interval, then the number of events in an interval of length
twill be a Poisson(λt )random variable.
•Hypergeometric Xis the number of white balls in a random sample of nballs
chosen without replacement from an urn of Nballs of which mare white.
P{X=i}=/parenleftbig
m
i/parenrightbig/parenleftbigN−m
n−i/parenrightbig
/parenleftbigN
n/parenrightbig ,i=0, 1, 2, ...

<<<PAGE 482>>>

The preceding uses the convention that/parenleftbigr
j/parenrightbig
=0 if either j<0o rj>r.
With p=m/N ,E[X]=np,V a r (X)=N−n
N−1np(1−p)
Note. If each ball were replaced before the next selection, then Xwould be a
Binomial (n,p)random variable.
•Negative Hypergeometric Xis the number of balls that need be removed from
an urn that contains n+mballs, of which nare white, until a total of rwhite balls
has been removed, where r…n.
P{X=k}=/parenleftbign
r−1/parenrightbig/parenleftbigm
k−r/parenrightbig
/parenleftbign+m
k−1/parenrightbign−r+1
n+m−k+1,kÚr
E[X]=rn+m+1
n+1,V a r (X)=mr(n+1−r )(n+m+1)
(n+1)2(n+2)

<<<PAGE 483>>>

Common Continuous Distributions
•Uniform (a,b)Xis equally likely to be near each value in the interval (a,b). Its
density function is
f(x)=1
b−a,a<x<b
E[X]=a+b
2,V a r (X)=(b−a)2
12.
•Normal (μ,σ2)Xis a random ﬂuctuation arising from many causes. Its density
function is
f(x)=1√
2πσe−(x−μ)2/2σ2,−q<x<q
E[X]=μ,V a r (X)=σ2
When μ=0,σ=1,Xis called a standard normal.
Notes .
1.IfXis Normal(μ, σ2), then Z=X−μ
σis standard normal.
2.Sum of independent normal random variables is also normal.
3.An important result is the central limit theorem, which states that the distri-
bution of the sum of the ﬁrst nof a sequence of independent and identically
distributed random variables becomes normal as ngoes to inﬁnity, for any dis-
tribution of these random variables that has a ﬁnite mean and variance.
•Exponential (λ)Xis the waiting time until an event occurs when events are always
occurring at a random rate λ> 0.Its density is
f(x)=λe−λx,x>0
E[X]=1
λ,V a r (X)=1
λ2,P(X>x)=e−λx,x>0.
Note. Xis memoryless, in that the remaining life of an item whose life distribution
is Exponential(λ) is also Exponential(λ), no matter what the current age of the
item is.
•Gamma(α ,λ)When α=n,Xis the waiting time until nevents occur when events
are always occurring at a random rate λ> 0.Its density is
f(t)=λe−λt(λt)α−1
/Gamma1(α),t>0
where /Gamma1(α)=/integraltextq
0e−xxα−1dxis called the gamma function.
E[X]=α
λ,V a r (X)=α
λ2.
Notes .
1.Gamma(1, λ)is exponential(λ).
2.If the random variables are independent, then the sum of a Gamma (α1,λ)and
a Gamma(α 2,λ)is a Gamma(α 1+α2,λ).
3.The sum of nindependent and identically distributed exponentials with para-
meter λis a Gamma(n, λ)random variable.
•Beta(a,b)Xis the distribution of a random variable taking on values in the inter-
val(0, 1). Its density is
f(x)=1
B(a, b)xa−1(1−x)b−1,0<x<1
where B(a, b)=/integraltext1
0xa−1(1−x)b−1dxis called the beta function.
E[X]=a
a+bVar(X)=ab
(a+b)2(a+b+1)

<<<PAGE 484>>>

Notes .
1.Beta(1, 1) and Uniform(0, 1) are identical.
2.Thejthsmallest of nindependent uniform (0, 1) random variables is a Beta (j,n−
j+1)random variable.
•Chi-Squared (n)Xis the sum of the squares of nindependent standard normal
random variables. Its density is
f(x)=e−x/ 2xn
2−1
2n/2/Gamma1(n/2),x>0
Notes .
1.The Chi-Squared(n) distribution is the same as the Gamma (n/2, 1/2) distribu-
tion.
2.The sample variance of nindependent and identically distributed Normal (μ,σ2)
random variables multiplied byn−1
σ2is a Chi-Squared (n−1)random variable, and
it is independent of the sample mean.
•Cauchy Xis the tangent of a uniformly distributed random angle between −π/ 2
andπ/2. Its density is
f(x)=1
π(1+x2),−q<x<q
E[X]=0V a r (X)=q.